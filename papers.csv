id,title,doi,publication_year,authors,abstract,open_access,host_venue
https://openalex.org/W1574901103,Foundations of Statistical Natural Language Processing,,1999,"Christopher D. Manning, Hinrich Schütze","Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.",False,
https://openalex.org/W2123442489,The Stanford CoreNLP Natural Language Processing Toolkit,https://doi.org/10.3115/v1/p14-5010,2014,"Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, David McClosky","Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, David McClosky. Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 2014.",True,
https://openalex.org/W2117130368,A unified architecture for natural language processing,https://doi.org/10.1145/1390156.1390177,2008,"Ronan Collobert, Jason Weston","We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.",False,
https://openalex.org/W2979826702,Transformers: State-of-the-Art Natural Language Processing,https://doi.org/10.18653/v1/2020.emnlp-demos.6,2020,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clément Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush","Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander Rush. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.",True,
https://openalex.org/W2158899491,Natural Language Processing (almost) from Scratch,https://doi.org/10.48550/arxiv.1103.0398,2011,"Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel P. Kuksa","We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",True,
https://openalex.org/W1521626219,Natural Language Processing with Python,,2009,"Steven Bird, Ewan Klein, Edward Loper","This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication. Packed with examples and exercises, Natural Language Processing with Python will help you: Extract information from unstructured text, either to guess the topic or identify named entities Analyze linguistic structure in text, including parsing and semantic analysis Access popular linguistic databases, including WordNet and treebanks Integrate techniques drawn from fields as diverse as linguistics and artificial intelligence This book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.",False,
https://openalex.org/W2952230511,Natural Language Processing (almost) from Scratch,,2011,"Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, Pavel P. Kuksa","We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.",False,
https://openalex.org/W2096175520,A maximum entropy approach to natural language processing,https://doi.org/10.5555/234285.234289,1996,"Adam Berger, Vincent J. Della Pietra, Stephen A. Della Pietra","The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.",False,
https://openalex.org/W1579838312,"Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition",,2000,"Daniel Jurafsky, James Martin","From the Publisher:
This book takes an empirical approach to language processing, based on applying statistical and other machine-learning algorithms to large corpora.Methodology boxes are included in each chapter. Each chapter is built around one or more worked examples to demonstrate the main idea of the chapter. Covers the fundamental algorithms of various fields, whether originally proposed for spoken or written language to demonstrate how the same algorithm can be used for speech recognition and word-sense disambiguation. Emphasis on web and other practical applications. Emphasis on scientific evaluation. Useful as a reference for professionals in any of the areas of speech and language processing.",False,
https://openalex.org/W2980282514,HuggingFace's Transformers: State-of-the-art Natural Language Processing,https://doi.org/10.48550/arxiv.1910.03771,2019,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clément Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Jamie Brew","Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.",True,
https://openalex.org/W2169818249,Natural language processing: an introduction,https://doi.org/10.1136/amiajnl-2011-000464,2011,"Prakash M. Nadkarni, Lucila Ohno‐Machado, Wendy W. Chapman",Objectives To provide an overview and tutorial of natural language processing (NLP) and modern NLP-system design.,True,
https://openalex.org/W1663984431,Advances in natural language processing,https://doi.org/10.1126/science.aaa8685,2015,"Julia Hirschberg, Christopher D. Manning","Natural language processing employs computational techniques for the purpose of learning, understanding, and producing human language content. Early computational approaches to language research focused on automating the analysis of the linguistic structure of language and developing basic technologies such as machine translation, speech recognition, and speech synthesis. Today’s researchers refine and make use of such tools in real-world applications, creating spoken dialogue systems and speech-to-speech translation engines, mining social media for information about health or finance, and identifying sentiment and emotion toward products and services. We describe successes and challenges in this rapidly advancing area.",False,
https://openalex.org/W2884001105,Recent Trends in Deep Learning Based Natural Language Processing [Review Article],https://doi.org/10.1109/mci.2018.2840738,2018,"Tom Young, Devamanyu Hazarika, Soujanya Poria, Erik Cambria","Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.",False,
https://openalex.org/W2930957955,Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP '08,https://doi.org/10.3115/1613715,2008,,"Welcome to the 2008 Conference on Empirical Methods in Natural Language Processing! The conference is organized under the auspices of SIGDAT, the ACL Special Interest Group for linguistic data and corpus-based approaches to natural language processing. It is co-located this year with AMTA 2008 and the International Workshop on Spoken Language Translation, in Honolulu, Hawaii. EMNLP received 385 submissions. We were able to accept 116 papers in total (an acceptance rate of 30%). 81 of the papers (21%) were accepted for oral presentation, and 35 (9%) for poster presentation. Two poster papers were subsequently withdrawn after acceptance. The papers were selected by a program committee of 15 area chairs, from Asia, Europe, and North America, assisted by a panel of 339 reviewers. This year EMNLP introduced an author response period. Authors were able to read and respond to the reviews of their paper before the program committee made a final decision. They were asked to correct factual errors in the reviews and answer questions raised in the reviewer comments. The intention was to help produce more accurate reviews. In some cases, reviewers changed their scores in view of the authors' response and the area chairs read all responses carefully prior to making recommendations for acceptance.",False,
https://openalex.org/W3098824823,Transformers: State-of-the-Art Natural Language Processing,https://doi.org/10.5281/zenodo.5347031,2020,"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clément Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush","Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",True,
https://openalex.org/W3019166713,A Survey of the Usages of Deep Learning for Natural Language Processing,https://doi.org/10.1109/tnnls.2020.2979670,2020,"Daniel W. Otter, Julian Richard Medina, Jugal Kalita","Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This article provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to many applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field.",True,
https://openalex.org/W1410460,Handbook of Natural Language Processing,https://doi.org/10.1201/9781420085938,2010,"Nitin Indurkhya, Fred J. Damerau","The Handbook of Natural Language Processing, Second Edition presents practical tools and techniques for implementing natural language processing in computer systems. Along with removing outdated material, this edition updates every chapter and expands the content to include emerging areas, such as sentiment analysis.New to the Second EditionGreater",False,
https://openalex.org/W3185341429,"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",https://doi.org/10.1145/3560815,2022,"Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig","This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P ( y|x ), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂ , from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.",True,
https://openalex.org/W2607303097,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,https://doi.org/10.18653/v1/d18-1,2018,,,False,
https://openalex.org/W2031213082,Natural language processing,https://doi.org/10.1002/aris.1440370103,2003,Gobinda Chowdhury,conducted domain-specific NLP studies,True,
https://openalex.org/W1516184288,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,https://doi.org/10.18653/v1/d13-1,2013,"Hang Li, Lluı́s Màrquez","Welcome to EMNLP 2010, Conference on Empirical Methods in Natural Language Processing! The conference is organized by SIGDAT, the Association for Computational Linguistics' special interest group on linguistic data and corpus-based approaches to NLP, and is held this year as a stand-alone conference at the MIT Stata Center, Massachusetts, USA on October 9-11.

EMNLP 2010 received 500 submissions, a new record for the conference. The program committee was able to accept 125 papers in total (an acceptance rate of 25%). Among them, 70 of the papers (14%) were accepted for oral presentations, and 55 (11%) for poster presentations. The PC, which consists of 18 area chairs and 460 PC members from Asia, Europe, and North America, worked together to create a strong program with high quality oral and poster presentations and enlightening invited talks.",False,
https://openalex.org/W2742947407,Recent Trends in Deep Learning Based Natural Language Processing,https://doi.org/10.48550/arxiv.1708.02709,2017,"Tom Young, Devamanyu Hazarika, Soujanya Poria, Erik Cambria","Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.",True,
https://openalex.org/W4239019441,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),https://doi.org/10.18653/v1/d19-1,2019,,,True,
https://openalex.org/W2740168486,A Survey on Hate Speech Detection using Natural Language Processing,https://doi.org/10.18653/v1/w17-1101,2017,"Anna Grau Schmidt, Michael Wiegand","This paper presents a survey on hate speech detection. Given the steadily growing body of social media content, the amount of online hate speech is also increasing. Due to the massive scale of the web, methods that automatically detect hate speech are required. Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing. We also discuss limits of those approaches.",True,
https://openalex.org/W2963042536,A Primer on Neural Network Models for Natural Language Processing,https://doi.org/10.1613/jair.4992,2016,Yoav Goldberg,"Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.",True,
https://openalex.org/W4205807230,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,https://doi.org/10.18653/v1/2021.emnlp-main,2021,,"EMNLP 2021 is one of the first hybrid conferences in the field of natural language processing.It is also for us, the organizing team, uncharted domain.Organizing a hybrid conference has felt like organizing two conferences, a virtual one and an in-person one, which seamlessly must work together and with a kind of multi-task objective make the conference experience synergistic and successful both remotely and in person.With this challenge come opportunities.The hybrid format allows remote participation in a conference that is held onsite in Punta Cana, The Dominican Republic, and allows creating a real conference feeling for those who do not want to travel the many miles from the other side of the world and increase their carbon footprint, and for those who have budget restrictions for traveling.We welcome you all!As in previous years, the purpose of the General Chair's preface is to express thanks to the amazing team of organizing chairs whose heroic efforts made this hybrid conference possible.The organizing team includes:• The Programme Chairs -Xuanjing Huang, Lucia Specia and Scott Yih -who did a tremendous job to manage the reviewing process and set up an outstanding scientific program.• The Senior Area Chairs, Area Chairs and Reviewers whose expertise enabled authors to learn from their reviews and to deliver papers that improved on their original submissions.• The Demonstration Chairs -Heike Adel and Shuming Shi -who selected outstanding demonstrations",True,
https://openalex.org/W3037109418,Stanza: A Python Natural Language Processing Toolkit for Many Human Languages,https://doi.org/10.18653/v1/2020.acl-demos.14,2020,"Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, Christopher D. Manning","We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at https://stanfordnlp.github.io/stanza/.",True,
https://openalex.org/W2963691697,AllenNLP: A Deep Semantic Natural Language Processing Platform,https://doi.org/10.18653/v1/w18-2501,2018,"Matt Gardner, Joël Grus, Mark E Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew E. Peters, Michael Schmitz, Luke Zettlemoyer","Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, Luke Zettlemoyer. Proceedings of Workshop for NLP Open Source Software (NLP-OSS). 2018.",True,
https://openalex.org/W3046375318,Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing,https://doi.org/10.1145/3458754,2021,"裕二 池谷, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon","Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning Benchmark) at https://aka.ms/BLURB .",True,
https://openalex.org/W1502957213,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,https://doi.org/10.18653/v1/d16-1,2016,"Christos Christodoulopoulos, Sharon Goldwater, Mark Steedman",,False,
https://openalex.org/W2117400858,Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging,,1995,Eric Brill,"Recently, there has been a rebirth of empiricism in the field of natural language processing. Manual encoding of linguistic information is being challenged by automated corpus-based learning as a method of providing a natural language processing system with linguistic knowledge. Although corpus-based approaches have been successful in many different areas of natural language processing, it is often the case that these methods capture the linguistic information they are modelling indirectly in large opaque tables of statistics. This can make it difficult to analyze, understand and improve the ability of these approaches to model underlying linguistic behavior. In this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part-of-speech tagging.",False,
https://openalex.org/W3144293453,Natural Language Processing,https://doi.org/10.1007/978-81-322-3972-7_19,2020,K. R. Chowdhary,,False,
https://openalex.org/W2747680751,"Natural language processing: state of the art, current trends and challenges",https://doi.org/10.1007/s11042-022-13428-4,2022,"Diksha Khurana, Aditya Koli, Kiran Khatter, Sukhdev Singh",,True,
https://openalex.org/W2147272182,Introduction to Arabic Natural Language Processing,https://doi.org/10.1007/978-3-031-02139-8,2010,Nizar Habash,This book provides system developers and researchers in natural language processing and computational linguistics with the necessary background information for working with the Arabic language. The go,False,
https://openalex.org/W4237040408,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,https://doi.org/10.18653/v1/d17-1,2017,"Chloé Braud, Ophélie Lacroix, Anders Søgaard","Discourse segmentation is the first step in building discourse parsers.Most work on discourse segmentation does not scale to real-world discourse parsing across languages, for two reasons: (i) models rely on constituent trees, and (ii) experiments have relied on gold standard identification of sentence and token boundaries.We therefore investigate to what extent constituents can be replaced with universal dependencies, or left out completely, as well as how state-of-the-art segmenters fare in the absence of sentence boundaries.Our results show that dependency information is less useful than expected, but we provide a fully scalable, robust model that only relies on part-of-speech information, and show that it performs well across languages in the absence of any gold-standard annotation.",True,
https://openalex.org/W2587019100,Comparative Study of CNN and RNN for Natural Language Processing,https://doi.org/10.48550/arxiv.1702.01923,2017,"Wenpeng Yin, Katharina Kann, Mo Yu, Hinrich Schütze","Deep neural networks (DNN) have revolutionized the field of natural language processing (NLP). Convolutional neural network (CNN) and recurrent neural network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state of the art on many NLP tasks often switches due to the battle between CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.",True,
https://openalex.org/W2606089314,Neural Network Methods for Natural Language Processing,https://doi.org/10.1007/978-3-031-02165-7,2017,Yoav Goldberg,Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) co,False,
https://openalex.org/W2045812729,Foundations of statistical natural language processing,https://doi.org/10.1145/601858.601867,2002,Gerhard Weikum,No abstract available.,False,
https://openalex.org/W1563635790,Strategies for Natural Language Processing,https://doi.org/10.4324/9781315802671,2014,"Wendy G. Lehnert, Martin Ringle",,False,
https://openalex.org/W4210984920,Neural Network Methods for Natural Language Processing,https://doi.org/10.2200/s00762ed1v01y201703hlt037,2017,Yoav Goldberg,Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) co,True,
https://openalex.org/W1984708705,Arabic Natural Language Processing,https://doi.org/10.1145/1644879.1644881,2009,"Ali Farghaly, Khaled Shaalan","The Arabic language presents researchers and developers of natural language processing (NLP) applications for Arabic text and speech with serious challenges. The purpose of this article is to describe some of these challenges and to present some solutions that would guide current and future practitioners in the field of Arabic natural language processing (ANLP). We begin with general features of the Arabic language in Sections 1, 2, and 3 and then we move to more specific properties of the language in the rest of the article. In Section 1 of this article we highlight the significance of the Arabic language today and describe its general properties. Section 2 presents the feature of Arabic Diglossia showing how the sociolinguistic aspects of the Arabic language differ from other languages. The stability of Arabic Diglossia and its implications for ANLP applications are discussed and ways to deal with this problematic property are proposed. Section 3 deals with the properties of the Arabic script and the explosion of ambiguity that results from the absence of short vowel representations and overt case markers in contemporary Arabic texts. We present in Section 4 specific features of the Arabic language such as the nonconcatenative property of Arabic morphology, Arabic as an agglutinative language, Arabic as a pro-drop language, and the challenge these properties pose to ANLP. We also present solutions that have already been adopted by some pioneering researchers in the field. In Section 5 we point out to the lack of formal and explicit grammars of Modern Standard Arabic which impedes the progress of more advanced ANLP systems. In Section 6 we draw our conclusion.",False,
https://openalex.org/W2028140375,Jumping NLP Curves: A Review of Natural Language Processing Research [Review Article],https://doi.org/10.1109/mci.2014.2307227,2014,"Erik Cambria, Bebo White","Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and representation of human language. NLP research has evolved from the era of punch cards and batch processing (in which the analysis of a sentence could take up to 7 minutes) to the era of Google and the likes of it (in which millions of webpages can be processed in less than a second). This review paper draws on recent developments in NLP research to look at the past, present, and future of NLP technology in a new light. Borrowing the paradigm of `jumping curves' from the field of business management and marketing prediction, this survey article reinterprets the evolution of NLP research as the intersection of three overlapping curves-namely Syntactics, Semantics, and Pragmatics Curveswhich will eventually lead NLP research to evolve into natural language understanding.",False,
https://openalex.org/W2596567068,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,https://doi.org/10.18653/v1/d15-1,2015,,,False,
https://openalex.org/W3031696893,Attention in Natural Language Processing,https://doi.org/10.1109/tnnls.2020.3019893,2020,"Andrea Galassi, Marco Lippi, Paolo Torroni","Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions: the representation of the input, the compatibility function, the distribution function, and the multiplicity of the input and/or output. We present the examples of how prior information can be exploited in attention models and discuss ongoing research efforts and open challenges in the area, providing the first extensive categorization of the vast body of literature in this exciting domain.",True,
https://openalex.org/W1554540371,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,,2011,"Kristian Woodsend, Mirella Lapata",,False,
https://openalex.org/W1603598191,Proceedings of the 2003 conference on Empirical methods in natural language processing -,https://doi.org/10.3115/1119355,2003,"Michael Collins, Mark Steedman","In this paper, we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models. The units of translation are blocks -- pairs of phrases. During decoding, we ...",False,
https://openalex.org/W2489487449,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),https://doi.org/10.18653/v1/2020.emnlp-main,2020,,"Welcome to EMNLP 2020!Due to the unprecedented situation with the Covid-19 pandemic, EMNLP 2020 will be held completely online this year.We decided to move EMNLP to a virtual format early on, when the pandemic just started, and postponed the paper submission deadlines by three weeks such that authors affected by the pandemic could have more time for their paper submissions.This resulted in a much tighter schedule for paper review and decisions, as well as publications, workshop programs, virtual infrastructure, etc.However, thanks to everyone's hard work, we made it.We received a record number of 3,677 submissions.This is a significant increase of 26% over EMNLP 2019, making it the largest NLP conference to date in terms of paper submissions.After removing withdrawals and desk rejecting papers which violated our formatting requirements, the anonymity policy, or double submission policy, 3,359 submissions were sent out for review.Despite the sharp increase in submissions, we kept the acceptance rates at a similar level as past years.",True,
https://openalex.org/W2953958347,Transfer Learning in Natural Language Processing,https://doi.org/10.18653/v1/n19-5004,2019,"Sebastian Ruder, Matthew E. Peters, Swabha Swayamdipta, Thomas Wolf","The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.",False,
https://openalex.org/W2964091467,Ask Me Anything: Dynamic Memory Networks for Natural Language Processing,https://doi.org/10.48550/arxiv.1506.07285,2015,"Ankit Kumar, Ozan İrsoy, Peter Ondrúška, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor W. Zhong, Romain Paulus, Richard Socher","Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.",True,
https://openalex.org/W2338526423,Natural Language Processing in Radiology: A Systematic Review,https://doi.org/10.1148/radiol.16142770,2016,"Ewoud Pons, Loes Braun, M. G. Myriam Hunink, Jan A. Kors","Radiological reporting has generated large quantities of digital content within the electronic health record, which is potentially a valuable source of information for improving clinical care and supporting research. Although radiology reports are stored for communication and documentation of diagnostic imaging, harnessing their potential requires efficient and automated information extraction: they exist mainly as free-text clinical narrative, from which it is a major challenge to obtain structured data. Natural language processing (NLP) provides techniques that aid the conversion of text into a structured representation, and thus enables computers to derive meaning from human (ie, natural language) input. Used on radiology reports, NLP techniques enable automatic identification and extraction of information. By exploring the various purposes for their use, this review examines how radiology benefits from NLP. A systematic literature search identified 67 relevant publications describing NLP methods that support practical applications in radiology. This review takes a close look at the individual studies in terms of tasks (ie, the extracted information), the NLP methodology and tools used, and their application purpose and performance results. Additionally, limitations, future challenges, and requirements for advancing NLP in radiology will be discussed.",False,
