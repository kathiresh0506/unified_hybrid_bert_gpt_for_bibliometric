{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sklearn as sk\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48757055",
   "metadata": {},
   "source": [
    "Core Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba83428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 404 - <!DOCTYPE html><html lang=\"en\"><head><meta charSet=\"utf-8\"/><meta name=\"description\" content=\"CORE is a service provided by the Knowledge Media Institute, based at The Open University, United Kingdom.\n",
      "\"/><link rel=\"shortcut icon\" href=\"/favicon/favicon-32px.png\"/><link rel=\"icon\" href=\"/favicon/favicon-128px.png\"/><link rel=\"icon\" href=\"/favicon/favicon.svg\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1, shrink-to-fit=no\"/><title>Page is not found</title><link rel=\"shortcut icon\" href=\"/favicon/favicon-32px.png\"/><link rel=\"icon\" href=\"/favicon/favicon-128px.png\"/><link rel=\"icon\" href=\"/favicon/favicon.svg\"/><meta name=\"next-head-count\" content=\"10\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/css/c4c21edf3247df18ba22.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"https://core.ac.uk/_next/static/css/c4c21edf3247df18ba22.css\" data-n-g=\"\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/css/69940f16912f38d7efe2.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"https://core.ac.uk/_next/static/css/69940f16912f38d7efe2.css\" data-n-g=\"\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/css/8f01378cc21cc6f8016a.css\" as=\"style\"/><link rel=\"stylesheet\" href=\"https://core.ac.uk/_next/static/css/8f01378cc21cc6f8016a.css\" data-n-p=\"\"/><noscript data-n-css=\"\"></noscript><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/webpack-53b2a1c2781a4aa3c425.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/framework.898ed9c4190ec62ba669.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/commons.d7ddf4d2b1bef08d6a43.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/main-dba0e33f318ab41063ce.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/b5f2ed29.441239f4fc365a69741a.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/e82996df.86a4b1a61816698443e2.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/c3f7f8607465e617b299e6671ee3987d5fa63bf3.58ced7faaca4d9aff03e.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/d4724269a42ff2517c4025d0beda2692c5594bc9.0b17ca494529028ea65e.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/69f785c1a9b276ccb372885623ae0ecb0053519b.c00ac77b0ffe249a74d2.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/69f785c1a9b276ccb372885623ae0ecb0053519b_CSS.ddce25b62cf34b951439.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/d46a588f3493bf286384f5488b19222037b6d567.ae093e90d5f38d3f1f45.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/9c353c8ac20eb1f739641e3bcf47b76d12dc6fca.d9558af2faaffeeb09b3.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/pages/_app-3fffe18f3d7f48d99642.js\" as=\"script\"/><link rel=\"preload\" href=\"https://core.ac.uk/_next/static/chunks/pages/404-2c39bf5a09df80115614.js\" as=\"script\"/></head><body><div id=\"__next\"><div class=\"bg-light\"><div class=\"container\"><a class=\"btn btn-outline-primary sr-only sr-only-focusable p-1 my-1\" href=\"#content\">Skip to main content</a></div></div><header class=\"styles-container-2d0MF styles-container-1KefU\" id=\"header\"><div class=\"styles-item-1WhX8 styles-logo-container-261sh\"><a href=\"https://core.ac.uk/\" class=\"styles-logo-link-3SQhe\"><span class=\"logo-logo-wCAGq styles-logo-3C3lk\"><svg class=\"icon-icon-35iz- logo-logo-icon-1JnTB\" role=\"img\" aria-label=\"CORE\" width=\"18\" height=\"24\"><use href=\"/design/icons.svg#core-symbol\"></use></svg>CORE</span><p class=\"styles-peace-1CGQO\"><span role=\"img\" aria-label=\"Ukranian Flag\">ðŸ‡ºðŸ‡¦Â </span><i>Â  make metadata, not war</i></p></a></div><div class=\"styles-item-1WhX8 styles-toggle-container-3kw9N\"><button class=\"button-button-3fr3r button-text-2Jjil styles-toggle-button-3MDvL\" type=\"button\" aria-controls=\"header-nav\" aria-expanded=\"false\" aria-label=\"Toggle navigation\"><svg class=\"icon-icon-35iz-\" role=\"img\" aria-label=\"Menu icon\" width=\"24\" height=\"24\" aria-hidden=\"true\"><use href=\"/design/icons.svg#menu\"></use></svg></button></div><nav class=\"styles-item-1WhX8 styles-nav-1DaTS\" id=\"header-nav\"><div class=\"styles-menu-2MrzW styles-services-menu-3Y6g_\"><button class=\"button-button-3fr3r button-text-2Jjil styles-menu-title-zh5md\" role=\"button\" id=\"services-title\" aria-haspopup=\"true\" aria-expanded=\"false\">Services</button><ul class=\"styles-menu-list-2qL0f\" aria-labelledby=\"services-title\"><li class=\"styles-item-34TYT styles-services-overview-item-9AF_0\"><a class=\"styles-item-link--QpIs\" href=\"/services\"><img src=\"data:image/svg+xml,&lt;svg viewBox=&quot;0 0 12953 10812&quot; width=&quot;160&quot; fill=&quot;none&quot; xmlns=&quot;http://www.w3.org/2000/svg&quot;&gt;&lt;path d=&quot;M10202.6 395.468c-1508.65-862.225-2844.81-152.39-3513.2 603.467C6021.02 1754.79 5523.01 2518.27 4683.95 2658.47c-839.07 140.2-2112.13-215.48-3060.61 452.6-948.481 668.08-944.824 2442.51-193.84 3448.59 750.98 1006.08 2693.95 1487.03 4935.62 1810.4a5909.74 5909.74 0 002411.12-133.8l1018.27-369.7c497.39-227.06 914.39-482.77 1267.29-690.33 537-316.97 726.3-1089.59 829-1655.57 149.1-848.13 136.2-1716.83-38.1-2560.16-224.9-1062.46-737.9-2044.161-1650.1-2565.032z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.8&quot; d=&quot;M10202.6 395.468c-1508.65-862.225-2844.81-152.39-3513.2 603.467C6021.02 1754.79 5523.01 2518.27 4683.95 2658.47c-839.07 140.2-2112.13-215.48-3060.61 452.6-948.481 668.08-944.824 2442.51-193.84 3448.59 750.98 1006.08 2693.95 1487.03 4935.62 1810.4a5909.74 5909.74 0 002411.12-133.8l1018.27-369.7c497.39-227.06 914.39-482.77 1267.29-690.33 537-316.97 726.3-1089.59 829-1655.57 149.1-848.13 136.2-1716.83-38.1-2560.16-224.9-1062.46-737.9-2044.161-1650.1-2565.032z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path opacity=&quot;.8&quot; d=&quot;M12508.9 1451.53c-34.1-563.231-418.8-747.929-662.6-479.721-57.9 65.421-96.7 145.531-112.2 231.551-15.4 86.01-6.9 174.62 24.7 256.1 85.7 231.33 232.9 641.87 320.7 963.11 129.2 474.23 85.9 840.58 236.8 646.74 150.8-193.84 235.6-906.42 192.6-1617.78z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.8&quot; d=&quot;M12508.9 1451.53c-34.1-563.231-418.8-747.929-662.6-479.721-57.9 65.421-96.7 145.531-112.2 231.551-15.4 86.01-6.9 174.62 24.7 256.1 85.7 231.33 232.9 641.87 320.7 963.11 129.2 474.23 85.9 840.58 236.8 646.74 150.8-193.84 235.6-906.42 192.6-1617.78z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M5999.99 1365.28c236.81-42.97 538.55-603.465 926.53-840.585 387.99-237.119 43.28-409.625-322.45-237.119 0 0-280.4 64.614-495.88 495.878-215.48 431.266-345.32 624.806-108.2 581.826z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.8&quot; d=&quot;M5999.99 1365.28c236.81-42.97 538.55-603.465 926.53-840.585 387.99-237.119 43.28-409.625-322.45-237.119 0 0-280.4 64.614-495.88 495.878-215.48 431.266-345.32 624.806-108.2 581.826z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M5612 1063.55c192.32-77.112 402.01-600.725 182.87-573.905a162.815 162.815 0 00-76.03 32.183 162.769 162.769 0 00-50.45 65.347c-70.1 155.744-229.5 545.555-56.39 476.375z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.8&quot; d=&quot;M5612 1063.55c192.32-77.112 402.01-600.725 182.87-573.905a162.815 162.815 0 00-76.03 32.183 162.769 162.769 0 00-50.45 65.347c-70.1 155.744-229.5 545.555-56.39 476.375z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M6786.33 10811.7c3405.57 0 6166.37-102.7 6166.37-229.2 0-126.6-2760.8-229.2-6166.37-229.2-3405.57 0-6166.332 102.6-6166.332 229.2 0 126.5 2760.762 229.2 6166.332 229.2z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.8&quot; d=&quot;M6786.33 10811.7c3405.57 0 6166.37-102.7 6166.37-229.2 0-126.6-2760.8-229.2-6166.37-229.2-3405.57 0-6166.332 102.6-6166.332 229.2 0 126.5 2760.762 229.2 6166.332 229.2z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M9750.01 1967.22a45.718 45.718 0 00-45.71 45.72v234.38h-835.41v-234.38a45.717 45.717 0 00-45.71-45.72c-12.13 0-23.76 4.82-32.33 13.39a45.76 45.76 0 00-13.39 32.33v5903.61c0 12.12 4.82 23.75 13.39 32.32a45.682 45.682 0 0032.33 13.39c12.12 0 23.75-4.81 32.32-13.39a45.676 45.676 0 0013.39-32.32V6595.93h835.41v1320.62c0 12.12 4.81 23.75 13.39 32.32a45.676 45.676 0 0032.32 13.39c12.13 0 23.76-4.81 32.33-13.39a45.676 45.676 0 0013.39-32.32V2012.03a45.715 45.715 0 00-13.71-31.74 45.706 45.706 0 00-32.01-13.07zm-45.71 4538.19h-835.41v-296.25h835.41v296.25zm0-387.99h-835.41v-295.94h835.41v295.94zm0-387.68h-835.41v-295.02h835.41v295.02zm0-387.37h-835.41v-293.81h835.41v293.81zm0-387.08h-835.41v-290.45h835.41v290.45zm0-385.85h-835.41v-292.28h835.41v292.28zm0-384.02h-835.41v-295.03h835.41v295.03zm0-387.68h-835.41v-295.03h835.41v295.03zm0-386.77h-835.41v-295.94h835.41v295.94zm0-387.68h-835.41v-296.55h835.41v296.55zm0-387.99h-835.41v-296.55h835.41v296.55z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M1836.68 6726.68s684.24-936.59 350.5-1158.17c-347.14-228.89-510.2 464.49-583.65 804.93-156.66 726.6-112.47 880.82-7.32 721.11l240.47-367.87z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M1836.68 6726.68s684.24-936.59 350.5-1158.17c-347.14-228.89-510.2 464.49-583.65 804.93-156.66 726.6-112.47 880.82-7.32 721.11l240.47-367.87z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M2093.31 7302.72s951.22-663.82 707.09-980.18c-253.88-329.47-633.33 274.3-813.46 571.77-384.33 636.38-392.25 796.7-241.08 679.66l347.45-271.25z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M2093.31 7302.72s951.22-663.82 707.09-980.18c-253.88-329.47-633.33 274.3-813.46 571.77-384.33 636.38-392.25 796.7-241.08 679.66l347.45-271.25z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M2114.03 8072.59s822.91-369.7 682.41-647.05c-146.3-288.62-523.92 117.34-704.65 319.11-386.16 430.96-415.42 552.26-282.84 484.91l305.08-156.97z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M2114.03 8072.59s822.91-369.7 682.41-647.05c-146.3-288.62-523.92 117.34-704.65 319.11-386.16 430.96-415.42 552.26-282.84 484.91l305.08-156.97z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M2162.8 8845.82s822.91-369.7 682.71-647.05c-146.6-288.32-523.92 117.34-704.96 319.41-385.85 430.96-415.42 552.27-282.53 484.61l304.78-156.97z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M2162.8 8845.82s822.91-369.7 682.71-647.05c-146.6-288.32-523.92 117.34-704.96 319.41-385.85 430.96-415.42 552.27-282.53 484.61l304.78-156.97z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M1197.56 7118.02S292.969 6391.73 57.983 6714.8c-243.825 335.25 440.713 531.23 777.496 620.22 718.671 189.88 874.721 152.4 719.891 40.24l-357.81-257.24z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M1197.56 7118.02S292.969 6391.73 57.983 6714.8c-243.825 335.25 440.713 531.23 777.496 620.22 718.671 189.88 874.721 152.4 719.891 40.24l-357.81-257.24z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M1224.38 7724.84S193.61 7192.69 27.504 7556.3c-172.811 378.23 537.329 433.09 883.865 453.2 742.141 43.28 887.831-24.38 713.801-103.62l-400.79-181.04z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M1224.38 7724.84S193.61 7192.69 27.504 7556.3c-172.811 378.23 537.329 433.09 883.865 453.2 742.141 43.28 887.831-24.38 713.801-103.62l-400.79-181.04z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M1349.03 8405.11s-883.86-426.69-1015.83-114.9c-137.456 323.98 464.181 353.85 758.9 362.69 628.46 19.5 750.07-41.45 601.03-102.41l-344.1-145.38z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M1349.03 8405.11s-883.86-426.69-1015.83-114.9c-137.456 323.98 464.181 353.85 758.9 362.69 628.46 19.5 750.07-41.45 601.03-102.41l-344.1-145.38z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M1392.31 8954.63s-833.877-518.13-998.154-222.18c-170.982 307.52 424.255 401.09 716.234 441.32 622.97 85.95 750.07 38.7 609.56-39.62l-327.64-179.52z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M1392.31 8954.63s-833.877-518.13-998.154-222.18c-170.982 307.52 424.255 401.09 716.234 441.32 622.97 85.95 750.07 38.7 609.56-39.62l-327.64-179.52z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M1313.37 6197.89s-156.65-1360.24-670.513-1221.26c-533.672 145.08-86.863 833.27 139.894 1166.09 481.859 708.31 659.239 786.34 621.139 565.07l-90.52-509.9z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M1313.37 6197.89s-156.65-1360.24-670.513-1221.26c-533.672 145.08-86.863 833.27 139.894 1166.09 481.859 708.31 659.239 786.34 621.139 565.07l-90.52-509.9z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M748.006 5472.2s782.064 1465.39 903.374 2145.66c139.59 781.46 162.14 1654.35 65.83 2338.28M1527.94 7165.26l383.11-1173.41M1651.38 7617.86l721.11-906.72M1605.36 7452.97L561.176 7014.4M1741.9 8231.99l785.11-601.63M1700.14 7973.85l-820.163-170.68M1739.46 8602l-635.17-100.58M1793.1 9008.58l427.3-382.81M1784.57 9180.78l-729.95-183.48&quot; stroke=&quot;%23B75400&quot; stroke-width=&quot;1.252&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M1243.88 9404.18h862.84v178.6h-75.28l-155.74 974.42h-400.79l-155.74-974.42h-75.29v-178.6z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M1243.88 9404.18h862.84v178.6h-75.28l-155.74 974.42h-400.79l-155.74-974.42h-75.29v-178.6z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M2664.77 203.152a281.004 281.004 0 00-279.79-55.775c-167.63 55.775-328.25 298.075-287.71 503.193 28.35 142.028 160.01 351.41 199.94 431.26 39.92 79.86-16.16 214.88-142.95 495.27-126.78 280.4-175.85 543.43 191.71 775.06 367.57 231.63 783.59-135.93 996.94-399.57 213.35-263.63-160.01-870.76-192.01-1127.686-32-256.93-86.86-749.456-486.13-621.752z&quot; fill=&quot;%23263238&quot;/&gt;&lt;path d=&quot;M3528.83 1970.27a880.115 880.115 0 01-323.66 205.66 880.242 880.242 0 01-380.39 48.53l-88.08 367.26 167.63 838.45 1094.47-567.2-247.79-640.04-222.18-252.66z&quot; fill=&quot;%23B75400&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path opacity=&quot;.3&quot; d=&quot;M3528.83 1970.27a880.115 880.115 0 01-323.66 205.66 880.242 880.242 0 01-380.39 48.53l-88.08 367.26 167.63 838.45 1094.47-567.2-247.79-640.04-222.18-252.66z&quot; fill=&quot;%23263238&quot;/&gt;&lt;path d=&quot;M3528.83 1970.27L3361.2 859.649l-496.8 101.797-247.78 159.704s-295.64 231.94-295.64 255.71c0 23.78 500.15 860.7 500.15 860.7a927.101 927.101 0 00382.35-58.18 927.269 927.269 0 00325.35-209.11z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M3528.83 1970.27L3361.2 859.649l-496.8 101.797-247.78 159.704s-295.64 231.94-295.64 255.71c0 23.78 500.15 860.7 500.15 860.7a927.101 927.101 0 00382.35-58.18 927.269 927.269 0 00325.35-209.11z&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M2862.27 1922.73s350.19-259.68 317.89-182.87c-32.31 76.8-274.31 268.51-339.22 274.3-64.92 5.79 21.33-91.43 21.33-91.43z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path d=&quot;M2872.94 1429.59l13.71 14.32 85.34 90.22 21.95 346.54 104.54-16.46-82.6-324.59 132.28 274.91 110.02-110.03-236.51-214.56-5.79-170.38-121-60.65&quot; fill=&quot;%23263238&quot;/&gt;&lt;path d=&quot;M2680.62 1018.14s-103.93 47.85-103.93 143.55c0 95.7 112.47 323.37 182.87 371.22 70.4 47.85 191.1-171.29 191.1-171.29s182.87 87.78 231.63 7.93c48.77-79.85-111.85-343.49-215.48-415.418-103.62-71.928-286.19 64.008-286.19 64.008z&quot; fill=&quot;%23fff&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M2592.84 922.129a1896.006 1896.006 0 0171.93 223.711c15.85 79.85 231.64 215.78 255.41 223.71 23.77 7.92 39.93-255.71 39.93-255.71l-63.7-311.489-303.57 119.778z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M2903.11 835.572a309.333 309.333 0 00-40.84-8.534l-220.36 77.719-28.95 75.586c14.63 40.837 33.22 95.697 44.5 140.197a245.334 245.334 0 00125.87 52.73c66.14 4.57 124.35-46.63 165.5-109.42l-45.72-228.278z&quot; fill=&quot;%23B75400&quot; opacity=&quot;.5&quot;/&gt;&lt;path d=&quot;M2592.84 922.129a1896.006 1896.006 0 0171.93 223.711c15.85 79.85 231.64 215.78 255.41 223.71 23.77 7.92 39.93-255.71 39.93-255.71l-63.7-311.489-303.57 119.778z&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M2448.99 786.197s-88.08-71.623-88.08 0c0 71.624 80.15 111.855 143.85 111.855&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M2448.99 786.197s-88.08-71.623-88.08 0c0 71.624 80.15 111.855 143.85 111.855&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M2920.18 554.564s56.08-87.777 71.93 8.229c7.3 62.5-9.85 125.403-47.85 175.554l-24.08-183.783z&quot; fill=&quot;%23fff&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;g opacity=&quot;.5&quot;&gt;&lt;path d=&quot;M2448.99 786.197s-88.08-71.623-88.08 0c0 71.624 80.15 111.855 143.85 111.855&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path d=&quot;M2448.99 786.197s-88.08-71.623-88.08 0c0 71.624 80.15 111.855 143.85 111.855&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;/g&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M2920.18 554.564s56.08-87.777 71.93 8.229c7.3 62.5-9.85 125.403-47.85 175.554l-24.08-183.783z&quot; fill=&quot;%23B75400&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M2349.93 555.173s68.58 285.58 215.79 426.694c147.21 141.113 304.78 142.333 359.34-109.112 44.8-207.251-66.45-483.687-66.45-483.687S2499.58 93.43 2349.93 555.173z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M2605.65 296.109l55.77 140.2s87.78-19.506 137.15 86.557c49.38 106.064 99.06 261.502 70.71 381.891-27.12 114.593-91.43 152.393-190.49 155.133 107.9 46.02 205.73 0 246.27-187.745 44.8-207.251-66.45-483.687-66.45-483.687s-121.6-99.663-252.96-92.349z&quot; fill=&quot;%23B75400&quot; opacity=&quot;.5&quot;/&gt;&lt;path d=&quot;M2349.93 555.173s68.58 285.58 215.79 426.694c147.21 141.113 304.78 142.333 359.34-109.112 44.8-207.251-66.45-483.687-66.45-483.687S2499.58 93.43 2349.93 555.173z&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M2622.41 706.04c8.53 16.153 7.01 33.526-3.35 39.012-10.37 5.486-25.61-3.353-33.83-19.506-8.23-16.154-7.01-33.222 3.35-38.708 10.36-5.486 25.6 3.353 33.83 19.202zM2781.5 656.665c6.1 17.068 2.14 34.136-8.83 37.793-10.98 3.657-25-6.705-30.48-23.773-5.49-17.068-2.14-34.135 8.84-37.793 10.97-3.657 24.38 7.315 30.47 23.773z&quot; fill=&quot;%23263238&quot;/&gt;&lt;path d=&quot;M2792.48 565.231a68.586 68.586 0 00-42.28-11.537 68.556 68.556 0 00-40.62 16.414c-43.89 34.135 39.01 205.117 39.01 205.117s1.52 53.946-67.66 28.954M2792.48 859.649a78.671 78.671 0 01-21.3 29.115 78.634 78.634 0 01-32.05 16.542 78.66 78.66 0 01-68.57-15.178M2606.86 610.643s-78.02-73.452-112.15 68.271&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.5&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M2863.18 298.548c-85.64-74.367-154.82-73.452-185-63.699a204.478 204.478 0 00-93.56-30.478c-239.56-32.002-271.56 152.39-271.56 327.639 0 175.249-152.39 383.414-47.86 407.492a124.96 124.96 0 0085.04-9.744 124.923 124.923 0 0058.82-62.184s111.86-182.869 111.86-311.791 139.28-116.731 139.28-116.731a74.345 74.345 0 0123.05-21.904 74.318 74.318 0 0130.09-10.276 74.356 74.356 0 0159.02 19.379c63.39 54.251 70.71 117.95 121.91 134.408 51.21 16.458 136.85-65.832 136.85-65.832a1205.57 1205.57 0 00-167.94-196.279z&quot; fill=&quot;%23263238&quot;/&gt;&lt;path d=&quot;M2666.91 346.398s-96.31-77.414-182.87 9.449c-86.56 86.862-77.11 251.139-106.07 357.508-28.95 106.368-106.36 125.569-106.36 125.569&quot; stroke=&quot;%23fff&quot; stroke-width=&quot;.5&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M2599.55 317.445s-103.93-103.016-234.68 27.43c-99.66 99.968-55.17 165.496-74.67 329.773M2715.37 297.938s182.87-86.862 260.89 135.323&quot; stroke=&quot;%23fff&quot; stroke-width=&quot;.5&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M2320.98 1377.47l-276.44 1112.45 810.72 229.81 39.93-143.86-532.45-245.65 180.73-640.04s-23.77-119.78-152.39-255.41&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M2896.41 2576.78l-532.46-245.65 180.74-640.04a491.4 491.4 0 00-67.97-149.04l-17.37-13.1v176.77c0 113.07-141.42 473.63-191.1 558.36-49.68 84.73-127.09 190.79-127.09 190.79l155.44-70.71 589.14 235.9 10.67-43.28z&quot; fill=&quot;%23B75400&quot; opacity=&quot;.5&quot;/&gt;&lt;path d=&quot;M2320.98 1377.47l-276.44 1112.45 810.72 229.81 39.93-143.86-532.45-245.65 180.73-640.04s-23.77-119.78-152.39-255.41&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M4600.44 1711.51l-613.83-128.92-626.94-724.465 92.66 582.745 384.63 366.34 772.93 35.36-9.45-131.06z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M3359.67 858.125l74.68 465.095 47.54 145.38 6.1 5.49c17.98-18.59 119.47-124.05 145.38-124.05 25.9 0 318.19 254.49 275.82 274.3l-42.67 21.03 728.13 120.09 5.18-55.78-612.61-128.61-627.55-722.945z&quot; fill=&quot;%23B75400&quot; opacity=&quot;.5&quot;/&gt;&lt;path d=&quot;M4600.44 1711.51l-613.83-128.92-626.94-724.465 92.66 582.745 384.63 366.34 772.93 35.36-9.45-131.06z&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M10330.6 7879.97L2513.91 6734.61a94.84 94.84 0 01-62.2-37.27 94.834 94.834 0 01-17.66-70.32l651.02-4440.96a94.248 94.248 0 0112.35-34.78 94.469 94.469 0 0194.93-45.08l148.12 21.64c18.76 2.82 37.94-.1 55.02-8.36a94.236 94.236 0 0040.68-37.96l196.59-339.83a89.711 89.711 0 0123.77-27.44 217.964 217.964 0 01159.4-39.01l1970.41 288.93a217.65 217.65 0 01117.95 56.69 91.31 91.31 0 0120.11 30.48l175.25 434.01a94.217 94.217 0 0029.23 39.03 94.115 94.115 0 0044.84 19.18l4835.38 708.92a94.666 94.666 0 0162.2 37.07c15.1 20.11 21.5 45.36 17.9 70.22l-651 4440.65c-3.8 24.8-17.2 47.09-37.4 62-20.1 14.91-45.4 21.22-70.2 17.55v0z&quot; fill=&quot;%23263238&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M10413.8 7858.33H2665.99c-22.49 0-44.77-4.43-65.55-13.04a171.153 171.153 0 01-55.57-37.13 171.259 171.259 0 01-37.13-55.57 171.394 171.394 0 01-13.03-65.54V3351.84c0-45.43 18.04-88.99 50.16-121.12a171.303 171.303 0 01121.12-50.16h21.03c34.29-.07 67.79-10.38 96.18-29.6a172.109 172.109 0 0063.22-78.3l117.95-296.55a166.732 166.732 0 0168.58-82.29 219.479 219.479 0 01110.94-30.48h1991.44c26.76-.03 53.3 4.83 78.33 14.33a166.07 166.07 0 0182.29 71.93l202.68 345.31c15.1 25.8 36.69 47.19 62.62 62.05a171.337 171.337 0 0085.19 22.68h4767.36c45.5 0 89 18.05 121.1 50.17 32.2 32.12 50.2 75.69 50.2 121.12v4336.12c0 22.49-4.4 44.76-13 65.54-8.6 20.79-21.2 39.67-37.2 55.57-15.9 15.91-34.7 28.53-55.5 37.13-20.8 8.61-43.1 13.04-65.6 13.04z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.1&quot; d=&quot;M10413.8 7858.33H2665.99c-22.49 0-44.77-4.43-65.55-13.04a171.153 171.153 0 01-55.57-37.13 171.259 171.259 0 01-37.13-55.57 171.394 171.394 0 01-13.03-65.54V3351.84c0-45.43 18.04-88.99 50.16-121.12a171.303 171.303 0 01121.12-50.16h21.03c34.29-.07 67.79-10.38 96.18-29.6a172.109 172.109 0 0063.22-78.3l117.95-296.55a166.732 166.732 0 0168.58-82.29 219.479 219.479 0 01110.94-30.48h1991.44c26.76-.03 53.3 4.83 78.33 14.33a166.07 166.07 0 0182.29 71.93l202.68 345.31c15.1 25.8 36.69 47.19 62.62 62.05a171.337 171.337 0 0085.19 22.68h4767.36c45.5 0 89 18.05 121.1 50.17 32.2 32.12 50.2 75.69 50.2 121.12v4336.12c0 22.49-4.4 44.76-13 65.54-8.6 20.79-21.2 39.67-37.2 55.57-15.9 15.91-34.7 28.53-55.5 37.13-20.8 8.61-43.1 13.04-65.6 13.04z&quot; fill=&quot;%23263238&quot;/&gt;&lt;path d=&quot;M10413.8 7858.33H2665.99c-22.49 0-44.77-4.43-65.55-13.04a171.153 171.153 0 01-55.57-37.13 171.259 171.259 0 01-37.13-55.57 171.394 171.394 0 01-13.03-65.54V3351.84c0-45.43 18.04-88.99 50.16-121.12a171.303 171.303 0 01121.12-50.16h21.03c34.29-.07 67.79-10.38 96.18-29.6a172.109 172.109 0 0063.22-78.3l117.95-296.55a166.732 166.732 0 0168.58-82.29 219.479 219.479 0 01110.94-30.48h1991.44c26.76-.03 53.3 4.83 78.33 14.33a166.07 166.07 0 0182.29 71.93l202.68 345.31c15.1 25.8 36.69 47.19 62.62 62.05a171.337 171.337 0 0085.19 22.68h4767.36c45.5 0 89 18.05 121.1 50.17 32.2 32.12 50.2 75.69 50.2 121.12v4336.12c0 22.49-4.4 44.76-13 65.54-8.6 20.79-21.2 39.67-37.2 55.57-15.9 15.91-34.7 28.53-55.5 37.13-20.8 8.61-43.1 13.04-65.6 13.04v0z&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M4683.95 3165.93H3444.1c-29.59 0-57.96-11.76-78.88-32.68a111.538 111.538 0 01-32.67-78.87c-.04-14.68 2.81-29.22 8.4-42.79 5.59-13.56 13.8-25.9 24.16-36.29a111.564 111.564 0 0178.99-32.78h1239.85c29.66 0 58.11 11.79 79.09 32.76a111.854 111.854 0 0132.76 79.1c0 14.67-2.9 29.2-8.52 42.76a111.628 111.628 0 01-24.26 36.22 111.589 111.589 0 01-79.07 32.57z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M10413.8 8614.8H2665.99c-45.43 0-88.99-18.05-121.12-50.17a171.286 171.286 0 01-50.16-121.12v-4335.2c0-45.43 18.04-89 50.16-121.12a171.313 171.313 0 01121.12-50.17h21.03c34.3-.03 67.81-10.31 96.22-29.54a171.9 171.9 0 0063.18-78.35l117.95-296.25a165.549 165.549 0 0168.58-82.29 216.728 216.728 0 01110.94-30.48h1991.44c26.76-.02 53.3 4.83 78.33 14.33a165.203 165.203 0 0182.29 72.23l202.68 345.62c15.1 25.8 36.69 47.19 62.62 62.05a171.337 171.337 0 0085.19 22.68h4767.36c45.5 0 89 18.05 121.1 50.17 32.2 32.12 50.2 75.69 50.2 121.12v4335.2c0 45.43-18 89-50.2 121.12a171.092 171.092 0 01-121.1 50.17v0z&quot; fill=&quot;%23263238&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M5277.96 2272l-1230.09-152.39a112.177 112.177 0 01-74.31-42.28 112.225 112.225 0 01-22.92-82.37 111.874 111.874 0 0142.42-74.33 111.815 111.815 0 0182.54-22.59l1229.79 152.39a111.804 111.804 0 0174.41 42.21 111.83 111.83 0 0122.82 82.44 111.478 111.478 0 01-13.72 41.36 111.285 111.285 0 01-28.51 32.95 111.467 111.467 0 01-82.43 22.61z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M5277.96 2272l-1230.09-152.39a112.177 112.177 0 01-74.31-42.28 112.225 112.225 0 01-22.92-82.37 111.874 111.874 0 0142.42-74.33 111.815 111.815 0 0182.54-22.59l1229.79 152.39a111.804 111.804 0 0174.41 42.21 111.83 111.83 0 0122.82 82.44 111.478 111.478 0 01-13.72 41.36 111.285 111.285 0 01-28.51 32.95 111.467 111.467 0 01-82.43 22.61z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M4683.95 3922.7H3444.1a111.564 111.564 0 01-78.99-32.78 111.631 111.631 0 01-24.16-36.3c-5.59-13.56-8.44-28.1-8.4-42.78a111.851 111.851 0 01111.55-111.85h1239.85a111.848 111.848 0 01111.85 111.85c0 29.67-11.78 58.12-32.76 79.1a111.89 111.89 0 01-79.09 32.76z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M4683.95 3922.7H3444.1a111.564 111.564 0 01-78.99-32.78 111.631 111.631 0 01-24.16-36.3c-5.59-13.56-8.44-28.1-8.4-42.78a111.851 111.851 0 01111.55-111.85h1239.85a111.848 111.848 0 01111.85 111.85c0 29.67-11.78 58.12-32.76 79.1a111.89 111.89 0 01-79.09 32.76z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M10667.1 5240.88H2412.41c-88.7 0-160.62 71.91-160.62 160.61v4995.01c0 88.8 71.92 160.7 160.62 160.7h8254.69c88.7 0 160.6-71.9 160.6-160.7V5401.49c0-88.7-71.9-160.61-160.6-160.61z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M10827.7 6671.82A12846.661 12846.661 0 017624.47 8863.2c-1732.68 828.69-3713.14 1399.9-4867.04 1694h7909.37c21.1 0 42-4.1 61.6-12.2 19.5-8 37.2-19.9 52.2-34.8 14.9-14.9 26.8-32.6 34.9-52.1 8.1-19.5 12.2-40.4 12.2-61.6V6671.82z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.2&quot; d=&quot;M10827.7 6671.82A12846.661 12846.661 0 017624.47 8863.2c-1732.68 828.69-3713.14 1399.9-4867.04 1694h7909.37c21.1 0 42-4.1 61.6-12.2 19.5-8 37.2-19.9 52.2-34.8 14.9-14.9 26.8-32.6 34.9-52.1 8.1-19.5 12.2-40.4 12.2-61.6V6671.82z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M10667.1 5240.88H2412.41c-88.7 0-160.62 71.91-160.62 160.61v4995.01c0 88.8 71.92 160.7 160.62 160.7h8254.69c88.7 0 160.6-71.9 160.6-160.7V5401.49c0-88.7-71.9-160.61-160.6-160.61z&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M2375.84 5707.8v-213.35c.08-42.63 17.07-83.48 47.24-113.59a160.91 160.91 0 01113.69-47.03h348.97M2375.84 8947.92V5967.78M10703.4 9964.67v337.43c0 21.1-4.1 42-12.2 61.5-8 19.5-19.8 37.3-34.8 52.2-14.9 15-32.6 26.8-52.1 34.9-19.5 8.1-40.4 12.3-61.5 12.3h-281.4M10703.4 5632.82v4081.02&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M7814.66 7184.77H5264.55c-45.95 0-83.2 37.25-83.2 83.2v1301.11c0 45.95 37.25 83.21 83.2 83.21h2550.11c45.95 0 83.2-37.26 83.2-83.21V7267.97c0-45.95-37.25-83.2-83.2-83.2z&quot; fill=&quot;%23263238&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M7814.66 7184.77H5264.55c-45.95 0-83.2 37.25-83.2 83.2v279.79c0 45.95 37.25 83.21 83.2 83.21h2550.11c45.95 0 83.2-37.26 83.2-83.21v-279.79c0-45.95-37.25-83.2-83.2-83.2zM2855.26 2719.73l55.78 103.93L3029.6 2909a46.07 46.07 0 0025.62 8.9c9.24.33 18.37-2.14 26.19-7.08l10.36-6.7a36.279 36.279 0 0015.79-20.16c2.63-8.41 2.11-17.5-1.46-25.56l-67.36-154.83 45.41 35.36a35.06 35.06 0 0025.34 7.07 35.052 35.052 0 0023.12-12.56v0a35.348 35.348 0 007.76-22.09c0-8.04-2.74-15.83-7.76-22.1l-105.76-133.8-132.27 20.42-39.32 143.86z&quot; fill=&quot;%23fff&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M3092.99 2904.12l-106.06-139.29M3030.81 2909l-83.2-110.33&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.5&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M4600.44 1711.51l165.8 9.75L4968 1850.19s11.28 39.31-16.76 44.8c-28.04 5.49-179.52-50.29-179.52-50.29l-161.84-2.13-9.44-131.06z&quot; fill=&quot;%23fff&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M4608.97 1822.15l174.03 78.63s28.04-39.32-11.28-56.08l-57.3-39.32&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M4608.97 1822.15l174.03 78.63s28.04-39.32-11.28-56.08l-57.3-39.32&quot; stroke=&quot;%23263238&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M10757.3 4794.37l-71.9-215.17-80.8-110.64c-1.3-2.02-3.3-3.54-5.6-4.33-2.2-.8-4.7-.81-7-.04-2.3.76-4.3 2.26-5.6 4.26-1.4 2.01-2 4.4-1.9 6.82v318.8l61 80.15 111.8-79.85z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M11929.5 5735.23l-603.5 21.33c-22.2.78-44.3-4.79-63.5-16.07a117.62 117.62 0 01-45-47.63l-460.2-898.49s-103.6 56.08-111.5 79.85c-8 23.78 340.7 898.19 447.7 1166.71 9 22.54 24.7 41.73 45.1 54.94 20.4 13.21 44.3 19.78 68.6 18.81l674.5-28.65&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M11929.5 5735.23l-603.5 21.33c-22.2.78-44.3-4.79-63.5-16.07a117.62 117.62 0 01-45-47.63l-460.2-898.49s-103.6 56.08-111.5 79.85c-8 23.78 340.7 898.19 447.7 1166.71 9 22.54 24.7 41.73 45.1 54.94 20.4 13.21 44.3 19.78 68.6 18.81l674.5-28.65&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M10801.5 4906.23l-94.5 39.01&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.75&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;g opacity=&quot;.5&quot;&gt;&lt;g opacity=&quot;.5&quot;&gt;&lt;path d=&quot;M11929.5 5735.23l-603.5 21.33c-22.2.78-44.3-4.79-63.5-16.07a117.62 117.62 0 01-45-47.63l-460.2-898.49s-103.6 56.08-111.5 79.85c-8 23.78 340.7 898.19 447.7 1166.71 9 22.54 24.7 41.73 45.1 54.94 20.4 13.21 44.3 19.78 68.6 18.81l674.5-28.65&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path d=&quot;M11929.5 5735.23l-603.5 21.33c-22.2.78-44.3-4.79-63.5-16.07a117.62 117.62 0 01-45-47.63l-460.2-898.49s-103.6 56.08-111.5 79.85c-8 23.78 340.7 898.19 447.7 1166.71 9 22.54 24.7 41.73 45.1 54.94 20.4 13.21 44.3 19.78 68.6 18.81l674.5-28.65&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;/g&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M10801.5 4906.23l-94.5 39.01&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.75&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;/g&gt;&lt;path d=&quot;M10757.3 4794.37l-71.9-215.17-80.8-110.64c-1.3-2.02-3.3-3.54-5.6-4.33-2.2-.8-4.7-.81-7-.04-2.3.76-4.3 2.26-5.6 4.26-1.4 2.01-2 4.4-1.9 6.82v318.8l61 80.15 111.8-79.85z&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path opacity=&quot;.5&quot; d=&quot;M10757.3 4794.37l-71.9-215.17-80.8-110.64c-1.3-2.02-3.3-3.54-5.6-4.33-2.2-.8-4.7-.81-7-.04-2.3.76-4.3 2.26-5.6 4.26-1.4 2.01-2 4.4-1.9 6.82v318.8l61 80.15 111.8-79.85z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M10757.3 4794.37l-71.9-215.17-80.8-110.64c-1.3-2.02-3.3-3.54-5.6-4.33-2.2-.8-4.7-.81-7-.04-2.3.76-4.3 2.26-5.6 4.26-1.4 2.01-2 4.4-1.9 6.82v318.8l61 80.15 111.8-79.85z&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M12116.3 5167.42c-28.9 36.73-49.7 79.23-60.9 124.66-8.9 58.52 5.2 283.75 5.2 283.75l-314 35.35s23.8-228.89 25-266.98c10.1-47.25 30.1-91.81 58.7-130.7 28.7-38.89 65.3-71.23 107.4-94.84 107.9-53.03 199.7-27.13 178.6 48.76z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M12114.2 5169.86c-57-15.54-126.2-32.92-184.7-46.94-40.2 23.95-75.1 55.93-102.4 93.95a341.651 341.651 0 00-56.4 127.02c0 14.63-4.3 57.3-8.5 104.24 30.5-7.01 78.9-17.68 114.9-24.08 28.5-4.6 56.5-11.64 83.8-21.03l-10.7 182.87h7l102.4-11.58s-14-225.24-5.1-283.76c11.1-44.01 31.5-85.15 59.7-120.69z&quot; fill=&quot;%23B75400&quot; opacity=&quot;.5&quot;/&gt;&lt;path d=&quot;M12116.3 5167.42c-28.9 36.73-49.7 79.23-60.9 124.66-8.9 58.52 5.2 283.75 5.2 283.75l-314 35.35s23.8-228.89 25-266.98c10.1-47.25 30.1-91.81 58.7-130.7 28.7-38.89 65.3-71.23 107.4-94.84 107.9-53.03 199.7-27.13 178.6 48.76z&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;1.004&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M12181.6 5206.13c260.2-343.79-367.3-701-532.2-359.34-23 42.14-41.2 86.71-54.2 132.89-25 15.24-91.5 56.08-103.7 67.35-12.2 11.28 30.5 40.24 54 54.56-44.8 110.03-38.1 225.84-25.3 252.05 22.2 45.11 132 45.72 204.2 32.31 72.2-13.41 100.9-82.59 121.9-101.49 21-18.9 125.9 5.18 162.7 16.15 36.9 10.97 122-26.51 172.6-94.48z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M11924 5173.21s5.5 25.91-46.9 46.94c-52.4 21.03-62.8 5.18-78.3 41.76-6.6 24.04-20 45.7-38.5 62.39-18.6 16.69-41.5 27.72-66.1 31.78-49.6 9.43-99.8 15.84-150.2 19.2 41.4 23.17 123.7 21.34 182.8 10.67 71.9-13.1 100.9-82.59 121.9-101.49 21.1-18.9 125.9 5.18 162.8 16.15 8.6 1.73 17.5 1.73 26.2 0 13.8-30.62 22-63.43 24.4-96.92l-138.1-30.48z&quot; fill=&quot;%23B75400&quot; opacity=&quot;.5&quot;/&gt;&lt;path d=&quot;M12181.6 5206.13c260.2-343.79-367.3-701-532.2-359.34-23 42.14-41.2 86.71-54.2 132.89-25 15.24-91.5 56.08-103.7 67.35-12.2 11.28 30.5 40.24 54 54.56-44.8 110.03-38.1 225.84-25.3 252.05 22.2 45.11 132 45.72 204.2 32.31 72.2-13.41 100.9-82.59 121.9-101.49 21-18.9 125.9 5.18 162.7 16.15 36.9 10.97 122-26.51 172.6-94.48z&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M11673.2 5009.85c-6.7 14.63-18 23.77-25.3 20.42-7.3-3.35-7.9-17.98 0-32.61 7.9-14.63 18-23.77 25.3-20.42 7.3 3.35 6.7 17.98 0 32.61z&quot; fill=&quot;%23263238&quot;/&gt;&lt;path d=&quot;M11667.1 4915.98s30.5-25.3 36.6 32.91M11539.4 5160.72s0 30.48 54.8 30.48&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.5&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M11465 7155.2l-24.1 298.08s-166.7 1186.82-158.8 1323.05c8 136.24 26 1543.67 26 1543.67l200.8 32.1 225.5-1561.14 316.7-1586.38-586.1-49.38z&quot; fill=&quot;%23B75400&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path opacity=&quot;.2&quot; d=&quot;M11465 7155.2l-24.1 298.08s-166.7 1186.82-158.8 1323.05c8 136.24 26 1543.67 26 1543.67l200.8 32.1 225.5-1561.14 316.7-1586.38-586.1-49.38z&quot; fill=&quot;%23263238&quot;/&gt;&lt;path d=&quot;M11719.2 8239.31s-77.1 453.82-135.3 676.31c-58.2 222.49-57.9 444.37-57.9 685.76 0 241.38-86.9 686.32-86.9 686.32&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.75&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M11872.5 7082.06c132.3-69.49 274.3-115.52 314.8-23.17 80.5 184.7-15.8 465.4-15.8 465.4L12245 8819l134.4 1562-200.9-32.3s-308.4-1318.18-356.6-1398.64c-48.1-80.46-124-1209.07-124-1209.07l-152.4-138.67&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path d=&quot;M11872.5 7082.06c132.3-69.49 274.3-115.52 314.8-23.17 80.5 184.7-15.8 465.4-15.8 465.4L12245 8819l134.4 1562-200.9-32.3s-308.4-1318.18-356.6-1398.64c-48.1-80.46-124-1209.07-124-1209.07l-152.4-138.67&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path opacity=&quot;.2&quot; d=&quot;M11872.5 7082.06c132.3-69.49 274.3-115.52 314.8-23.17 80.5 184.7-15.8 465.4-15.8 465.4L12245 8819l134.4 1562-200.9-32.3s-308.4-1318.18-356.6-1398.64c-48.1-80.46-124-1209.07-124-1209.07l-152.4-138.67&quot; fill=&quot;%23263238&quot;/&gt;&lt;path d=&quot;M12029.2 8062.84c4.5 300.21 35 444.68 58.8 523.92 28.9 96.61 19.2 280.09 19.2 365.74 0 85.64-38.7-19.21 48.4 377.01 87.2 396.22 144.8 946.69 144.8 946.69M12029.2 7408.48s9.4-19.51 0 434.61v70.71&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.75&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M11583.3 7322.22l-6.4 135.94-31.4 94.48 46.9 54.86M11521.7 7403.29s78.6 78.64 282.8 54.87&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.75&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M12004.2 7335.33c152.4-30.48 162.1-72.54 214.8-107.89 1-57.64-9.3-114.91-30.4-168.55-40-91.43-182.9-46.32-314.9 23.17l-64 101.18-343.5-28.04-10.9 134.72c176.3 60.5 365 76.11 548.9 45.41z&quot; fill=&quot;%23263238&quot;/&gt;&lt;path d=&quot;M11499.5 7248.47l-3.4 53.64 87.2 20.11 6.7-70.4-90.5-3.35z&quot; fill=&quot;%23fff&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.75&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M12107.8 7328.02l-14.6-133.5 44.2-14.63 22.2 118.56&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path d=&quot;M12107.8 7328.02l-14.6-133.5 44.2-14.63 22.2 118.56&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M11767.1 7372.51l-29.6-155.74h74.1l29.5 133.49&quot; fill=&quot;%23B75400&quot;/&gt;&lt;path d=&quot;M11767.1 7372.51l-29.6-155.74h74.1l29.5 133.49&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;g opacity=&quot;.2&quot; fill=&quot;%23263238&quot;&gt;&lt;path opacity=&quot;.2&quot; d=&quot;M12107.8 7328.02l-14.6-133.5 44.2-14.63 22.2 118.56M11767.1 7372.51l-29.6-155.74h74.1l29.5 133.49&quot;/&gt;&lt;/g&gt;&lt;path d=&quot;M11425.7 5973.87s-25.3 169.16-101.2 457.17c-75.9 288.02-152.4 633.34-152.4 633.34l42.4 177.38 126.5-185.91 84.7-1081.98zM11636.9 5729.13s-177.4 42.06-194.1 126.49c-9.1 38.86-14.8 78.42-17.1 118.25l270.3-219.44 338-135.32-397.1 110.02z&quot; fill=&quot;%23263238&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M11685.4 5663.91s-144.2 163.06-201.5 230.11c-57.3 67.05-57.6 786.64-57.6 892.4 0 105.76-57.3 392.86 38.4 450.46 95.7 57.61 681.2 38.41 738.8-57.6 57.6-96 95.7-1246.86 86.3-1352.31-7.2-47.31-25.9-92.15-54.3-130.61-28.5-38.46-66-69.38-109.1-90.05-66.8-30.79-441 57.6-441 57.6z&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M12126.4 5606.31c-8-3.22-16.4-5.08-25-5.49l-41.5 21.64a393.295 393.295 0 01136 224.93c19.3 128.35 17.5 258.98-5.2 386.77-20.7 141.11-355.4 0-292.6 52.42 62.8 52.42 297.8 141.11 308.4 188.05 10.7 46.94-116.4 125.26-162.1 130.75-45.7 5.49 30.5 109.72 78.3 198.41 47.9 88.69 30.5 213.35-78.3 298.08-108.8 84.73-361.5 126.48-470.6 130.75-35.3 0-57.3 9.14-64 19.5 167.6 37.49 642.2 13.11 693.7-72.84 57.6-96 95.7-1246.86 86.3-1352.31-7.2-47.31-25.9-92.15-54.3-130.61-28.5-38.46-66-69.38-109.1-90.05z&quot; fill=&quot;%23B75400&quot; opacity=&quot;.5&quot;/&gt;&lt;path d=&quot;M11685.4 5663.91s-144.2 163.06-201.5 230.11c-57.3 67.05-57.6 786.64-57.6 892.4 0 105.76-57.3 392.86 38.4 450.46 95.7 57.61 681.2 38.41 738.8-57.6 57.6-96 95.7-1246.86 86.3-1352.31-7.2-47.31-25.9-92.15-54.3-130.61-28.5-38.46-66-69.38-109.1-90.05-66.8-30.79-441 57.6-441 57.6z&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M11980.1 6393.86c-154.5-48.15-289.9-182.87-289.9-182.87 34.3 160.08 127.4 301.47 260.9 396.22&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M11980.1 6393.86c-154.5-48.15-289.9-182.87-289.9-182.87 34.3 160.08 127.4 301.47 260.9 396.22&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.75&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M12097.7 5500.85l-460.5 105.46-28.6 134.41 517.8-134.41-28.7-105.46z&quot; fill=&quot;%23fff&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M11535.7 10553.2l-26.8-202.4-200.8-32-167.4 119.2-149.9 82.3v32.9h544.9zM12404.4 10569.1l-25-188.1-200.9-32.3-167.3 119.2-150 82.6v18.6h543.2z&quot; fill=&quot;%23263238&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M10669.5 4618.82l-123.7-71.63s-48.2-5.79-27.4 47.85l123.7 95.4&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M10669.5 4618.82l-123.7-71.63s-48.2-5.79-27.4 47.85l123.7 95.4&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M10651.3 4805.04c-5.9-14.7-14.5-28.15-25.3-39.62l-23.2-25-125.9-156.04s-39-26.21-30.5 30.47l78.1 152.39-138.7-130.14s-26.2-21.64-26.2 8.84 125.9 164.89 125.9 164.89l-139-65.22s-30.5 21.64-8.6 43.27c22 21.64 138.7 65.23 138.7 65.23l-103.9-4.27s-30.5 25.91-8.9 34.75c21.7 8.83 143.3 34.44 143.3 34.44 15.4 13.13 33.1 23.45 52.1 30.47l30.5 56.39 116.7-58.82-55.1-142.03z&quot; fill=&quot;%23fff&quot; stroke=&quot;%23263238&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;path d=&quot;M11897.5 6157.96l-695.5 163.06c-35.6 6.27-72.3.01-103.8-17.72a156.197 156.197 0 01-69-79.51l-438.9-1219.12 116.7-58.82 484.9 898.19c11 20.54 28.3 37 49.4 46.93 21.1 9.94 44.8 12.82 67.7 8.23l531.8-150.26&quot; fill=&quot;%23fff&quot;/&gt;&lt;path d=&quot;M11897.5 6157.96l-695.5 163.06c-35.6 6.27-72.3.01-103.8-17.72a156.197 156.197 0 01-69-79.51l-438.9-1219.12 116.7-58.82 484.9 898.19c11 20.54 28.3 37 49.4 46.93 21.1 9.94 44.8 12.82 67.7 8.23l531.8-150.26&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M10772.6 5060.75l-135.4 67.66&quot; stroke=&quot;%23263238&quot; stroke-width=&quot;.75&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M11810.9 4850.45s-74 74.06-37.1 129.53c21.8 32.04 38.5 67.32 49.3 104.54 0 0-30.4 74.06-6.1 74.06 24.4 0 12.2-55.47 55.5-42.97s86.3 86.25 49.4 110.94c-27.6 15.42-56.8 27.69-87.2 36.57a20.75 20.75 0 00-9.9 6.58c-2.6 3.11-4.2 6.9-4.6 10.92-.5 4.02.3 8.08 2.1 11.68 1.8 3.61 4.6 6.6 8.1 8.62 40.1 27.08 86.3 43.84 134.5 48.76 39.3.84 78.6-3.25 117-12.19 0 0 121.9-166.41 209.7-209.69 87.8-43.28 49.4-240.47 49.4-265.16 0-24.68-86.3-80.16-86.3-80.16s24.7-55.47-86.2-178.6c-111-123.13-320.7-148.12-326.8-104.84-2.1 11.46-.9 23.29 3.4 34.09a61.285 61.285 0 0021.3 26.87s-185-80.16-271.2 6.09c-86.3 86.25 215.7 284.36 215.7 284.36z&quot; fill=&quot;%23263238&quot; stroke=&quot;%23263238&quot; stroke-miterlimit=&quot;10&quot;/&gt;&lt;path d=&quot;M11810.9 4850.45s150 182.87 372.2 200.55M11757.9 4906.23s69.8 202.98 358.4 260.89&quot; stroke=&quot;%23fff&quot; stroke-width=&quot;.5&quot; stroke-linecap=&quot;round&quot; stroke-linejoin=&quot;round&quot;/&gt;&lt;/svg&gt;\" alt=\"\"/><span class=\"styles-services-overview-item-label-2qM2Y\">Services overview</span><span class=\"styles-services-overview-item-description-1SQ-o\">Explore all CORE services</span></a></li><li class=\"styles-group-3_h4T\"><h3 id=\"access-to-raw-data-title\" class=\"styles-group-title-1v3Na\">Access to raw data</h3><ul class=\"styles-group-list-37aX1\" aria-labelledby=\"access-to-raw-data-title\"><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/services/api\">API</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/services/dataset\">Dataset</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/services/fastsync\">FastSync</a></li></ul></li><li class=\"styles-group-3_h4T\"><h3 id=\"content-discovery-title\" class=\"styles-group-title-1v3Na\">Content discovery</h3><ul class=\"styles-group-list-37aX1\" aria-labelledby=\"content-discovery-title\"><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/services/recommender\">Recommender</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/services/discovery\">Discovery</a></li></ul></li><li class=\"styles-group-3_h4T\"><h3 id=\"oai-resolver-title\" class=\"styles-group-title-1v3Na\">OAI identifiers</h3><ul class=\"styles-group-list-37aX1\" aria-labelledby=\"oai-resolver-title\"><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/documentation/oai-resolver\">OAI Resolver</a></li></ul></li><li class=\"styles-group-3_h4T\"><h3 id=\"managing-content-title\" class=\"styles-group-title-1v3Na\">Managing content</h3><ul class=\"styles-group-list-37aX1\" aria-labelledby=\"managing-content-title\"><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/services/repository-dashboard\">Dashboard</a></li></ul></li><li class=\"styles-group-3_h4T\"><h3 id=\"contracts-title\" class=\"styles-group-title-1v3Na\">Bespoke contracts</h3><ul class=\"styles-group-list-37aX1\" aria-labelledby=\"contracts-title\"><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/services/consultancy\">Consultancy services</a></li></ul></li></ul></div><div class=\"styles-menu-2MrzW styles-communities-menu-3a1Ao\"><button class=\"button-button-3fr3r button-text-2Jjil styles-menu-title-zh5md\" role=\"button\" id=\"communities-title\" aria-haspopup=\"true\" aria-expanded=\"false\">Support us</button><ul class=\"styles-menu-list-2qL0f\" aria-labelledby=\"communities-title\"><li class=\"styles-group-3_h4T styles-support-menu-36qjb\"><h3 id=\"membership-title\" class=\"styles-group-title-1v3Na\">Support us</h3><ul class=\"styles-group-list-37aX1\" aria-labelledby=\"membership-title\"><a class=\"button-button-3fr3r button-contained-pmIp- styles-member-menu-item-tvwnB\" href=\"/membership\"><span>Membership</span></a><a class=\"button-button-3fr3r button-contained-pmIp- styles-member-menu-item-tvwnB\" href=\"/sponsorship\"><span>Sponsorship</span></a><a class=\"button-button-3fr3r button-contained-pmIp- styles-member-menu-item-tvwnB\" href=\"/governance/research\"><span>Research partnership</span></a></ul></li></ul></div><div class=\"styles-menu-2MrzW styles-about-menu-3ZQZB\"><button class=\"button-button-3fr3r button-text-2Jjil styles-menu-title-zh5md\" role=\"button\" id=\"about-title\" aria-haspopup=\"true\" aria-expanded=\"false\">About</button><ul class=\"styles-menu-list-2qL0f\" aria-labelledby=\"about-title\"><li class=\"styles-group-3_h4T\"><h3 id=\"about-title\" class=\"styles-group-title-1v3Na\">About</h3><ul class=\"styles-group-list-37aX1\" aria-labelledby=\"about-title\"><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/about\">About us</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/about#mission\">Our mission</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/about#team\">Team</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"//blog.core.ac.uk\">Blog</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/faq\">FAQs</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/contact\">Contact us</a></li></ul></li><li class=\"styles-group-3_h4T\"><h3 id=\"community-title\" class=\"styles-group-title-1v3Na\">Community governance</h3><ul class=\"styles-group-list-37aX1\" aria-labelledby=\"community-title\"><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/governance\">Governance</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/governance/advisory\">Advisory Board</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/governance/supporters\">Board of supporters</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/governance/research\">Research network</a></li></ul></li><li class=\"styles-group-3_h4T\"><h3 id=\"innovations-title\" class=\"styles-group-title-1v3Na\">Innovations</h3><ul class=\"styles-group-list-37aX1\" aria-labelledby=\"innovations-title\"><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/about/research-outputs\">Our research</a></li><li class=\"styles-item-34TYT\"><a class=\"styles-item-link--QpIs\" href=\"/innovations/labs\">Labs</a></li></ul></li></ul></div></nav></header><div id=\"content\"><main class=\"article fullHeight\"><div class=\"article-header container \"><h1>Uh-oh</h1></div><div class=\"section\"><p>The page you were looking for could not be found.</p><p>To help you find what you are looking for, why not</p><div role=\"toolbar\" class=\"btn-toolbar-left  btn-toolbar\"><a href=\"/\" class=\"btn btn-primary\">Go back to the homepage</a><a href=\"/about#contact\" class=\"btn btn-outline-primary\">Contact us</a></div></div></main></div><footer class=\"footer-footer-7joNV\" id=\"footer\"><div class=\"footer-container-1XrUr\"><div class=\"footer-section-logos-1_Ejr\"><div class=\"footer-logos-28CCb\"><a href=\"https://www.open.ac.uk\"><svg class=\"icon-icon-35iz- footer-footer-main-logo-1jMdX\" role=\"img\" aria-label=\"Ou logo\" width=\"24\" height=\"24\"><use href=\"/design/icons.svg#ou-logo\"></use></svg></a></div><p>CORE is a not-for-profit service delivered by The Open University supported by<!-- --> <a href=\"https://core.ac.uk/governance/supporters#membership-list\"><span>CORE Members.</span></a></p></div><menu class=\"footer-menu-2JIRS\"><div><h6 class=\"footer-menu-title-2NNki\">Product</h6><ul class=\"undefined \"><li class=\"footer-item-1F33t\"><a href=\"/services\">Services</a></li><li class=\"footer-item-1F33t\"><a href=\"/membership\">Membership</a></li><li class=\"footer-item-1F33t\"><a href=\"/sponsorship\">Sponsorship</a></li><li class=\"footer-item-1F33t\"><a href=\"/innovations/labs\">Labs</a></li></ul></div><div><h6 class=\"footer-menu-title-2NNki\">Organisation</h6><ul class=\"undefined \"><li class=\"footer-item-1F33t\"><a href=\"/about\">About us</a></li><li class=\"footer-item-1F33t\"><a href=\"/about#our-mission\">Governance</a></li><li class=\"footer-item-1F33t\"><a href=\"/about#team\">Team</a></li><li class=\"footer-item-1F33t\"><a href=\"/benefits\">Data providers</a></li></ul></div><div><h6 class=\"footer-menu-title-2NNki\">Support</h6><ul class=\"undefined \"><li class=\"footer-item-1F33t\"><a href=\"/terms\">Terms</a></li><li class=\"footer-item-1F33t\"><a href=\"/faq\">FAQs</a></li><li class=\"footer-item-1F33t\"><a href=\"https://blog.core.ac.uk/\">Blog</a></li><li class=\"footer-item-1F33t\"><a href=\"/about#contact\">Contact us</a></li></ul></div></menu><div class=\"footer-section-card-3esBH\"><div class=\"footer-card-1yZdd\"><div><h6 class=\"footer-card-title-3Uaog\">Writing about CORE?</h6><p class=\"footer-card-desc-30YMW\">Discover our<!-- --> <a href=\"/about/research-outputs\">research outputs</a> and cite our work.</p> </div><svg class=\"icon-icon-35iz- footer-card-img-o-6kR\" role=\"img\" aria-label=\"writing\" width=\"24\" height=\"24\"><use href=\"/design/icons.svg#writing\"></use></svg></div><div class=\"footer-contact-wrapper-17ZHP\"><span class=\"footer-contact-2b5lt\">Follow us:</span><div class=\"footer-footer-icons-2NPaF\"><a href=\"https://twitter.com/oacore\" target=\"_blank\"><svg class=\"icon-icon-35iz- footer-footer-icon-2g_ak\" role=\"img\" aria-label=\"twitter\" width=\"24\" height=\"24\"><use href=\"/design/icons.svg#twitter\"></use></svg></a><a href=\"https://www.linkedin.com/company/coreoa\" target=\"_blank\"><svg class=\"icon-icon-35iz- footer-footer-icon-2g_ak\" role=\"img\" aria-label=\"linkedin\" width=\"24\" height=\"24\"><use href=\"/design/icons.svg#linkedin\"></use></svg></a><a href=\"https://github.com/oacore\" target=\"_blank\"><svg class=\"icon-icon-35iz- footer-footer-icon-2g_ak\" role=\"img\" aria-label=\"github\" width=\"24\" height=\"24\"><use href=\"/design/icons.svg#github\"></use></svg></a></div></div></div></div><div class=\"footer-bottom-3E5D_\"><a href=\"/\" class=\"footer-logo-link-j8Agm\"><span class=\"logo-logo-wCAGq\"><svg class=\"icon-icon-35iz- logo-logo-icon-1JnTB\" role=\"img\" aria-label=\"CORE\" width=\"18\" height=\"24\"><use href=\"/design/icons.svg#core-symbol\"></use></svg>CORE</span></a><a href=\"/accessibility\" class=\"footer-bottom-link-PaLMR\">Accessibility</a><a href=\"/cookies\" class=\"footer-bottom-link-PaLMR\">Cookies</a><a href=\"/privacy\" class=\"footer-bottom-link-PaLMR\">Privacy</a></div></footer></div><script id=\"__NEXT_DATA__\" type=\"application/json\">{\"props\":{\"pageProps\":{}},\"page\":\"/404\",\"query\":{},\"buildId\":\"Mi7r0zB098bfuAsFzq6DM\",\"assetPrefix\":\"https://core.ac.uk\",\"nextExport\":true,\"autoExport\":true,\"isFallback\":false}</script><script nomodule=\"\" src=\"https://core.ac.uk/_next/static/chunks/polyfills-1fa0ce8a429103f107e7.js\"></script><script src=\"https://core.ac.uk/_next/static/chunks/webpack-53b2a1c2781a4aa3c425.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/framework.898ed9c4190ec62ba669.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/commons.d7ddf4d2b1bef08d6a43.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/main-dba0e33f318ab41063ce.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/b5f2ed29.441239f4fc365a69741a.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/e82996df.86a4b1a61816698443e2.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/c3f7f8607465e617b299e6671ee3987d5fa63bf3.58ced7faaca4d9aff03e.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/d4724269a42ff2517c4025d0beda2692c5594bc9.0b17ca494529028ea65e.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/69f785c1a9b276ccb372885623ae0ecb0053519b.c00ac77b0ffe249a74d2.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/69f785c1a9b276ccb372885623ae0ecb0053519b_CSS.ddce25b62cf34b951439.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/d46a588f3493bf286384f5488b19222037b6d567.ae093e90d5f38d3f1f45.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/9c353c8ac20eb1f739641e3bcf47b76d12dc6fca.d9558af2faaffeeb09b3.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/pages/_app-3fffe18f3d7f48d99642.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/chunks/pages/404-2c39bf5a09df80115614.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/Mi7r0zB098bfuAsFzq6DM/_buildManifest.js\" async=\"\"></script><script src=\"https://core.ac.uk/_next/static/Mi7r0zB098bfuAsFzq6DM/_ssgManifest.js\" async=\"\"></script></body></html>\n",
      "Saved metadata to core_papers_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "CORE_API_KEY = 'ZLQojgG1uJDYRdprWS8UhEzsIPM03cNi' \n",
    "\n",
    "query = \"natural+language+processing\"\n",
    "page_size = 50  # Max results per page (CORE allows up to 100)\n",
    "\n",
    "# CORE Search API endpoint\n",
    "CORE_SEARCH_URL = \"https://core.ac.uk/api-v3/search/works\"\n",
    "\n",
    "CORe = f\"https://core.ac.uk/search/?q={query}&page=1\"\n",
    "# Function to query CORE and collect results\n",
    "def fetch_core_papers(query, max_pages=5):\n",
    "    all_results = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            'q': query,\n",
    "            'page': page,\n",
    "            'pageSize': page_size,\n",
    "            'apiKey': CORE_API_KEY\n",
    "        }\n",
    "\n",
    "        response = requests.get(CORe, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        if 'data' not in data:\n",
    "            print(\"No data found.\")\n",
    "            break\n",
    "\n",
    "        for item in data['data']:\n",
    "            paper_info = {\n",
    "                'title': item.get('title'),\n",
    "                'authors': item.get('authors'),\n",
    "                'year': item.get('publishedDate', '')[:4],\n",
    "                'journal': item.get('journals', [{}])[0].get('name') if item.get('journals') else None,\n",
    "                'abstract': item.get('description'),\n",
    "                'fullTextLink': item.get('fullTextLink')\n",
    "            }\n",
    "            all_results.append(paper_info)\n",
    "\n",
    "        print(f\"Page {page} fetched, total results: {len(all_results)}\")\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "# Fetch papers and save to CSV\n",
    "df = fetch_core_papers(query=\"machine learning\", max_pages=3)\n",
    "df.to_csv(\"core_papers_metadata.csv\", index=False)\n",
    "print(\"Saved metadata to core_papers_metadata.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e566c",
   "metadata": {},
   "source": [
    "OpenAlex retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8d83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching OpenAlex for: Large Language Model\n",
      "âœ… Retrieved 50 papers.\n",
      "Saved metadata to papers.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "def fetch_openalex_papers(query: str, max_results: int = 50) -> pd.DataFrame:\n",
    "    print(f\"Searching OpenAlex for: {query}\")\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    per_page = 25\n",
    "    all_results = []\n",
    "    cursor = \"*\"\n",
    "\n",
    "    # âœ… Real, properly formatted User-Agent\n",
    "    headers = {\n",
    "        \"User-Agent\": \"LLMResearchBot/1.0 (mailto:your.email@example.com)\"\n",
    "    }\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "\n",
    "    while len(all_results) < max_results:\n",
    "        url = (\n",
    "            f\"{base_url}?filter=title.search:{encoded_query}\"\n",
    "            f\"&per-page={per_page}&cursor={cursor}\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            break\n",
    "\n",
    "        if response.status_code == 403:\n",
    "            print(\"âŒ Access Forbidden (403) â€” Check your User-Agent and network.\")\n",
    "            break\n",
    "        elif response.status_code != 200:\n",
    "            print(f\"Failed to fetch data: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        results = data.get(\"results\", [])\n",
    "\n",
    "        for work in results:\n",
    "            paper = {\n",
    "                \"id\": work.get(\"id\"),\n",
    "                \"title\": work.get(\"title\"),\n",
    "                \"doi\": work.get(\"doi\"),\n",
    "                \"publication_year\": work.get(\"publication_year\"),\n",
    "                \"authors\": \", \".join([a['author']['display_name'] for a in work.get(\"authorships\", [])]),\n",
    "                \"abstract\": work.get(\"abstract_inverted_index\", None),\n",
    "                \"open_access\": work.get(\"open_access\", {}).get(\"is_oa\", False),\n",
    "                \"host_venue\": work.get(\"host_venue\", {}).get(\"display_name\", \"\")\n",
    "            }\n",
    "\n",
    "            # Rebuild abstract from inverted index\n",
    "            if paper[\"abstract\"]:\n",
    "                abstract_words = sorted(\n",
    "                    [(v, k) for k, vlist in paper[\"abstract\"].items() for v in vlist]\n",
    "                )\n",
    "                paper[\"abstract\"] = \" \".join([w for _, w in abstract_words])\n",
    "            else:\n",
    "                paper[\"abstract\"] = \"\"\n",
    "\n",
    "            all_results.append(paper)\n",
    "\n",
    "        cursor = data.get('meta', {}).get('next_cursor', None)\n",
    "        if not cursor:\n",
    "            break\n",
    "\n",
    "        time.sleep(1)  # rate-limiting\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "    print(f\"âœ… Retrieved {len(df)} papers.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, filename: str = \"papers.csv\"):\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved metadata to {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter your research topic: \")\n",
    "    df = fetch_openalex_papers(query, max_results=50)\n",
    "    save_to_csv(df) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4a4fb",
   "metadata": {},
   "source": [
    "Bidirectional Encoder Representation for Transformers(BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99209be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"fill-mask\", model=\"google-bert/bert-base-uncased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06492b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6786b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1386,  0.1583, -0.2967,  ..., -0.2708, -0.2844,  0.4581],\n",
      "         [ 0.5364, -0.2327,  0.1754,  ...,  0.5540,  0.4981, -0.0024],\n",
      "         [ 0.3002, -0.3475,  0.1208,  ..., -0.4562,  0.3288,  0.8773],\n",
      "         ...,\n",
      "         [ 0.3799,  0.1203,  0.8283,  ..., -0.8624, -0.5957,  0.0471],\n",
      "         [-0.0252, -0.7177, -0.6950,  ...,  0.0757, -0.6668, -0.3401],\n",
      "         [ 0.7535,  0.2391,  0.0717,  ...,  0.2467, -0.6458, -0.3213]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9377, -0.5043, -0.9799,  0.9030,  0.9329, -0.2438,  0.8926,  0.2288,\n",
      "         -0.9531, -1.0000, -0.8862,  0.9906,  0.9855,  0.7155,  0.9455, -0.8645,\n",
      "         -0.6035, -0.6666,  0.3020, -0.1587,  0.7455,  1.0000, -0.4022,  0.4261,\n",
      "          0.6151,  0.9996, -0.8773,  0.9594,  0.9585,  0.6950, -0.6718,  0.3325,\n",
      "         -0.9954, -0.2268, -0.9658, -0.9951,  0.6127, -0.7670,  0.0873,  0.0824,\n",
      "         -0.9518,  0.4713,  1.0000,  0.3299,  0.7583, -0.2670, -1.0000,  0.3166,\n",
      "         -0.9364,  0.9910,  0.9719,  0.9893,  0.2190,  0.6048,  0.5849, -0.4123,\n",
      "         -0.0063,  0.1719, -0.3988, -0.6190, -0.6603,  0.5069, -0.9757, -0.9039,\n",
      "          0.9926,  0.9323, -0.3687, -0.4869, -0.3143,  0.0499,  0.9129,  0.3396,\n",
      "         -0.1879, -0.9235,  0.8675,  0.3228, -0.6406,  1.0000, -0.7989, -0.9931,\n",
      "          0.9629,  0.9124,  0.4827, -0.7276,  0.5996, -1.0000,  0.7548, -0.1600,\n",
      "         -0.9941,  0.3386,  0.8394, -0.4158,  0.2943,  0.6111, -0.5745, -0.7185,\n",
      "         -0.4768, -0.9681, -0.4327, -0.6732,  0.1248, -0.2093, -0.5882, -0.4186,\n",
      "          0.5447, -0.6125, -0.6138,  0.4712,  0.4779,  0.7633,  0.3974, -0.4148,\n",
      "          0.7063, -0.9680,  0.7389, -0.4270, -0.9948, -0.6019, -0.9950,  0.7459,\n",
      "         -0.6343, -0.2753,  0.9522, -0.5724,  0.6218, -0.1295, -0.9905, -1.0000,\n",
      "         -0.8710, -0.7506, -0.5008, -0.4827, -0.9872, -0.9847,  0.7214,  0.9694,\n",
      "          0.3013,  1.0000, -0.4427,  0.9699, -0.5431, -0.8189,  0.9180, -0.5132,\n",
      "          0.9026,  0.5274, -0.5940,  0.2928, -0.6933,  0.7179, -0.9318, -0.2776,\n",
      "         -0.9160, -0.9457, -0.3287,  0.9556, -0.7927, -0.9860, -0.1904, -0.2760,\n",
      "         -0.6062,  0.9005,  0.9266,  0.4353, -0.6858,  0.4720,  0.2851,  0.7685,\n",
      "         -0.8647, -0.5626,  0.5127, -0.5468, -0.9490, -0.9907, -0.5809,  0.7146,\n",
      "          0.9948,  0.7981,  0.3463,  0.9349, -0.4238,  0.9333, -0.9754,  0.9936,\n",
      "         -0.2597,  0.4665, -0.5400,  0.4947, -0.8723,  0.0034,  0.8378, -0.9134,\n",
      "         -0.8432, -0.2516, -0.5177, -0.4687, -0.9491,  0.5691, -0.4856, -0.4857,\n",
      "         -0.2245,  0.9609,  0.9823,  0.7496,  0.6256,  0.8552, -0.9073, -0.5802,\n",
      "          0.2874,  0.3017,  0.3016,  0.9974, -0.8503, -0.2108, -0.9261, -0.9907,\n",
      "         -0.0252, -0.9488, -0.3972, -0.8097,  0.8707, -0.7512,  0.8107,  0.5488,\n",
      "         -0.9830, -0.8569,  0.4852, -0.6156,  0.4846, -0.2893,  0.9647,  0.9858,\n",
      "         -0.7064,  0.7120,  0.9593, -0.9590, -0.8708,  0.7893, -0.3561,  0.8603,\n",
      "         -0.7243,  0.9882,  0.9876,  0.9282, -0.9547, -0.8329, -0.7993, -0.8398,\n",
      "         -0.2333,  0.2315,  0.9712,  0.6055,  0.6388,  0.2429, -0.7884,  0.9981,\n",
      "         -0.9448, -0.9804, -0.8184, -0.3534, -0.9951,  0.9729,  0.4165,  0.8094,\n",
      "         -0.6227, -0.8183, -0.9817,  0.8532,  0.1242,  0.9826, -0.6376, -0.9450,\n",
      "         -0.8094, -0.9748,  0.0412, -0.3097, -0.8153, -0.0306, -0.9255,  0.5677,\n",
      "          0.6217,  0.6652, -0.9682,  0.9997,  1.0000,  0.9826,  0.9013,  0.8950,\n",
      "         -1.0000, -0.8081,  1.0000, -0.9995, -1.0000, -0.9361, -0.8200,  0.4755,\n",
      "         -1.0000, -0.2698, -0.0111, -0.9297,  0.8492,  0.9879,  0.9950, -1.0000,\n",
      "          0.8653,  0.9513, -0.5679,  0.9966, -0.6713,  0.9815,  0.6008,  0.7414,\n",
      "         -0.3265,  0.5574, -0.9801, -0.8956, -0.8082, -0.9267,  0.9999,  0.2542,\n",
      "         -0.7970, -0.8854,  0.7831, -0.1391, -0.0060, -0.9786, -0.4503,  0.8895,\n",
      "          0.9021,  0.3021,  0.2650, -0.5750,  0.5099,  0.1216,  0.1170,  0.6484,\n",
      "         -0.9505, -0.3889, -0.6938,  0.2508, -0.7526, -0.9831,  0.9646, -0.2742,\n",
      "          0.9865,  1.0000,  0.3756, -0.9045,  0.8847,  0.4860, -0.5515,  1.0000,\n",
      "          0.9092, -0.9904, -0.4959,  0.7900, -0.7156, -0.8280,  0.9999, -0.4197,\n",
      "         -0.9282, -0.7733,  0.9945, -0.9956,  0.9998, -0.8985, -0.9838,  0.9735,\n",
      "          0.9655, -0.8103, -0.8325,  0.1020, -0.6722,  0.4561, -0.9412,  0.8396,\n",
      "          0.6979, -0.1201,  0.9288, -0.8345, -0.6312,  0.4356, -0.8901, -0.4565,\n",
      "          0.9874,  0.5709, -0.2111, -0.0206, -0.4182, -0.9116, -0.9781,  0.8246,\n",
      "          1.0000, -0.4229,  0.9489, -0.5226, -0.0986,  0.2202,  0.7459,  0.7152,\n",
      "         -0.3528, -0.8800,  0.9299, -0.9716, -0.9949,  0.7278,  0.2206, -0.4944,\n",
      "          1.0000,  0.6285,  0.3795,  0.7228,  0.9993,  0.0301,  0.5936,  0.9816,\n",
      "          0.9914, -0.3465,  0.5882,  0.8365, -0.9824, -0.4488, -0.7612,  0.1331,\n",
      "         -0.9479, -0.0559, -0.9697,  0.9846,  0.9960,  0.5818,  0.3121,  0.8577,\n",
      "          1.0000, -0.9274,  0.6693, -0.1365,  0.8035, -1.0000, -0.8057, -0.4504,\n",
      "         -0.1711, -0.9512, -0.5899,  0.3991, -0.9754,  0.9563,  0.8806, -0.9937,\n",
      "         -0.9923, -0.4979,  0.8853,  0.1439, -0.9994, -0.8986, -0.6272,  0.8385,\n",
      "         -0.3239, -0.9470, -0.7009, -0.4768,  0.5742, -0.2216,  0.5665,  0.9667,\n",
      "          0.7935, -0.9401, -0.6746, -0.1753, -0.9163,  0.9409, -0.8701, -0.9894,\n",
      "         -0.2514,  1.0000, -0.4087,  0.9385,  0.6050,  0.8219, -0.2712,  0.3326,\n",
      "          0.9827,  0.3613, -0.8314, -0.9850, -0.2861, -0.5398,  0.8254,  0.8414,\n",
      "          0.7590,  0.9412,  0.9627,  0.2765, -0.0737,  0.0399,  0.9998, -0.3095,\n",
      "         -0.1933, -0.4689, -0.2511, -0.4629, -0.2914,  1.0000,  0.3963,  0.7777,\n",
      "         -0.9950, -0.9808, -0.9303,  1.0000,  0.8822, -0.6848,  0.8124,  0.6242,\n",
      "         -0.2551,  0.8266, -0.2791, -0.3167,  0.2294,  0.1682,  0.9627, -0.6738,\n",
      "         -0.9904, -0.7910,  0.7099, -0.9770,  1.0000, -0.7030, -0.3960, -0.5981,\n",
      "         -0.6683, -0.2727, -0.0183, -0.9882, -0.3841,  0.5605,  0.9745,  0.3505,\n",
      "         -0.4898, -0.9298,  0.9578,  0.9533, -0.9859, -0.9597,  0.9777, -0.9784,\n",
      "          0.7551,  1.0000,  0.3446,  0.6786,  0.3947, -0.5349,  0.5541, -0.6754,\n",
      "          0.8078, -0.9595, -0.4484, -0.3901,  0.3983, -0.1319, -0.2896,  0.7860,\n",
      "          0.3500, -0.5530, -0.7294, -0.2361,  0.4663,  0.9332, -0.3048, -0.1916,\n",
      "          0.2318, -0.3230, -0.9323, -0.4672, -0.6315, -1.0000,  0.8068, -1.0000,\n",
      "          0.8035,  0.4066, -0.3700,  0.8760,  0.7829,  0.8298, -0.8628, -0.9795,\n",
      "          0.1322,  0.8529, -0.5029, -0.9057, -0.6918,  0.5017, -0.2052,  0.1564,\n",
      "         -0.7397,  0.8156, -0.3414,  1.0000,  0.2659, -0.8292, -0.9821,  0.2491,\n",
      "         -0.3009,  1.0000, -0.8952, -0.9832,  0.3330, -0.9180, -0.8493,  0.5868,\n",
      "          0.1653, -0.8522, -0.9961,  0.9220,  0.8661, -0.6477,  0.7927, -0.3991,\n",
      "         -0.7691,  0.1512,  0.9868,  0.9924,  0.7317,  0.9083, -0.1227, -0.5258,\n",
      "          0.9840,  0.4009, -0.0436,  0.1361,  1.0000,  0.4004, -0.9497, -0.1309,\n",
      "         -0.9788, -0.3522, -0.9551,  0.3755,  0.3099,  0.9195, -0.4460,  0.9738,\n",
      "         -0.9714,  0.1901, -0.8894, -0.7863,  0.4757, -0.9463, -0.9892, -0.9938,\n",
      "          0.8142, -0.4077, -0.1895,  0.2102,  0.1715,  0.6322,  0.5566, -1.0000,\n",
      "          0.9642,  0.6150,  0.9768,  0.9768,  0.9115,  0.8108,  0.3251, -0.9920,\n",
      "         -0.9910, -0.5438, -0.3567,  0.7960,  0.7648,  0.8900,  0.6470, -0.4875,\n",
      "         -0.4792, -0.7756, -0.8423, -0.9972,  0.5961, -0.8679, -0.9678,  0.9718,\n",
      "         -0.3461, -0.1534, -0.2139, -0.9586,  0.9321,  0.7627,  0.4636,  0.0862,\n",
      "          0.5071,  0.9170,  0.9597,  0.9882, -0.9231,  0.8555, -0.9196,  0.6712,\n",
      "          0.9381, -0.9606,  0.2335,  0.8301, -0.5560,  0.3696, -0.4752, -0.9740,\n",
      "          0.8174, -0.4268,  0.7773, -0.4798,  0.0639, -0.4718, -0.2607, -0.7624,\n",
      "         -0.8742,  0.6576,  0.6207,  0.9219,  0.9360, -0.0496, -0.8942, -0.3701,\n",
      "         -0.8944, -0.9526,  0.9536, -0.0851, -0.2961,  0.9031,  0.1321,  0.9324,\n",
      "          0.4289, -0.4989, -0.4174, -0.7639,  0.8887, -0.7894, -0.7639, -0.7093,\n",
      "          0.8105,  0.3595,  1.0000, -0.9188, -0.9878, -0.8268, -0.6012,  0.4992,\n",
      "         -0.7880, -1.0000,  0.3609, -0.8314,  0.8524, -0.9398,  0.9500, -0.9339,\n",
      "         -0.9851, -0.3495,  0.8436,  0.9375, -0.5159, -0.8989,  0.5196, -0.8797,\n",
      "          0.9979,  0.8753, -0.8277, -0.0012,  0.6013, -0.9184, -0.7398,  0.9228]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f323d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ce5a1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    BertModel, \n",
    "    BertTokenizerFast, \n",
    "    GPT2LMHeadModel, \n",
    "    GPT2Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8ab5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36afb1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\aravi\\AppData\\Local\\Temp\\ipykernel_31296\\833307183.py:10: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  TRAIN_FILE = \"A:\\ml_research\\sample_train_data.csv\"\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    BERT_MODEL_NAME = \"bert-base-uncased\"\n",
    "    GPT_MODEL_NAME = \"gpt2\"\n",
    "    MAX_LEN = 512\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 2e-5\n",
    "    EPOCHS = 3\n",
    "    WARMUP_STEPS = 0\n",
    "    SAVE_PATH = \"./bert_gpt_hybrid\"\n",
    "    TRAIN_FILE = \"A:\\ml_research\\sample_train_data.csv\"\n",
    "    \n",
    "    # Create directory for saving models\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Load tokenizers\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(config.BERT_MODEL_NAME)\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained(config.GPT_MODEL_NAME)\n",
    "if gpt_tokenizer.pad_token is None:\n",
    "    gpt_tokenizer.pad_token = gpt_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b5dac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, targets, bert_tokenizer, gpt_tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.gpt_tokenizer = gpt_tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        target = str(self.targets[idx])\n",
    "        \n",
    "        # Tokenize for BERT\n",
    "        bert_inputs = self.bert_tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize for GPT2\n",
    "        gpt_inputs = self.gpt_tokenizer(\n",
    "            target,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"bert_input_ids\": bert_inputs[\"input_ids\"].squeeze(),\n",
    "            \"bert_attention_mask\": bert_inputs[\"attention_mask\"].squeeze(),\n",
    "            \"gpt_input_ids\": gpt_inputs[\"input_ids\"].squeeze(),\n",
    "            \"gpt_attention_mask\": gpt_inputs[\"attention_mask\"].squeeze(),\n",
    "            \"target_text\": target\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4a31f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertGPTHybrid(nn.Module):\n",
    "    def __init__(self, bert_model_name, gpt_model_name):\n",
    "        super(BertGPTHybrid, self).__init__()\n",
    "        # Load pre-trained models\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained(gpt_model_name)\n",
    "        \n",
    "        # Adapter layer between BERT and GPT2\n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        gpt_hidden_size = self.gpt.config.n_embd\n",
    "        \n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(bert_hidden_size, gpt_hidden_size),\n",
    "            nn.LayerNorm(gpt_hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, bert_input_ids, bert_attention_mask, gpt_input_ids=None, gpt_attention_mask=None):\n",
    "        # Get BERT embeddings\n",
    "        bert_outputs = self.bert(\n",
    "            input_ids=bert_input_ids,\n",
    "            attention_mask=bert_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Get the [CLS] token representation which summarizes the input\n",
    "        bert_cls_output = bert_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Adapt BERT output to GPT2 input dimensions\n",
    "        adapted_output = self.adapter(bert_cls_output)\n",
    "        \n",
    "        # Reshape to match what GPT2 expects for embeddings\n",
    "        # This becomes our \"prompt\" embedding for GPT2\n",
    "        batch_size = adapted_output.shape[0]\n",
    "        adapted_output = adapted_output.unsqueeze(1)  # Shape: [batch_size, 1, gpt_hidden_size]\n",
    "        \n",
    "        # Set up for GPT2 generation\n",
    "        if gpt_input_ids is not None:\n",
    "            # Training mode - we have targets\n",
    "            gpt_outputs = self.gpt(\n",
    "                input_ids=gpt_input_ids,\n",
    "                attention_mask=gpt_attention_mask,\n",
    "                labels=gpt_input_ids,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Extract the loss and logits\n",
    "            loss = gpt_outputs.loss\n",
    "            logits = gpt_outputs.logits\n",
    "            \n",
    "            return {\n",
    "                \"loss\": loss,\n",
    "                \"logits\": logits\n",
    "            }\n",
    "        else:\n",
    "            # Inference mode - generate text\n",
    "            # We use the adapted BERT output to condition the GPT2 generation\n",
    "            # This is a simplified approach and can be extended\n",
    "            return self.gpt.generate(\n",
    "                input_embeddings=adapted_output,\n",
    "                max_length=50,\n",
    "                num_return_sequences=1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65a6e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Assuming the CSV has 'input' and 'output' columns\n",
    "    train_texts, val_texts, train_targets, val_targets = train_test_split(\n",
    "        df['input'].values, df['output'].values, test_size=0.1, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TextDataset(\n",
    "        texts=train_texts,\n",
    "        targets=train_targets,\n",
    "        bert_tokenizer=bert_tokenizer,\n",
    "        gpt_tokenizer=gpt_tokenizer,\n",
    "        max_len=config.MAX_LEN\n",
    "    )\n",
    "    \n",
    "    val_dataset = TextDataset(\n",
    "        texts=val_texts,\n",
    "        targets=val_targets,\n",
    "        bert_tokenizer=bert_tokenizer,\n",
    "        gpt_tokenizer=gpt_tokenizer,\n",
    "        max_len=config.MAX_LEN\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "359b95bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config):\n",
    "    # Set up optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    \n",
    "    # Calculate total steps for scheduler\n",
    "    total_steps = len(train_loader) * config.EPOCHS\n",
    "    \n",
    "    # Create scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=config.WARMUP_STEPS,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        logger.info(f\"Starting epoch {epoch + 1}/{config.EPOCHS}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            # Move batch to device\n",
    "            bert_input_ids = batch[\"bert_input_ids\"].to(device)\n",
    "            bert_attention_mask = batch[\"bert_attention_mask\"].to(device)\n",
    "            gpt_input_ids = batch[\"gpt_input_ids\"].to(device)\n",
    "            gpt_attention_mask = batch[\"gpt_attention_mask\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                bert_input_ids=bert_input_ids,\n",
    "                bert_attention_mask=bert_attention_mask,\n",
    "                gpt_input_ids=gpt_input_ids,\n",
    "                gpt_attention_mask=gpt_attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = outputs[\"loss\"]\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        train_loss = np.mean(train_losses)\n",
    "        logger.info(f\"Epoch {epoch + 1} - Training loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}\"):\n",
    "                # Move batch to device\n",
    "                bert_input_ids = batch[\"bert_input_ids\"].to(device)\n",
    "                bert_attention_mask = batch[\"bert_attention_mask\"].to(device)\n",
    "                gpt_input_ids = batch[\"gpt_input_ids\"].to(device)\n",
    "                gpt_attention_mask = batch[\"gpt_attention_mask\"].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    bert_input_ids=bert_input_ids,\n",
    "                    bert_attention_mask=bert_attention_mask,\n",
    "                    gpt_input_ids=gpt_input_ids,\n",
    "                    gpt_attention_mask=gpt_attention_mask\n",
    "                )\n",
    "                \n",
    "                loss = outputs[\"loss\"]\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        val_loss = np.mean(val_losses)\n",
    "        logger.info(f\"Epoch {epoch + 1} - Validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save model checkpoint\n",
    "        checkpoint_path = os.path.join(config.SAVE_PATH, f\"checkpoint_epoch_{epoch + 1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss\n",
    "        }, checkpoint_path)\n",
    "        logger.info(f\"Model checkpoint saved to {checkpoint_path}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = os.path.join(config.SAVE_PATH, \"final_model.pt\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    logger.info(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "913b28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation function for testing\n",
    "def generate_text(model, input_text, bert_tokenizer, gpt_tokenizer, max_len=100):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input text for BERT\n",
    "    bert_inputs = bert_tokenizer(\n",
    "        input_text,\n",
    "        max_length=config.MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    bert_input_ids = bert_inputs[\"input_ids\"].to(device)\n",
    "    bert_attention_mask = bert_inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        bert_outputs = model.bert(\n",
    "            input_ids=bert_input_ids,\n",
    "            attention_mask=bert_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    # Get the [CLS] token representation\n",
    "    bert_cls_output = bert_outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    # Adapt BERT output to GPT2 input dimensions\n",
    "    adapted_output = model.adapter(bert_cls_output)\n",
    "    adapted_output = adapted_output.unsqueeze(1)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.gpt.generate(\n",
    "            inputs_embeds=adapted_output,\n",
    "            max_length=max_len,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    # Decode output tokens\n",
    "    output_text = gpt_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f7901c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading data...\n",
      "INFO:__main__:Initializing model...\n",
      "INFO:__main__:Starting training...\n",
      "INFO:__main__:Starting epoch 1/3\n",
      "Training Epoch 1:   0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining and evaluation completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[52], line 13\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Example of how to use the model for inference\u001b[39;00m\n\u001b[0;32m     16\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting model with sample input...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[45], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, config)\u001b[0m\n\u001b[0;32m     21\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Move batch to device\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     bert_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert_input_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     bert_attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m     gpt_input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    logger.info(\"Loading data...\")\n",
    "    train_loader, val_loader = load_and_prepare_data(config.TRAIN_FILE)\n",
    "    \n",
    "    logger.info(\"Initializing model...\")\n",
    "    model = BertGPTHybrid(\n",
    "        bert_model_name=config.BERT_MODEL_NAME,\n",
    "        gpt_model_name=config.GPT_MODEL_NAME\n",
    "    )\n",
    "    model = model.to('cpu')\n",
    "    \n",
    "    logger.info(\"Starting training...\")\n",
    "    model = train_model(model, train_loader, val_loader, config)\n",
    "    \n",
    "    # Example of how to use the model for inference\n",
    "    logger.info(\"Testing model with sample input...\")\n",
    "    sample_input = \"This is a test input for our BERT-GPT hybrid model.\"\n",
    "    generated_text = generate_text(model, sample_input, bert_tokenizer, gpt_tokenizer)\n",
    "    logger.info(f\"Input: {sample_input}\")\n",
    "    logger.info(f\"Generated: {generated_text}\")\n",
    "    \n",
    "    logger.info(\"Training and evaluation completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55fa7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e1a62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudagpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
