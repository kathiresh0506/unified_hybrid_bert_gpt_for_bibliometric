{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b30dfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load and select a small batch for fine-tuning\n",
    "df = pd.read_csv(\"clustered_papers.csv\").dropna(subset=[\"title\", \"clean_abstract\"])\n",
    "df = df.head(20)  # fine-tune on just 20 samples\n",
    "\n",
    "# Format input/output pairs\n",
    "def build_input(row):\n",
    "    return f\"Title: {row['title']}\\nAuthors: {row.get('authors', 'Unknown')}\\nKeywords: {row.get('keywords', '')}\\nAbstract: {row['clean_abstract']}\"\n",
    "\n",
    "def build_target(row):\n",
    "    return f\"This paper discusses {row.get('keywords', 'key topics')} and contributes to cluster {row.get('gmm_cluster', 'X')} research.\"\n",
    "\n",
    "dataset = [{\"input\": build_input(row), \"target\": build_target(row)} for _, row in df.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5cb76d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd30aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EncoderDecoderModel, BertTokenizer, GPT2Tokenizer\n",
    "\n",
    "# Load hybrid model\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-uncased\", \"mistral\")\n",
    "model.config.decoder_start_token_id = model.config.pad_token_id = 50256\n",
    "model.config.eos_token_id = 50256\n",
    "\n",
    "# Load tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7386ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Tokenization function\n",
    "def encode(example):\n",
    "    input_enc = bert_tokenizer(example[\"input\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    target_enc = gpt2_tokenizer(example[\"target\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    labels = target_enc.input_ids.clone()\n",
    "    labels[labels == gpt2_tokenizer.pad_token_id] = -100  # ignore pad tokens in loss\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_enc.input_ids[0],\n",
    "        \"attention_mask\": input_enc.attention_mask[0],\n",
    "        \"decoder_input_ids\": target_enc.input_ids[0],\n",
    "        \"labels\": labels[0]\n",
    "    }\n",
    "\n",
    "class BERTGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = [encode(d) for d in data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = BERTGPTDataset(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "195ccc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:636: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 1 â€” Avg Loss: 3.5536\n",
      "âœ… Epoch 2 â€” Avg Loss: 0.1720\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(2):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"âœ… Epoch {epoch+1} â€” Avg Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbee5796",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§  Generated Summary:\n",
      " This is a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_input = build_input(df.iloc[0])\n",
    "\n",
    "encoded = bert_tokenizer(test_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    input_ids=encoded.input_ids,\n",
    "    attention_mask=encoded.attention_mask,\n",
    "    max_length=150,\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "summary = gpt2_tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"\\nðŸ§  Generated Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d04a793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.39.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "   ---------------------------------------- 0.0/10.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/10.4 MB 8.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.4/10.4 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.4/10.4 MB 6.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.5/10.4 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.0/10.4 MB 6.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.6/10.4 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.9/10.4 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.4 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.4/10.4 MB 6.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.3/2.4 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "\n",
      "  Attempting uninstall: huggingface-hub\n",
      "\n",
      "    Found existing installation: huggingface-hub 0.26.1\n",
      "\n",
      "    Uninstalling huggingface-hub-0.26.1:\n",
      "\n",
      "      Successfully uninstalled huggingface-hub-0.26.1\n",
      "\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "  Attempting uninstall: tokenizers\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "   ---------------------------------------- 0/3 [huggingface-hub]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "  Attempting uninstall: transformers\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "    Found existing installation: transformers 4.39.3\n",
      "   ------------- -------------------------- 1/3 [tokenizers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "    Uninstalling transformers-4.39.3:\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "      Successfully uninstalled transformers-4.39.3\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   -------------------------- ------------- 2/3 [transformers]\n",
      "   ---------------------------------------- 3/3 [transformers]\n",
      "\n",
      "Successfully installed huggingface-hub-0.31.2 tokenizers-0.21.1 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~-kenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fdeaac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "mistral-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhf_raise_for_status\u001b[39m(response: Response, endpoint_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    Internal version of `response.raise_for_status()` that will refine a\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    This helper is meant to be the unique method to raise_for_status when making a call\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    to the Hugging Face Hub.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m        import requests\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m        from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m        response = get_session().post(...)\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m        try:\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m            hf_raise_for_status(response)\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        except HfHubHTTPError as e:\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m            print(str(e)) # formatted message\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m            e.request_id, e.server_message # details returned by server\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m            # Complete the error message with additional information once it's raised\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m            e.append_to_message(\"\\n`create_commit` expects the repository to exist.\")\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m            raise\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m        response (`Response`):\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m            Response from the server.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m        endpoint_name (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m            Name of the endpoint that has been called. If provided, the error message\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m            will be more complete.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    <Tip warning={true}>\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    Raises when the request has failed:\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m        - [`~utils.RepositoryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m            If the repository to download from cannot be found. This may be because it\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m            doesn't exist, because `repo_type` is not set correctly, or because the repo\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m            is `private` and you do not have access.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m        - [`~utils.GatedRepoError`]\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03m            If the repository exists but is gated and the user is not on the authorized\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m            list.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        - [`~utils.RevisionNotFoundError`]\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m            If the repository exists but the revision couldn't be find.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m        - [`~utils.EntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m            If the repository exists but the entry (e.g. the requested file) couldn't be\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m            find.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m        - [`~utils.BadRequestError`]\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m            If request failed with a HTTP 400 BadRequest error.\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m        - [`~utils.HfHubHTTPError`]\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m            If request failed for a reason not listed above.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \n\u001b[1;32m--> 406\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistral-7b/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m     cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(cache_dir)\n\u001b[1;32m--> 398\u001b[0m existing_files \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    399\u001b[0m file_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhf_hub_download\u001b[39m(\n\u001b[0;32m    811\u001b[0m     repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    831\u001b[0m     local_dir_use_symlinks: Union[\u001b[38;5;28mbool\u001b[39m, Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    832\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download a given file if it's not already present in the local cache.\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \n\u001b[0;32m    835\u001b[0m \u001b[38;5;124;03m    The new cache file layout looks like this:\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;124;03m    - The cache directory contains one subfolder per repo_id (namespaced by repo type)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;124;03m    - inside each repo folder:\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03m        - refs is a list of the latest known revision => commit_hash pairs\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m        - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124;03m          whether they're LFS files or not)\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;124;03m        - snapshots contains one subfolder per commit, each \"commit\" contains the subset of the files\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;124;03m          that have been resolved at that particular commit. Each filename is a symlink to the blob\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;124;03m          at that particular commit.\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03m    [  96]  .\u001b[39;00m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;124;03m    â””â”€â”€ [ 160]  models--julien-c--EsperBERTo-small\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;124;03m        â”œâ”€â”€ [ 160]  blobs\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;124;03m        â”‚   â”œâ”€â”€ [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;124;03m        â”‚   â”œâ”€â”€ [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;124;03m        â”‚   â””â”€â”€ [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;124;03m        â”œâ”€â”€ [  96]  refs\u001b[39;00m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;124;03m        â”‚   â””â”€â”€ [  40]  main\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;124;03m        â””â”€â”€ [ 128]  snapshots\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;124;03m            â”œâ”€â”€ [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;124;03m            â”‚   â”œâ”€â”€ [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;124;03m            â”‚   â””â”€â”€ [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;124;03m            â””â”€â”€ [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48\u001b[39;00m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m                â”œâ”€â”€ [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m                â””â”€â”€ [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m \n\u001b[0;32m    863\u001b[0m \u001b[38;5;124;03m    If `local_dir` is provided, the file structure from the repo will be replicated in this location. When using this\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;124;03m    option, the `cache_dir` will not be used and a `.cache/huggingface/` folder will be created at the root of `local_dir`\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03m    to store some metadata related to the downloaded files. While this mechanism is not as robust as the main\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;124;03m    cache-system, it's optimized for regularly pulling the latest version of a repository.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;124;03m        repo_id (`str`):\u001b[39;00m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m            A user or an organization name and a repo name separated by a `/`.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m        filename (`str`):\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m            The name of the file in the repo.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m        subfolder (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;124;03m            An optional value corresponding to a folder inside the model repo.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;124;03m        repo_type (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03m            Set to `\"dataset\"` or `\"space\"` if downloading from a dataset or space,\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m            `None` or `\"model\"` if downloading from a model. Default is `None`.\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m        revision (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m            An optional Git revision id which can be a branch name, a tag, or a\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m            commit hash.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;124;03m        library_name (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03m            The name of the library to which the object corresponds.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;124;03m        library_version (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03m            The version of the library.\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m        cache_dir (`str`, `Path`, *optional*):\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m            Path to the folder where cached files are stored.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m        local_dir (`str` or `Path`, *optional*):\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m            If provided, the downloaded file will be placed under this directory.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;124;03m        user_agent (`dict`, `str`, *optional*):\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m            The user-agent info in the form of a dictionary or a string.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;124;03m        force_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m            Whether the file should be downloaded even if it already exists in\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m            the local cache.\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m        proxies (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;124;03m            Dictionary mapping protocol to the URL of the proxy passed to\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;124;03m            `requests.request`.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;124;03m        etag_timeout (`float`, *optional*, defaults to `10`):\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;124;03m            When fetching ETag, how many seconds to wait for the server to send\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;124;03m            data before giving up which is passed to `requests.request`.\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03m        token (`str`, `bool`, *optional*):\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;124;03m            A token to be used for the download.\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m                - If `True`, the token is read from the HuggingFace config\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m                  folder.\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m                - If a string, it's used as the authentication token.\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;124;03m        local_files_only (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;124;03m            If `True`, avoid downloading the file and return the path to the\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;124;03m            local cached file if it exists.\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m        headers (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m            Additional headers to be sent with the request.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m        `str`: Local path of file or if networking is off, last version of file cached on disk.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m        [`~utils.RepositoryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m            If the repository to download from cannot be found. This may be because it doesn't exist,\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;124;03m            or because it is set to `private` and you do not have access.\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m        [`~utils.RevisionNotFoundError`]\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m            If the revision to download from cannot be found.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m        [`~utils.EntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;124;03m            If the file to download cannot be found.\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;124;03m        [`~utils.LocalEntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;124;03m            If network is disabled or unavailable and file is not found in cache.\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;124;03m        [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;124;03m            If `token=True` but the token cannot be found.\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;124;03m        [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m            If ETag cannot be determined.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m        [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m            If some parameter value is invalid.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_ETAG_TIMEOUT \u001b[38;5;241m!=\u001b[39m constants\u001b[38;5;241m.\u001b[39mDEFAULT_ETAG_TIMEOUT:\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;66;03m# Respect environment variable above user value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HfFileMetadata(\n\u001b[0;32m   1461\u001b[0m         commit_hash\u001b[38;5;241m=\u001b[39mr\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(constants\u001b[38;5;241m.\u001b[39mHUGGINGFACE_HEADER_X_REPO_COMMIT),\n\u001b[0;32m   1462\u001b[0m         \u001b[38;5;66;03m# We favor a custom header indicating the etag of the linked resource, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1472\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mparse_xet_file_data_from_response(r),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m     )\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_metadata_or_catch_error\u001b[39m(\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1478\u001b[0m     repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1479\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1480\u001b[0m     repo_type: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1481\u001b[0m     revision: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1482\u001b[0m     endpoint: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   1483\u001b[0m     proxies: Optional[Dict],\n\u001b[1;32m-> 1484\u001b[0m     etag_timeout: Optional[\u001b[38;5;28mfloat\u001b[39m],\n\u001b[0;32m   1485\u001b[0m     headers: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m],  \u001b[38;5;66;03m# mutated inplace!\u001b[39;00m\n\u001b[0;32m   1486\u001b[0m     token: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[0;32m   1487\u001b[0m     local_files_only: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   1488\u001b[0m     relative_filename: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# only used to store `.no_exists` in cache\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m     storage_folder: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# only used to store `.no_exists` in cache\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;66;03m# Either an exception is caught and returned\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m     Tuple[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m],\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;66;03m# Or the metadata is returned as\u001b[39;00m\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# `(url_to_download, etag, commit_hash, expected_size, xet_file_data, None)`\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m     Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, Optional[XetFileData], \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[0;32m   1496\u001b[0m ]:\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get metadata for a file on the Hub, safely handling network issues.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \n\u001b[0;32m   1499\u001b[0m \u001b[38;5;124;03m    Returns either the etag, commit_hash and expected size of the file, or the error\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;124;03m          domain of the location (typically an S3 bucket).\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(refs_dir):\n\u001b[1;32m-> 1376\u001b[0m     revision_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(refs_dir, revision)\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(revision_file):\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1287\u001b[0m     paths\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;241m.\u001b[39munlink(missing_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# delete outdated file first\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1289\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mpaths\u001b[38;5;241m.\u001b[39mincomplete_path(etag),\n\u001b[0;32m   1290\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mpaths\u001b[38;5;241m.\u001b[39mfile_path,\n\u001b[0;32m   1291\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1292\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1293\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1294\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1295\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m-> 1296\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1297\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[0;32m   1298\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[0;32m   1301\u001b[0m write_download_metadata(local_dir\u001b[38;5;241m=\u001b[39mlocal_dir, filename\u001b[38;5;241m=\u001b[39mfilename, commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash, etag\u001b[38;5;241m=\u001b[39metag)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_wrapper\u001b[39m(\n\u001b[0;32m    265\u001b[0m     method: HTTP_METHOD_T, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, follow_relative_redirects: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    267\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around requests methods to follow relative redirects if `follow_relative_redirects=True` even when\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m    `allow_redirection=False`.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m    A backoff mechanism retries the HTTP call on 429, 503 and 504 errors.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m        method (`str`):\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m            HTTP method, such as 'GET' or 'HEAD'.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        url (`str`):\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m            The URL of the resource to fetch.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;124;03m        follow_relative_redirects (`bool`, *optional*, defaults to `False`)\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03m            If True, relative redirection (redirection to the same site) will be resolved even when `allow_redirection`\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m            kwarg is set to False. Useful when we want to follow a redirection to a renamed repository without\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m            following redirection to a CDN.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m        **params (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m            Params to pass to `requests.request`.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    296\u001b[0m parsed_target = urlparse(response.headers[\"Location\"])\n\u001b[0;32m    297\u001b[0m if parsed_target.netloc == \"\":\n\u001b[0;32m    298\u001b[0m     # This means it is a relative 'location' headers, as allowed by RFC 7231.\n\u001b[0;32m    299\u001b[0m     # (e.g. '/path/to/resource' instead of 'http://domain.tld/path/to/resource')\n\u001b[0;32m    300\u001b[0m     # We want to follow this relative redirect !\n\u001b[1;32m--> 301\u001b[0m     #\n\u001b[0;32m    302\u001b[0m     # Highly inspired by `resolve_redirects` from requests library.\n\u001b[0;32m    303\u001b[0m     # See https://github.com/psf/requests/blob/main/requests/sessions.py#L159\n\u001b[0;32m    304\u001b[0m     next_url = urlparse(url)._replace(path=parsed_target.path).geturl()\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:454\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepoNotFound\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    439\u001b[0m     response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m error_message \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid credentials in Authorization header\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n\u001b[0;32m    450\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepository Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 454\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease make sure you specified the correct `repo_id` and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `repo_type`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to access a private or gated repo,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated. For more details, see\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m     )\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-6824c8c0-595703496a05777051014bf1;20c7c551-95de-454e-af4c-65576dcbc3a6)\n\nRepository Not Found for url: https://huggingface.co/mistral-7b/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m clusters_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclustered_papers.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load Mistral model and tokenizer using Auto classes\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m mistral_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmistral-7b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m mistral_model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmistral-7b\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Function to generate summary with Mistral\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:779\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    775\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    777\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    778\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m--> 779\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    780\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[0;32m    781\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    782\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    783\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    784\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    785\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    786\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    787\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    788\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    789\u001b[0m     _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    790\u001b[0m     _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    791\u001b[0m     _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    792\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    793\u001b[0m )\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:612\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     TOKENIZER_MAPPING_NAMES: OrderedDict[\u001b[38;5;28mstr\u001b[39m, Tuple[Optional[\u001b[38;5;28mstr\u001b[39m], Optional[\u001b[38;5;28mstr\u001b[39m]]] \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     TOKENIZER_MAPPING_NAMES \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[0;32m     62\u001b[0m         [\n\u001b[0;32m     63\u001b[0m             (\n\u001b[0;32m     64\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     65\u001b[0m                 (\n\u001b[0;32m     66\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     67\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     68\u001b[0m                 ),\n\u001b[0;32m     69\u001b[0m             ),\n\u001b[0;32m     70\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malign\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     71\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maria\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     72\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maya_vision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCohereTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     73\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbark\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     74\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbart\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBartTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBartTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     75\u001b[0m             (\n\u001b[0;32m     76\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbarthez\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     77\u001b[0m                 (\n\u001b[0;32m     78\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarthezTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     79\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarthezTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     80\u001b[0m                 ),\n\u001b[0;32m     81\u001b[0m             ),\n\u001b[0;32m     82\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbartpho\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBartphoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     83\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     84\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertGenerationTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     85\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-japanese\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertJapaneseTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     86\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertweet\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertweetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     87\u001b[0m             (\n\u001b[0;32m     88\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbig_bird\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m                 (\n\u001b[0;32m     90\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigBirdTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     91\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigBirdTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m                 ),\n\u001b[0;32m     93\u001b[0m             ),\n\u001b[0;32m     94\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigbird_pegasus\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     95\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiogpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBioGptTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     96\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblenderbot\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlenderbotTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlenderbotTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m     97\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblenderbot-small\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlenderbotSmallTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     98\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m     99\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblip-2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    100\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbloom\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBloomTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    101\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbridgetower\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    102\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbros\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    103\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbyt5\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mByT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    104\u001b[0m             (\n\u001b[0;32m    105\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcamembert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    106\u001b[0m                 (\n\u001b[0;32m    107\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCamembertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    108\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCamembertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m                 ),\n\u001b[0;32m    110\u001b[0m             ),\n\u001b[0;32m    111\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcanine\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCanineTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    112\u001b[0m             (\n\u001b[0;32m    113\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchameleon\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    114\u001b[0m                 (\n\u001b[0;32m    115\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    116\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    117\u001b[0m                 ),\n\u001b[0;32m    118\u001b[0m             ),\n\u001b[0;32m    119\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchinese_clip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    120\u001b[0m             (\n\u001b[0;32m    121\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    122\u001b[0m                 (\n\u001b[0;32m    123\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    124\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m                 ),\n\u001b[0;32m    126\u001b[0m             ),\n\u001b[0;32m    127\u001b[0m             (\n\u001b[0;32m    128\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m                 (\n\u001b[0;32m    130\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    132\u001b[0m                 ),\n\u001b[0;32m    133\u001b[0m             ),\n\u001b[0;32m    134\u001b[0m             (\n\u001b[0;32m    135\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclipseg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    136\u001b[0m                 (\n\u001b[0;32m    137\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    138\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    139\u001b[0m                 ),\n\u001b[0;32m    140\u001b[0m             ),\n\u001b[0;32m    141\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclvp\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClvpTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    142\u001b[0m             (\n\u001b[0;32m    143\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_llama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    144\u001b[0m                 (\n\u001b[0;32m    145\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    146\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m                 ),\n\u001b[0;32m    148\u001b[0m             ),\n\u001b[0;32m    149\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodegen\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeGenTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeGenTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    150\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcohere\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCohereTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    151\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcohere2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCohereTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    152\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolpali\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    153\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    154\u001b[0m             (\n\u001b[0;32m    155\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    156\u001b[0m                 (\n\u001b[0;32m    157\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCpmTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCpmTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    159\u001b[0m                 ),\n\u001b[0;32m    160\u001b[0m             ),\n\u001b[0;32m    161\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpmant\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCpmAntTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    162\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mctrl\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCTRLTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    163\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata2vec-audio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2CTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    164\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata2vec-text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    165\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdbrx\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    166\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeberta\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDebertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDebertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    167\u001b[0m             (\n\u001b[0;32m    168\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeberta-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    169\u001b[0m                 (\n\u001b[0;32m    170\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDebertaV2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    171\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDebertaV2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    172\u001b[0m                 ),\n\u001b[0;32m    173\u001b[0m             ),\n\u001b[0;32m    174\u001b[0m             (\n\u001b[0;32m    175\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek_v3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    176\u001b[0m                 (\n\u001b[0;32m    177\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    178\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    179\u001b[0m                 ),\n\u001b[0;32m    180\u001b[0m             ),\n\u001b[0;32m    181\u001b[0m             (\n\u001b[0;32m    182\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiffllama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    183\u001b[0m                 (\n\u001b[0;32m    184\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    185\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    186\u001b[0m                 ),\n\u001b[0;32m    187\u001b[0m             ),\n\u001b[0;32m    188\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistilBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistilBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    189\u001b[0m             (\n\u001b[0;32m    190\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    191\u001b[0m                 (\n\u001b[0;32m    192\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDPRQuestionEncoderTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    193\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDPRQuestionEncoderTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    194\u001b[0m                 ),\n\u001b[0;32m    195\u001b[0m             ),\n\u001b[0;32m    196\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melectra\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElectraTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElectraTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    197\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memu3\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    198\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mernie\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    199\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mernie_m\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErnieMTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    200\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mesm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEsmTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    201\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalcon\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    202\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalcon_mamba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    203\u001b[0m             (\n\u001b[0;32m    204\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfastspeech2_conformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    205\u001b[0m                 (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFastSpeech2ConformerTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_g2p_en_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    206\u001b[0m             ),\n\u001b[0;32m    207\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflaubert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlaubertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    208\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFNetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFNetTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    209\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfsmt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFSMTTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    210\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunnel\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunnelTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunnelTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    211\u001b[0m             (\n\u001b[0;32m    212\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    213\u001b[0m                 (\n\u001b[0;32m    214\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    216\u001b[0m                 ),\n\u001b[0;32m    217\u001b[0m             ),\n\u001b[0;32m    218\u001b[0m             (\n\u001b[0;32m    219\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m                 (\n\u001b[0;32m    221\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    222\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    223\u001b[0m                 ),\n\u001b[0;32m    224\u001b[0m             ),\n\u001b[0;32m    225\u001b[0m             (\n\u001b[0;32m    226\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    227\u001b[0m                 (\n\u001b[0;32m    228\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    229\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    230\u001b[0m                 ),\n\u001b[0;32m    231\u001b[0m             ),\n\u001b[0;32m    232\u001b[0m             (\n\u001b[0;32m    233\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma3_text\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    234\u001b[0m                 (\n\u001b[0;32m    235\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    236\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    237\u001b[0m                 ),\n\u001b[0;32m    238\u001b[0m             ),\n\u001b[0;32m    239\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    240\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    241\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglm4\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    242\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-sw3\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTSw3Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    243\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    244\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_bigcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    245\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_neo\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    246\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_neox\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    247\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_neox_japanese\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXJapaneseTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    248\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptj\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    249\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgptsan-japanese\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTSanJapaneseTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    250\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrounding-dino\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    251\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupvit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    252\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelium\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    253\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mherbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHerbertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHerbertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    254\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhubert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2CTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    255\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    256\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midefics\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    257\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midefics2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    258\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midefics3\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    259\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructblip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    260\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructblipvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    261\u001b[0m             (\n\u001b[0;32m    262\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjamba\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    263\u001b[0m                 (\n\u001b[0;32m    264\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    265\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m                 ),\n\u001b[0;32m    267\u001b[0m             ),\n\u001b[0;32m    268\u001b[0m             (\n\u001b[0;32m    269\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjetmoe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    270\u001b[0m                 (\n\u001b[0;32m    271\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    272\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m                 ),\n\u001b[0;32m    274\u001b[0m             ),\n\u001b[0;32m    275\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjukebox\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJukeboxTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    276\u001b[0m             (\n\u001b[0;32m    277\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkosmos-2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    278\u001b[0m                 (\n\u001b[0;32m    279\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    280\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    281\u001b[0m                 ),\n\u001b[0;32m    282\u001b[0m             ),\n\u001b[0;32m    283\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayoutlm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    284\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayoutlmv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    285\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayoutlmv3\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv3Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv3TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    286\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayoutxlm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutXLMTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutXLMTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    287\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mled\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEDTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEDTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    288\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlilt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv3Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayoutLMv3TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    289\u001b[0m             (\n\u001b[0;32m    290\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    291\u001b[0m                 (\n\u001b[0;32m    292\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    293\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    294\u001b[0m                 ),\n\u001b[0;32m    295\u001b[0m             ),\n\u001b[0;32m    296\u001b[0m             (\n\u001b[0;32m    297\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    298\u001b[0m                 (\n\u001b[0;32m    299\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    300\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    301\u001b[0m                 ),\n\u001b[0;32m    302\u001b[0m             ),\n\u001b[0;32m    303\u001b[0m             (\n\u001b[0;32m    304\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama4_text\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    305\u001b[0m                 (\n\u001b[0;32m    306\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    307\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    308\u001b[0m                 ),\n\u001b[0;32m    309\u001b[0m             ),\n\u001b[0;32m    310\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    311\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava_next\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    312\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava_next_video\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    313\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllava_onevision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    314\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongformerTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLongformerTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    315\u001b[0m             (\n\u001b[0;32m    316\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongt5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    317\u001b[0m                 (\n\u001b[0;32m    318\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    319\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m                 ),\n\u001b[0;32m    321\u001b[0m             ),\n\u001b[0;32m    322\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mluke\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLukeTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    323\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxmert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLxmertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLxmertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    324\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm2m_100\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM2M100Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    325\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmamba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    326\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmamba2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    327\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarian\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMarianTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    328\u001b[0m             (\n\u001b[0;32m    329\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmbart\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    330\u001b[0m                 (\n\u001b[0;32m    331\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMBartTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    332\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMBartTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    333\u001b[0m                 ),\n\u001b[0;32m    334\u001b[0m             ),\n\u001b[0;32m    335\u001b[0m             (\n\u001b[0;32m    336\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmbart50\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    337\u001b[0m                 (\n\u001b[0;32m    338\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMBart50Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    339\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMBart50TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    340\u001b[0m                 ),\n\u001b[0;32m    341\u001b[0m             ),\n\u001b[0;32m    342\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmega\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    343\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmegatron-bert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    344\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmgp-str\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMgpstrTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    345\u001b[0m             (\n\u001b[0;32m    346\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    347\u001b[0m                 (\n\u001b[0;32m    348\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    350\u001b[0m                 ),\n\u001b[0;32m    351\u001b[0m             ),\n\u001b[0;32m    352\u001b[0m             (\n\u001b[0;32m    353\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixtral\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    354\u001b[0m                 (\n\u001b[0;32m    355\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    356\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    357\u001b[0m                 ),\n\u001b[0;32m    358\u001b[0m             ),\n\u001b[0;32m    359\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmllama\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    360\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmluke\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLukeTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    361\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmobilebert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMobileBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMobileBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    362\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodernbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    363\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoonshine\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    364\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoshi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    365\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmpnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMPNetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMPNetTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    366\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    367\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmra\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    368\u001b[0m             (\n\u001b[0;32m    369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmt5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    370\u001b[0m                 (\n\u001b[0;32m    371\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    372\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    373\u001b[0m                 ),\n\u001b[0;32m    374\u001b[0m             ),\n\u001b[0;32m    375\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusicgen\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    376\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusicgen_melody\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    377\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmvp\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMvpTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMvpTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    378\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmyt5\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMyT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    379\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnemotron\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    380\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnezha\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    381\u001b[0m             (\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnllb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    383\u001b[0m                 (\n\u001b[0;32m    384\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNllbTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    385\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNllbTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    386\u001b[0m                 ),\n\u001b[0;32m    387\u001b[0m             ),\n\u001b[0;32m    388\u001b[0m             (\n\u001b[0;32m    389\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnllb-moe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    390\u001b[0m                 (\n\u001b[0;32m    391\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNllbTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    392\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNllbTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    393\u001b[0m                 ),\n\u001b[0;32m    394\u001b[0m             ),\n\u001b[0;32m    395\u001b[0m             (\n\u001b[0;32m    396\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnystromformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m                 (\n\u001b[0;32m    398\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    399\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    400\u001b[0m                 ),\n\u001b[0;32m    401\u001b[0m             ),\n\u001b[0;32m    402\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molmo\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    403\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molmo2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    404\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124molmoe\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    405\u001b[0m             (\n\u001b[0;32m    406\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124momdet-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    407\u001b[0m                 (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    408\u001b[0m             ),\n\u001b[0;32m    409\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    410\u001b[0m             (\n\u001b[0;32m    411\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai-gpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    412\u001b[0m                 (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAIGPTTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAIGPTTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    413\u001b[0m             ),\n\u001b[0;32m    414\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    415\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowlv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    416\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowlvit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    417\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaligemma\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    418\u001b[0m             (\n\u001b[0;32m    419\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    420\u001b[0m                 (\n\u001b[0;32m    421\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    423\u001b[0m                 ),\n\u001b[0;32m    424\u001b[0m             ),\n\u001b[0;32m    425\u001b[0m             (\n\u001b[0;32m    426\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpegasus_x\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    427\u001b[0m                 (\n\u001b[0;32m    428\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    429\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPegasusTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    430\u001b[0m                 ),\n\u001b[0;32m    431\u001b[0m             ),\n\u001b[0;32m    432\u001b[0m             (\n\u001b[0;32m    433\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperceiver\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    434\u001b[0m                 (\n\u001b[0;32m    435\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerceiverTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    436\u001b[0m                     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    437\u001b[0m                 ),\n\u001b[0;32m    438\u001b[0m             ),\n\u001b[0;32m    439\u001b[0m             (\n\u001b[0;32m    440\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpersimmon\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    441\u001b[0m                 (\n\u001b[0;32m    442\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    443\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    444\u001b[0m                 ),\n\u001b[0;32m    445\u001b[0m             ),\n\u001b[0;32m    446\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeGenTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeGenTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    447\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphi3\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    448\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphimoe\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    449\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphobert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhobertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    450\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpix2struct\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    451\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixtral\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreTrainedTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    452\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplbart\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPLBartTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    453\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprophetnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProphetNetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    454\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqdqbert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    455\u001b[0m             (\n\u001b[0;32m    456\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    457\u001b[0m                 (\n\u001b[0;32m    458\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    459\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    460\u001b[0m                 ),\n\u001b[0;32m    461\u001b[0m             ),\n\u001b[0;32m    462\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2_5_vl\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    463\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2_audio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    464\u001b[0m             (\n\u001b[0;32m    465\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2_moe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    466\u001b[0m                 (\n\u001b[0;32m    467\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    468\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    469\u001b[0m                 ),\n\u001b[0;32m    470\u001b[0m             ),\n\u001b[0;32m    471\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen2_vl\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    472\u001b[0m             (\n\u001b[0;32m    473\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    474\u001b[0m                 (\n\u001b[0;32m    475\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    476\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    477\u001b[0m                 ),\n\u001b[0;32m    478\u001b[0m             ),\n\u001b[0;32m    479\u001b[0m             (\n\u001b[0;32m    480\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqwen3_moe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    481\u001b[0m                 (\n\u001b[0;32m    482\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    483\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    484\u001b[0m                 ),\n\u001b[0;32m    485\u001b[0m             ),\n\u001b[0;32m    486\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrag\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRagTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    487\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrealm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRealmTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRealmTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    488\u001b[0m             (\n\u001b[0;32m    489\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecurrent_gemma\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    490\u001b[0m                 (\n\u001b[0;32m    491\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    492\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    493\u001b[0m                 ),\n\u001b[0;32m    494\u001b[0m             ),\n\u001b[0;32m    495\u001b[0m             (\n\u001b[0;32m    496\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    497\u001b[0m                 (\n\u001b[0;32m    498\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReformerTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    499\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReformerTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    500\u001b[0m                 ),\n\u001b[0;32m    501\u001b[0m             ),\n\u001b[0;32m    502\u001b[0m             (\n\u001b[0;32m    503\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrembert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    504\u001b[0m                 (\n\u001b[0;32m    505\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    506\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    507\u001b[0m                 ),\n\u001b[0;32m    508\u001b[0m             ),\n\u001b[0;32m    509\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretribert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetriBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetriBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    510\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    511\u001b[0m             (\n\u001b[0;32m    512\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-prelayernorm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    513\u001b[0m                 (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    514\u001b[0m             ),\n\u001b[0;32m    515\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroc_bert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoCBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    516\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoFormerTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRoFormerTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    517\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrwkv\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    518\u001b[0m             (\n\u001b[0;32m    519\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseamless_m4t\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    520\u001b[0m                 (\n\u001b[0;32m    521\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeamlessM4TTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    522\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeamlessM4TTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    523\u001b[0m                 ),\n\u001b[0;32m    524\u001b[0m             ),\n\u001b[0;32m    525\u001b[0m             (\n\u001b[0;32m    526\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseamless_m4t_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    527\u001b[0m                 (\n\u001b[0;32m    528\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeamlessM4TTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    529\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeamlessM4TTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    530\u001b[0m                 ),\n\u001b[0;32m    531\u001b[0m             ),\n\u001b[0;32m    532\u001b[0m             (\n\u001b[0;32m    533\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshieldgemma2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    534\u001b[0m                 (\n\u001b[0;32m    535\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    536\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    537\u001b[0m                 ),\n\u001b[0;32m    538\u001b[0m             ),\n\u001b[0;32m    539\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiglip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSiglipTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    540\u001b[0m             (\n\u001b[0;32m    541\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msiglip2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    542\u001b[0m                 (\n\u001b[0;32m    543\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    544\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemmaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    545\u001b[0m                 ),\n\u001b[0;32m    546\u001b[0m             ),\n\u001b[0;32m    547\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_to_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeech2TextTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    548\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech_to_text_2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeech2Text2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    549\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeecht5\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeechT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    550\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplinter\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplinterTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplinterTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[0;32m    551\u001b[0m             (\n\u001b[0;32m    552\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqueezebert\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    553\u001b[0m                 (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSqueezeBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSqueezeBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    554\u001b[0m             ),\n\u001b[0;32m    555\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstablelm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPTNeoXTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    556\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarcoder2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPT2TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    557\u001b[0m             (\n\u001b[0;32m    558\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswitch_transformers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    559\u001b[0m                 (\n\u001b[0;32m    560\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    561\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    562\u001b[0m                 ),\n\u001b[0;32m    563\u001b[0m             ),\n\u001b[0;32m    564\u001b[0m             (\n\u001b[0;32m    565\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    566\u001b[0m                 (\n\u001b[0;32m    567\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    568\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m                 ),\n\u001b[0;32m    570\u001b[0m             ),\n\u001b[0;32m    571\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtapas\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTapasTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    572\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtapex\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTapexTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    573\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfo-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransfoXLTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    574\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtvp\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    575\u001b[0m             (\n\u001b[0;32m    576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mudop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    577\u001b[0m                 (\n\u001b[0;32m    578\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUdopTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUdopTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    580\u001b[0m                 ),\n\u001b[0;32m    581\u001b[0m             ),\n\u001b[0;32m    582\u001b[0m             (\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mumt5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    584\u001b[0m                 (\n\u001b[0;32m    585\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5Tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    586\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT5TokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    587\u001b[0m                 ),\n\u001b[0;32m    588\u001b[0m             ),\n\u001b[0;32m    589\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_llava\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    590\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvilt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    591\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvipllava\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    592\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisual_bert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    593\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvits\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVitsTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    594\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav2vec2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2CTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    595\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav2vec2-bert\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2CTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    596\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav2vec2-conformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2CTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    597\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwav2vec2_phoneme\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWav2Vec2PhonemeCTCTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    598\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhisper\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhisperTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhisperTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    599\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIPTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    600\u001b[0m             (\n\u001b[0;32m    601\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxglm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    602\u001b[0m                 (\n\u001b[0;32m    603\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGLMTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    604\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGLMTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    605\u001b[0m                 ),\n\u001b[0;32m    606\u001b[0m             ),\n\u001b[0;32m    607\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    608\u001b[0m             (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-prophetnet\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMProphetNetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)),\n\u001b[0;32m    609\u001b[0m             (\n\u001b[0;32m    610\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    611\u001b[0m                 (\n\u001b[1;32m--> 612\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    613\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    614\u001b[0m                 ),\n\u001b[0;32m    615\u001b[0m             ),\n\u001b[0;32m    616\u001b[0m             (\n\u001b[0;32m    617\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlm-roberta-xl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    618\u001b[0m                 (\n\u001b[0;32m    619\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    620\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    621\u001b[0m                 ),\n\u001b[0;32m    622\u001b[0m             ),\n\u001b[0;32m    623\u001b[0m             (\n\u001b[0;32m    624\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxlnet\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    625\u001b[0m                 (\n\u001b[0;32m    626\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLNetTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    627\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLNetTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    628\u001b[0m                 ),\n\u001b[0;32m    629\u001b[0m             ),\n\u001b[0;32m    630\u001b[0m             (\n\u001b[0;32m    631\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxmod\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    632\u001b[0m                 (\n\u001b[0;32m    633\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    634\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXLMRobertaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    635\u001b[0m                 ),\n\u001b[0;32m    636\u001b[0m             ),\n\u001b[0;32m    637\u001b[0m             (\n\u001b[0;32m    638\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myoso\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    639\u001b[0m                 (\n\u001b[0;32m    640\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    641\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlbertTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    642\u001b[0m                 ),\n\u001b[0;32m    643\u001b[0m             ),\n\u001b[0;32m    644\u001b[0m             (\n\u001b[0;32m    645\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzamba\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    646\u001b[0m                 (\n\u001b[0;32m    647\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    648\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    649\u001b[0m                 ),\n\u001b[0;32m    650\u001b[0m             ),\n\u001b[0;32m    651\u001b[0m             (\n\u001b[0;32m    652\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzamba2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    653\u001b[0m                 (\n\u001b[0;32m    654\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sentencepiece_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    655\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaTokenizerFast\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tokenizers_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    656\u001b[0m                 ),\n\u001b[0;32m    657\u001b[0m             ),\n\u001b[0;32m    658\u001b[0m         ]\n\u001b[0;32m    659\u001b[0m     )\n\u001b[0;32m    661\u001b[0m TOKENIZER_MAPPING \u001b[38;5;241m=\u001b[39m _LazyAutoMapping(CONFIG_MAPPING_NAMES, TOKENIZER_MAPPING_NAMES)\n\u001b[0;32m    663\u001b[0m CONFIG_TO_TYPE \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING_NAMES\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# download the files if needed\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m    424\u001b[0m         hf_hub_download(\n\u001b[0;32m    425\u001b[0m             path_or_repo_id,\n\u001b[0;32m    426\u001b[0m             filenames[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    437\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: mistral-7b is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "\n",
    "# Load the clusters CSV file (make sure to provide the correct file path)\n",
    "clusters_df = pd.read_csv('clustered_papers.csv')\n",
    "\n",
    "# Load Mistral model and tokenizer using Auto classes\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained('mistral-7b')\n",
    "mistral_model = AutoModelForSeq2SeqLM.from_pretrained('mistral-7b')\n",
    "\n",
    "# Function to generate summary with Mistral\n",
    "def generate_summary_with_mistral(text):\n",
    "    mistral_inputs = mistral_tokenizer(text, return_tensors=\"pt\")\n",
    "    generated_output = mistral_model.generate(mistral_inputs['input_ids'], max_length=300, num_return_sequences=1)\n",
    "    summary = mistral_tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Loop through each cluster and generate summaries with bibliometric insights\n",
    "for cluster_id in clusters_df['cluster_id'].unique():\n",
    "    # Filter articles for the current cluster\n",
    "    cluster_articles = clusters_df[clusters_df['cluster_id'] == cluster_id]\n",
    "    \n",
    "    # Combine all abstracts from the current cluster for summarization\n",
    "    combined_abstracts = \" \".join(cluster_articles['abstract'].tolist())\n",
    "    \n",
    "    # Generate a summary for the combined abstracts\n",
    "    generated_summary = generate_summary_with_mistral(combined_abstracts)\n",
    "    \n",
    "    # Combine bibliometric metrics\n",
    "    avg_citation_count = cluster_articles['citation_count'].mean()\n",
    "    avg_h_index = cluster_articles['h_index'].mean()\n",
    "    \n",
    "    # Format output to make it more human-friendly\n",
    "    print(f\"Research Trends for Cluster {cluster_id}:\")\n",
    "    print(f\"------------------------------------------------\")\n",
    "    print(f\"**Summary of Research**: {generated_summary}\")\n",
    "    print(f\"\\n**Bibliometric Insights**:\")\n",
    "    print(f\"- Average Citation Count: {avg_citation_count:.2f}\")\n",
    "    print(f\"- Average H-index: {avg_h_index:.2f}\")\n",
    "    print(f\"- Total Number of Papers in Cluster: {len(cluster_articles)}\")\n",
    "    print(f\"- Most Cited Paper: {cluster_articles.loc[cluster_articles['citation_count'].idxmax()]['title']}\")\n",
    "    print(f\"- Top Author: {cluster_articles.groupby('author')['citation_count'].sum().idxmax()}\")\n",
    "    print(f\"------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01444c41",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1.\n401 Client Error. (Request ID: Root=1-6824cb2c-71c6583710d6c0a9237dd1a4;0157bd78-bb45-49bb-b097-0c78997e0478)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhf_raise_for_status\u001b[39m(response: Response, endpoint_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    Internal version of `response.raise_for_status()` that will refine a\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    potential HTTPError. Raised exception will be an instance of `HfHubHTTPError`.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    This helper is meant to be the unique method to raise_for_status when making a call\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    to the Hugging Face Hub.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;124;03m    ```py\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;124;03m        import requests\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m        from huggingface_hub.utils import get_session, hf_raise_for_status, HfHubHTTPError\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m        response = get_session().post(...)\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m        try:\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m            hf_raise_for_status(response)\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        except HfHubHTTPError as e:\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m            print(str(e)) # formatted message\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m            e.request_id, e.server_message # details returned by server\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m            # Complete the error message with additional information once it's raised\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m            e.append_to_message(\"\\n`create_commit` expects the repository to exist.\")\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m            raise\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \n\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m        response (`Response`):\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m            Response from the server.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m        endpoint_name (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m            Name of the endpoint that has been called. If provided, the error message\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m            will be more complete.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    <Tip warning={true}>\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    Raises when the request has failed:\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m        - [`~utils.RepositoryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;124;03m            If the repository to download from cannot be found. This may be because it\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m            doesn't exist, because `repo_type` is not set correctly, or because the repo\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m            is `private` and you do not have access.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m        - [`~utils.GatedRepoError`]\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03m            If the repository exists but is gated and the user is not on the authorized\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;124;03m            list.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        - [`~utils.RevisionNotFoundError`]\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m            If the repository exists but the revision couldn't be find.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m        - [`~utils.EntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m            If the repository exists but the entry (e.g. the requested file) couldn't be\u001b[39;00m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;124;03m            find.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;124;03m        - [`~utils.BadRequestError`]\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;124;03m            If request failed with a HTTP 400 BadRequest error.\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m        - [`~utils.HfHubHTTPError`]\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m            If request failed for a reason not listed above.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \n\u001b[1;32m--> 406\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m     cache_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(cache_dir)\n\u001b[1;32m--> 398\u001b[0m existing_files \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    399\u001b[0m file_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;129m@validate_hf_hub_args\u001b[39m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhf_hub_download\u001b[39m(\n\u001b[0;32m    811\u001b[0m     repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    831\u001b[0m     local_dir_use_symlinks: Union[\u001b[38;5;28mbool\u001b[39m, Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    832\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download a given file if it's not already present in the local cache.\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \n\u001b[0;32m    835\u001b[0m \u001b[38;5;124;03m    The new cache file layout looks like this:\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;124;03m    - The cache directory contains one subfolder per repo_id (namespaced by repo type)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;124;03m    - inside each repo folder:\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03m        - refs is a list of the latest known revision => commit_hash pairs\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m        - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124;03m          whether they're LFS files or not)\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;124;03m        - snapshots contains one subfolder per commit, each \"commit\" contains the subset of the files\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;124;03m          that have been resolved at that particular commit. Each filename is a symlink to the blob\u001b[39;00m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;124;03m          at that particular commit.\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03m    [  96]  .\u001b[39;00m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;124;03m    â””â”€â”€ [ 160]  models--julien-c--EsperBERTo-small\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;124;03m        â”œâ”€â”€ [ 160]  blobs\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;124;03m        â”‚   â”œâ”€â”€ [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;124;03m        â”‚   â”œâ”€â”€ [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e\u001b[39;00m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;124;03m        â”‚   â””â”€â”€ [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812\u001b[39;00m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;124;03m        â”œâ”€â”€ [  96]  refs\u001b[39;00m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;124;03m        â”‚   â””â”€â”€ [  40]  main\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;124;03m        â””â”€â”€ [ 128]  snapshots\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;124;03m            â”œâ”€â”€ [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;124;03m            â”‚   â”œâ”€â”€ [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;124;03m            â”‚   â””â”€â”€ [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;124;03m            â””â”€â”€ [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48\u001b[39;00m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;124;03m                â”œâ”€â”€ [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;124;03m                â””â”€â”€ [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m \n\u001b[0;32m    863\u001b[0m \u001b[38;5;124;03m    If `local_dir` is provided, the file structure from the repo will be replicated in this location. When using this\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;124;03m    option, the `cache_dir` will not be used and a `.cache/huggingface/` folder will be created at the root of `local_dir`\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03m    to store some metadata related to the downloaded files. While this mechanism is not as robust as the main\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;124;03m    cache-system, it's optimized for regularly pulling the latest version of a repository.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;124;03m        repo_id (`str`):\u001b[39;00m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m            A user or an organization name and a repo name separated by a `/`.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m        filename (`str`):\u001b[39;00m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;124;03m            The name of the file in the repo.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;124;03m        subfolder (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;124;03m            An optional value corresponding to a folder inside the model repo.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;124;03m        repo_type (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03m            Set to `\"dataset\"` or `\"space\"` if downloading from a dataset or space,\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m            `None` or `\"model\"` if downloading from a model. Default is `None`.\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m        revision (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;124;03m            An optional Git revision id which can be a branch name, a tag, or a\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m            commit hash.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;124;03m        library_name (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03m            The name of the library to which the object corresponds.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;124;03m        library_version (`str`, *optional*):\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03m            The version of the library.\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;124;03m        cache_dir (`str`, `Path`, *optional*):\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m            Path to the folder where cached files are stored.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m        local_dir (`str` or `Path`, *optional*):\u001b[39;00m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m            If provided, the downloaded file will be placed under this directory.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \u001b[38;5;124;03m        user_agent (`dict`, `str`, *optional*):\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m            The user-agent info in the form of a dictionary or a string.\u001b[39;00m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;124;03m        force_download (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;124;03m            Whether the file should be downloaded even if it already exists in\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;124;03m            the local cache.\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;124;03m        proxies (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;124;03m            Dictionary mapping protocol to the URL of the proxy passed to\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;124;03m            `requests.request`.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \u001b[38;5;124;03m        etag_timeout (`float`, *optional*, defaults to `10`):\u001b[39;00m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;124;03m            When fetching ETag, how many seconds to wait for the server to send\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;124;03m            data before giving up which is passed to `requests.request`.\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03m        token (`str`, `bool`, *optional*):\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;124;03m            A token to be used for the download.\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;124;03m                - If `True`, the token is read from the HuggingFace config\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;124;03m                  folder.\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m                - If a string, it's used as the authentication token.\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;124;03m        local_files_only (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;124;03m            If `True`, avoid downloading the file and return the path to the\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[38;5;124;03m            local cached file if it exists.\u001b[39;00m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m        headers (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m            Additional headers to be sent with the request.\u001b[39;00m\n\u001b[0;32m    910\u001b[0m \n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m        `str`: Local path of file or if networking is off, last version of file cached on disk.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \n\u001b[0;32m    914\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m        [`~utils.RepositoryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m            If the repository to download from cannot be found. This may be because it doesn't exist,\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;124;03m            or because it is set to `private` and you do not have access.\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m        [`~utils.RevisionNotFoundError`]\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m            If the revision to download from cannot be found.\u001b[39;00m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;124;03m        [`~utils.EntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;124;03m            If the file to download cannot be found.\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;124;03m        [`~utils.LocalEntryNotFoundError`]\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;124;03m            If network is disabled or unavailable and file is not found in cache.\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;124;03m        [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;124;03m            If `token=True` but the token cannot be found.\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;124;03m        [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m            If ETag cannot be determined.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m        [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m            If some parameter value is invalid.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \n\u001b[0;32m    931\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_ETAG_TIMEOUT \u001b[38;5;241m!=\u001b[39m constants\u001b[38;5;241m.\u001b[39mDEFAULT_ETAG_TIMEOUT:\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;66;03m# Respect environment variable above user value\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1484\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HfFileMetadata(\n\u001b[0;32m   1461\u001b[0m         commit_hash\u001b[38;5;241m=\u001b[39mr\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(constants\u001b[38;5;241m.\u001b[39mHUGGINGFACE_HEADER_X_REPO_COMMIT),\n\u001b[0;32m   1462\u001b[0m         \u001b[38;5;66;03m# We favor a custom header indicating the etag of the linked resource, and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1472\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mparse_xet_file_data_from_response(r),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m     )\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_metadata_or_catch_error\u001b[39m(\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   1478\u001b[0m     repo_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1479\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1480\u001b[0m     repo_type: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1481\u001b[0m     revision: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   1482\u001b[0m     endpoint: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   1483\u001b[0m     proxies: Optional[Dict],\n\u001b[1;32m-> 1484\u001b[0m     etag_timeout: Optional[\u001b[38;5;28mfloat\u001b[39m],\n\u001b[0;32m   1485\u001b[0m     headers: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m],  \u001b[38;5;66;03m# mutated inplace!\u001b[39;00m\n\u001b[0;32m   1486\u001b[0m     token: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[0;32m   1487\u001b[0m     local_files_only: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   1488\u001b[0m     relative_filename: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# only used to store `.no_exists` in cache\u001b[39;00m\n\u001b[0;32m   1489\u001b[0m     storage_folder: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# only used to store `.no_exists` in cache\u001b[39;00m\n\u001b[0;32m   1490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\n\u001b[0;32m   1491\u001b[0m     \u001b[38;5;66;03m# Either an exception is caught and returned\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m     Tuple[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m],\n\u001b[0;32m   1493\u001b[0m     \u001b[38;5;66;03m# Or the metadata is returned as\u001b[39;00m\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;66;03m# `(url_to_download, etag, commit_hash, expected_size, xet_file_data, None)`\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m     Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, Optional[XetFileData], \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[0;32m   1496\u001b[0m ]:\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get metadata for a file on the Hub, safely handling network issues.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \n\u001b[0;32m   1499\u001b[0m \u001b[38;5;124;03m    Returns either the etag, commit_hash and expected size of the file, or the error\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;124;03m          domain of the location (typically an S3 bucket).\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(refs_dir):\n\u001b[1;32m-> 1376\u001b[0m     revision_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(refs_dir, revision)\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(revision_file):\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1287\u001b[0m     paths\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;241m.\u001b[39munlink(missing_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# delete outdated file first\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1289\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mpaths\u001b[38;5;241m.\u001b[39mincomplete_path(etag),\n\u001b[0;32m   1290\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mpaths\u001b[38;5;241m.\u001b[39mfile_path,\n\u001b[0;32m   1291\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1292\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1293\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1294\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1295\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m-> 1296\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1297\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[0;32m   1298\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[0;32m   1301\u001b[0m write_download_metadata(local_dir\u001b[38;5;241m=\u001b[39mlocal_dir, filename\u001b[38;5;241m=\u001b[39mfilename, commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash, etag\u001b[38;5;241m=\u001b[39metag)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:277\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_wrapper\u001b[39m(\n\u001b[0;32m    265\u001b[0m     method: HTTP_METHOD_T, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, follow_relative_redirects: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    267\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around requests methods to follow relative redirects if `follow_relative_redirects=True` even when\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;124;03m    `allow_redirection=False`.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m    A backoff mechanism retries the HTTP call on 429, 503 and 504 errors.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m        method (`str`):\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m            HTTP method, such as 'GET' or 'HEAD'.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        url (`str`):\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m            The URL of the resource to fetch.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;124;03m        follow_relative_redirects (`bool`, *optional*, defaults to `False`)\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03m            If True, relative redirection (redirection to the same site) will be resolved even when `allow_redirection`\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m            kwarg is set to False. Useful when we want to follow a redirection to a renamed repository without\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m            following redirection to a CDN.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m        **params (`dict`, *optional*):\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m            Params to pass to `requests.request`.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:301\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    296\u001b[0m parsed_target = urlparse(response.headers[\"Location\"])\n\u001b[0;32m    297\u001b[0m if parsed_target.netloc == \"\":\n\u001b[0;32m    298\u001b[0m     # This means it is a relative 'location' headers, as allowed by RFC 7231.\n\u001b[0;32m    299\u001b[0m     # (e.g. '/path/to/resource' instead of 'http://domain.tld/path/to/resource')\n\u001b[0;32m    300\u001b[0m     # We want to follow this relative redirect !\n\u001b[1;32m--> 301\u001b[0m     #\n\u001b[0;32m    302\u001b[0m     # Highly inspired by `resolve_redirects` from requests library.\n\u001b[0;32m    303\u001b[0m     # See https://github.com/psf/requests/blob/main/requests/sessions.py#L159\n\u001b[0;32m    304\u001b[0m     next_url = urlparse(url)._replace(path=parsed_target.path).geturl()\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:423\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 423\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m     )\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-6824cb2c-71c6583710d6c0a9237dd1a4;0157bd78-bb45-49bb-b097-0c78997e0478)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load Mistral model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m mistral_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmistral_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      9\u001b[0m     mistral_id,\n\u001b[0;32m     10\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m     11\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load your clustered data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:794\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    778\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    779\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m    780\u001b[0m     TOKENIZER_CONFIG_FILE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    792\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    793\u001b[0m )\n\u001b[1;32m--> 794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1138\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         config_class \u001b[38;5;241m=\u001b[39m CONFIG_MAPPING[config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1134\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1135\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1136\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1137\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1138\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1139\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1140\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers from source with the command \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1141\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1142\u001b[0m         )\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class\u001b[38;5;241m.\u001b[39mfrom_dict(config_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwargs)\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\configuration_utils.py:631\u001b[0m, in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    628\u001b[0m     )\n\u001b[0;32m    630\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_auto_class\u001b[39m\u001b[38;5;124m\"\u001b[39m: from_auto_class}\n\u001b[1;32m--> 631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pipeline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     user_agent[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m from_pipeline\n\u001b[0;32m    634\u001b[0m pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(pretrained_model_name_or_path)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\configuration_utils.py:686\u001b[0m, in \u001b[0;36m_get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m         \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n\u001b[0;32m    684\u001b[0m         config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dict_from_json_file(resolved_config_file)\n\u001b[1;32m--> 686\u001b[0m     config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m commit_hash\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m):\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like the config file at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid JSON file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:416\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inside \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Either all the files were found, or some were _CACHED_NO_EXIST but we do not raise for missing entries\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_counter \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(full_filenames):\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m existing_files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(existing_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    419\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n",
      "\u001b[1;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1.\n401 Client Error. (Request ID: Root=1-6824cb2c-71c6583710d6c0a9237dd1a4;0157bd78-bb45-49bb-b097-0c78997e0478)\n\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load Mistral model\n",
    "mistral_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    mistral_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load your clustered data\n",
    "df = pd.read_csv(\"clustered_papers.csv\")  # replace with your actual file\n",
    "\n",
    "# Assume you have a function or mapping that returns keywords for each cluster\n",
    "cluster_keywords = {\n",
    "    0: [\"crowdsourced data\", \"urban mobility\", \"transportation modeling\"],\n",
    "    1: [\"machine learning\", \"citation analysis\", \"bibliometric networks\"],\n",
    "    # Add all your cluster -> keyword mappings\n",
    "}\n",
    "\n",
    "# Generate and print summary for each cluster\n",
    "for cluster_id in sorted(df['cluster_id'].unique()):\n",
    "    cluster_df = df[df['cluster_id'] == cluster_id]\n",
    "    abstracts = cluster_df['abstract'].dropna().tolist()\n",
    "    if not abstracts:\n",
    "        continue\n",
    "\n",
    "    combined_text = \" \".join(abstracts)[:2000]  # Truncate if too long\n",
    "    keywords = cluster_keywords.get(cluster_id, [\"science\", \"research\"])\n",
    "\n",
    "    # Build instruct prompt\n",
    "    prompt = f\"<s>[INST] Generate a bibliometric summary using the topics: {', '.join(keywords)}. Text:\\n{combined_text}\\n[/INST]\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300)\n",
    "\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nðŸ“˜ Cluster {cluster_id} Summary\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(summary)\n",
    "    print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0a5cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embedding: [-3.03823858e-01 -2.94588953e-01 -3.76790494e-01  5.01689222e-03\n",
      " -3.54408994e-02  1.06439918e-01 -3.23471457e-01  2.17532516e-01\n",
      "  2.92097963e-03 -7.44312882e-01 -4.35850620e-01 -3.50625992e-01\n",
      "  2.00050063e-02 -2.46841222e-01 -1.59674272e-01  5.49915060e-02\n",
      " -2.40113243e-01  6.04893386e-01  4.65398401e-01  5.87469265e-02\n",
      " -3.19653422e-01 -7.89481580e-01  1.41855806e-01  2.59826928e-01\n",
      " -1.24172002e-01 -3.95769417e-01 -7.99091160e-02  4.51406538e-01\n",
      " -7.91762024e-02  1.85417414e-01 -4.29062903e-01  3.09045583e-01\n",
      " -5.20451665e-01 -8.73078465e-01  7.41924167e-01 -7.56629258e-02\n",
      " -5.66660017e-02 -4.74565268e-01 -5.19070625e-01  4.38661039e-01\n",
      " -3.53880256e-01  3.36756796e-01  4.73318577e-01 -3.30739498e-01\n",
      " -2.30438918e-01  6.87498897e-02 -2.97834635e+00 -2.10847571e-01\n",
      " -8.75047624e-01 -1.28071606e-01  3.81216854e-01  4.08959866e-01\n",
      "  3.36281300e-01  1.29074126e-01  6.62514269e-01  7.79323637e-01\n",
      "  2.08158702e-01 -5.16489595e-02  3.43803972e-01  1.69889957e-01\n",
      "  5.51769316e-01  4.24994111e-01 -2.14304030e-01 -5.35979211e-01\n",
      "  1.90755606e-01 -6.43610209e-02  5.47206998e-02  4.96894479e-01\n",
      " -1.86606094e-01  3.67489755e-02 -5.50784171e-01  1.43669158e-01\n",
      "  5.18800020e-01 -8.66626352e-02 -2.19657600e-01 -3.38900946e-02\n",
      "  1.65021107e-01  4.66869235e-01 -8.07627559e-01 -1.64124861e-01\n",
      " -4.41753775e-01  8.41248453e-01  1.09165795e-01  1.17643267e-01\n",
      "  5.80080807e-01  7.92051256e-01 -9.70901430e-01 -4.78672832e-01\n",
      " -1.40501127e-01  4.45860445e-01 -2.10405946e-01 -3.04221839e-01\n",
      "  3.54654014e-01  1.11477005e+00  7.84012079e-01  3.58697236e-01\n",
      " -1.30415529e-01 -2.64885068e-01 -2.08544940e-01  6.15005851e-01\n",
      "  5.91294467e-01 -2.97800004e-01  2.65872836e-01 -3.22568297e-01\n",
      "  4.06827807e-01 -1.51383504e-02 -5.54092228e-01 -4.92251009e-01\n",
      " -1.23656601e-01 -2.69746614e+00  5.47028840e-01 -1.67653672e-02\n",
      " -2.85866261e-01 -1.77994788e-01  1.48758769e-01  4.89803404e-01\n",
      "  7.20278740e-01  2.15709254e-01 -4.52702269e-02 -6.47674873e-02\n",
      "  1.18794814e-01  1.44697383e-01 -1.32495329e-01  5.85410953e-01\n",
      " -3.43137234e-01  2.75409102e-01  2.25036308e-01  1.50559187e-01\n",
      "  3.22888345e-01  2.57087618e-01  3.78596783e-01  6.61356747e-01\n",
      " -1.34976059e-01 -5.85843503e-01 -3.71585906e-01  2.45749384e-01\n",
      "  4.72799122e-01 -6.50312379e-02  1.03827089e-01 -2.42710449e-02\n",
      " -1.61127567e-01 -7.94591725e-01 -2.10137796e+00 -5.65133512e-01\n",
      "  9.99554455e-01  1.97853357e-01  2.83387154e-01 -1.78595394e-01\n",
      "  1.94912940e-01  1.02922007e-01  2.71759927e-01  6.88619688e-02\n",
      " -7.25524187e-01 -3.99493039e-01 -4.28509004e-02  1.11385517e-01\n",
      " -8.41510236e-01  5.08089811e-02  6.40503943e-01  3.04388493e-01\n",
      " -6.12770170e-02 -1.59682423e-01 -4.55193728e-01 -9.55761075e-02\n",
      " -4.41343069e-01  1.41912133e-01  5.37566960e-01  4.37040254e-02\n",
      "  2.81829208e-01  1.31740540e-01 -2.05661908e-01  2.07400724e-01\n",
      "  6.62233114e-01  2.85458058e-01  4.90685165e-01  1.84566736e-01\n",
      "  4.49979722e-01  1.80129901e-01  3.74054313e-01  4.95090872e-01\n",
      " -2.44576976e-01  6.07260168e-01  9.72533897e-02 -9.37253982e-02\n",
      "  4.00745600e-01 -4.28710580e-01 -1.26358718e-01 -4.71248806e-01\n",
      " -5.64967275e-01  5.60319573e-02 -7.20048249e-02 -1.23202875e-02\n",
      " -2.10176483e-02  9.74207371e-02  1.10258490e-01 -4.30400342e-01\n",
      "  9.30555701e-01 -1.01523809e-01  2.85602123e-01 -4.15301621e-02\n",
      "  7.48717785e-02 -1.72861680e-01 -1.09371006e-01  4.48875040e-01\n",
      " -4.10333753e-01  3.37432790e+00 -2.87095606e-01 -4.76357937e-01\n",
      " -4.42594528e-01 -1.17385298e-01 -4.25485581e-01 -2.73876131e-01\n",
      "  3.14201355e-01 -6.57704413e-01  2.26520717e-01 -4.71419692e-01\n",
      "  3.71921003e-01  5.24181187e-01 -4.40325737e-01 -2.06464320e-01\n",
      "  6.30145431e-01  6.35774493e-01 -6.52629197e-01  5.44244170e-01\n",
      " -2.82690018e-01  2.39978552e-01 -5.97796142e-02  8.54915619e-01\n",
      "  6.00129604e-01 -1.09372818e+00 -9.90878940e-02 -4.58210222e-02\n",
      " -3.11456978e-01  2.77797788e-01 -7.13510215e-01 -4.21125144e-01\n",
      " -8.74434114e-01 -3.36560160e-01  2.15873480e-01 -2.42640406e-01\n",
      " -4.69421923e-01  1.97370306e-01  1.51250750e-01  1.42509937e-01\n",
      "  3.78442913e-01  1.08452030e-02  5.81535041e-01  4.40722227e-01\n",
      "  1.27532721e-01 -8.95353556e-02  7.39166498e-01 -3.70861232e-01\n",
      "  1.89236686e-01 -4.68166918e-01 -2.60707974e-01 -5.73436856e-01\n",
      "  6.51094913e-02  2.68359780e-01 -1.82223946e-01  9.69101936e-02\n",
      "  1.71345383e-01 -1.45429730e-01  3.33489805e-01 -4.26005721e-01\n",
      " -7.76545465e-01 -7.37197459e-01  1.44036040e-01 -4.78814095e-01\n",
      "  1.76023588e-01 -1.49703681e-01  3.54002655e-01 -5.69597147e-02\n",
      "  1.60185277e-01 -3.06181550e+00 -1.53075606e-01 -5.72886348e-01\n",
      "  3.72718900e-01  2.65904754e-01 -7.43334472e-01 -3.34909648e-01\n",
      "  5.35075307e-01  4.95426089e-01 -6.81253314e-01  5.32151222e-01\n",
      "  1.91439055e-02  7.03206241e-01  1.86690971e-01 -3.41856211e-01\n",
      "  1.39765084e-01  1.60684347e-01 -1.73364252e-01  1.68120354e-01\n",
      " -1.63153559e-01  1.38858601e-01  7.40036368e-02  2.27854222e-01\n",
      " -4.17960808e-03  3.71486396e-01 -4.31283355e-01 -3.82834226e-01\n",
      " -3.82469833e-01 -9.37596560e-01  2.14142278e-01  3.81438613e-01\n",
      " -1.36773497e-01  1.67250991e-01  9.40938741e-02 -1.58943430e-01\n",
      " -2.27256823e+00  8.74977410e-02  6.83586299e-02 -2.62213707e-01\n",
      " -2.07681179e-01  1.79981917e-01  6.50544941e-01 -4.82416958e-01\n",
      " -8.67757499e-01 -3.85069132e-01 -8.24159384e-02  1.86037987e-01\n",
      "  2.50081360e-01 -2.50337124e-02  4.05665159e-01  5.58602035e-01\n",
      "  3.51490289e-01 -1.08947076e-01  4.96256709e-01  4.05098870e-03\n",
      "  1.17049038e-01 -4.46413666e-01 -5.37404597e-01  3.13501775e-01\n",
      " -5.66248670e-02  5.21882594e-01 -9.82223213e-01  7.10331425e-02\n",
      " -3.75795454e-01 -4.09534037e-01  4.66200769e-01 -6.72969222e-01\n",
      " -2.77400702e-01  2.38609567e-01 -7.29408264e-02  1.57251060e-01\n",
      "  1.44245982e-01  4.86383438e-01  8.53721559e-01  9.28595066e-02\n",
      "  1.44029930e-01  1.26498985e+00 -2.79999554e-01  1.84412450e-01\n",
      "  8.95227253e-01  9.07380879e-02  9.75967586e-01  2.24406958e-01\n",
      " -1.51531339e-01  3.39259207e-01  2.18420342e-01  2.55489945e-01\n",
      "  1.38760459e+00 -1.12043172e-01 -4.88151424e-02 -3.77121210e-01\n",
      "  4.90213275e-01  2.32067689e-01 -2.33816460e-01  3.89621079e-01\n",
      "  1.06561100e+00 -5.45746148e-01  2.14555860e-01 -2.25018382e-01\n",
      "  9.91273671e-03 -1.01733625e+00 -2.12945834e-01 -5.80432117e-01\n",
      "  1.14616603e-01  3.52275312e-01  1.74265936e-01  1.00929439e+00\n",
      " -2.64209270e-01 -1.08852112e+00 -2.11574450e-01 -3.33137751e-01\n",
      " -1.17299333e-03 -1.76978469e-01  6.71120286e-02 -4.46681261e-01\n",
      " -4.02579993e-01 -1.10299431e-01 -7.47526884e-01  1.28722847e-01\n",
      " -6.41264796e-01 -2.27028072e-01  1.07649677e-01 -1.36430532e-01\n",
      " -1.42240429e+00 -4.63469446e-01 -1.86222434e-01  3.05613786e-01\n",
      "  2.84615278e-01  2.09360570e-01  1.66598447e-02 -1.18049547e-01\n",
      "  4.79218185e-01 -5.94101846e-01  2.67336249e-01  7.77873099e-02\n",
      " -4.12358820e-01 -5.60837567e-01 -7.07649708e-01  3.53718102e-01\n",
      "  1.73673928e-01  3.19273062e-02 -2.15477735e-01 -1.93510488e-01\n",
      "  2.93597519e-01  5.13518751e-02 -1.38250962e-01  1.22219570e-01\n",
      "  1.66278824e-01 -1.91558555e-01  5.62437892e-01  3.82898808e-01\n",
      " -2.28021555e-02  9.63744640e-01  8.39688420e-01  5.86799324e-01\n",
      "  5.17765820e-01  7.15042472e-01  5.18150628e-01 -4.56401818e-02\n",
      "  4.41424847e-01  3.31310928e-02 -1.52724907e-01 -3.37495178e-01\n",
      " -5.01755297e-01 -5.08557558e-01  3.33946794e-01 -7.20271528e-01\n",
      " -3.24351043e-01 -3.90101671e-01 -3.14785615e-02 -8.30262661e-01\n",
      " -2.79336691e-01  5.52389503e-01 -2.19960034e-01  6.25406802e-01\n",
      "  5.38597107e-01 -4.56903249e-01 -3.21185365e-02  1.59916401e-01\n",
      "  2.54068673e-01  7.68879831e-01 -1.01516485e-01  6.22536987e-03\n",
      " -2.71685809e-01  8.47205520e-01 -2.91768074e-01 -2.32147962e-01\n",
      " -7.91487277e-01 -2.97860414e-01  3.73831689e-01  9.03848037e-02\n",
      " -3.77684027e-01 -2.99598217e-01 -1.10735960e-01 -2.96604633e-01\n",
      "  2.86643356e-01  2.93127000e-01 -2.13755465e+00  1.72231644e-01\n",
      "  5.80931962e-01  6.72860026e-01  3.25232685e-01 -3.36615294e-02\n",
      " -5.17170787e-01  6.92778409e-01  4.24069129e-02 -1.25111952e-01\n",
      " -1.41847819e-01 -9.97623861e-01 -2.48993754e-01  1.05955768e-02\n",
      " -7.20817149e-02  8.99536908e-02 -2.56312937e-01 -3.30127180e-01\n",
      "  2.58050542e-02 -1.56628281e-01 -2.72992730e-01  3.80017787e-01\n",
      "  1.90809652e-01 -6.39048517e-02  5.49922138e-03 -6.79573178e-01\n",
      "  3.54787916e-01  4.71579760e-01  1.73889071e-01  1.40344739e-01\n",
      " -1.40151963e-01 -3.82846445e-01 -6.93244576e-01 -9.62501615e-02\n",
      "  5.48109055e-01  1.79084480e-01  2.74240017e-01 -3.88765752e-01\n",
      "  6.95122838e-01 -2.81543583e-02 -4.93431091e-01  7.08428562e-01\n",
      " -9.22967196e-02  1.95841670e-01  2.23768845e-01 -1.41412497e-01\n",
      " -3.38614881e-01 -1.63368180e-01 -4.15521324e-01 -1.52449340e-01\n",
      " -3.67466539e-01  3.45857702e-02  3.77637185e-02 -6.30373731e-02\n",
      "  1.53604642e-01 -4.50040996e-02 -1.92984223e-01  2.23364532e-01\n",
      " -5.29767215e-01  2.00826257e-01  3.97735357e-01 -3.19702953e-01\n",
      " -4.84797508e-02  1.15604013e-01 -3.43546420e-01 -1.02862966e+00\n",
      " -3.31704050e-01 -6.10948205e-01 -7.40960777e-01 -3.00927728e-01\n",
      "  6.50627315e-01  8.65414888e-02 -4.83936876e-01  6.73789144e-01\n",
      " -5.71556926e-01  3.92382368e-02  5.23419201e-01 -1.18905552e-01\n",
      "  5.41147470e-01 -2.56521910e-01 -1.36582881e-01 -4.95750368e-01\n",
      " -3.91434759e-01  6.52786553e-01 -2.83701926e-01 -1.31263316e-01\n",
      " -3.97048533e-01 -3.18676531e-01  9.12800059e-02 -2.30428457e-01\n",
      " -7.33456910e-01 -3.79245013e-01  2.45084420e-01  1.03215203e-01\n",
      " -2.03761622e-01 -6.88308537e-01  1.63904279e-01 -4.04506177e-01\n",
      " -8.35094154e-02 -2.79382616e-01  5.37252389e-02  5.96332550e-01\n",
      "  4.65979367e-01  3.86881888e-01  3.48968863e-01 -6.10448793e-02\n",
      "  5.73337018e-01 -9.94527340e-03  1.97490454e-01 -5.92190087e-01\n",
      " -2.42805377e-01 -3.69883567e-01 -2.13024855e-01  2.47948125e-01\n",
      "  6.82375789e-01 -9.12577391e-01  3.97825867e-01 -3.95976931e-01\n",
      "  1.60022473e+00  4.84747827e-01  4.28680152e-01 -3.50376487e-01\n",
      "  7.78658450e-01  9.69223157e-02 -4.49009359e-01  8.62805545e-01\n",
      " -6.82071626e-01 -5.01335375e-02 -2.53381848e-01  3.27517778e-01\n",
      " -4.40697312e-01  5.13285518e-01  4.98526692e-01  9.87671494e-01\n",
      " -3.59471083e-01 -5.72911382e-01 -3.43150198e-01  1.68191940e-01\n",
      " -7.14259505e-01  1.12951815e+00  5.40782988e-01  8.75323117e-02\n",
      "  1.77787513e-01  6.41454935e-01 -3.85245025e-01  4.81645465e-01\n",
      "  5.02464771e-01  3.48949730e-02 -4.80260670e-01  2.29665935e-01\n",
      "  6.58189356e-02  1.00322294e+00 -3.03836644e-01  1.12223059e-01\n",
      " -2.97916025e-01 -3.99048865e-01  6.91226363e-01  1.76347539e-01\n",
      " -1.89142719e-01 -4.04415131e-01  5.17377853e-01 -1.55545384e-01\n",
      "  1.04448453e-01  5.28798223e-01 -3.42488855e-01 -6.55166626e-01\n",
      "  4.68079656e-01  6.13815010e-01 -4.29856740e-02  3.29212487e-01\n",
      " -3.51444870e-01  5.70579708e-01 -9.71956924e-02  1.82344645e-01\n",
      " -3.37710157e-02  1.21567518e-01 -1.69176549e-01  8.69155228e-02\n",
      "  1.72598064e-01  5.76301455e-01  2.73743808e-01 -6.75247982e-03\n",
      " -1.78825364e-01  1.50688201e-01 -3.86660546e-01  4.37192023e-01\n",
      "  1.97378904e-01 -3.23726207e-01  3.10098886e-01  1.97389320e-01\n",
      "  4.56643790e-01  3.94443721e-01  4.93506610e-01  1.33734584e-01\n",
      "  5.39645672e-01  5.66998184e-01 -1.09497458e-01 -2.22327590e+00\n",
      "  1.71253443e-01  5.31610847e-01  4.14107919e-01 -5.05743265e-01\n",
      " -2.27950923e-02  3.09769601e-01  2.55310774e-01 -2.49603629e-01\n",
      "  5.20329969e-03 -9.26829427e-02  8.10874224e-01  9.66617048e-01\n",
      " -1.13633126e-01  1.70411661e-01  7.40716994e-01  2.14106500e-01\n",
      " -1.02572048e+00 -2.04366624e-01 -3.48678857e-01  5.46867132e-01\n",
      "  1.73461482e-01 -3.42002243e-01 -4.06353138e-02 -6.69542253e-01\n",
      "  6.34213462e-02  1.39105931e-01 -8.11827064e-01  5.00545144e-01\n",
      "  1.14766550e+00 -2.50892967e-01  2.79290289e-01  9.77494121e-02\n",
      "  8.25779021e-01  1.56420261e-01 -4.77133840e-01  1.74098849e-01\n",
      "  1.50189206e-01 -3.00052673e-01  1.51577458e-01 -4.98352557e-01\n",
      "  4.45586532e-01 -7.53745437e-02  7.63495862e-02 -2.85221398e-01\n",
      "  7.40827769e-02  4.42952424e-01 -3.00074041e-01  6.32266283e-01\n",
      " -3.60712737e-01 -2.04998404e-01 -1.46959364e-01  5.83032481e-02\n",
      " -1.38016552e-01  6.28215730e-01 -8.39272738e-02  7.44105995e-01\n",
      " -1.23712048e-03  5.05161762e-01 -3.92003208e-01 -1.15079045e-01\n",
      "  5.90554357e-01 -3.02465022e-01  4.90493178e-01  1.37736261e-01\n",
      " -3.94927770e-01 -5.11058807e-01 -3.48522961e-01  2.24246264e-01\n",
      "  2.00705603e-01 -9.79637355e-03 -2.42050290e-01  3.36554170e-01\n",
      " -2.56607383e-02  4.89471048e-01  2.60042906e-01  2.92030573e-02\n",
      " -3.23106974e-01  6.08169496e-01  1.44946828e-01  1.18433356e-01\n",
      " -3.92132401e-01 -6.56875789e-01  1.19904369e-01  3.13865155e-01\n",
      " -6.76669455e+00 -1.54439926e-01 -6.14286065e-01 -3.30135822e-01\n",
      " -8.49660933e-02 -4.70077127e-01  7.47107863e-02  1.54274181e-02\n",
      " -1.97616071e-01 -2.90094823e-01 -1.81808963e-01 -2.76574850e-01\n",
      " -4.75893825e-01 -5.28998673e-01 -8.01036730e-02  1.27828076e-01]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example input text (replace with your actual article or text)\n",
    "input_text = \"Research trends in urban mobility using crowd-sourced data.\"\n",
    "\n",
    "# Tokenize input text and convert it to input IDs\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Get the model's output (embeddings)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state  # Embeddings of the input text\n",
    "\n",
    "# Optional: If you want a single vector representation (e.g., using the [CLS] token)\n",
    "sentence_embedding = embeddings[:, 0, :].squeeze().numpy()  # [CLS] token representation\n",
    "print(\"Sentence Embedding:\", sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b1cd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with Grok API: 401\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Grok API URL (replace with your actual endpoint)\n",
    "grok_api_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "\n",
    "# Prepare the payload with BERT embeddings\n",
    "data = {\n",
    "    \"text_embedding\": sentence_embedding.tolist(),  # Convert numpy array to list for JSON compatibility\n",
    "    \"additional_info\": \"Research trends in urban mobility\"  # Optional additional info\n",
    "}\n",
    "\n",
    "# Send the POST request to Grok API\n",
    "response = requests.post(grok_api_url, json=data)\n",
    "\n",
    "# Check the response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Grok API Response:\", result)\n",
    "else:\n",
    "    print(\"Error with Grok API:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abdcbda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.78.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.4.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.78.1-py3-none-any.whl (680 kB)\n",
      "   ---------------------------------------- 0.0/680.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 680.9/680.9 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl (207 kB)\n",
      "Installing collected packages: jiter, distro, openai\n",
      "\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   -------------------------- ------------- 2/3 [openai]\n",
      "   ---------------------------------------- 3/3 [openai]\n",
      "\n",
      "Successfully installed distro-1.9.0 jiter-0.9.0 openai-1.78.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9efc4881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: keybert in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: openai in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.78.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keybert) (13.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keybert) (1.5.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keybert) (3.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas torch transformers keybert openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1fc1a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['id', 'title', 'doi', 'publication_year', 'authors', 'abstract', 'open_access', 'host_venue', 'clean_abstract', 'keywords', 'entities', 'gmm_cluster', 'gmm_probs']\n",
      "\n",
      "ðŸ”¹ Cluster 0 ðŸ”¹\n",
      "Research Summary: Unpacking the Consequences of Overreliance on AI-Generated Content in the Era of Infodemics\n",
      "\n",
      "**Research Methodology:**\n",
      "\n",
      "This study employed a mixed-methods approach, combining both qualitative and quantitative data collection and analysis methods. The research design consisted of three stages:\n",
      "\n",
      "1. **Content Analysis**: A dataset of 500 AI-generated articles was collected from online platforms, focusing on topics related to health, politics, and education. These articles were analyzed using natural language processing (NLP) techniques to identify patterns of plagiarism, misinformation, and inequity.\n",
      "2. **Survey Research**: A total of 500 participants were recruited through online surveys, comprising of individuals from diverse backgrounds and age groups. The survey aimed to understand users' perceptions and experiences with AI-generated content, including their ability to distinguish between human-written and AI-generated articles.\n",
      "3. **Expert Interviews**: In-depth interviews were conducted with 20 experts from academia, journalism, and AI development, to gather insights into the development and deployment of AI models, as well as the potential consequences of overreliance on AI-generated content.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "1. **Plagiarism and Misinformation**: The content analysis revealed that 60% of the generated articles exhibited plagiarism, with 40% containing misinformation. These findings suggest that the models are not adequately designed to handle unstructured data, leading to the spread of misinformation.\n",
      "2. **Indistinguishability**: The survey results showed that 70% of participants were unable to distinguish between human-written and AI-generated articles, highlighting the need for more user-centered AI design.\n",
      "3. **Inequity and Mishandling**: Expert interviews revealed that AI models are often developed and deployed without considering the potential inequities in data, leading to the exacerbation of existing social biases.\n",
      "4. **Retrace and Unresolved Issues**: The study found that the AI models lack the ability to retrace the sources of information, further complicating the issue of misinformation and plagiarism.\n",
      "5. **Overreliance and Consequences**: The findings suggest that the overreliance on AI-generated content can lead to the erosion of trust in institutions, exacerbate the infodemic, and have significant consequences for individuals and society as a whole.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "This study highlights the urgent need for more rigorous development, deployment, and regulation of AI-generated content. The findings suggest that the overreliance on AI-generated content can have far-reaching consequences, including the exacerbation of misinformation, plagiarism, and social inequities. It is essential to develop more user-centered AI models that can mitigate these risks and promote a more informed and equitable society.\n",
      "\n",
      "ðŸ”¹ Cluster 1 ðŸ”¹\n",
      "**Research Summary: Exploring the Frontiers of Language and Vision**\n",
      "\n",
      "**Research Methodology:**\n",
      "\n",
      "This study employed a qualitative, deconstructive approach to investigate the intricacies of language and vision. Specifically, a within-subjects design was utilized, where participants engaged in self-evaluating tasksolving abilities using exemplars of language and vision snippets. The research leveraged cutting-edge tools, such as the Tree of Thought LLM (https://github.com/princeton-nlp/tree-of-thought-llm) and MiniGPT4 (https://minigpt4.github.io), to facilitate the debugging and rethinking of language and vision interactions.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "1. **Unexplored Dimensions of Language and Vision:** The study uncovered previously unexplored dimensions of language and vision, highlighting the importance of token-level analysis in understanding the complex relationships between language and vision.\n",
      "2. **Adversarial Effects on Language and Vision:** The research revealed that adversarial attacks on language and vision models can have significant implications for tasksolving abilities, emphasizing the need for robust debugging and rethinking of language and vision interactions.\n",
      "4. **Cowriting and Hand-Drawn Approaches:** The study demonstrated the effectiveness of cowriting and hand-drawn approaches in facilitating deeper understanding and tasksolving abilities in language and vision domains.\n",
      "5. **Log-Linear Relationships:** The research identified log-linear relationships between language and vision snippets, highlighting the importance of considering the nuances of language and vision interactions in tasksolving.**\n",
      "\n",
      "**Implications and Future Directions:**\n",
      "\n",
      "The study's findings have significant implications for the development of more effective language and vision models, emphasizing the need for continued research in this area. Future directions include exploring the applications of MiniGPT4 and other cutting-edge tools in language and vision research, as well as further investigating the log-linear relationships to improve tasksolving abilities.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "This study contributes to our understanding of the complex relationships between language and vision, it also highlights the need for continued research in this area. The findings have significant implications for the development of more effective language and vision models, and the study's methodology provides a framework for future research in this area.\n",
      "\n",
      "ðŸ”¹ Cluster 2 ðŸ”¹\n",
      "**Research Summary:**\n",
      "\n",
      "**Title:** \"Vision-to-Language: A Comprehensive Study of Domain-Specific Transformer-Based Models\"\n",
      "\n",
      "**Research Objective:**\n",
      "The study aims to investigate the effectiveness of transformer-based models for vision-to-language tasks, focusing on domain-specific models and their performance on various benchmark datasets.\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "1. **Model Selection:** The study utilizes three state-of-the-art transformer-based models, namely, OPT-175B, Flamingo-80B, and GPT-NeoX-20B.\n",
      "2. **Domain-Specific Training:** Each model is fine-tuned on specific domains, including VQA-2, BLIP-2, and Image-to-Text datasets.\n",
      "3. **Rank Deficiency:** The study explores the effect of rank deficiency on model performance, using LORA (github.com/microsoft/LORA) and CodeLMS (github.com/vhellendoorn/CodeLMS) techniques.\n",
      "4. **Compute-Optimal Training:** The models are trained using compute-optimal techniques to reduce computational resources.\n",
      "6. **Evaluation Metrics:** The study uses standard evaluation metrics, including n-grams, BLEU, METEOR, and CIDEr, to assess model performance.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "1. **Domain-Specific Performance:** The study shows that transformer-based models outperform generic models on domain-specific tasks, highlighting the importance of domain-specific training.\n",
      "2. **Rank Deficiency:** The results indicate that rank deficiency techniques, such as LORA and CodeLMS, can significantly reduce computational resources without compromising performance.\n",
      "3. **Compute-Optimal Training:** The study demonstrates that compute-optimal training techniques can reduce training time and resources while maintaining model performance.\n",
      "4. **Extrapolating to New Domains:** The results suggest that transformer-based models can extrapolate to new domains, achieving competitive performance without additional training data.\n",
      "5. **Multilingual Performance:** The study finds that transformer-based models can handle multilingual inputs, achieving comparable performance across languages.\n",
      "\n",
      "**Conclusion:**\n",
      "The study provides a comprehensive analysis of domain-specific transformer-based models, highlighting their effectiveness in various vision-to-language tasks. The findings have significant implications for the development of efficient and accurate models for real-world applications.\n",
      "\n",
      "**Limitations:**\n",
      "The study is limited to transformer-based models and specific domain datasets. Further research is needed to explore other model architectures and domains to generalize the findings.\n",
      "\n",
      "ðŸ”¹ Cluster 3 ðŸ”¹\n",
      "**Research Summary: Investigating the Efficacy of Advanced Language Models in Healthcare and Information Retrieval**\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "This study employed a mixed-methods approach, combining both qualitative and quantitative methods to investigate the performance of advanced language models in healthcare and information retrieval. The research design involved a multi-task learning framework, where various models were trained on multiple tasks simultaneously. The models were evaluated on several datasets, including but not limited to MedMCQA, HealthSearchQA, and a custom dataset (plt001).\n",
      "\n",
      "**Quantitative Methods:**\n",
      "\n",
      "* **Model architectures:** The study utilized various models, including NBMefreeStep1, NBMefreeStep2, AmbossStep1, AmbossStep2, and InstructGPT.\n",
      "* **Parameter efficiency:** The parameter-efficient models, such as those with 175 billion and 540 billion parameters, were compared to smaller models.\n",
      "* **Evaluation metrics:** The performance of the models was evaluated using accuracy, lost applications, and reinforcing learning metrics.\n",
      "\n",
      "**Qualitative Methods:**\n",
      "\n",
      "* **Exemplars:** A qualitative analysis of exemplars from the output was conducted to assess the quality and relevance of the generated by the models.\n",
      "* **Reshape:** The impact of reshaping the input data on the model's performance was investigated.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "1. **Parameter efficiency:** Larger models with 175 billion and 540 billion parameters outperformed smaller models in terms of accuracy and reinforcing learning metrics.\n",
      "2. **Multitask learning:** The multi-task learning approach improved the performance of the models on individual tasks, particularly in the healthcare domain (MedMCQA and HealthSearchQA datasets).\n",
      "3. **NBMefreeStep1 and NBMefreeStep2:** These models demonstrated superior performance on the plt001 dataset, suggesting their potential in handling complex queries (5687 queries).\n",
      "4. **AmbossStep1 and AmbossStep2:** These models showed improved performance on the HealthSearchQA dataset, indicating their effectiveness in healthcare information retrieval.\n",
      "5. **Lost applications:** The study found that the lost applications metric is a reliable measure of model efficacy in handling out-of-distribution queries.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "This study contributes to the understanding of advanced language models in healthcare and information retrieval. The findings suggest that larger, parameter-efficient models, such as those with 175 billion and 540 billion parameters, can outperform smaller models. The multi-task learning approach and reshaping input data can further improve model performance. The study's results have implications for the development of more accurate and efficient language models in healthcare and information retrieval.\n",
      "\n",
      "ðŸ”¹ Cluster 4 ðŸ”¹\n",
      "**Research Summary:**\n",
      "\n",
      "**Title:** A Comprehensive Analysis of Domain-Specific and Task-Specific Approaches in Knowledge-Intensive Natural Language Processing\n",
      "\n",
      "**Research Methodology:**\n",
      "\n",
      "This study employed a mixed-methods approach, combining both quantitative and qualitative methods to scrutinize the current state of domain-specific and task-specific approaches in knowledge-intensive natural language processing (NLP). The research design involved a comprehensive survey of existing literature, benchmarking of state-of-the-art models, and expert interviews.\n",
      "\n",
      "* A systematic review of 100 research articles and conference papers was conducted to identify the current trends, challenges, and opportunities in domain-specific and task-specific approaches in NLP.\n",
      "* A benchmarking study was carried out using the Text-DaVinci-002 dataset to evaluate the performance of six state-of-the-art models, including Instruct-GPT and LLM-Augmented, in various knowledge-intensive NLP tasks, such as multitask learning and embedding.\n",
      "* Expert interviews were conducted with 10 leading researchers in the field to gather insights on the understudied and untraceable aspects of domain-specific and task-specific approaches.\n",
      "\n",
      "**Key Findings:**\n",
      "\n",
      "1. **Domain-specific approaches outperform task-specific approaches**: The benchmarking study revealed that domain-specific models outperformed task-specific models in knowledge-intensive NLP tasks, such as question answering and text classification.\n",
      "2. **Knowledge-intensive tasks require multitask learning**: The study highlighted the importance of multitask learning in knowledge-intensive NLP tasks, as it enables models to leverage shared knowledge across tasks and improve overall performance.\n",
      "4. **LLM-Augmented models demonstrate forward-looking capabilities**: The benchmarking study showed that LLM-Augmented models exhibited forward-looking capabilities, enabling them to generalize well to unseen tasks and datasets.\n",
      "5. **Instruct-GPT models require fine-tuning for optimal performance**: The study found that Instruct-GPT models require fine-tuning on specific tasks and datasets to achieve optimal results.\n",
      "6. **Embedding techniques are invaluable for knowledge-intensive tasks**: The study demonstrated that embedding techniques, such as XML-NS-MML and xlink, are essential for representing complex knowledge structures and relationships in NLP tasks.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "This study provides a comprehensive analysis of domain-specific and task-specific approaches in knowledge-intensive NLP. The findings highlight the need for a more nuanced understanding of the strengths and limitations of each approach. The results have important implications for the development of more effective and efficient NLP models that can tackle complex, knowledge-intensive tasks.**\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "* The study was limited to a specific set of NLP tasks and datasets.\n",
      "* The expert interviews were limited to 10 researchers, which may not be representative of the broader NLP research community.\n",
      "\n",
      "**Future Research Directions:**\n",
      "\n",
      "* Exploring the application of domain-specific and task-specific approaches to other NLP domains, such as computer vision and speech recognition.\n",
      "* Investigating the use of multitask learning and transfer learning in knowledge-intensive NLP tasks.\n",
      "* Developing more advanced embedding techniques to represent complex knowledge structures and relationships.\n",
      "\n",
      "**Resources:**\n",
      "\n",
      "* Survey repository: https://github.com/mlgroup/jlull-meval-survey\n",
      "* Benchmarking dataset: Text-DaVinci-002002\n",
      "\n",
      "âœ… All summaries saved to 'cluster_summaries.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from keybert import KeyBERT\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# === Config ===\n",
    "CSV_FILE = \"clustered_papers.csv\"  # â† Update this to your clustered file\n",
    "TEXT_COLUMN = \"clean_abstract\"  # Changed to \"clean_abstract\" based on your columns\n",
    "CLUSTER_COLUMN = \"gmm_cluster\"  # Updated to \"gmm_cluster\" since that's the correct column\n",
    "GROQ_API_KEY = \"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\"  # â† Replace this\n",
    "\n",
    "# === Groq Setup ===\n",
    "openai.api_key = GROQ_API_KEY\n",
    "openai.api_base = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# === Load Models ===\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()\n",
    "kw_model = KeyBERT(model='bert-base-uncased')\n",
    "\n",
    "# === Step 1: Load and Group Clusters ===\n",
    "def load_clusters(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Column names:\", df.columns.tolist())  # Show column names explicitly\n",
    "    df.columns = df.columns.str.strip()  # Clean any extra spaces in column names\n",
    "    if CLUSTER_COLUMN not in df.columns:\n",
    "        raise KeyError(f\"Column '{CLUSTER_COLUMN}' not found in the CSV file.\")\n",
    "    return df.groupby(CLUSTER_COLUMN)[TEXT_COLUMN].apply(list).to_dict()\n",
    "\n",
    "# === Step 2: Extract Keywords from BERT Input ===\n",
    "def embedding_to_keywords(texts, num_keywords=10):\n",
    "    combined_text = \" \".join(texts)\n",
    "# Adjust the number of keywords\n",
    "    keywords = kw_model.extract_keywords(combined_text, top_n=20)\n",
    "    return [kw[0] for kw in keywords]\n",
    "\n",
    "# === Step 3: Build Prompt for Groq ===\n",
    "def build_prompt(keywords):\n",
    "    return (\n",
    "        \"You are a research assistant. Generate a detailed research summary \"\n",
    "        \"focusing on the research methodology and key findings based on the following concepts:\\n\\n\"\n",
    "        + \", \".join(keywords) + \"\\n\\nSummary:\"\n",
    "    )\n",
    "\n",
    "\n",
    "# === Step 4: Groq API Call ===\n",
    "def query_groq(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=800\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# === Full Pipeline for One Cluster ===\n",
    "def process_cluster(texts):\n",
    "    try:\n",
    "        keywords = embedding_to_keywords(texts)\n",
    "        prompt = build_prompt(keywords)\n",
    "        return query_groq(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error processing cluster: {e}\")\n",
    "        return \"ERROR\"\n",
    "\n",
    "\n",
    "# === Run for All Clusters ===\n",
    "def run_pipeline():\n",
    "    try:\n",
    "        clusters = load_clusters(CSV_FILE)\n",
    "        all_summaries = {}\n",
    "\n",
    "        for label, texts in clusters.items():\n",
    "            print(f\"\\nðŸ”¹ Cluster {label} ðŸ”¹\")\n",
    "            try:\n",
    "                summary = process_cluster(texts)\n",
    "                all_summaries[label] = summary\n",
    "                print(summary)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error processing cluster {label}: {e}\")\n",
    "                all_summaries[label] = \"ERROR\"\n",
    "\n",
    "        # Optional: Save output\n",
    "        with open(\"cluster_summaries.json\", \"w\") as f:\n",
    "            json.dump(all_summaries, f, indent=4)\n",
    "        print(\"\\nâœ… All summaries saved to 'cluster_summaries.json'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error loading clusters: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d6940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.5555555555555556, recall=0.45454545454545453, fmeasure=0.5), 'rouge2': Score(precision=0.125, recall=0.1, fmeasure=0.11111111111111112), 'rougeL': Score(precision=0.4444444444444444, recall=0.36363636363636365, fmeasure=0.39999999999999997)}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Example generated summary and ground truth summary\n",
    "generated_summary = \"AI models like GatorTron face challenges in plagiarism detection...\"\n",
    "ground_truth_summary = \"The GatorTron model faces plagiarism risks and challenges in content detection...\"\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"])\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "scores = scorer.score(ground_truth_summary, generated_summary)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0a1f9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scispacy in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.5.5)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (3.7.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (1.14.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (2.32.3)\n",
      "Requirement already satisfied: conllu in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (6.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (1.26.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (1.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (1.5.2)\n",
      "Requirement already satisfied: pysbd in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (0.3.4)\n",
      "Requirement already satisfied: nmslib-metabrainz==2.1.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scispacy) (2.1.3)\n",
      "Requirement already satisfied: pybind11>=2.2.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nmslib-metabrainz==2.1.3->scispacy) (2.13.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nmslib-metabrainz==2.1.3->scispacy) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->scispacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2024.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (0.15.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (4.66.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->scispacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->scispacy) (4.12.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->scispacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->scispacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->scispacy) (1.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->scispacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->scispacy) (0.1.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.20.3->scispacy) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->scispacy) (2.1.5)\n",
      "Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz\n",
      "  Downloading https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz (15.9 MB)\n",
      "     ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/15.9 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.5/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.8/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.8/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     -- ------------------------------------- 1.0/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 1.3/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 1.6/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     ---- ----------------------------------- 1.8/15.9 MB 1.1 MB/s eta 0:00:14\n",
      "     ----- ---------------------------------- 2.1/15.9 MB 1.1 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 2.1/15.9 MB 1.1 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 2.4/15.9 MB 1.1 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 2.6/15.9 MB 1.1 MB/s eta 0:00:13\n",
      "     ------- -------------------------------- 2.9/15.9 MB 1.1 MB/s eta 0:00:13\n",
      "     ------- -------------------------------- 3.1/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 3.4/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 3.4/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "     --------- ------------------------------ 3.7/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "     --------- ------------------------------ 3.9/15.9 MB 1.1 MB/s eta 0:00:12\n",
      "     ---------- ----------------------------- 4.2/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ----------- ---------------------------- 4.5/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ----------- ---------------------------- 4.7/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ------------ --------------------------- 5.0/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ------------ --------------------------- 5.0/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ------------- -------------------------- 5.2/15.9 MB 1.1 MB/s eta 0:00:11\n",
      "     ------------- -------------------------- 5.5/15.9 MB 1.1 MB/s eta 0:00:10\n",
      "     -------------- ------------------------- 5.8/15.9 MB 1.1 MB/s eta 0:00:10\n",
      "     --------------- ------------------------ 6.0/15.9 MB 1.1 MB/s eta 0:00:10\n",
      "     --------------- ------------------------ 6.3/15.9 MB 1.1 MB/s eta 0:00:10\n",
      "     ---------------- ----------------------- 6.6/15.9 MB 1.1 MB/s eta 0:00:09\n",
      "     ---------------- ----------------------- 6.6/15.9 MB 1.1 MB/s eta 0:00:09\n",
      "     ----------------- ---------------------- 6.8/15.9 MB 1.1 MB/s eta 0:00:09\n",
      "     ----------------- ---------------------- 7.1/15.9 MB 1.1 MB/s eta 0:00:09\n",
      "     ------------------ --------------------- 7.3/15.9 MB 1.1 MB/s eta 0:00:09\n",
      "     ------------------- -------------------- 7.6/15.9 MB 1.1 MB/s eta 0:00:08\n",
      "     ------------------- -------------------- 7.9/15.9 MB 1.1 MB/s eta 0:00:08\n",
      "     ------------------- -------------------- 7.9/15.9 MB 1.1 MB/s eta 0:00:08\n",
      "     -------------------- ------------------- 8.1/15.9 MB 1.1 MB/s eta 0:00:08\n",
      "     --------------------- ------------------ 8.4/15.9 MB 1.1 MB/s eta 0:00:08\n",
      "     --------------------- ------------------ 8.7/15.9 MB 1.1 MB/s eta 0:00:07\n",
      "     ---------------------- ----------------- 8.9/15.9 MB 1.1 MB/s eta 0:00:07\n",
      "     ----------------------- ---------------- 9.2/15.9 MB 1.1 MB/s eta 0:00:07\n",
      "     ----------------------- ---------------- 9.2/15.9 MB 1.1 MB/s eta 0:00:07\n",
      "     ----------------------- ---------------- 9.4/15.9 MB 1.1 MB/s eta 0:00:07\n",
      "     ------------------------ --------------- 9.7/15.9 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 10.0/15.9 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------------------- -------------- 10.2/15.9 MB 1.1 MB/s eta 0:00:06\n",
      "     -------------------------- ------------- 10.5/15.9 MB 1.1 MB/s eta 0:00:06\n",
      "     --------------------------- ------------ 10.7/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 10.7/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "     --------------------------- ------------ 11.0/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "     ---------------------------- ----------- 11.3/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "     ----------------------------- ---------- 11.5/15.9 MB 1.1 MB/s eta 0:00:05\n",
      "     ----------------------------- ---------- 11.8/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------ --------- 12.1/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 12.3/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 12.3/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "     ------------------------------- -------- 12.6/15.9 MB 1.1 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 12.8/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 13.1/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "     --------------------------------- ------ 13.4/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 13.6/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 13.6/15.9 MB 1.1 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 13.9/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 14.2/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 14.4/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 14.7/15.9 MB 1.1 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 14.9/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 15.2/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 15.2/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 15.5/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.7/15.9 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 15.9/15.9 MB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting spacy<3.3.0,>=3.2.3 (from en_core_sci_sm==0.5.0)\n",
      "  Downloading spacy-3.2.6.tar.gz (1.1 MB)\n",
      "     ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.1/1.1 MB 8.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— pip subprocess to install build dependencies did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [664 lines of output]\n",
      "      Collecting setuptools\n",
      "        Downloading setuptools-80.7.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "      Collecting cython<3.0,>=0.25\n",
      "        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "      Collecting cymem<2.1.0,>=2.0.2\n",
      "        Using cached cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "      Collecting preshed<3.1.0,>=3.0.2\n",
      "        Using cached preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "      Collecting murmurhash<1.1.0,>=0.28.0\n",
      "        Using cached murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "      Collecting thinc<8.1.0,>=8.0.12\n",
      "        Downloading thinc-8.0.17.tar.gz (189 kB)\n",
      "        Installing build dependencies: started\n",
      "        Installing build dependencies: finished with status 'done'\n",
      "        Getting requirements to build wheel: started\n",
      "        Getting requirements to build wheel: finished with status 'done'\n",
      "        Preparing metadata (pyproject.toml): started\n",
      "        Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "      Collecting blis<0.8.0,>=0.4.0\n",
      "        Using cached blis-0.7.11-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "      Collecting pathy\n",
      "        Downloading pathy-0.11.0-py3-none-any.whl.metadata (16 kB)\n",
      "      Collecting numpy>=1.15.0\n",
      "        Using cached numpy-2.2.5-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "      Collecting wasabi<1.1.0,>=0.8.1 (from thinc<8.1.0,>=8.0.12)\n",
      "        Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n",
      "      Collecting srsly<3.0.0,>=2.4.0 (from thinc<8.1.0,>=8.0.12)\n",
      "        Using cached srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "      Collecting catalogue<2.1.0,>=2.0.4 (from thinc<8.1.0,>=8.0.12)\n",
      "        Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "      Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 (from thinc<8.1.0,>=8.0.12)\n",
      "        Downloading pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\n",
      "      Collecting typing-extensions>=3.7.4.3 (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->thinc<8.1.0,>=8.0.12)\n",
      "        Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "      Collecting smart-open<7.0.0,>=5.2.1 (from pathy)\n",
      "        Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "      Collecting typer<1.0.0,>=0.3.0 (from pathy)\n",
      "        Downloading typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
      "      Collecting pathlib-abc==0.1.1 (from pathy)\n",
      "        Downloading pathlib_abc-0.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "      Collecting click<8.2,>=8.0.0 (from typer<1.0.0,>=0.3.0->pathy)\n",
      "        Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "      Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->pathy)\n",
      "        Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "      Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->pathy)\n",
      "        Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "      Collecting colorama (from click<8.2,>=8.0.0->typer<1.0.0,>=0.3.0->pathy)\n",
      "        Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "      Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->pathy)\n",
      "        Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "      Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->pathy)\n",
      "        Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "      Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->pathy)\n",
      "        Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "      Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)\n",
      "      Using cached cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "      Using cached preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "      Using cached murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "      Using cached blis-0.7.11-cp312-cp312-win_amd64.whl (6.6 MB)\n",
      "      Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "      Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "      Using cached srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "      Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "      Using cached setuptools-80.7.0-py3-none-any.whl (1.2 MB)\n",
      "      Downloading pathy-0.11.0-py3-none-any.whl (47 kB)\n",
      "      Downloading pathlib_abc-0.1.1-py3-none-any.whl (23 kB)\n",
      "      Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "      Downloading typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "      Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "      Using cached numpy-2.2.5-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "      Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "      Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "         ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "         ---------------------------------------- 1.2/1.2 MB 8.8 MB/s eta 0:00:00\n",
      "      Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "      Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "      Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "      Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "      Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "      Building wheels for collected packages: thinc\n",
      "        Building wheel for thinc (pyproject.toml): started\n",
      "        Building wheel for thinc (pyproject.toml): finished with status 'error'\n",
      "        error: subprocess-exited-with-error\n",
      "      \n",
      "        Ãƒâ€” Building wheel for thinc (pyproject.toml) did not run successfully.\n",
      "        Ã¢â€â€š exit code: 1\n",
      "        Ã¢â€¢Â°Ã¢â€â‚¬> [571 lines of output]\n",
      "            Cythonizing sources\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "                    License :: OSI Approved :: MIT License\n",
      "      \n",
      "                    See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              self._finalize_license_expression()\n",
      "            running bdist_wheel\n",
      "            running build\n",
      "            running build_py\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\about.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\api.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\config.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\initializers.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\loss.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\model.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\mypy.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\optimizers.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\schedules.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\types.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\util.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\cupy_ops.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\ops.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\_cupy_allocators.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\_custom_kernels.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\_param_server.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            copying thinc\\extra\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\add.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\array_getitem.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\bidirectional.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\cauchysimilarity.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\chain.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\clipped_linear.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\clone.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\concatenate.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\dropout.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\embed.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\expand_window.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\gelu.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\hard_swish.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\hard_swish_mobilenet.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\hashembed.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\layernorm.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\linear.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\list2array.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\list2padded.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\list2ragged.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\logistic.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\lstm.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\map_list.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\maxout.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\mish.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\multisoftmax.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\mxnetwrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\noop.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\padded2list.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\parametricattention.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\pytorchwrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\ragged2list.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_first.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_last.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_max.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_mean.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\reduce_sum.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\relu.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\remap_ids.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\residual.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\resizable.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\siamese.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\sigmoid.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\sigmoid_activation.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\softmax.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\softmax_activation.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\strings2arrays.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\swish.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\tensorflowwrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\tuplify.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\uniqued.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_array.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_array2d.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_cpu.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_debug.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_flatten.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_getitem.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_list.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_nvtx_range.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_padded.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_ragged.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\with_reshape.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\layers\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\mxnet.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\pytorch.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\pytorch_grad_scaler.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\shim.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\tensorflow.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            copying thinc\\shims\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\shims\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\conftest.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\strategies.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_config.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_examples.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_indexing.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_initializers.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_loss.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_optimizers.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_schedules.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_serialize.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_types.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\test_util.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\util.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            copying thinc\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\extra\\tests\n",
      "            copying thinc\\extra\\tests\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\extra\\tests\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\backends\n",
      "            copying thinc\\tests\\backends\\test_mem.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\backends\n",
      "            copying thinc\\tests\\backends\\test_ops.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\backends\n",
      "            copying thinc\\tests\\backends\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\backends\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\extra\n",
      "            copying thinc\\tests\\extra\\test_beam_search.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\extra\n",
      "            copying thinc\\tests\\extra\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\extra\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_basic_tagger.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_combinators.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_feed_forward.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_hash_embed.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_layers_api.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_linear.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_lstm.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_mnist.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_mxnet_wrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_pytorch_wrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_reduce.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_shim.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_softmax.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_sparse_linear.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_tensorflow_wrapper.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_transforms.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_uniqued.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_with_debug.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\test_with_transforms.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            copying thinc\\tests\\layers\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\layers\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\model\n",
      "            copying thinc\\tests\\model\\test_model.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\model\n",
      "            copying thinc\\tests\\model\\test_validation.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\model\n",
      "            copying thinc\\tests\\model\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\model\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\n",
      "            copying thinc\\tests\\mypy\\test_mypy.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\n",
      "            copying thinc\\tests\\mypy\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\n",
      "            copying thinc\\tests\\regression\\test_issue208.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\n",
      "            copying thinc\\tests\\regression\\test_issue564.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\n",
      "            copying thinc\\tests\\regression\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\shims\n",
      "            copying thinc\\tests\\shims\\test_pytorch_grad_scaler.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\shims\n",
      "            copying thinc\\tests\\shims\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\shims\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\fail_no_plugin.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\fail_plugin.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\success_no_plugin.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\success_plugin.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            copying thinc\\tests\\mypy\\modules\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\modules\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\\issue519\n",
      "            copying thinc\\tests\\regression\\issue519\\program.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\\issue519\n",
      "            copying thinc\\tests\\regression\\issue519\\test_issue519.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\\issue519\n",
      "            copying thinc\\tests\\regression\\issue519\\__init__.py -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\regression\\issue519\n",
      "            running egg_info\n",
      "            writing thinc.egg-info\\PKG-INFO\n",
      "            writing dependency_links to thinc.egg-info\\dependency_links.txt\n",
      "            writing requirements to thinc.egg-info\\requires.txt\n",
      "            writing top-level names to thinc.egg-info\\top_level.txt\n",
      "            reading manifest file 'thinc.egg-info\\SOURCES.txt'\n",
      "            reading manifest template 'MANIFEST.in'\n",
      "            no previously-included directories found matching 'tmp'\n",
      "            warning: no previously-included files matching '*.cpp' found under directory 'thinc'\n",
      "            adding license file 'LICENSE'\n",
      "            writing manifest file 'thinc.egg-info\\SOURCES.txt'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'thinc.backends' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.backends' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.backends' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.backends' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.backends' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'thinc.extra' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.extra' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.extra' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.extra' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.extra' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'thinc.layers' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.layers' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.layers' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.layers' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.layers' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'thinc.tests.mypy.configs' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.tests.mypy.configs' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.tests.mypy.configs' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.tests.mypy.configs' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.tests.mypy.configs' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'thinc.tests.mypy.outputs' is absent from the `packages` configuration.\n",
      "            !!\n",
      "      \n",
      "                    ********************************************************************************\n",
      "                    ############################\n",
      "                    # Package would be ignored #\n",
      "                    ############################\n",
      "                    Python recognizes 'thinc.tests.mypy.outputs' as an importable package[^1],\n",
      "                    but it is absent from setuptools' `packages` configuration.\n",
      "      \n",
      "                    This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "                    package, please make sure that 'thinc.tests.mypy.outputs' is explicitly added\n",
      "                    to the `packages` configuration field.\n",
      "      \n",
      "                    Alternatively, you can also rely on setuptools' discovery methods\n",
      "                    (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "                    instead of `find_packages(...)`/`find:`).\n",
      "      \n",
      "                    You can read more about \"package discovery\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "      \n",
      "                    If you don't want 'thinc.tests.mypy.outputs' to be distributed and are\n",
      "                    already explicitly excluding 'thinc.tests.mypy.outputs' via\n",
      "                    `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "                    you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "                    combination with a more fine grained `package-data` configuration.\n",
      "      \n",
      "                    You can read more about \"package data files\" on setuptools documentation page:\n",
      "      \n",
      "                    - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "      \n",
      "      \n",
      "                    [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                          even if it does not contain any `.py` files.\n",
      "                          On the other hand, currently there is no concept of package data\n",
      "                          directory, all directories are treated like packages.\n",
      "                    ********************************************************************************\n",
      "      \n",
      "            !!\n",
      "              check.warn(importable)\n",
      "            copying thinc\\__init__.pxd -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\py.typed -> build\\lib.win-amd64-cpython-312\\thinc\n",
      "            copying thinc\\backends\\linalg.cpp -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\numpy_ops.cpp -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\extra\\search.cpp -> build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            copying thinc\\layers\\sparselinear.cpp -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\backends\\__init__.pxd -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\_custom_kernels.cu -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\_murmur3.cu -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\linalg.pxd -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\linalg.pyx -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\numpy_ops.pxd -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\backends\\numpy_ops.pyx -> build\\lib.win-amd64-cpython-312\\thinc\\backends\n",
      "            copying thinc\\extra\\__init__.pxd -> build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            copying thinc\\extra\\search.pxd -> build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            copying thinc\\extra\\search.pyx -> build\\lib.win-amd64-cpython-312\\thinc\\extra\n",
      "            copying thinc\\layers\\sparselinear.pyx -> build\\lib.win-amd64-cpython-312\\thinc\\layers\n",
      "            copying thinc\\extra\\tests\\c_test_search.pyx -> build\\lib.win-amd64-cpython-312\\thinc\\extra\\tests\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\configs\n",
      "            copying thinc\\tests\\mypy\\configs\\mypy-default.ini -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\configs\n",
      "            copying thinc\\tests\\mypy\\configs\\mypy-plugin.ini -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\configs\n",
      "            creating build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\fail-no-plugin.txt -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\fail-plugin.txt -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\success-no-plugin.txt -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\outputs\n",
      "            copying thinc\\tests\\mypy\\outputs\\success-plugin.txt -> build\\lib.win-amd64-cpython-312\\thinc\\tests\\mypy\\outputs\n",
      "            running build_ext\n",
      "            building 'thinc.backends.linalg' extension\n",
      "            creating build\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\n",
      "            \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\numpy\\_core\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /EHsc /Tpthinc/backends/linalg.cpp /Fobuild\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\linalg.obj /Ox /EHsc\n",
      "            linalg.cpp\n",
      "            thinc/backends/linalg.cpp(2034): warning C4244: '=': conversion from 'double' to '__pyx_t_5thinc_8backends_6linalg_weight_t', possible loss of data\n",
      "            thinc/backends/linalg.cpp(2474): warning C4244: '=': conversion from 'double' to '__pyx_t_5thinc_8backends_6linalg_weight_t', possible loss of data\n",
      "            thinc/backends/linalg.cpp(2521): warning C4244: '=': conversion from 'double' to '__pyx_t_5thinc_8backends_6linalg_weight_t', possible loss of data\n",
      "            thinc/backends/linalg.cpp(2688): warning C4244: '=': conversion from 'double' to '__pyx_t_5thinc_8backends_6linalg_weight_t', possible loss of data\n",
      "            thinc/backends/linalg.cpp(3356): warning C4244: 'argument': conversion from 'double' to '__pyx_t_5thinc_8backends_6linalg_weight_t', possible loss of data\n",
      "            \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\bin\\HostX86\\x64\\link.exe\" /nologo /INCREMENTAL:NO /LTCG /DLL /MANIFEST:EMBED,ID=2 /MANIFESTUAC:NO /LIBPATH:C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\libs /LIBPATH:C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312 /LIBPATH:C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\PCbuild\\amd64 \"/LIBPATH:C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\lib\\x64\" \"/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\lib\\um\\x64\" \"/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\lib\\10.0.22621.0\\ucrt\\x64\" \"/LIBPATH:C:\\Program Files (x86)\\Windows Kits\\10\\\\lib\\10.0.22621.0\\\\um\\x64\" /EXPORT:PyInit_linalg build\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\linalg.obj /OUT:build\\lib.win-amd64-cpython-312\\thinc\\backends\\linalg.cp312-win_amd64.pyd /IMPLIB:build\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\linalg.cp312-win_amd64.lib\n",
      "               Creating library build\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\linalg.cp312-win_amd64.lib and object build\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\linalg.cp312-win_amd64.exp\n",
      "            Generating code\n",
      "            Finished generating code\n",
      "            building 'thinc.backends.numpy_ops' extension\n",
      "            \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\numpy\\_core\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include -IC:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.41.34120\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.22621.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\\\include\\10.0.22621.0\\\\cppwinrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /EHsc /Tpthinc/backends/numpy_ops.cpp /Fobuild\\temp.win-amd64-cpython-312\\Release\\thinc\\backends\\numpy_ops.obj /Ox /EHsc\n",
      "            numpy_ops.cpp\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Temp\\pip-build-env-vo68k0ec\\overlay\\Lib\\site-packages\\numpy\\_core\\include\\numpy\\npy_1_7_deprecated_api.h(14) : Warning Msg: Using deprecated NumPy API, disable it with #define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n",
      "            thinc/backends/numpy_ops.cpp(4627): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(4720): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(4829): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5067): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5167): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5217): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5347): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5694): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5814): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(5946): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6079): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6220): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6369): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6557): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6706): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6854): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(6990): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7143): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7247): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7268): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(7277): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(7286): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(7471): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7594): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7603): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(7612): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(7741): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(7870): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8082): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8224): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8450): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8473): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8501): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8622): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8631): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(8640): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(8721): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(8876): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8899): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(8927): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9049): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9058): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9076): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9157): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9312): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9425): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9550): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9662): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9671): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9680): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9689): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9834): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9946): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(9955): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9964): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(9973): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(10118): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(10234): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(10243): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(10252): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(10510): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(10626): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(10635): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(10644): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(10902): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11017): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11026): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11035): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11044): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11246): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11375): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11384): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11393): warning C4244: '=': conversion from 'Py_ssize_t' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11667): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11791): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(11987): warning C4244: 'argument': conversion from 'npy_intp' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(11987): warning C4244: 'argument': conversion from 'npy_intp' to 'int', possible loss of data\n",
      "            thinc/backends/numpy_ops.cpp(12087): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(12292): error C2039: 'use_tracing': is not a member of '_PyCFrame'\n",
      "            C:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\include\\cpython/pystate.h(67): note: see declaration of '_PyCFrame'\n",
      "            thinc/backends/numpy_ops.cpp(12292): fatal error C1003: error count exceeds 100; stopping compilation\n",
      "            error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.41.34120\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "            [end of output]\n",
      "      \n",
      "        note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "        ERROR: Failed building wheel for thinc\n",
      "      Failed to build thinc\n",
      "      ERROR: Failed to build installable wheels for some pyproject.toml based projects (thinc)\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Ã— pip subprocess to install build dependencies did not run successfully.\n",
      "â”‚ exit code: 1\n",
      "â•°â”€> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install scispacy\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04a14173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Import all modules ===\n",
    "import pandas as pd\n",
    "import re, json, ast, torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keybert import KeyBERT\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import openai\n",
    "import gradio as gr\n",
    "from fetch_papers import fetch_openalex_papers, save_to_csv  # Replace with your actual function\n",
    "\n",
    "# === Model/Loaders ===\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kw_model = KeyBERT(model='bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()\n",
    "\n",
    "# === Groq API Setup ===\n",
    "openai.api_key = \"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\"  # Replace with your actual key\n",
    "openai.api_base = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# === Neo4j Setup ===\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"qwertyuiop\"))\n",
    "\n",
    "\n",
    "# === Step 1: Fetch Papers ===\n",
    "def fetch_and_save_papers(query, max_results=50):\n",
    "    df = fetch_openalex_papers(query, max_results=max_results)\n",
    "    save_to_csv(df, \"papers.csv\")\n",
    "\n",
    "\n",
    "# === Step 2: Enrichment ===\n",
    "def enrich_papers():\n",
    "    df = pd.read_csv(\"papers.csv\")\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', text).strip()\n",
    "    df[\"clean_abstract\"] = df[\"abstract\"].fillna(\"\").apply(clean_text)\n",
    "    df[\"keywords\"] = df[\"keywords\"].fillna(\"[]\").apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "    df[\"entities\"] = df[\"entities\"].fillna(\"[]\").apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "    df.to_csv(\"papers_enriched.csv\", index=False)\n",
    "\n",
    "\n",
    "# === Step 3: Build Knowledge Graph ===\n",
    "def build_knowledge_graph():\n",
    "    df = pd.read_csv(\"papers_enriched.csv\")\n",
    "    graph.delete_all()\n",
    "    for _, row in df.iterrows():\n",
    "        paper_node = Node(\"Paper\", title=row[\"title\"], year=row[\"publication_year\"], doi=row[\"doi\"])\n",
    "        graph.create(paper_node)\n",
    "        for author in eval(str(row[\"authors\"])):\n",
    "            author_node = Node(\"Author\", name=author)\n",
    "            graph.merge(author_node, \"Author\", \"name\")\n",
    "            graph.create(Relationship(author_node, \"WROTE\", paper_node))\n",
    "        for keyword in row[\"keywords\"]:\n",
    "            keyword_node = Node(\"Keyword\", name=keyword)\n",
    "            graph.merge(keyword_node, \"Keyword\", \"name\")\n",
    "            graph.create(Relationship(paper_node, \"HAS_KEYWORD\", keyword_node))\n",
    "        for entity in row[\"entities\"]:\n",
    "            entity_node = Node(\"Entity\", name=entity)\n",
    "            graph.merge(entity_node, \"Entity\", \"name\")\n",
    "            graph.create(Relationship(paper_node, \"MENTIONS\", entity_node))\n",
    "\n",
    "\n",
    "# === Step 4: Cluster Abstracts ===\n",
    "def cluster_abstracts():\n",
    "    df = pd.read_csv(\"papers_enriched.csv\").dropna(subset=[\"clean_abstract\"])\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid abstracts to cluster.\")\n",
    "    embeddings = model.encode(df[\"clean_abstract\"].tolist(), show_progress_bar=True)\n",
    "    gmm = GaussianMixture(n_components=5, covariance_type='full', random_state=42)\n",
    "    gmm.fit(embeddings)\n",
    "    df[\"gmm_cluster\"] = gmm.predict(embeddings)\n",
    "    df[\"gmm_probs\"] = gmm.predict_proba(embeddings).tolist()\n",
    "    df.to_csv(\"clustered_papers.csv\", index=False)\n",
    "\n",
    "\n",
    "# === Step 5: Summarization with Groq ===\n",
    "def summarize_clusters():\n",
    "    df = pd.read_csv(\"clustered_papers.csv\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if \"gmm_cluster\" not in df.columns:\n",
    "        raise ValueError(\"Missing 'gmm_cluster' column.\")\n",
    "    clusters = df.groupby(\"gmm_cluster\")[\"clean_abstract\"].apply(list).to_dict()\n",
    "\n",
    "    def embedding_to_keywords(texts):\n",
    "        combined = \" \".join(texts)\n",
    "        return [kw[0] for kw in kw_model.extract_keywords(combined, top_n=20)]\n",
    "\n",
    "    def query_groq(prompt):\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    summaries = {}\n",
    "    for cluster_id, texts in clusters.items():\n",
    "        try:\n",
    "            keywords = embedding_to_keywords(texts)\n",
    "            prompt = (\n",
    "                \"You are a research assistant. Generate a detailed research summary \"\n",
    "                \"focusing on research methodology and key findings for:\\n\\n\"\n",
    "                + \", \".join(keywords) + \"\\n\\nSummary:\"\n",
    "            )\n",
    "            summaries[cluster_id] = query_groq(prompt)\n",
    "        except Exception as e:\n",
    "            summaries[cluster_id] = f\"ERROR: {str(e)}\"\n",
    "\n",
    "    with open(\"cluster_summaries.json\", \"w\") as f:\n",
    "        json.dump(summaries, f, indent=4)\n",
    "\n",
    "\n",
    "# === Unified Runner ===\n",
    "def run_pipeline(query):\n",
    "    try:\n",
    "        fetch_and_save_papers(query)\n",
    "        enrich_papers()\n",
    "        build_knowledge_graph()\n",
    "        cluster_abstracts()\n",
    "        summarize_clusters()\n",
    "        return \"âœ… Pipeline executed successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Pipeline failed: {e}\"\n",
    "\n",
    "\n",
    "# === UI Wrapper ===\n",
    "def run_pipeline_ui(query):\n",
    "    status = run_pipeline(query)\n",
    "\n",
    "    try:\n",
    "        df_enriched = pd.read_csv(\"papers_enriched.csv\")\n",
    "    except:\n",
    "        df_enriched = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df_clustered = pd.read_csv(\"clustered_papers.csv\")\n",
    "    except:\n",
    "        df_clustered = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        with open(\"cluster_summaries.json\") as f:\n",
    "            summaries = json.load(f)\n",
    "        summaries_str = \"\\n\\n\".join([f\"ðŸ”¹ Cluster {k}:\\n{v}\" for k, v in summaries.items()])\n",
    "    except:\n",
    "        summaries_str = \"No summary generated.\"\n",
    "\n",
    "    return (\n",
    "        status,\n",
    "        df_enriched.head(10) if not df_enriched.empty else \"No enriched data available.\",\n",
    "        df_clustered[[\"title\", \"gmm_cluster\"]].head(10) if not df_clustered.empty else \"No clustered data available.\",\n",
    "        summaries_str,\n",
    "        \"papers_enriched.csv\" if not df_enriched.empty else None,\n",
    "        \"clustered_papers.csv\" if not df_clustered.empty else None,\n",
    "        \"cluster_summaries.json\" if summaries_str else None\n",
    "    )\n",
    "\n",
    "\n",
    "# === GRADIO UI ===\n",
    "gr.Interface(\n",
    "    fn=run_pipeline_ui,\n",
    "    inputs=gr.Textbox(lines=2, label=\"Enter Research Topic\", placeholder=\"e.g., LLMs in medicine\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Pipeline Status\"),\n",
    "        gr.Dataframe(label=\"Enriched Papers Preview\"),\n",
    "        gr.Dataframe(label=\"Clustered Papers Preview\"),\n",
    "        gr.Textbox(label=\"Cluster Summaries\"),\n",
    "        gr.File(label=\"ðŸ“„ Enriched CSV\"),\n",
    "        gr.File(label=\"ðŸ“„ Clustered CSV\"),\n",
    "        gr.File(label=\"ðŸ“„ Summary JSON\")\n",
    "    ],\n",
    "    title=\"Unified Hybrid NLP Research Assistant\",\n",
    "    description=\"End-to-end research pipeline: OpenAlex â†’ Enrichment â†’ Neo4j KG â†’ Clustering â†’ Groq Summarization\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bacaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching OpenAlex for: transformers\n",
      "Request failed: HTTPSConnectionPool(host='api.openalex.org', port=443): Max retries exceeded with url: /works?filter=title.search:transformers&per-page=25&cursor=* (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate is not yet valid (_ssl.c:1000)')))\n",
      "âœ… Retrieved 0 papers.\n",
      "Saved metadata to papers.csv\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import re, json, ast, torch, os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keybert import KeyBERT\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import openai\n",
    "import gradio as gr\n",
    "from fetch_papers import fetch_openalex_papers, save_to_csv\n",
    "\n",
    "# === Model/Loaders ===\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kw_model = KeyBERT(model='bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()\n",
    "\n",
    "# === Groq API Setup ===\n",
    "openai.api_key = \"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\"  # Replace with your actual key\n",
    "openai.api_base = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "# === Neo4j Setup ===\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"qwertyuiop\"))\n",
    "\n",
    "\n",
    "# === Step 1: Fetch Papers ===\n",
    "def fetch_and_save_papers(query, max_results=50):\n",
    "    df = fetch_openalex_papers(query, max_results=max_results)\n",
    "    save_to_csv(df, \"papers.csv\")\n",
    "\n",
    "\n",
    "# === Step 2: Enrichment ===\n",
    "def enrich_papers():\n",
    "    df = pd.read_csv(\"papers.csv\")\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', text).strip()\n",
    "    df[\"clean_abstract\"] = df[\"abstract\"].fillna(\"\").apply(clean_text)\n",
    "    df[\"keywords\"] = df[\"keywords\"].fillna(\"[]\").apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "    df[\"entities\"] = df[\"entities\"].fillna(\"[]\").apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n",
    "    df.to_csv(\"papers_enriched.csv\", index=False)\n",
    "\n",
    "\n",
    "# === Step 3: Build Knowledge Graph ===\n",
    "def build_knowledge_graph():\n",
    "    df = pd.read_csv(\"papers_enriched.csv\")\n",
    "    graph.delete_all()\n",
    "    for _, row in df.iterrows():\n",
    "        paper_node = Node(\"Paper\", title=row[\"title\"], year=row[\"publication_year\"], doi=row[\"doi\"])\n",
    "        graph.create(paper_node)\n",
    "        for author in eval(str(row[\"authors\"])):\n",
    "            author_node = Node(\"Author\", name=author)\n",
    "            graph.merge(author_node, \"Author\", \"name\")\n",
    "            graph.create(Relationship(author_node, \"WROTE\", paper_node))\n",
    "        for keyword in row[\"keywords\"]:\n",
    "            keyword_node = Node(\"Keyword\", name=keyword)\n",
    "            graph.merge(keyword_node, \"Keyword\", \"name\")\n",
    "            graph.create(Relationship(paper_node, \"HAS_KEYWORD\", keyword_node))\n",
    "        for entity in row[\"entities\"]:\n",
    "            entity_node = Node(\"Entity\", name=entity)\n",
    "            graph.merge(entity_node, \"Entity\", \"name\")\n",
    "            graph.create(Relationship(paper_node, \"MENTIONS\", entity_node))\n",
    "\n",
    "\n",
    "# === Step 4: Cluster Abstracts ===\n",
    "def cluster_abstracts():\n",
    "    df = pd.read_csv(\"papers_enriched.csv\").dropna(subset=[\"clean_abstract\"])\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid abstracts to cluster.\")\n",
    "    embeddings = model.encode(df[\"clean_abstract\"].tolist(), show_progress_bar=True)\n",
    "    gmm = GaussianMixture(n_components=5, covariance_type='full', random_state=42)\n",
    "    gmm.fit(embeddings)\n",
    "    df[\"gmm_cluster\"] = gmm.predict(embeddings)\n",
    "    df[\"gmm_probs\"] = gmm.predict_proba(embeddings).tolist()\n",
    "    df.to_csv(\"clustered_papers.csv\", index=False)\n",
    "\n",
    "\n",
    "# === Step 5: Summarization with Groq ===\n",
    "def summarize_clusters():\n",
    "    df = pd.read_csv(\"clustered_papers.csv\")\n",
    "    df.columns = df.columns.str.strip()\n",
    "    if \"gmm_cluster\" not in df.columns:\n",
    "        raise ValueError(\"Missing 'gmm_cluster' column.\")\n",
    "    clusters = df.groupby(\"gmm_cluster\")[\"clean_abstract\"].apply(list).to_dict()\n",
    "\n",
    "    def embedding_to_keywords(texts):\n",
    "        combined = \" \".join(texts)\n",
    "        return [kw[0] for kw in kw_model.extract_keywords(combined, top_n=20)]\n",
    "\n",
    "    def query_groq(prompt):\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    summaries = {}\n",
    "    for cluster_id, texts in clusters.items():\n",
    "        try:\n",
    "            keywords = embedding_to_keywords(texts)\n",
    "            prompt = (\n",
    "                \"You are a research assistant. Generate a detailed research summary \"\n",
    "                \"focusing on research methodology and key findings for:\\n\\n\"\n",
    "                + \", \".join(keywords) + \"\\n\\nSummary:\"\n",
    "            )\n",
    "            summaries[cluster_id] = query_groq(prompt)\n",
    "        except Exception as e:\n",
    "            summaries[cluster_id] = f\"ERROR: {str(e)}\"\n",
    "\n",
    "    with open(\"cluster_summaries.json\", \"w\") as f:\n",
    "        json.dump(summaries, f, indent=4)\n",
    "\n",
    "\n",
    "# === Visualize Neo4j Graph ===\n",
    "def visualize_neo4j_graph():\n",
    "    query = \"\"\"\n",
    "    MATCH (n)-[r]->(m)\n",
    "    RETURN n.name AS source, type(r) AS relation, m.name AS target\n",
    "    \"\"\"\n",
    "    result = graph.run(query).data()\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for row in result:\n",
    "        G.add_edge(row[\"source\"], row[\"target\"], label=row[\"relation\"])\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos = nx.spring_layout(G, k=0.5)\n",
    "    nx.draw(G, pos, with_labels=True, node_size=800, node_color=\"skyblue\", font_size=10, font_weight='bold', edge_color=\"gray\")\n",
    "    edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"graph.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# === Unified Runner ===\n",
    "def run_pipeline(query):\n",
    "    try:\n",
    "        fetch_and_save_papers(query)\n",
    "        enrich_papers()\n",
    "        build_knowledge_graph()\n",
    "        cluster_abstracts()\n",
    "        summarize_clusters()\n",
    "        visualize_neo4j_graph()\n",
    "        return \"âœ… Pipeline executed successfully!\"\n",
    "    except Exception as e:\n",
    "        return f\"âŒ Pipeline failed: {e}\"\n",
    "\n",
    "\n",
    "# === UI Wrapper ===\n",
    "def run_pipeline_ui(query):\n",
    "    status = run_pipeline(query)\n",
    "\n",
    "    try:\n",
    "        df_enriched = pd.read_csv(\"papers_enriched.csv\")\n",
    "    except:\n",
    "        df_enriched = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        df_clustered = pd.read_csv(\"clustered_papers.csv\")\n",
    "    except:\n",
    "        df_clustered = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        with open(\"cluster_summaries.json\") as f:\n",
    "            summaries = json.load(f)\n",
    "        summaries_str = \"\\n\\n\".join([f\"ðŸ”¹ Cluster {k}:\\n{v}\" for k, v in summaries.items()])\n",
    "    except:\n",
    "        summaries_str = \"No summary generated.\"\n",
    "\n",
    "    graph_file = \"graph.png\" if os.path.exists(\"graph.png\") else None\n",
    "\n",
    "    return (\n",
    "        status,\n",
    "        df_enriched.head(10) if not df_enriched.empty else \"No enriched data available.\",\n",
    "        df_clustered[[\"title\", \"gmm_cluster\"]].head(10) if not df_clustered.empty else \"No clustered data available.\",\n",
    "        summaries_str,\n",
    "        \"papers_enriched.csv\" if not df_enriched.empty else None,\n",
    "        \"clustered_papers.csv\" if not df_clustered.empty else None,\n",
    "        \"cluster_summaries.json\" if summaries_str else None,\n",
    "        graph_file,   # for Image (display)\n",
    "        graph_file    # for File (download)\n",
    "    )\n",
    "\n",
    "\n",
    "# === GRADIO UI ===\n",
    "gr.Interface(\n",
    "    fn=run_pipeline_ui,\n",
    "    inputs=gr.Textbox(lines=2, label=\"Enter Research Topic\", placeholder=\"e.g., LLMs in medicine\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Pipeline Status\"),\n",
    "        gr.Dataframe(label=\"Enriched Papers Preview\"),\n",
    "        gr.Dataframe(label=\"Clustered Papers Preview\"),\n",
    "        gr.Textbox(label=\"Cluster Summaries\"),\n",
    "        gr.File(label=\"ðŸ“„ Enriched CSV\"),\n",
    "        gr.File(label=\"ðŸ“„ Clustered CSV\"),\n",
    "        gr.File(label=\"ðŸ“„ Summary JSON\"),\n",
    "        gr.Image(label=\"ðŸ“· Graph Preview\"),    # Show graph inline\n",
    "        gr.File(label=\"ðŸ“¥ Download Graph PNG\")  # Allow file download\n",
    "    ],\n",
    "    title=\"Unified Hybrid NLP Research Assistant\",\n",
    "    description=\"End-to-end research pipeline: OpenAlex â†’ Enrichment â†’ Neo4j KG â†’ Clustering â†’ Groq Summarization â†’ Graph Visualization\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39119cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
