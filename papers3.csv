title,authors,year,doi,full_text
"Implications of Computer Vision Driven Assistive Technologies Towards
  Individuals with Visual Impairment","['Linda Wang', 'Alexander Wong']",2019,http://arxiv.org/abs/1905.07844v1,"Implications of Computer Vision Driven Assistive Technologies Towards
Individuals with Visual Impairment
Linda Wang and Alexander Wong
Waterloo ArtiÔ¨Åcial Intelligence Institute
University of Waterloo
fly8wang,a28wong g@uwaterloo.ca
Abstract
Computer vision based technology is becoming ubiqui-
tous in society. One application area that has seen an in-
crease in computer vision is assistive technologies, speciÔ¨Å-
cally for those with visual impairment. Research has shown
the ability of computer vision models to achieve tasks such
provide scene captions, detect objects and recognize faces.
Although assisting individuals with visual impairment with
these tasks increases their independence and autonomy,
concerns over bias, privacy and potential usefulness arise.
This paper addresses the positive and negative implications
computer vision based assistive technologies have on indi-
viduals with visual impairment, as well as considerations
for computer vision researchers and developers in order to
mitigate the amount of negative implications.
1. Introduction
In recent years, the rise of deep learning has made previ-
ously unsolvable tasks possible. One particular area where
deep learning has made tremendous progress is computer
vision, such as in image recognition, object detection and
image understanding. As computer vision results are be-
coming more promising, larger issues regarding the use of
this technology need to be considered. An important area to
consider is assistive technologies for those with visual im-
pairments, as computer vision technologies have the poten-
tial to aid in tasks where previous solutions have struggled.
Although there are positive aspects of computer vision
applications, there are also negative aspects that should be
addressed. The use of black box artiÔ¨Åcial intelligence so-
lutions raises many concerns such as fairness and bias of
the models [13]. There are also ethical concerns related to
privacy protection as many computer vision models rely on
camera input. In addition, the exclusion of certain groups
during the development process may also lead to negative
aspects of the technology [15] and as a result, lead to low
adoption rates.
We thank NSERC, Canada Research Chairs program, and Microsoft.
Figure 1. Positive implications: computer vision-based devices al-
low blind individuals to navigate independently, recognize faces
and read text, which helps them overcome social barriers.
As AI is becoming more ubiquitous, it is crucial to ad-
dress issues related to the implications of AI, speciÔ¨Åcally
computer vision driven assistive technology towards indi-
viduals with visual impairment. The goal of the paper is
to review what implications computer vision has on assis-
tive technologies for individuals with visual impairment and
considerations for computer vision researchers. The paper
will be guided by the following questions:
What are the positive and negative aspects of using
computer vision in assistive technologies with respect
to the impact on the lives of individuals with visual
impairment?
What should researchers consider while conducting
computer vision research to reduce negative implica-
tions of AI-powered assistive technology on the lives
of individuals with visual impairment?
2. Positive Implications
Vision impairment and blindness cause a considerable
amount of economic and emotional burden for not only
the affected persons but also their caregivers and society at
large [11]. The recent rise in computer vision based assis-
tive technologies show the potential to reduce some burden
placed on the individuals, as well as on caregivers and so-
ciety. By assisting visually impaired individuals with tasks
1arXiv:1905.07844v1  [cs.CV]  20 May 2019
Negative Implications
Gender
Age Bias
Race/Ethnicity
Exploitation of personal
information
Obtrusiveness of camerasPrivacyTradeoff between autonomy
and privacy costs
Poor device evaluation
Age and condition dependentExclusion in
development
processInefÔ¨Åciency in development
process
Table 1. Negative implications: bias in computer vision algo-
rithms, privacy concerns related to data collection and cameras,
and exclusion in development process.
they would otherwise need help in, as shown in Figure 1,
their level of independence and autonomy are increased.
Overcoming social barriers: One area assistive tech-
nologies have become an integral part in the lives of those
with visual impairment is overcoming barriers faced in ev-
eryday life. These individuals face adversity in all stages of
life. For instance, severely visually impaired young people
use their assistive technology as more than just a device to
overcome environmental barriers but also a means of com-
munication for peers in their school [18].
Face recognition and optical character recognition:
The ever growing presence of smartphones and advance-
ments in computer vision are transforming the accessibil-
ity of assistive technologies, allowing individuals to over-
come social barriers and have autonomy over when and how
they access information. Smartphone applications, such as
SeeingAI and Lookout, use auditory cues to assist users
in identifying scenes, recognizing faces, reading short text,
documents and currency [8, 6].
Navigation assistance: Individuals with visual impair-
ment also face difÔ¨Åculty localizing themselves in unknown
indoor and outdoor environments. Research projects are us-
ing cameras and sensors to give directions so these indi-
viduals can navigate outdoor and indoor environments in-
dependently. For instance, a prototype was developed for
guiding the visually impaired across streets in a straight line
using a wearable computer-based orientation and wayÔ¨Ånd-
ing aid [16]. For indoor navigation, Tian et al. developed a
proof of concept computer-vision based indoor wayÔ¨Ånding
aid that detects doors and elevators, as well as text on signs,
to Ô¨Ånd different rooms [19].
3. Negative Implications
Although the advancement of technology is evident, only
a limited number of assistive technology solutions haveemerged to make a social or economic impact and im-
prove quality of life. Fundamental challenges, such as those
shown in Table 1, are still be to thoroughly addressed before
deploying into assistive technologies.
Bias: In machine learning, bias refers to statistics that
lead to a skew and as a result, brings an unjust outcome
for a population [13]. Bias often stems from training data
sample sets that are non-representative of the general popu-
lation. When algorithms are trained with biased data, they
are inherently bound to produce skewed results [5].
One of the biggest implications in applying AI systems
with bias is the potential for adversely impacting already
marginalized groups. In 2012, Klare et al. conducted a
study on the inÔ¨Çuence of gender, race/ethnicity and age
on the performance of six different face recognition algo-
rithms, three of which are commercial [10]. The results
found that there are lower matching accuracies for females
than males, Blacks compared to other race/ethnicities, and
18 to 30 year olds compared to other age groups.
In recent years, the low errors rates achieved by fa-
cial recognition models led to even more commercializa-
tion. However, studies have shown consistent bias in ar-
eas of gender, race and age from these commercial mod-
els. Buolamwini and Gebru evaluated bias present in
three commercial automated facial analysis algorithms from
IBM, Microsoft and Megvii with respect to phenotypic sub-
groups [5]. The results showed that there is a signiÔ¨Åcant
drop in performance of state of the art models when applied
to images of a particular gender and/or ethnicity group. For
instance, male subjects were more accurately classiÔ¨Åed than
females and lighter subjects were more accurately classiÔ¨Åed
than darker subjects. All three commercial classiÔ¨Åer per-
formed the worst on darker female subjects.
Raji and Buolamwini conducted a second audit of com-
mercial facial analysis models [14]. In this study, perfor-
mances from target companies, ones that were in the Ô¨Årst
audit, and non-target companies, Amazon and Kairos, are
presented. The results showed all targets had the greatest
reduction in error rates for female and darker faces. In terms
of non-target companies, the performance results were sim-
ilar to the Ô¨Årst audit, with the largest disparity gap between
black females and white males.
Although the awareness of disparity improved the facial
recognition models from target companies and produced a
lower error rate than non-target companies, the commercial-
ization of these models before evaluating biases and poten-
tial impacts on protected groups raises a concern.
Privacy: As shown in the Section 2, computer vision
based assistive technologies for the visually impaired allow
these individuals to gain independence and autonomy over
different aspects of their life. However, these devices also
pose privacy risks because of the vast amounts of personal
data stored. Although individuals with visual impairment
felt that smartphones help them communicate and achieve
greater independence, these devices create privacy risks be-
cause of the amount of personal data stored. As well, their
poor visual acuity makes it hard to safeguard their informa-
tion, such as if someone is around and eavesdropping [4].
Home-monitoring for older adults, who represent major-
ity of those with visual impairment [1], reliefs caregivers
burden and allows individuals with severe visual impair-
ment to live independently, but the devices for monitor-
ing also store personal data. Studies have found that older
adults are willing to have activity monitoring shared with
family members and doctors if the collected data is useful,
but expressed that the greatest concern is exploitation and
misuse of their personal health information [9].
Based on the studies, the greatest fear associated with the
collection of personal data is the concern that their collected
data could end up in the wrong hands and be misused. In
addition to the fear of personal information being exploited,
the use of cameras is obtrusive and found to elicit greater
fears than wearable solutions. In a comparison of four am-
bient intelligent systems, the camera-based behaviour and
emergency detection system was perceived with the great-
est fear and highest level of concern [9]. However, studies
have also shown that there is a tradeoff between gained au-
tonomy and privacy costs. Older adults with lower levels of
functioning are willing to accept video cameras and trade-
off the privacy lost if camera-based solution could prevent
transfer to a long term care facility [20].
The different perceptions of privacy over the use of data,
as well as the potential beneÔ¨Åts of using cameras for home
monitoring, suggest that privacy is a complex topic. Under-
standing the variables that inÔ¨Çuence privacy concerns and
how these concerns can be mediated by potential beneÔ¨Åts
are important when developing computer vision based as-
sistive technologies.
Exclusion in development process: The main goal of
assistive technologies is to improve the lives of end users.
However, when the design of form or function of the tech-
nology is poor, or when inequality exists between techno-
logical accessibility, the lives of those affected can be nega-
tively impacted, as well as perceptions of their abilities [12].
For instance, a device that has good design, usability and ac-
cessibility can be poorly evaluated. The user‚Äôs lifestyle and
aspirations have to be taken into consideration to receive a
positive user evaluation [12].
The lifestyle and desired function of assistive technolo-
gies depend on age and level of adaption to their condition.
A predominant want for young disabled people is the sig-
niÔ¨Åcance of being ordinary [18]. No matter the degree of
visual impairment, all the participants expressed that inclu-
sion by peers and being ordinary is a big part in their daily
lives [18]. In addition to age, how the user has adapted to
their condition also impacts the desired functionality of the
Figure 2. Design considerations for computer vision researchers.
assistive technology. As users become more accustomed to
their condition, they may prefer to perform some activities
independently [16].
Not only does including users during the development
process point out which areas to focus on, but also saves
development time. When testing the usability of the indoor
wayÔ¨Ånding device on blind participants, the researchers
found that the participants were able to Ô¨Ånd doors without
any problem since the participants use canes, and realized
that text localization and recognition were more useful for
indoor navigation [19]. By including the users earlier in
the development process could have identiÔ¨Åed that locating
doors are not a problem and use the saved time to address
text localization and recognition.
4. Design Considerations for Researchers
Computer vision has the potential to impact people‚Äôs
lives. However, just algorithmic advances to the accuracies
of computer vision models are insufÔ¨Åcient for assistive tech-
nologies, which interact with and around humans. Recently,
the term human-centered artiÔ¨Åcial intelligence is used to re-
fer to intelligent systems that are aware of the interaction
with humans and are designed with social responsibility in
mind [15]. As researchers, it is important to uphold soci-
ety‚Äôs moral and legal obligations to treat citizens fairly, es-
pecially those in protected groups that face discrimination.
Figure 2 illustrates some considerations to reduce the nega-
tive implications mentioned in Section 3.
Bias mitigation: One method to uphold fairness is by
mitigating bias. For instance, researchers can use tools,
such as Google‚Äôs What-If tool [3] and IBM AI Fairness 360
kit [2], to analyze and identify unwanted bias in datasets and
ML models in order to mitigate such bias. For age, gender
and ethnicity, there are different methods to reduce the neg-
ative impacts of bias. Das et al. proposed a Multi-Task Con-
volution Neural Network that employs joint dynamic loss
weight adjustments to minimize bias when classifying gen-
der, age and race [7]. There are also methods to reduce bias
at the dataset level. Salimi et al. introduced a database re-
pair algorithm, which uses causal pre-processing to reduce
or eliminate sources of discrimination for fair ML [17].
Disability discrimination: Like age, gender and race,
disability status is also a protected characteristic. How-
ever, disability discrimination has not been explored in lit-
erature [21]. Similar to under-representation of age, gender
and racial groups in datasets, as shown in Section 3, there
is also potential for under-representation of individuals with
disabilities. Ways to mitigate disability bias have also not
been explored. Compared to gender, race and age, gather-
ing a balanced training dataset is not enough to address the
biased outcomes for those with a disability [21]. The many
different forms and degrees of disability makes it difÔ¨Åcult
for a machine learning model to Ô¨Ånd patterns, form groups
and generalize. With the rise of machine learning based
assistive technologies, understanding and assessing the im-
pact towards people with disabilities is crucial, especially
since disability bias has not been widely explored.
Inclusion of end users: Taking into account where end
users will use the assistive technologies, as well as the needs
and goals, a task speciÔ¨Åc training set and appropriate model
architecture can allow computer vision based devices to be
perceived as useful, allowing individuals to gain indepen-
dence and autonomy. Based on the studies mentioned in
Section 3, users are willing to tradeoff privacy for more au-
tonomy. Thus, by including users in the development pro-
cess, the devices will be perceived as more useful, gaining
more adoption since users are willing to tradeoff privacy
concerns to have more independence and autonomy.
Diverse skill set: The ethical implications presented in
this paper are difÔ¨Åcult to address by just computer vision
researchers. Instead, a team with a diverse set of skills is
required to address both the positive and negative impli-
cations of an assistive technology. The underlying bias in
the models can cause protected groups to feel more iso-
lated. Researchers should be aware of the possible biases
the dataset and algorithm may have before the system be-
comes commercialized and interacts with people in every-
day context. In addition to bias, the use of cameras raise
privacy concerns over who has access to the data stored
and the amount of security measures taken to protect per-
sonal data. Before deployment, developers should ensure
that measures are in place to reduce the chances of data
exploitation. By understanding the needs and goals of in-
dividuals with visual impairment, designers can effectively
address these requirements in the design of the computer
vision system, resulting in a more useful device for the end
users.
References
[1] Blindness and vision impairment, Oct 2018. 3
[2] Introducing ai fairness 360, Sep 2018. 3
[3] The what-if tool, Sep 2018. 3
[4] T. Ahmed, R. Hoyle, K. Connelly, D. Crandall, and A. Ka-
padia. Privacy concerns and behaviors of people with vi-sual impairments. In Proceedings of the 33rd Annual ACM
Conference on Human Factors in Computing Systems , pages
3523‚Äì3532, 2015. 3
[5] J. Buolamwini and T. Gebru. Gender shades: Intersectional
accuracy disparities in commercial gender classiÔ¨Åcation. In
S. A. Friedler and C. Wilson, editors, Proceedings of the 1st
Conference on FAT , volume 81 of PMLR , pages 77‚Äì91, 23‚Äì
24 Feb 2018. 2
[6] P. Clary. Lookout: an app to help blind and visually impaired
people learn about their surroundings, May 2018. 2
[7] A. Das, A. Dantcheva, and F. Bremond. Mitigating Bias in
Gender, Age and Ethnicity ClassiÔ¨Åcation: a Multi-Task Con-
volution Neural Network Approach. In ECCVW 2018 , Sept.
2018. 3
[8] S. Kelley. Seeing ai: ArtiÔ¨Åcial intelligence for blind and
visually impaired users, 2019. 2
[9] F. Kirchbuchner, T. Grosse-Puppendahl, M. R. Hastall,
M. Distler, and A. Kuijper. Ambient intelligence from se-
nior citizens‚Äô perspectives. In Ambient Intelligence , pages
48‚Äì59, Cham, 2015. Springer International Publishing. 3
[10] B. F. Klare, M. J. Burge, J. C. Klontz, R. W. V order Bruegge,
and A. K. Jain. Face recognition performance: Role of de-
mographic information. IEEE Transactions on Information
Forensics and Security , 7(6):1789‚Äì1801, Dec 2012. 2
[11] J. K ¬®oberlein, K. Beifus, C. Schaffert, and R. P. Finger. The
economic burden of visual impairment and blindness: a sys-
tematic review. BMJ Open , 3(11), 2013. 1
[12] M. Leo, G. Medioni, M. Trivedi, T. Kanade, and G. Farinella.
Computer vision for assistive technologies. Computer Vision
and Image Understanding , 154:1 ‚Äì 15, 2017. 3
[13] K. Lloyd. Bias ampliÔ¨Åcation in artiÔ¨Åcial intelligence sys-
tems. CoRR , abs/1809.07842, 2018. 1, 2
[14] I. D. Raji and J. Buolamwini. Actionable auditing: Inves-
tigating the impact of publicly naming biased performance
results of commercial ai products. In Conference on AIES ,
2019. 2
[15] M. O. Riedl. Human-centered artiÔ¨Åcial intelligence and ma-
chine learning. CoRR , abs/1901.11184, 2019. 1, 3
[16] D. A. Ross. Implementing assistive technology on wear-
able computers. IEEE Intelligent Systems , 16(3):47‚Äì53, May
2001. 2, 3
[17] B. Salimi, L. Rodriguez, B. Howe, and D. Suciu. Capuchin:
Causal database repair for algorithmic fairness, 2019. 4
[18] S. S ¬®oderstr ¬®om and B. Ytterhus. The use and nonuse of as-
sistive technologies from the world of information and com-
munication technology by visually impaired young people: a
walk on the tightrope of peer inclusion. Disability & Society ,
25(3):303‚Äì315, 2010. 2, 3
[19] Y . Tian, X. Yang, C. Yi, and A. Arditi. Toward a computer
vision-based wayÔ¨Ånding aid for blind persons to access un-
familiar indoor environments. Machine vision and applica-
tions , 24:521‚Äì535, 04 2013. 2, 3
[20] D. Townsend, F. Knoefel, and R. Goubran. Privacy ver-
sus autonomy: A tradeoff model for smart home monitoring
technologies. In 2011 IEEE EMBC , pages 4749‚Äì4752, Aug
2011. 3
[21] S. Trewin. AI fairness for people with disabilities: Point of
view. CoRR , abs/1811.10670, 2018. 4"
Second Croatian Computer Vision Workshop (CCVW 2013),"['Sven Lonƒçariƒá', 'Sini≈°a ≈†egviƒá']",2013,http://arxiv.org/abs/1310.0319v3,
"Multiband NFC for High-Throughput Wireless Computer Vision Sensor
  Network","['F. Li', 'J. Du']",2017,http://arxiv.org/abs/1707.03720v1,"Multiband NFC for High-Throughput Wireless Computer Vision Sensor Network Fei Y. Li, Jason Y . Du 09212020027@fudan.edu.cn  Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR [1], non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput.     1. Computer Vision and Sensors  With emerging machine learning algorithms [1-5], computer vision tasks such as face recognition, object detection and simultaneous localization and mapping (SLAM) are now using more and more machine learning methods. In order to achieve good results, large amount of data is required to feed into algorithm. This not only requires energy-efficient computation [6-7], but energy-efficient data transfer is also necessary.   Point-to-point file exchange among smart devices has attracted people‚Äôs attention today. The smart devices support high-definition photo/video capture (some smart phones even support 4K definition photo/video capture), and many people are actually using smart devices to replace camera. As 4K photos and videos have large file size (>10 MB per photo and >300 MB for video per minute), the most convenient way to share those photos with friends is by high-speed point-to-point link. In addition to file transfer, many computer vision application categories such as embedded systems and military applications also desire to have vision sensors coupled to computation unit wirelessly. Therefore, the connection/coupling between sensor and processor can be more flexible and more reliable. The distance between sensor and processor does not necessarily to be long, and near-field communication is a good match for this case (Fig. 1).        Fig. 1 Processor and vision sensor coupled by NFC  However, data rate of NFC is usually low. Currently several standards and products have been proposed for high-speed point-to-point communication. Bluetooth 4.0 has one HS mode, which uses ad-hoc point-to-point WiFi link to transfer data. The highest data rate can reach 25 Mbps. WiFi Alliance also proposed similar WiFI Direct technology, which supports data rate as high as 250 Mbps. However, the protocols of Bluetooth and WiFi both require a long time (can be as long as >10 seconds) to establish link (searching and pairing), and the user experience is not very smooth. In addition the bandwidth of WiFi and Bluetooth is not scalable due to spectrum regulation, and the data rate upgrade of Bluetooth/WiFi is much slower than the growth of 
Vision	Sensor	
Processor	
NFC	
computer vision requirements   2. Multiband RF Interconnect for NFC  High-speed NFC has the potential to solve above issues. Per FCC regulation, NFC must work in several pre-defined license-free industry, science and medic (ISM) bands, such as 900 MHz, 2.4 GHz, 5.8 GHz, etc [8]. Each frequency band only has limited bandwidth, so the conventional NFC which only uses one frequency band has limited data rate. On the other hand, NFC limits communication range to <3 cm, and therefore output power of TX can be low enough to pass spectrum regulation. This means that NFC can use broader bandwidth for data transfer. In addition, high speed NFC can use a simple protocol for link. By selecting frequency band properly, we can make sure there is no interference from other communications. There is only one TX-RX pair within communication range, and protocol can be simplified. This means the link establishment can be very quick, and user experience for data sharing will be very smooth. The diagram using high-speed NFC is shown in Fig. 2. A designed coupler is placed between sensor and processor, and the other part of device is shielded to reduce RF power leakage.  
 Fig. 2 Processor and vision sensor interface with coupler/shielding.  On the other hand, multiband RF signaling can be used to enhance data rate. By divide data stream into multiple sub-streams and each up-converted to one ISM band, multiple data streams can be transferred simultaneously. The multiple RF signals can be combined by power combiner [9].          Fig. 3 Multi-band NFC 
RF	signal	1	(data	stream1)	
Output	RF	signal	
RF	signal	2	(data	stream2)	
3. All-Digital Transmitter (ADTX) with FPGA  An all-digital transmitter design methodology has been proposed by Li et al. in [10]. In [10], transmitter is implemented by synthesis, therefore greatly reduces the turnaround time of transmitter design. It is an ideal solution to our prototyping of high-speed NFC.  We used the same transmitter architecture as in [10], using SDM and XOR mixing to convert analog signal into high-speed RF signal. The transmitter architecture is shown in Fig. 4.  fLO,IfLO,Qmixerbuffer	chainPAI-path	dataQ-path	dataŒ£-Œî1-bit	pulsesŒ£-Œî1-bit	pulsesn-bitn-bitADTX	chip	Fig. 4 ADTX architecture  Further, we used FPGA to fast implement ADTX circuit. Also, due to flexible reconfiguration of FPGA, we can repeatedly iterate our design until design goal is met.  4. Implementation Results  We implemented our high-speed NFC system, including one coupler and one FPGA-based ADTX. The coupler is shown in Fig. 5, and its coupling loss is around 10-20 dB for near-field (Fig. 6).  
  Fig. 5. Coupler design. 

 Fig. 6 Simulation results of coupling loss  The output spectrum of NFC system is captured by spectrum analyzer, as shown in Fig. 7. We used 250MHz and 400 MHz as carrier bands, and two peaks appear in spectrum analyzer.  
 Fig. 7 Spectrum of TX output  The demodulated waveform is shown in Fig. 8. QAM-16 is used for modulation, and it can be demodulated successfully. The data rate per band is 16 Mbps, and total data rate is 32 Mbps. The data rate is limited by carrier frequency, and carrier frequency is limited by speed of FPGA. With ASIC implementation, we expect carrier frequency as high as 6 GHz [10], as total data 

rate of greater than 300 Mbps can be achieved, which can meet the requirement of computer vision.  
 Fig. 8 Demodulated QAM-16 data at 450 MHz.  Conclusion In this work, we proposed a high-speed NFC system for computer vision applications. This system includes coupler and ADTX to achieve high speed data transfer, while achieves faster prototyping for non-contact data transfer. The data rate can be further boosted with ASIC implementation.  References [1] B. Kang et al., ""Hand Segmentation for Hand-Object Interaction from Depth map"", arXiv preprint arXiv:1603.02345, 2016. [2] P. Ballester and R. Araujo, ""On the performance of GoogLeNet and AlexNet applied to sketches"", in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI Press, 2016. [3] Hongxiang Li et al., ""A convolutional neural network cascade for face detection"", in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015. [4] C. Szegedy, et al., ""Going deeper with convolutions"", in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015. [5] T. N. Sainath et al., ""Deep convolutional neural networks for large-scale speech tasks"", Neural Networks, pp.39-48, 2015. 

[6] Y.-H. Chen et al., ""Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks"", IEEE Journal of Solid-State Circuits vol. 52, no. 1, 2017. [7] C.-P. Lu, ""AI, Native Supercomputing and The Revival of Moore's Law"", arXiv preprint arXiv:1705.05983, 2017.  [8] Emission Mask/Analog Capability Requirements on Public Safety Channels [Online]. Available: https://www.fcc.gov [9] D. M. Pozar, ""Microwave Engineering"", third edition, 2005. [10] Y. L i e t a l., ""A Novel Fully Synthesizable All-Digital RF Transmitter for IoT Applications"", IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2017.  "
Deep Learning vs. Traditional Computer Vision,"[""Niall O' Mahony"", 'Sean Campbell', 'Anderson Carvalho', 'Suman Harapanahalli', 'Gustavo Velasco-Hernandez', 'Lenka Krpalkova', 'Daniel Riordan', 'Joseph Walsh']",2019,http://arxiv.org/abs/1910.13796v1,"Deep Learning  vs. Traditional Computer Vision  
Niall O‚Äô Mahony, Sean Campbell,  Anderson Carvalho, Suman Harapanahalli, 
Gustavo Velasco Hernandez, Lenka Krpalkova,  Daniel Riordan, Joseph Walsh  
IMaR Technology Gateway, Institute of Technology Tralee, Tralee, Ireland  
niall.omahony@research.ittralee.ie  
Abstract. Deep Learning  has pushed the limits of what was possible  in the 
domain of Digital Image Processing . However,  that is not  to say that the 
traditional computer vision techniques which had been undergoing progressive 
development in years prior to the rise of DL have become obsolete.  This paper 
will analyse  the benefits and drawbacks of each approach . The aim of th is paper 
is to promote a discussion on whether  knowledge of classical computer vision 
techniques should be maintained . The paper will also explore  how the two sides 
of computer vision can be combined . Several recent hybrid methodologies are 
reviewed  which have demonstrated the ability to improve computer vision 
performance  and to tackle problems not suited to Deep Learning . For example , 
combining traditional computer vision techniques with Deep Learning  has been  
popular  in emerging domains  such as Panoramic Vision and 3D vision for which  
Deep Learning models have  not yet  been fully optimised . 
Keywords:  Computer Vision , Deep Learning , Hybrid techniques . 
1 Introduction  
Deep Learning ( DL) is used in the domain of digital image processing to solve difficult 
problems ( e.g. image colourization , classification, segmentation and detection ). DL 
methods such as C onvolutiona l Neural Networks  (CNN s) mostly improve prediction 
performance using  big data and plentiful computin g resources  and have pushed the 
boundar ies of what was possible . Problems which were assumed to be unsolvable are 
now being solved with super -human accuracy . Image classification is a prime example 
of this. Since being reignited by Krizhevsky, Sutskever and Hinton in 2012 [1],  DL 
has dominated the domain ever since due to a substantial ly better performance  
compared to traditional methods  . 
Is DL making traditional Computer Vision (CV) techniques obsole te? Has DL 
superseded traditional computer vision? Is there still a need to study traditional CV 
techniques when DL seems to be so effective? These are all question s which have been 
brought up in the community  in recent years   [2], which this paper intends to address .  
Additionally , DL is not going t o solve all CV problems . There are some problems 
where  traditional techniques with global features  are a better solution . The advent of 
DL may open many  doors to do something with traditional techniques to overcome the 
many challenges DL brings  (e.g. compu ting power, time, accuracy, characteristics and 
quantity of inputs, and among others) . 
 
This paper will provide a comparison of  deep learning  to the more traditional hand -
crafted feature definition approaches which dominated CV prior to it. There has been 
so much progress in Deep Learning in recent years that it is impossible for this paper 
to capture the many facets and sub -domains of Deep Lea rning which are tackling the 
most pertinent problems in CV today. This paper will review traditional algorithmic 
approaches in CV, and more particularly, the applications in which they have been used 
as an adequate substitute for DL, to complement DL and to tackle problems DL cannot.  
The paper will then move on to review some of the recent activities in combining 
DL with  CV, with a focus on the state -of-the-art techniques for emerging technology 
such as  3D perception,  namely object registration, object det ection and semantic 
segmentation of 3D point clouds. Finally , developments and possible directions of 
getting the performance of 3D DL to the same heights as 2D DL are discussed along 
with an outlook on the impact the increased use of 3D will have on CV in  general.  
2 A Comparison of Deep Learning  and Traditional Computer 
Vision  
2.1 What is Deep Learning  
To gain a fundamental understanding of DL we need to consider the difference  between 
descriptive analysis and predictive analysis.  
Descriptive  analysis involves defining a comprehensible mathematical model which 
describes the phenomenon that we wish to observe. This entails collecting data about a 
process , forming hypotheses  on patterns in the data  and validating these hypotheses  
through comparing the out come of descriptive models we form with the real outcome 
[3]. Producing such models is precarious however because there is always a risk of un -
modelled variables that scientists and engineers neglect to include due to ignorance or 
failure to understand some complex, hidden or non -intuitive phenomena  [4].  
Predictive  analysis  involves the discovery of rules that underlie a phenomenon and 
form a predictive model which minimise the error between the actual and the predicted 
outcome considering all possible interfering factors [3]. Machine learning rejects the 
traditional programming paradigm where problem analysis is replaced by a training 
framework where the system is fed a large number of training patterns (sets of inputs 
for which the desired outputs are known) which it learns and uses t o compute new 
patterns [5].  
DL is a subset of machine learning . DL is based largely on Artificial Neural 
Networks (ANNs), a computing paradigm inspired by the functioning of the human 
brain. Like the human brain, it is composed of many computing cells or ‚Äòneurons‚Äô that 
each perform  a simple operation and interact  with each other to make a decision  [6]. 
Deep Learning is all about learning  or ‚Äòcredit assignment‚Äô across many layers of a 
neural network accurately, efficiently and without supervision and is of recent interest 
due to enabling advancements in processing hardware [7]. Self-organisation and the 
exploitation of interactions between small units have proven to perform better than 
central control, particularly for complex non -linear process models in that better fault 
tolerance and adaptability to new data is achievable [7]. 
 
2.2 Advantages  of Deep Learning  
Rapid p rogression s in DL and improvements  in device capabilities including 
computing power, memory capacity, power consumption, image sensor resolution, and 
optics have improved the performance and cost -effectiveness of  further quickened  the 
spread  of vision -based application s. Compared to traditional CV techniques, DL 
enable s CV engineers to achieve  greater accuracy in tasks such as image  classification , 
semantic segmentation , object detection  and Simultaneous Localization and Mapping 
(SLAM) . Since neural networks used in DL are trained rather than programmed,  
applications using this approach often require less expert analysis and fine-tuning  and 
exploit  the tremendous amount o f video data available in today‚Äôs systems.  DL also 
provides superior flexibility  because CNN  models  and frameworks can be re -trained 
using a custom dataset  for any use case, contrary  to CV algorithms, which tend to be 
more domain -specific.  
 
Taking  the prob lem of object detection on a mobile robot  as an example, we can  
compare  the two types of algorithms for computer vision:  
 
The traditional  approach is  to use well -established  CV techniques such as  feature 
descriptors (SIFT, SURF, BRIEF, etc.) for object detection. Before the emergence of 
DL, a step called feature extraction  was carried out for tasks such as image  
classification . Features are small ‚Äúinteresting‚Äù, descriptive or informative patches i n 
images. Several CV algorithms , such as  edge detection, corner detection  or threshold 
segmentation  may be involved in this step . As many features  as practicable are 
extracted from images and these features form a definition (known as a bag -of-words) 
of each object class . At the deployment stage , these definitions are search ed for in other 
images. If a significant number of features from one bag -of-words are in  another image, 
the image is classified as containing that specific object (i.e. chair, horse, etc .). 
The difficulty with this traditional approach is that it is necessary  to choose which 
features are important  in each given image.  As the number of classes to classify 
increases, feature extraction  becomes more and more cumbersome . It is up to the CV 
engineer‚Äôs judgment and a long trial and error process to decide which features best 
describe  different classes of objects. Moreover, each feature  definition requires dealing 
with a plethora of parameters, all of which must  be fine -tuned by  the CV engineer . 
DL introduced the concept of end -to-end learning where the machine is  just given a 
dataset of images  which have been annotated with what classes of object are present in 
each image  [7].  Thereby a DL m odel is ‚Äòtrained‚Äô on the given data , where  neural 
networks discover  the underlying patterns in classes of images and automatically works 
out the most descriptive and salient features with respect to each specific class of object 
for each object. It has been well -established  that DNN s perform  far better than 
traditional algorithms, albeit with  trade -offs with respect to computing requirements  
and training time . With  all the state -of-the-art approaches in CV employing this 
methodology, the workflow of the CV engineer has changed dramatically  where the 
knowledge and expertise in extracting hand -crafted features has been replaced by 
knowledge and expertise in iterating thro ugh deep learning architectures  as depicted in 
Fig. 1.  
 
 
Fig. 1.  (a) Traditional Computer Vision workflow vs. (b) Deep Learning wo rkflow . Figure from [8]. 
 
The development of CNNs  has had a tremendous influence in the field of CV in recent 
years and is responsible for a big jump in the ability to recognize objects  [9]. This burst 
in progress has been enabled by a n increase in computing power, as well as an increase 
in the amount of data available for training neural networks.  The recent explosion in 
and wide -spread adoption of various deep -neural network architectures for CV is 
apparent in the fact that the semina l paper ImageNet Classification with Deep 
Convolutional Neural Networks has been cited over 3000 times [2].  
CNN s make use of kernels (also known as filters), to detect features (e.g. edges) 
throughout an image. A kernel is just a matrix of values, called weights, which  are 
trained to detect specific features. As their name indicates, the main idea behind the 
CNNs  is to spatially convolve the kernel on a given input image check if the feature it 
is meant to detect is present.  To provide a value representing how confident it is that a 
specific feature is present, a convolution operation is carried out by computing the  dot 
product of the kernel and the input area where kernel is overlapped (the area of the 
original image the kernel is looking at is known as the receptive field [10]). 
To facilitate the learning of kernel  weights , the convolution layer‚Äôs output is   
summed  with a bias term and then fed to a non -linear activation function. Activation 
Functions are usually non -linear func tions like Sigmoid, TanH  and ReLU (Rectified 
Linear Unit). Depending on the nature of data and classification tasks, these activation 
functi ons are selected accordingly  [11]. For example,  ReLUs are known to have more 
biological representation  (neurons in th e brain either fire or they don‚Äôt). As a result, it 
yields favourab le results for image recognition tasks  as it is less susceptible to  the 
vanishing gradient problem and it produces sparser, more efficient representations  [7].  

To speed up the training process and reduce the amount of memory consumed by 
the network, the convolutional layer is often followed by a pooling layer to remove 
redundancy present in the input feature. For example, max pooling moves a window 
over the input and  simply outputs the maximum value in that window effectively 
reducing  to the important pixels in an image  [7]. As shown in Fig. 2 , deep CNNs may 
have several pairs of convolutional and pooling layers . Finally, a  Fully Connected 
layer flattens the previous layer volume into a feature vector and then an output layer 
which computes the scores (confidence or probabilities) for th e output classes/features 
through a dense network. This output is then passed to a regression function such as 
Softmax  [12], for example, which  maps everything to a vector whose elements sum 
up to one  [7].  
 
Fig. 2.  Building blocks of a CNN. Figure from [13] 
But DL is still only a tool of CV For example, the most common neural network used 
in CV is the  CNN. But what is a convolution? It‚Äôs in fact a widely used image 
processing technique (e.g. see  Sobel edge detection ). The advantages of DL are clear,  
and it would be beyond the scope of this paper to review the state -of-the-art. DL is 
certainly not the panacea for all problems either, as we will see in following sections of 
this paper, there are problems and applications where the more conventional CV 
algorithms are more suitable . 
 
 
2.3 Advantages of Traditional  Computer Vision Techniques  
This section will detail how the t raditional feature -based  approaches such as those listed 
below have been shown to be useful in improving performance in CV tasks : 
‚Ä¢ Scale Invariant Feature Transform (SIFT)  [14] 
‚Ä¢ Speeded Up Robust Features (SURF) [15] 
‚Ä¢ Features from Accelerated Segment Test (FAST)  [16] 
‚Ä¢ Hough transforms  [17] 
‚Ä¢ Geometric hashing  [18] 
 
Feature descriptors such as SIFT  and SURF  are generally combined with traditional 
machine learning classification algorithms such as Support Vector Machines and K -
Nearest Neighbours  to solve the aforementioned CV problems .  

DL is sometimes overkill  as often traditional CV techniques can solve a problem 
much more efficiently and in fewer lines of code than DL . Algorithms  like SIFT and 
even simple colour thresholding and pixel counting algorithms are not class -specific, 
that is, they are very general and perform the same for any image . In contrast , features 
learned from a deep neural net are specific  to your train ing dataset  which, if not well 
constructed, probably won‚Äôt perform well for images different from the training set . 
Therefore , SIFT  and other algorithms  are often used for applications such as  image 
stitching/3D mesh reconstruction  which  don‚Äôt require specific class knowledge. These 
tasks have been shown to be achievable by training large datasets , however this requires 
a huge research effort and it is not practical to go through this effort for a closed  
application . One needs to practice common sense when it comes to choosing which 
route to take for a given CV application. For example, to classify two classes of product 
on an assembly line conveyor belt , one with red paint  and one with blue paint . A deep 
neural  net will work given that enough  data can be collected to train from. However,  
the same can be achieved  by using simple colour thresholding . Some  problems can be 
tackled with  simpler and faster techniques.  
What if a DNN perform s poorly outside o f the training data ? If the training dataset 
is limited,  then the machine may overfit to the training data and  not be able to generalize 
for the task  at hand . It would be too difficult to manually tweak the parameters of the 
model  because  a DNN  has million s of parameters inside of it  each with complex inter -
relationships . In this way, DL model s have been criticised to be a black box  in this way 
[5]. Traditional CV has full transparency and the one can  judge whether your solution 
will work outside of a training environment.  The CV engineer can  have insight s into a 
problem that they can transfer to their  algorithm  and if anything fails, the parameters 
can be tweaked to perform well for a wider range of images.  
Today, the traditional techniques are used when the problem can be simplified so 
that they can be deployed on low cost  microcontrollers or to limit the problem for deep 
learning techniques by highlighting certain features in data, augmenting data  [19] or 
aiding in dataset annotation   [20]. We will discuss later in this paper how many image 
transformatio n techniques can be used to improve your neural net training. Finally, 
there  are many more cha llenging problems in CV such as: Robotic s [21], augmented 
reality  [22], automatic panorama stit ching  [23], virtual reality  [24], 3D modelling  [24], 
motion estimation  [24], video stabilization  [21], motion capture  [24], video processing  
[21] and scene understanding  [25] which cannot simply be easily implemented in a 
differentiable manner with deep learning but benefit from solutions  using ""traditional"" 
techniques.  
3 Challenges for Traditional C omputer Vision  
3.1 Mixing Hand -Crafted Approaches  with DL for Better Performance  
There are clear trade -offs between traditional CV and deep learning -based 
approaches. Classic CV algorithms are well-established , transparent , and optimized for 
performance and power efficiency, while DL offers greater accuracy and versa tility at 
the cost of  large amounts of computing resources.   
Hybrid approaches combine traditional CV and deep learning  and offer the 
advantages traits of both methodologies. They are especially practical in high 
performance  systems which need to be implem ented quickly . For example, in a security 
camera, a CV algorithm can efficiently detect faces or other features [26] or moving 
objects  [27] in the scene. These detections can then be passed to a DNN for identity 
verification or object classification . The DNN need only be applied on a small patch of 
the image  saving significant computing resources  and training effort  compared to what 
would be required to process the entire frame.  
The fusion of Machine Learning metr ics and Deep Network have become very 
popular, due to the simple fact that it can generate better models.  Hybrid vision 
processing implementation s can introduce  performance advantage and ‚Äò can deliver a 
130X -1,000X reduction in multiply -accumulate operation s and about 10X 
improvement in frame rates compared to a pure DL solution. Furthermore, the hybrid 
implementation uses about half of the memory bandwidth and requires significantly 
lower CPU resources ‚Äô [28].  
3.2 Overcoming the Challenges of Deep Learning  
There are also challenges introduced by DL. The latest DL approaches may achieve 
substantially better accuracy;  however  this jump comes at the cost of billions of 
additional math operations and an increased requirement for processing power.  DL 
requires a these  computing resources f or training and to a lesser extent for inference.  It 
is essential to have dedicated hardware (e.g. high -powered GPUs [29] and TPUs [30] 
for training and AI accelerated platforms such as VPUs for inference [31]) for 
developers of AI.  
Vision process ing results using DL are also dependent on image resolution. 
Achieving adequate performance in object classification, for example, requires high-
resolution  images or video ‚Äì with the consequent increase in the amount of data that 
needs to be processed, sto red, and transferred. Image resolution is especially important 
for applications in which it is necessary to detect and classify objects in the distance , 
e.g. in security camera  footage . The frame reduction techniques discussed previously 
such as using SIFT  features [26, 32]  or optical flow for moving objects [27] to first 
identify a region of interest are useful with respect to image resolution and also with 
respect to reducing t he time and data required for training . 
DL needs big data.  Often millions of data records are required. For example, 
PASCAL  VOC Dataset consists of 500K images with 20 object categories  [26][33], 
ImageNet consists of 1.5 million images with 1000 object categories  [34] and Microsoft 
Common Objects in Context (COCO) consists of 2.5 million images with 91 object 
categories  [35]. When big datasets  or high computing facility are unavailable,  
traditional methods will come into play.  
Training a DNN takes a very long time. Depending on computing hardware 
availability , training can take a matter of hours or days. Moreover, training for any 
given application often requires many iterations  as it entails trial and error with  different 
training parameters. The most common technique to reduce training time is tr ansfer 
learning [36]. With respect to traditional CV, the discrete Fourier transform is another 
CV technique which once experienced major popularity but now seems obscure. The 
algorithm can be used to speed up convolutions  as  demonstrated by [37, 38]  and hence 
may again become of major importance.   
However, it must be said that e asier , more domain -specific  tasks than general image 
classification will not re quire as much data (in the order of hundreds or thousands rather 
than millions) . This is still a considerable amount of data and CV techniques  are often 
used to boost training data  through  data augmentation or reduce the data down to a 
particular type of f eature through other pre -processing steps . 
Pre-processing entails transforming the data (usually with traditional CV techniques) 
to allow relationships/patterns to be more easily interpreted before training your model. 
Data augmentation is a common pre -processing task which is used when there is limited 
training data. It can involve performing random rotations, shifts, shears, etc. on the 
images in your training set to effectively increase the number of training images  [19]. 
Another approach is to highlight features of interest before passing the data to a CNN 
with CV -based methods such as background subtraction and segmentation [39]. 
3.3 Making Best Use of Edge Computing  
If algorithms and neural network inferences can be run at the edge, latency, costs, cloud 
storage and processing requirements, and bandwidth requirements are reduced 
compared to cloud -based implementations. Edge computing can also privacy and 
security requirements by avoiding transmission  of sensitive or identifiable data over the 
network.  
Hybrid or composite approaches involving conventional  CV and DL  take great 
advantage of the heterogeneous computing capabilities  available at the edge . A 
heterogeneous compute  architecture consists of a combination of CPUs, 
microcontroller coprocessors, Digital Signal Processors (DSPs), Field Programmable 
Gate Arrays (FPGAs) and AI accelerating devices [31] and can be power efficient by  
assigning different workloads to the most efficient compute engine. Test 
implementations show 10x latency reductions in object detection when DL inferences 
are executed on a DSP versus a CPU  [28].  
Several hybrids  of deep learning and hand -crafted features based approach es have 
demonstrated their benefits in edge applications. For example,  for facial -expression 
recognition , [41] propose a new feature lo ss to embed the information of hand -crafted 
features into the training process of network, which tries to reduce the difference 
between  hand -crafted features and features learned by  the deep neural network . The use 
of hybrid approaches has also been shown to be advantageous in incorporating data 
from other sensors on edge nodes. Such a  hybrid model where th e deep learning is 
assisted by additional sensor sources like synthetic aperture radar (SAR) imagery and 
elevation like synthetic aperture radar (S AR) imagery and elevation  is presented by 
[40]. In the context of 3D robot vision, [42] have shown that combining both linear 
subspace methods and deep convolutional prediction achieves improved performance 
along with several orders of magnitude faster runtime performance compared  to the 
state of the art.  
3.4 Problems Not Suited to Deep Learning  
There are many more changing problems in CV such as: Robotic, augmented reality, 
automatic panorama stitching, virtual reality, 3D modelling, motion stamation , video 
stabilization, motion captu re, video processing and scene understanding which cannot 
simply be easily implemented in a differentiable manner with deep learning but need 
to be solved using the other ""traditional"" techniques.  
DL excels at solving closed -end classification problems, in  which a wide range of 
potential signals must be mapped onto a limited number of categories, given that there 
is enough data available and the test set closely resembles the training set. However, 
deviations from these assumptions can cause problems and it  is critical to acknowledge 
the problems which DL is not good at. Marcus et al. present ten concerns for deep 
learning, and suggest that deep learning must be supplemented by other techniques if 
we are to reach artificial general intelligence  [43]. As well  as discussing the limitations 
of the training procedure and intense computing and data requirements as we do in our 
paper, key to their discussion is identifying problems where DL performs poorly and 
where it can be supplemented by other techniques .  
One such problem is the limited ability  of DL algorithms  to learn visual relations , 
i.e. identifying whether multiple objects in an image are the same or different. This 
limitation has been demonstrated by [43] who argue that feedback mechanisms 
including attention and perceptual grouping may be the key computational components 
to realising  abstract visual reasoning.   
It is also worth noting that ML models find it difficult to deal with priors, that is, not 
everythin g can be learnt from data,  so some priors must be injected into the models  
[44], [45]. Solutions that have to do with 3D CV need strong priors in order to work 
well, e.g. image -based 3D modelling requires smoothness, silhouette and illumination 
information [46].  
Below are  some emerging fields in CV where  DL faces new challenges and where  
classic CV will have a more prominent role.  
3.5 3D Vision  
3D vision systems are becoming increasingly accessible and as such there has been a 
lot of progress in the design of 3D Convolutional Neural Networks (3D CNNs). This 
emerging field is known as Geometric Deep Learning and has multiple applications 
such as video classification, computer graphics, vision and robotics. This paper will 
focus on 3DCNNs for processing data from 3D Vision System s. Wherein 2D 
convolutional layers the kernel has the  same depth so as to output a 2D matrix, the 
depth of a 3D convolutional kernel must be less than that of the 3D input volume so 
that the output of the convolution is also 3D and so preserve the spatial information.  
 
Fig. 3.  2DCNN vs. 3D CNN [47] 
 
The size of the input is much larger in terms of memory than conventional RGB images 
and the kernel m ust also be convolved through the input space in 3 dimensions  (see Fig. 
3). As a result, the computational complexity of 3D CNNs grows cubically with 
resolution.  Compared to 2D image processing, 3D CV is made even more difficult as 
the extra dimension introduces more u ncertainties, such as occlusions and different 
cameras angles as shown in Fig. 4 . 
 
 
 
Fig. 4.  3D object detection in point clouds is a challenging problem due to discrete sampling, noisy scans, 
occlusions and cluttered scenes. Figure from [48]. 
FFT based methods can optimise 3D CNNs reduce the amount of computation, at the 
cost of increased memory requirements however . Recent research h as seen the 

implementation of the Winograd Minimal Filtering Algorithm (WMFA) achieve a two -
fold speedup compared to cuDNN  (NVIDIA‚Äôs language/API for programming on 
their graphics cards)  without increasing the required memory [49].  The next section 
will include some solutions with novel architectures and pre -processing steps to various 
3D data representations which have been proposed to overcome these challenges.  
Geometric Deep Learning (GDL) deals with the extension of DL techniques to 3D 
data. 3D data can be represented in a variety of different ways  which can be classified 
as Euclidean  or non-Euclidean [50].3D Euclidean -structured data has an underlying 
grid structure that allows for a global parametrization and having a common  system of 
coordinates  as in 2D images . This allows  existing 2D DL paradigms and 2DCNNs can 
be applied to 3D data .  3D Euclidean data is more suitable for analysing  simple rigid 
objects such as, chairs, planes, etc  e.g. with voxel -based approaches  [51]. On the other 
hand, 3D non -Euclidean data do not have the grid ded array structure where there is no 
global parametrization. Therefore, extending classical DL techniques to such 
representations is a challenging task  and has only recently been realized with 
architectures such as Point net [52].  
 Continuous shape information that is useful for recognition is often lost in their 
conversion to a voxel representation. W ith respect to tradit ional CV algorithms,  [53] 
propose a single dimensional feature that can be applied to vo xel CNNs. A novel 
rotation -invariant feature based on mean curvature that improves shape recognition for 
voxel CNNs  was proposed . The method was very suc cessful in that when it was applied 
to the state-of-the-art recent voxel CNN Octnet  architecture a 1% o verall accuracy 
increase on the ModelNet10 dataset  was achieved .  
 
3.6 SLAM  
Visual SLAM is a subset  of SLAM where a vision system is used instead of LiDAR for 
the registration of landmarks in a scene. Visual SLAM has the advantages of 
photogrammetry (rich visual data, low -cost, lightweight and low power consumption) 
without the associated heavy computat ional workload involved in post -processing. The 
visual SLAM problem consists of steps such as environment sensing, data matching, 
motion estimation, as well as location update and registration of new landmarks  [54]. 
Building a model of how visual objects appear in different conditions such as 3D 
rotation, scaling, lighting and extending from that representation using a strong form of 
transfer learning to achieve zero/ one shot  learning  is a challenging problem in this 
domain. Feature extraction and data representation  method s can be useful to reduce the 
amount of training examples needed for an ML model  [55].  
A two -step a pproach is commonly used in image based  localization ; place 
recognition followed by pose estimation . The former computes  a global descriptor for 
each of the images by aggregating local image descriptors, e.g. SIFT, using the bag -of-
words approac h. Each glo bal descriptor is stored in the database together with the 
camera pose of its associated image with respect to the 3D point cloud reference map. 
Similar global descriptor s are extracted from the query image and the closest global 
descriptor  in the database  can be retrieved via an efficient search. The camera pose of 
the closest global descriptor would give us a coarse localization of the query image with 
respect to the reference map. In pose estimation, the exact pose of the query image 
calculated more prec isely with algorithms such as the Perspective -n-Point (PnP) [13] 
and geometric verification [18] algorithms. [56] 
The success of image based  place recognition is largely attributed to the ability to 
extract image feature descriptors. Unfortunately, there is no algor ithm to extract local 
features similar to SIFT for LiDAR scans. A 3D scene is composed of 3D points and 
database images. One approach has associated e ach 3D point to a set of SIFT 
descriptors corresponding to the image features from which the point was tri angulated. 
These descriptors can then be averaged into a single SIFT descriptor that describes the 
appearance of that point  [57]. 
Another approach constructs multi -modal f eatures from RGB -D data rather than the 
depth processing. For the depth processing part, they adopt the well-known  
colourization method based on surface normals, since it has been proved to be effective 
and robust across tasks  [58]. Another alternative approach ut ilizing traditional CV 
techniques  present s the Force Histogram Decomposition (FHD), a graph -based 
hierarchical descriptor that allows the spatial relations and shape information between 
the pairwise structural subparts of objects  to be characterized . An ad vantage of this 
learning procedure is its compatibility with traditional bags -of-features frameworks, 
allowing for hybrid representations gathering structural and local features  [59]. 
 
3.7 360 camera s 
A 360 camera, also known as  an  omnidirectional or spherical  or panoramic  camera is 
a camera with a 360-degree  field of view  in the horizontal plane, or with a visual field 
that covers (approximately) the entire sphere. Omnidirectional cameras are important 
in applications suc h as robotics  where large visual field coverage is needed . A 360 
camera can replace multiple monocular cameras  and eliminate blind spots which 
obviously advantageous in omnidirectional Unmanned Ground Vehicles (UGVs) and 
Unmanned Aerial Vehicles (UAVs).  Thanks to the imaging characteristic of spherical 
cameras, each image captures the 360‚ó¶ panorama  of the scene, eliminating the 
limitation on available steering choices. One of the major challenges with spherical 
images is the heavy barrel distortion due to t he ultra-wide -angle fisheye lens, which 
complicates the implementation of conventional human vision inspired methods such 
as lane detection and trajectory tracking. Additional pre-processing  steps such as prior 
calibration and deworming  are often required.  An alternative approach which has been 
presented by [60], who circumvent  these pre-processing  steps by formulating 
navigation as a classification problem on finding the optimal potential path orientation 
directly based on the raw, uncalibrated spherical images.   
Panorama stitching is an other open research problem  in this area.  A real -time 
stitchi ng methodology [61]  uses a group of deformable meshes  and the final image  and 
combine the inputs using a robust pixel -shader. Another approach  [62], combine the 
accuracy provided by geometric reasoning (lines and vanishing points) with the higher 
level of data abstraction and pattern recognition achieved by DL techniques (edge and 
normal maps)  to extract structural and generate layout hypotheses for indoor scenes . In 
sparsely structured scenes , feature -based image alignment methods often fail due to 
shortage  of distinct image features. Instead, direct image alignment methods, such as 
those based on phase correlation, can be applied. Correlation -based image alignment 
techniques based on discriminative correlation filters (DCF)  have been investigat ed 
by [23] who show that the proposed DCF -based methods outper form phase 
correlation -based approaches on these datasets.  
3.8 Dataset Annotation  and Augmentation  
There are arguments against the combination of CV and DL and they summarize to the 
conclusion that we need to re -evaluate our methods from rule -based to data -driven. 
Traditionally, from the perspective of signal processing, we know the operational 
connotations of CV algorithms such as SIFT and SURF method s, but DL leads such 
meaning nowhere, all you need is more data. This can be seen as a huge step forward,  
but may be  also a backward move. Some of the pros and cons of each side of this debate 
have been discussed already in this paper, however, if future -methods are to be purely 
data-driven then focus should be placed on more intelligent methods for dataset 
creation.  
 The fundamental problem of current research is that there is no longer enough data 
for advanced algorithms or models for special applications . Coupling custom datasets 
and DL models  will be the future theme to many research papers. So many researchers‚Äô 
outputs consist of not only algorithms or architectures, but also datasets or methods  to 
amass data. Dataset annotation is a major bottleneck  in the DL workflow which requires 
many hours of manual labelling. Nowhere is this more problematic than in  semantic 
segmentation  applications where every pixel needs to be annotated accurately. There 
are many useful tools available to semi -automate the process  as reviewed by [20], many  
of which take advantage of algorith mic approaches  such as ORB features   [55], polygon 
morphing [63], semi -automatic Area of Interest (AOI) fitting  [55] and all of the above 
[63]. 
The easiest and most common method to overcome limited datasets and reduce 
overfitting of deep learning models for image classification is to artificially enlarge the 
dataset using label -preserving transformations . This process is known as dataset 
augmentatio n and it involves the artificial generation of extra training data from the 
available ones, for example, by cropping, scaling, or rotating images [64]. It is desirable 
for data augmentation procedures to require very little computation  and to be 
implementable within the DL training pipeline so that the transformed images do not 
need to be stor ed on disk . Traditional algorithmic approaches that have been employed 
for dataset augmentation include Principle Component Analysis (PCA) [1], adding 
noise, interpolating or extrapolating  between samples in a feature space [65] and 
modelling  the visual context surrounding objects from segmentation annotations [66]. 
Conclusion  
A lot of the CV techniques invented in the past 20 years have become irrelevant in 
recent years because of DL. However, knowledge is never obsolete and there is always 
something worth learning from e ach generation of innovation. That knowledge can give 
you more intuitions and tools to use especially when you wish to deal with 3D CV 
problems for example. Knowing only DL for CV will dramatically limit the kind of 
solutions in a CV engin eer‚Äôs arsenal.  
In this paper  we have laid down m any arguments for why traditional CV techniques 
are still very much useful  even in the age of DL . We have compared and contrasted 
traditional CV and DL for typical  applications and d iscussed how s ometimes traditional 
CV can be considered as an alternative in situations  where DL  is overkill for a specific 
task.  
The paper also highlight ed some areas where traditional CV techniques remain 
relevant  such as being utilized in hybrid approaches to improv e performance . DL 
innovations are driving exciting breakthroughs for the IoT  (Internet of Things) , as well 
as hybrid techniques that combine the technologies with traditional algorithms. 
Additionally, we reviewed how traditional CV techniques can actually improve DL 
performance in a wide range of applications from reducing training time, processing 
and data requirements to being applied in emerging fields such as SLAM, Panoramic -
stitching, Geometric Deep Learning and 3D vision where DL  is not yet well 
established . 
The digital image processing domain has undergone some very dramatic changes 
recently and in a very short period . So much so it has led us to question whether the CV 
techniques that were in vogue prior to the AI explosion are still relevant. This paper 
hopefully highlight  some cases where traditional CV techniques are useful and that 
there is something still to gain from the years of effort put in to their develop ment even 
in the age of data -driven intelligence.   
References  
1.  Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet Classification with Deep 
Convolutional Neural Networks. NIPS‚Äô12 Proc 25th Int Conf Neural Inf Process Syst 
1:1097 ‚Äì1105  
2.  Nash W, Drummond T, Birbilis N (2018) A Review of Deep Learning in the Study of 
Materials Degradation. npj Mater Degrad 2:37. https://doi.org/10.1038/s41529 -018-
0058 -x 
3.  Bonaccorso G (2018) Machine Learning Algorithms Popular Algorithms for Data 
Science and Machine Learning, 2nd Editi on. Packt Publishing Ltd  
4.  Mahony NO, Murphy T, Panduru K, et al (2017) Improving controller performance in 
a powder blending process using predictive control. In: 2017 28th Irish Signals and 
Systems Conference (ISSC). IEEE, pp 1 ‚Äì6 
5.  O‚ÄôMahony N, Murphy  T, Panduru K, et al (2017) Real -time monitoring of powder blend 
composition using near infrared spectroscopy. In: 2017 Eleventh International 
Conference on Sensing Technology (ICST). IEEE, pp 1 ‚Äì6 
6.  O‚Äô Mahony N, Murphy T, Panduru K, et al (2016) Adaptive  process control and sensor 
fusion for process analytical technology. In: 2016 27th Irish Signals and Systems 
Conference (ISSC). IEEE, pp 1 ‚Äì6 
7.  Koehn P, Koehn P (1994) Combining Genetic Algorithms and Neural Networks: The 
Encoding Problem  
8.  Wang J, Ma Y, Zhang L, Gao RX (2018) Deep learning for smart manufacturing: 
Methods and applications. J Manuf Syst 48:144 ‚Äì156. 
https://doi.org/10.1016/J.JMSY.2018.01.003  
9.  Voulodimos A, Doulamis N, Doulamis A, Protopapadakis E (2018) Deep Learning for 
Computer Visi on: A Brief Review. Comput Intell Neurosci 2018:1 ‚Äì13. 
https://doi.org/10.1155/2018/7068349  
10.  Dumoulin V, Visin F, Box GEP (2018) A guide to convolution arithmetic for deep 
learning. arXiv Prepr arXiv arXiv160307285v2  
11.  Hayou S, Doucet A, Rousseau J ( 2018) On The Selection of Initialization and Activation 
Function for Deep Neural Networks. arXiv Prepr arXiv 180508266v2  
12.  Horiguchi S, Ikami D, Aizawa K (2017) Significance of Softmax -based Features in 
Comparison to Distance Metric Learning -based Featu res 
13.  Adit Deshpande A Beginner‚Äôs Guide To Understanding Convolutional Neural Networks 
‚Äì Adit Deshpande ‚Äì CS Undergrad at UCLA ('19). https://adeshpande3.github.io/A -
Beginner%27s -Guide -To-Understanding -Convolutional -Neural -Networks/. Accessed 
19 Jul 201 8 
14.  Karami E, Shehata M, Smith A (2017) Image Identification Using SIFT Algorithm: 
Performance Analysis against Different Image Deformations  
15.  Bay H, Tuytelaars T, Van Gool L (2006) SURF: Speeded Up Robust Features. Springer, 
Berlin, Heidelberg, pp 4 04‚Äì417 
16.  Rosten E, Drummond T (2006) Machine Learning for High -Speed Corner Detection. 
Springer, Berlin, Heidelberg, pp 430 ‚Äì443 
17.  Goldenshluger A, Zeevi A (2004) The Hough Transform Estimator. 32:. 
https://doi.org/10.1214/009053604000000760  
18.  Tsai FCD (1994) Geometric hashing with line features. Pattern Recognit 27:377 ‚Äì389. 
https://doi.org/10.1016/0031 -3203(94)90115 -5 
19.  Wang J, Perez L The Effectiveness of Data Augmentation in Image Classification using 
Deep Learning  
20.  Sch√∂ning J, Faion P, He idemann G (2016) Pixel -wise Ground Truth Annotation in 
Videos - An Semi -automatic Approach for Pixel -wise and Semantic Object Annotation. 
In: Proceedings of the 5th International Conference on Pattern Recognition Applications 
and Methods. SCITEPRESS - Science and and Technology Publications, pp 690 ‚Äì697 
21.  Zhang X, Lee J -Y, Sunkavalli K, Wang Z (2017) Photometric Stabilization for Fast -
forward Videos  
22.  Alhaija HA, Mustikovela SK, Mescheder L, et al (2017) Augmented Reality Meets 
Computer Vision‚ÄØ: Effici ent Data Generation for Urban Driving Scenes  
23.  Meneghetti G, Danelljan M, Felsberg M, Nordberg K (2015) Image Alignment for 
Panorama Stitching in Sparsely Structured Environments. Springer, Cham, pp 428 ‚Äì439 
24.  Alldieck T, Kassubeck M, Magnor M (2017) Optical Flow -based 3D Human Motion 
Estimation from Monocular Video  
25.  Zheng B, Zhao Y, Yu J, et al (2015) Scene Understanding by Reasoning Stability and 
Safety. Int J Comput Vis 112:221 ‚Äì238. https://doi.org/10.1007/s11263 -014-0795 -4 
26.  Zheng L, Yang Y,  Tian Q SIFT Meets CNN: A Decade Survey of Instance Retrieval  
27.  AlDahoul N, Md Sabri AQ, Mansoor AM (2018) Real -Time Human Detection for 
Aerial Captured Video Sequences via Deep Models. Comput Intell Neurosci 2018:1 ‚Äì
14. https://doi.org/10.1155/2018/1639 561 
28.  Conventional computer vision coupled with deep learning makes AI better | Network 
World. https://www.networkworld.com/article/3239146/internet -of-
things/conventional -computer -vision -coupled -with-deep -learning -makes -ai-
better.html. Accessed 12 Sep 2018  
29.  Bahrampour S, Ramakrishnan N, Schott L, Shah M (2015) Comparative Study of Deep 
Learning  Software Frameworks  
30.  (2017) An in -depth look at Google‚Äôs first Tensor Processing Unit (TPU) | Google Cloud 
Big Data and Machine Learning Blog | Google Cloud Platform. 
https://cloud.google.com/blog/big -data/2017/05/an -in-depth -look-at-googles -first-
tensor-processing -unit-tpu. Accessed 11 Jan 2018  
31.  Vision Processing Unit | Machine Vision Technology | Movidius. 
https://www.movidius.com/solutions/vision -processing -unit. Accessed 11 Jan 2018  
32.  Ng H -W, Nguyen D, Vonikakis V, Winkler S Deep Learning for Emotion Recognition 
on Small Datasets Using Transfer Learning. https://doi.org/10.1145/2818346.2830593  
33.  Pepik B, Stark M, Gehler P, Schiele B (2012) Teaching 3D geometry to deformable part 
models. In: Proceedings of the IEEE Computer Socie ty Conference on Computer Vision 
and Pattern Recognition  
34.  Russakovsky O, Deng J, Su H, et al (2015) ImageNet Large Scale Visual Recognition 
Challenge. Int J Comput Vis 115:211 ‚Äì252. https://doi.org/10.1007/s11263 -015-0816 -y 
35.  Lin T -Y, Maire M, Belong ie S, et al (2014) Microsoft COCO: Common Objects in 
Context  
36.  CS231n Convolutional Neural Networks for Visual Recognition. 
http://cs231n.github.io/transfer -learning/. Accessed 9 Mar 2018  
37.  Highlander TC Efficient Training of Small Kernel Convolution al Neural Networks 
using Fast Fourier Transform  
38.  Highlander T, Rodriguez A (2016) Very Efficient Training of Convolutional Neural 
Networks using Fast Fourier Transform and Overlap -and-Add 
39.  Li F, Wang C, Liu X, et al (2018) A Composite Model of Woun d Segmentation Based 
on Traditional Methods and Deep Neural Networks. Comput Intell Neurosci 2018:1 ‚Äì
12. https://doi.org/10.1155/2018/4149103  
40.  Nijhawan R, Das J, Raman B (2018) A hybrid of deep learning and hand -crafted features 
based approach for snow cover mapping. Int J Remote Sens 1 ‚Äì15. 
https://doi.org/10.1080/01431161.2018.1519277  
41.  Zeng G, Zhou J, Jia X, et al (2018) Hand -Crafted Feature Guided Deep Learning for 
Facial Expression Recognition. In: 2018 13th IEEE International Conference on 
Automa tic Face & Gesture Recognition (FG 2018). IEEE, pp 423 ‚Äì430 
42.  Burchfiel B, Konidaris G Hybrid Bayesian Eigenobjects: Combining Linear Subspace 
and Deep Network Methods for 3D Robot Vision  
43.  Marcus G, thank Christina I, Chollet F, et al Deep Learning: A Critical Appraisal  
44.  Nalisnick E, Smyth P (2018) Learning Priors for Invariance. 366 ‚Äì375 
45.  Diligenti M, Roychowdhury S, Gori M (2017) Integrating Prior Knowledge into Deep 
Learning. In: 2017 16th IEEE International Conference on Machine Learning an d 
Applications (ICMLA). IEEE, pp 920 ‚Äì923 
46.  Zhu H, Nie Y, Yue T, Cao X (2017) The role of prior in image based 3D modeling: a 
survey. Front Comput Sci 11:175 ‚Äì191. https://doi.org/10.1007/s11704 -016-5520 -8 
47.  Tran D, Bourdev L, Fergus R, et al (2015) Le arning Spatiotemporal Features with 3D 
Convolutional Networks. arXiv Prepr arXiv 14120767  
48.  Pang G, Neumann U (2016) 3D point cloud object detection with multi -view 
convolutional neural network. In: 2016 23rd International Conference on Pattern 
Recognit ion (ICPR). IEEE, pp 585 ‚Äì590 
49.  Lan Q, Wang Z, Wen M, et al (2017) High Performance Implementation of 3D 
Convolutional Neural Networks on a GPU. Comput Intell Neurosci 2017:1 ‚Äì8. 
https://doi.org/10.1155/2017/8348671  
50.  Ahmed E, Saint A, Shabayek AER, et  al (2018) Deep Learning Advances on Different 
3D Data Representations: A Survey. arXiv Prepr arXiv 180801462  
51.  Zhou Y, Tuzel O (2017) VoxelNet: End -to-End Learning for Point Cloud Based 3D 
Object Detection. arXiv Prepr arXiv 171106396  
52.  Qi CR, Yi L,  Su H, Guibas LJ (2017) PointNet++: Deep Hierarchical Feature Learning 
on Point Sets in a Metric Space. arXiv Prepr arXiv170602413v1  
53.  Braeger S, Foroosh H (2018) Curvature Augmented Deep Learning for 3D Object 
Recognition. In: 2018 25th IEEE Internatio nal Conference on Image Processing (ICIP). 
IEEE, pp 3648 ‚Äì3652  
54.  Niall O‚Äô Mahony (Institute of Technology Tralee), Sean Campbell (Institute of 
Technology Tralee), Lenka Krpalkova (Institute of Technology Tralee), et al (2018) 
Deep Learning for Visual Nav igation of Unmanned Ground Vehicles; A review  
55.  Karami E, Prasad S, Shehata M Image Matching Using SIFT, SURF, BRIEF and ORB: 
Performance Comparison for Distorted Images  
56.  Angelina M, Gim U, Lee H PointNetVLAD: Deep Point Cloud Based Retrieval for 
Large -Scale Place Recognition  
57.  Camposeco F, Cohen A, Pollefeys M, Sattler T Hybrid scene Compression for Visual 
Localization  
58.  Loghmani MR, Planamente M, Caputo B, Vincze M Recurrent Convolutional Fusion 
for RGB -D Object Recognition  
59.  Cl√©ment M, Kurtz C, Wendling L (2018) Learning spatial relations and shapes for 
structural object description and scene recognition. Pattern Recognit 84:197 ‚Äì210. 
https://doi.org/10.1016/J.PATCOG.2018.06.017  
60.  Ran L, Zhang Y, Zhang Q, et al (2017) Con volutional Neural Network -Based Robot 
Navigation Using Uncalibrated Spherical Images. Sensors 17:1341. 
https://doi.org/10.3390/s17061341  
61.  Silva RMA, Feij√≥ B, Gomes PB, et al (2016) Real time 360¬∞ video stitching and 
streaming. In: ACM SIGGRAPH 2016 Pos ters on - SIGGRAPH ‚Äô16. ACM Press, New 
York, New York, USA, pp 1 ‚Äì2 
62.  Fernandez -Labrador C, Perez -Yus A, Lopez -Nicolas G, Guerrero JJ Layouts from 
Panoramic Images with Geometry and Deep Learning  
63.  Sch√∂ning J, Faion P, Heidemann G (2016) Pixel -wise Gr ound Truth Annotation in 
Videos - An Semi -automatic Approach for Pixel -wise and Semantic Object Annotation. 
In: Proceedings of the 5th International Conference on Pattern Recognition Applications 
and Methods. SCITEPRESS - Science and and Technology Publica tions, pp 690 ‚Äì697 
64.  Ioannidou A, Chatzilari E, Nikolopoulos S, Kompatsiaris I (2017) Deep Learning 
Advances in Computer Vision with 3D Data. ACM Comput Surv 50:1 ‚Äì38. 
https://doi.org/10.1145/3042064  
65.  Devries T, Taylor GW (2017) Dataset Augmentation i n Feature Space. arXiv Prepr 
arXiv 170205538v1  
66.  Dvornik N, Mairal J, Schmid C Modeling Visual Context is Key to Augmenting Object 
Detection Datasets  
 "
Enhancing camera surveillance using computer vision: a research note,"['Haroon Idrees', 'Mubarak Shah', 'Ray Surette']",2018,http://arxiv.org/abs/1808.03998v1," 
   
 
 
Enhancing camera surveillance  
using computer vision: a research note 
 
Haroon Idrees and Mubarak Shah 
Center for Research in Computer Vision,  
University of Central Florida,  
Orlando, Florida, USA, and  
 
Ray Surette  
Department of Criminal Justice,  
University of Central Florida,  
Orlando, Florida, USA  
1 Enhancing Camera Surveillance using Computer Vision: A Research Note 
 Int
roduction  
Police Surveillance of Public Spaces.  Historically called the ‚Äústake-out‚Äù, police 
surveillance has a long history and evidence gained from surveillance has been an important part 
of investigations for nearly two centuries (Marx, 1988).  Similarly, the use of visual technology 
by police began in the nineteenth century with the photographing of inmates and evolved to 
include crime scene photographs as standard police procedures (Buckland, 2001; Norris & 
Armstrong, 1999a). While both practices became law enforcement mainstays, police surveillance 
and visual evidence remained separate realms well into the twentieth century (Norris & 
Armstrong, 1999a).   Surveillance cameras operated by law enforcement are therefore a 
relatively recent phenomenon and their marriage has moved surveillance from a human based 
activity to a heavily technological one. 
As evolved, police surveillance has two goals: proactively deterring offenders and aiding 
in investigations.   Initially, the investigative goal dominated and surveillance was aimed at 
solving crimes, not preventing them.  But as cameras became less expensive and more pervasive, 
deterrence and risk reduction became important (Kroener, 2014).  Current camera surveillance 
projects aim to provide some combination of retrospective crime scene analysis, deterrence of 
future crimes, and facilitation of real-time intervention and force deployment (Haggerty & 
Gozso, 2005). 
It is unknown how many public space surveillance cameras are operated by law 
enforcement agencies but a 2014 U.S. estimate was about 30 million (Staples, 2014, p. 71 citing 
Vlahos, 2009).  Despite their limitations surveillance cameras have emerged as a popular law 
enforcement choice to address crime and security concerns and much of the gap between what 
2 was promised and what was delivered has been linked to their rapid adoption (Surette, 2005).1
The n
umber of cameras installed quickly outpaced the capacity to monitor them and thus to 
effectively respond to what was visually captured (Piza, Caplan & Kennedy, 2014a, 2014b; Gill 
et al, 2005; Keval & Sasse, 2010).  It is apparent that a weak link in the information chain from 
camera to police response is the human monitor tasked with watching the screens (Surette, 
2005). 
Humans as Camera Monitors: A Poor Match.  Humans are not particularly good as 
camera monitors (Hier, Greenberg, Walby, & Lett, 2007; N√§sholm, Rohlfing, & Sauer, 2014; 
Sutton & Wilson, 2004).  Surveillance camera monitors are most frequently tasked with general 
camera monitoring.  They sit at a desk before a bank of monitor screens and conduct on-going 
non-specific assessment of live video feeds. The review of a surveillance camera‚Äôs archived 
video is also sometimes required to determine if an event of interest was recorded.   In this 
second task, monitors are asked to search for a specific event, person, or object.  Again, the 
human monitor is often asked to watch hours of video.  The monitoring difficulty is further 
compounded in that many criminal activities have subtle precursors that are easily overlooked 
when humans are tasked with monitoring multiple cameras (La Vigne et al, 2011; Piza Caplan & 
Kennedy, 2014a; Piza & Sytsma, 2016).  Humans quickly become image swamped, missing 
more than they observe even when vigilant (Boksem, Meijman & Lorist, 2005; Faber, Maurits & 
Lorist, 2012; Gill et al 2005;  Sasse, 2010; Surette, 2005). 
The deficiency of human monitors occurs because perception failure occurs when there is 
little visual change present in long video stretches.  The monitor‚Äôs attention shifts from visual 
review to other non-visual tasks such as conversing or daydreaming resulting in ‚Äúinattentional 
blindness‚Äù (Johnston, Hawley, Plewe, Elliott, & DeWitt, 1990; Sasse, 2010).  In these instances, 
3 monitors have their eyes open and are looking at a video stream but their minds are cognitively 
el
sewhere, the visual images failing to reach psychological ‚Äúattention capture‚Äù levels necessary 
for effective monitoring.2  Significant amounts of time and video can pass, the images passing in
pl
ain view but unseen (Driver, 1998). 
Relevant for specific event searches,  perceptual blindness more often occurs when a 
monitor‚Äôs cognitive attention is focused on finding one type of activity to the exclusion of other 
significant events (Bredemeier & Simons, 2012; Fougnie & Marois, 2007; Most, Scholl, Clifford 
& Simons, 2005).  In this situation, unexpected and even bizarre events are more likely to fail to 
capture the attention of monitors.  Important for noting anomalies, such as unexpected crimes, 
when a monitor is looking intently for a particular element in a video stream, failure to see other 
things of interest increases (see for example Piza, Caplan, & Kennedy, 2014a, p. 10).   The more 
different the unexpected event is from what is being looked for, the more likely it is to be missed 
(Memmert, 2006).3  Hence, different but serious crimes than one being searched for are less
li
kely to be noticed, the opposite of the case in general monitoring tasks where the lack of visual 
change contributes to monitor error.  In addition to these cognitive barriers, a number of 
surveillance barriers that reduce potential deterrent effects from surveillance cameras have been 
described.  In addition to defensive actions taken by offenders (Piza & Sytsma, 2016) and 
camera related contextual factors (Lim & Wilcox, 2016), two additional noted barriers are high 
camera to operator ratios (Piza, Caplan, Kennedy, & Gilchrist, 2015; Piza, Caplan & Kennedy, 
2014a, 2014b) and poor integration into agency practices (La Vigne, Lowry, Markman, & 
Dwyer, 2011; Piza, Caplan & Kennedy 2014b; Piza, Caplan, Kennedy, & Gilchrist, 2015). 
The cumulative result is that ‚Äúa high camera-to-operator ratio has the predictable result of 
crime occurring within sight of a camera going undetected and the detection of criminal events 
4 by CCTV operators as rare‚Äù (Piza, Caplan, & Kennedy, 2014a, p. 1019-1020 citing Norris & 
Arm
strong, 1999a, 1999b).  Faced with significant competition for attention, the camera systems 
currently are ‚Äòhit or miss‚Äô tools regarding the detection of on-going incidents and expensive time 
and human capital consuming drudgery-laden search platforms for finding useful investigative 
evidence.  Additionally criticized for using sworn personnel as monitors and for instances of 
monitor abuses such as voyeurism and profiling, for police agencies the need for an alternative to 
human monitors is apparent (Bredemeier & Simons, 2012; Surette, 2005).  Computer vision 
app
lications can potentially address these issues and increase the deterrent impact of cameras and 
their organizational benefits.  However, the lack of computer vision use in law enforcement is 
exacerbated by a lack of computer vision software development designed with law enforcement 
needs in mind and the absence of field trials to justify agency costs for upgrading to computer 
vision enhanced camera networks. 
Computer Vision as Solution.  An emerging approach to the shortfalls of human 
monitored camera systems is computer vision (also known as machine vision).  A literature 
search reveals little current use of computer vision capabilities by law enforcement agencies 
although calls for its incorporation and discussions of potential applications have been forwarded 
(see Baldwin & Baird, 2001; Barett, Todd, Miller & Blythe, 2005; Piza, Caplan, & Kennedy, 
2014b; Shah, Javed & Shafique, 2007; Thomas & Cook, 2006).  The bulk of law enforcement 
computer applications have concentrated not on computer vision but on data-mining coupled 
with crime mapping to identify crime ‚Äòhot spots‚Äô (Lohr, 2012; Wang, Ding, Lo, Stepinski, 
Salazar & Morabito, 2013; Yu, Ward, Morabito & Ding, 2011).  The common current computer 
vision uses are facial recognition applications and license plate readers. While computer vision 
as a public safety tool remains under-explored, the recent coupling of surveillance cameras to 
5 fast, inexpensive computers have made computer vision solutions feasible.  The primary benefit 
th
at computer vision offers law enforcement agencies is the substitution of automated analysis of 
camera video streams for human monitors.  With computer vision, the human in a computer 
vision enhanced security camera network assumes a supervisory assessment and response 
decision role. 
The first step in understanding computer vision involves comprehending digitization of a 
visual image into a grid of pixels where each pixel is assigned a numerical value representing its 
color.  This initial process generates for an image (or in the case of a video, each frame) a two-
dimensional grid of numbers which mathematically renders the original image as digits, hence 
the term ‚Äòdigital photo‚Äô. A simplified example is provided in Figure 1. These assigned numbers 
are the foundation for all subsequent manipulation, analysis, interpretation, labeling and other 
higher-level computer vision capabilities.  When a digital photo is opened for viewing, the 
process is reversed by a photo processing program which uses each pixel‚Äôs value to instruct the 
viewing device (a computer, smart phone, or digital camera) on how to color a corresponding 
screen pixel ‚Äì converting numbers back to colored pixels and reconstructing the picture in a form 
that humans can see.  This ‚Äúpicture to number to picture‚Äù process makes computer vision 
possible. 
Figure 1 about here 
The key to computer vision is the analysis made possible by the pixel values when the 
state of the image is not visual but numerical.   The art of computer vision moves quickly from 
input that looks loosely analogous to an ‚Äòimage‚Äô (for example, the numbers assigned to each 
pixel in Figure 1) to working with data and outputs that do not appear to correspond to the 
6 original image in any straightforward fashion as the photo and histogram in Figure 2 
dem
onstrate. 
Figure 2 about here 
In essence, computer vision involves determining what a quantitative analysis of pixel 
values can tell about an image.  From its numerical foundation the core tasks of computer vision 
proceed and in turn allow the development of common computer vision applications such as 
locating people and places in images, face recognition, and image stabilization. 
An example of a core computer vision task with direct criminal justice applications is the 
tracking of objects across a set of video frames (Yilmaz, Javed, & Shah, 2006).  The 
mathematical representations of the tracked objects are derived from determining key points 
(unique sets of pixels in an image) so that a ‚Äúprobability distribution function‚Äù (PDF) can be 
generated. A PDF is analogous to a visual fingerprint without being as individually unique. 
Thus, an object will usually have a similar PDF that can be tracked across video frames.  The 
objects tracked can be automatically set by a computer vision object detector or can be assigned 
manually by a human placing an outline around a region of interest within a video frame.  Recent 
tracking methods can lose and reacquire objects as they move into and out of camera fields of 
view (Comaniciu, Ramesh, & Meer, 2000) and work well when the tracked object is 
substantially different from its background.  Multiple objects that are similar in appearance or 
that cross in front or behind one another are more difficult to track.4
Anot
her useful computer vision task is the assignment of a name to objects and actions. 
Not only is it useful to be able to name objects in a picture but it is an important goal to 
determine whether a human is present and to determine who they are and what they are doing, a 
crucial public safety surveillance task. For a computer vision program to be able to recognize and 
7 name objects, a set of images are initially used to train ‚Äòclassifiers‚Äô, computer vision sub-routines 
tha
t assign labels to images.  Once developed, classifiers can be used to answer queries about 
unlabeled images such as: Is there a handgun in this video?  The classifier training process for a 
previously unknown object proceeds after manual annotation during which training data is 
created by humans who assign labels to a set of representative images of the object (positive 
visual examples as shown in Figure 3) as well as those which do not contain the object (negative 
examples). The training images allow the computer vision program to mathematically 
differentiate among objects. For example, providing examples of ‚Äúweapons‚Äù and ‚Äúnot weapons‚Äù 
trains a classifier that can then better calculate the probability of a weapon being in an image. 
Such an approach is termed ‚Äòsupervised‚Äô as it assumes availability of annotated training data in 
contrast to ‚Äòsemi-supervised‚Äô and ‚Äòunsupervised‚Äô approaches that require partially-annotated or 
no training data, respectively. In general, the performance of a particular computer vision task is 
proportional to amount of human-labeled data available. 
Figure 3 about here 
The now common task of matching a particular individual‚Äôs face with a face in an image 
database is also useful (Turk & Kentland, 1991).  To accurately match a face, the computer 
vision program must consider ‚Äúbetween class variation‚Äù (different people who share features 
such as blue eyes) and ‚Äúwithin class variation‚Äù (the same person who looks differently due to 
differences in image aspects (frontal face compared with profile for example).  As with object 
and action labelling, face recognition is set as a probabilistic outcome with some threshold level 
of image similarity needed to be reached before a match is declared.  A recent improvement 
works to maximize the distinction between the faces of different persons and takes into account 
8 differences (i.e., normalizes within class variation) observed across multiple images of the same 
per
son.5   With these and other capabilities under development, potential computer vision
sol
utions for police surveillance camera tasks are now within reach. 
Computer Vision Applications in Policing.   General law enforcement surveillance needs 
that computer vision can address fall under two umbrellas.  The first set revolves around the need 
for automated real-time, live video stream analysis.  The second involves the need for post-hoc 
searches of archived video files. Computer vision based automated identification of public safety 
events of interest addresses the first task and query-based searches of video files addresses the 
second. 
Related to the first task, live real-time video analysis involves the need to rapidly identify 
and correctly respond to ongoing incidents.  This capability is important because effectiveness of 
a surveillance system in reducing crime has been linked to real-time intervention. Unless 
surveillance results in someone showing up to address an observed problem, camera deterrent 
effects wane (Ariel, 2016; Gill, 2003; Goold, 2004; Piza et al 2014a, 2014b; Welsh & Farrington, 
2002). The development of live event analysis is conducted along two computer vision paths: 
action and event detection of pre-identified events of interest and detection of anomalous new, 
unanticipated but potentially noteworthy events.  For real-time detection of activities, it is 
imperative that the system analyzes the surveillance video as it is captured and classifies actions 
and events as they appear.  Of particular interest to public safety monitors are many activities 
which occur infrequently and are precursors to criminal activity (for example, ‚Äòcar hopping‚Äô 
where a person pulls on car door handles as they walk along a street would be a precursor to theft 
from vehicles).  These activities are more difficult to program because first they are rare and 
therefore have a limited number of examples available for analysis and second, they can be 
9 ambiguous and difficult to define mathematically.  Hence, a murderous assault will likely occur 
onl
y once in the lifetime of a camera‚Äôs view-shed but it is crucial that it be noted by a computer 
vision program and that it be distinguished from one person giving another a vigorous friendly 
hug.  Humans quickly distinguish the two activities; however, computer vision programs must be 
quantitatively trained to do so. To be useful, anomaly models also must continuously update and 
incorporate environmental changes, for instance changes in weather, crowd density, or lightning 
conditions at different times of the day. 
Computer vision can also reduce the immense amount of time currently spent reviewing 
and searching videos.  Even when it is known that a video contains specific images such as 
weapons, the minutes or seconds of interest are often buried within hours of output.  A computer 
vision solution to this issue is query-based searches.  To be useful, query-based searches require 
search options that permits retrieval of objects with particular properties such as a person with 
specific height, weight, race, gender, or appearance; or ‚Äòobjects‚Äô such as an item a person was 
carrying like an umbrella or back-pack.  The ability to submit an object and attribute-based 
search would significantly reduce the number of irrelevant video clips that an investigator must 
review.  Independent of specific query-linked searches, it is also useful to have computer vision 
based video summarization programs for the distillation of videos into shortened but accurate 
summaries.  An eight-hour video can typically be reduced to an edited ‚Äòchange only‚Äô video 
lasting minutes (Chen, Wang & Wang, 2009; Evangelopoulos et al, 2009; Gao, Wang, Yong & 
Gu, 2009). 
In another potential use of computer vision, recent criminal justice research has used 
camera footage to study pre-crime visual cues.  For example, Piza, Caplan, & Kennedy (2014b) 
and Levine, Taylor and Best (2011) used video footage to examine violence precursors and 
10 Moeller (2016) and Piza and Sytsma (2016) searched for correlates of illegal drug sales. 
Com
puter vision has the potential to significantly aid these research efforts and increase the use 
of surveillance videos as a data source. A number of prior research efforts have employed 
surveillance video as data (see Piza & Sytsma 2016; Piza, Caplan, & Kennedy, 2014a; Sampson 
& Raudenbush 1999; St. Jean, 2007) but their usefulness has been limited by heavy processing 
and time demands.  Despite having a number of years of video, Piza and Sytsma (2016) had to 
limit analysis to a single year and 62 incidents due to processing workload; in their study, each 
minute of video equaled 20 minutes of transcription time.  Lastly, as implied by Piza & Sytsma 
(2016) and Moeller (2016) a set of criminological theories and concepts such as routine 
activities, environmental crime, crime displacement, and hotspot analysis could benefit from the 
exploration of computer vision generated data. 
On-going Research.  A National Institute of Justice funded study is underway to address 
three research questions associated with police use of computer vision (Shah, Idees, & Surette, 
2015).  In this study computer vision analytics for a large surveillance camera network is being 
developed and their integration into a Public Safety Visual Analytics Workstation (PSVAW) 
within a municipal police department will be field tested.  The law enforcement targeted 
computer vision analytics under development include the retrieval of objects, concepts and 
events (Mazaheri, Kalayeh, Idrees, & Shah, 2015); the localization of actions in long untrimmed 
videos (Soomro, Idrees, & Shah, 2016); the interactive detection of anomalies without annotated 
training examples (Zavesky & Chang, 2008); and multiple methods for video summarization 
(Rodriguez, 2010).  The research questions addressed are: what is the accuracy and speed of the 
analytics; what is the organizational fit of computer vision in a police department; and what is 
the impact of a computer vision capability on a municipal criminal justice system? 
11 Research Question 1: How well do the computer vision algorithms work in the lab? 
Com
puter vision algorithms are being evaluated on standard pre-curated, annotated datasets 
which are partitioned for training and testing. For many computer vision tasks, prior algorithm 
accuracy has been high, above 90 percent for easy action recognition datasets.  However, for 
challenging datasets accuracy drops to around 60 percent (Kuehne, Jhuang, Garrote, Poggio, & 
Serre, 2011), a level that would generate numerous false hits in police applications.  The goal is 
to produce computer vision algorithms that are sensitive enough to not miss significant events 
but also do not swamp human reviewers with large number of erroneously flagged video clips. A 
second programming goal is to achieve significant reduction in storage and computational cost 
over large-scale surveillance video archives.  The practical impact for a law enforcement agency 
would be significant gains in search speed and the ability to search thousands of hours of video 
data (Ye, Liu, Wang, & Chang, 2013). 
A computer vision based method for detection of static concepts and dynamic events is 
also being developed for object detection such as weapons, police officers, police vehicles; and 
complex event detection like assaults, thefts, and car crashes.  Both use features from deep neural 
networks for processing images and video frames.6  In the static concept search, a human can
que
ry a single concept such as ‚Äòpolice officer‚Äô and the system will return a sorted list of video 
clips in which the concept ‚Äòpolice officer‚Äô appears. The complex event detection categorizes 
video into broad categorical classes of behaviors beyond a brief appearance of single objects. 
Thus, more challenging activities can be dealt with and video clips can be robustly classified into 
events such as ‚Äòrobberies‚Äô and ‚Äòassaults‚Äô. 
Regarding the need for real-time video analysis, computer vision software for live online 
abnormality detection is additionally being created (Javan, Roshtkhari, & Levine, 2013). The 
12 quantitative problem amounts to finding patterns in the digital data that significantly deviate 
fro
m behaviors previously identified empirically. The detection of abnormal behaviors is a 
difficult task.  First, the quantitative definition of a normal versus abnormal visual pattern is not 
well defined.  Second, normal behavior evolves over time and may change significantly as time 
passes (for instance, many people walking during daylight versus few people walking during 
nighttime differ visually but both may be normal activity when it comes to crime detection (Lim 
and Wilcox, 2016; Moeller, 2016).   Third, because abnormal events are rare it is difficult to 
obtain enough examples to train classifiers. 
To cope with these challenges, an online dictionary learning approach to detect 
abnormalities is being pursued which divides long videos into small non-overlapping meaningful 
clips. Since these segments are computed based on appearance and motion information, many 
will contain tracked vehicles and people, which are then compared with existing elements in a 
dictionary thereby permitting the detection of abnormalities (Tran, Bourdev, Fergus, Torresani, 
& Paluri, 2015). If the flagged anomaly is deemed a normal event, it is added to the dictionary. 
This allows the computer program to interactively update and recognize a ‚Äúnew normal‚Äù such as 
when a crowded day time street becomes a sparsely populated nighttime scene.  The anomaly 
detection process flags anomalous events from an unsupervised approach so that labeled training 
data is not required. The normalcy models will also be unique for each camera, since abnormal 
behavior may vary by camera across a network. 
In addition to detection, a computer vision benefit is the ability to automatically 
summarize video files and screen out irrelevant information (McCarthy & O‚ÄôMahony, 2015). 
One approach is to use computer vision to identify a small set of suspicious video clips in real 
time from multiple camera feeds or from a large video archive. A promising approach being 
13 pursued is built on semantic indexing (SIN) which uses ideas from deep learning and foreground 
obje
ct detection (Shah, Idees, & Surette, 2015).  A temporal action localization (finding an action 
in long videos) approach automatically decomposes an action into several sub-actions, models 
each sub-action on appearance and duration into distinct steps, and detects sub-actions in an 
original untrimmed video.  An action event usually consists of a sequence of sub-actions/sub-
events in a specific order. For example, a robbery action can be decomposed into person A 
approaching person B, person A producing a weapon and gesturing at person B, person B 
holding hands aloft, handing over wallet or phone, and the two separating.  The approach for 
localization automatically discovers the number of sub-actions for each action/event from a set 
of training videos, registering the point in time the action begins and ends in a video. Once 
identified, these segments can be flagged for human monitor review and deployment decisions. 
A second video summarization approach renders a new video that highlights interesting 
activities in the original video and skims through redundant information to save viewing time. 
Along these lines, a hierarchical video summarization method is being created which will first 
identify small video regions termed supervoxels (regions with similar appearance and coherent 
motion) based on information such as color and motion.  Next, high-level objects of interest such 
as moving humans or vehicles will be incorporated.  These information sources will be combined 
and the defined video segments matched with previously detected and labeled objects . By 
detecting interesting regions as well as objects, analysis of human and object interactions are 
possible (e.g. a theft involving a person in a car or a fight involving multiple people). 
Research Question 2: Does computer vision work in the field?   If a large automated 
camera system results in event swamping from the flagging of numerous events for review or has 
no significant impact on daily agency operations, computer vision‚Äôs promise will be unmet. To 
14 address this issue, a set of events of interest to law enforcement and the design and installation of 
a co
mputer vision workstation in a municipal police department will be evaluated.  Table 1 lists 
18 objects, events, and interactions of interest to law enforcement that computer vision 
algorithms are being developed to detect. 
Table 1 about here 
Some of the events of interest are rare and have proven difficult to locate sufficient 
numbers of training examples from police surveillance cameras.  In addition, some events occur 
in conjunction with other crimes or are ambiguous. These events seldom happen without other 
confounding criminal activity or they are hard to identify by annotators (e.g. injured officer and 
custody events).  Thus, it is difficult to train detectors for such less straightforward events to flag 
them reliably.  Correspondingly, anomaly detection in the real-world assumes greater 
importance. 
In terms of agency impact, the key computer vision field component will be a ‚ÄúPublic 
Safety Visual Analytics Workstation (PSVAW ‚Äì see Figure 4).  The PSVAW will have multiple 
capabilities ranging from detection and localizing objects in camera feeds, labeling actions and 
events associated with training data, and allowing query based searches for specific events in 
videos.  It will also be programmed to flag pre-trained criminal and new non-trained abnormal 
events.   Using human monitor feedback, the PSVAW will refine the retrieval parameters and 
improve its search results over time. After repeating a number of iterations, the PSVAW will 
create an inductive model to detect new activity of interest in real-time so that an initial anomaly 
will over time become a computer vision trained, recognized, and labeled event. 
Figure 4 about here 
15 Research Question 3.  What are computer vision impacts on a criminal justice system? 
The   p
resence of surveillance cameras has been forwarded as both possibly increasing the 
reporting of events to the police or suppressing citizen guardianship levels (Surette, 2006). 
Besides issues of loss of privacy, costs, and effectiveness, because computer vision surveillance 
cameras are expected to catch events that humans would miss, more people may be arrested as 
the criminal justice net becomes wider and finer (Surette, 2005). 
The system-wide impact of a computer vision enhanced camera network will be assessed 
along two dimensions: its impact on the policing of a community through time series analysis of 
‚Äòall reported crime‚Äô and ‚Äòcalls for service‚Äô data, and the local criminal justice system‚Äôs use of 
computer vision generated video for investigations and evidence. Utilizing measures of time 
spent by human monitors on video requests and processing, flagged events and response times, 
and use of camera video for investigations and case evidence the system-wide impact of 
computer vision capabilities will be evaluated. 
 Conclusion.  Computer vision has the potential to address a wide set of problems 
associated with current public space camera surveillance systems from inappropriate use such as 
profiling and voyeurism to inherent unintended errors by human monitors.  An automated 
camera monitor system will not view what it has not been programmed to view and when 
appropriately programmed will reduce the surveillance gaze from falling on unsuitable subjects. 
Computer vision systems can also greatly reduce the two sources of error and ineffectiveness in 
the use of public space surveillance cameras.  A computer algorithm will not become bored or 
distracted during real-time monitoring so events of interest are less likely to go unseen. 
Simultaneously, an algorithm will not be so focused on a search for a specific event that other 
16 important events go unnoticed.  These benefits are currently being developed and tested in 
com
puter vision lab settings.
Computer vision should not be expected to be a panacea  f or law enforcement‚Äôs 
surveillance needs and software gaps remain such as algorithms confidently misidentifying 
images (Nguyen, Yosinski & Clune, 2014).  Additional shortfalls are due to object size (number 
of pixels) presenting detection errors in labeling small objects such as guns and tracking can be 
hampered by the occlusion of people and objects. Hence, following computer vision 
developments from the lab to the field will be an important step.  The promise of computer 
vision is that the automation of monitoring can upgrade the current reality of a poorly utilized 
technology expenditure to a reliable public safety tool.  To already budget conscious, low-on-
manpower agencies a field evaluated computer vision capability stands as potentially invaluable. 
References 
Alla
rd, T., Wortley, R. & Stewart, A. (2008). The effect of CCTV on prisoner misbehavior. The 
Prison Journal 88(3), 404‚Äì422. 
Ariel, B. (2016).  Do police body cameras really work: IEEE Spectrum .  Posted May 4, 2016. 
Downloaded May 24, 2016 from http://spectrum.ieee.org/consumer-electronics/portable-
devices/do-police-body-cameras-really-work 
Ariel, B., Farrar, W., & Sutherland, A. (2015). The effect of police body-worn cameras on use of 
force and citizens‚Äô complaints against the police: a randomized controlled trial. Journal of 
quantitative criminology , 31(3), 509-535. 
Assari, S. M., Idrees, H., & Shah M. (2016). Human re-identification in crowd videos using 
personal, social and environmental constraints.  European Conference on Computer Vision 
(ECCV). 
17 Baldwin, D. & Baird, J. (2001).  Discerning intentions in dynamic human action. Tre nds in 
Cognitive Sciences, 5, 171-178. 
Barrett, H., Todd, P., Miller, G., & Blythe, P. (2005).  Accurate judgments of intention from 
motion cues alone: A cross-cultural study. Evolution and Human Behavior, 26, 313-331. 
Becklen, R. & Cervone, D. (1983).  Selective looking and the noticing of unexpected events. 
Memory & Cognition, 11, 601-608. 
Boksem, M., Meijman, T. & Lorist, F. (2005). Effects of mental fatigue on attention: An ERP 
study. Cognitive Brain Research, 25(1), 107-116. 
Bredemeier, K & Simons, D. (2012). Working memory and inattentional blindness. 
Psychological Bulletin Review , 19, 239-244. 
Buckland, G.  (2001). Shots in the dark: True crime pictures.  NY: Little Brown & Co. 
Chen B., Wang J. & Wang J. (2009). A novel video summarization based on mining the story-
structure & semantic relations among concept entities. Multimedia, IEEE Transactions on, 
11(2), 295-312. 
Comaniciu, D., Ramesh, V., & Meer, P. (2000). Real-time tracking of non-rigid objects using 
mean shift. Computer Vision and Pattern Recognition, 2, 142-149. 
Driver, J. (1998). The neuropsychology of spatial attention. In H. Pashler (Ed.), Attention. (pp. 
297-340). London: Taylor Francis. 
Evangelopoulos, G., Zlatintsi, A., Skoumas, G., Rapantzikos, K., Potamianos, A., Maragos, P., & 
Avrithis, Y. (2009). Video event detection and summarization using audio, visual and text 
saliency. Acoustics, Speech and Signal Processing, ICASSP IEEE International Conference. 
Faber, L., Maurits, N., & Lorist, M. (2012). Mental fatigue affects visual selective attention. 
PloS one , 7(10), e48073. 
Fougnie, D. & Marois, R. (2007). Executive working memory load induces inattentional 
blindness.  Psychonomic Bulletin and Review, 14(1), 142‚Äì147. 
Gao, Y., Wang, D., Yong, J., & Gu, H. (2009). Dynamic video summarization using two level 
redundancy detection. Multimedia Tools and Applications , 42(2), 233-250. 
18 Gill, M. (2003). CCTV. Leicester, UK: Perpetuity Press; 
Gil
l, M. & Loveday, K. (2003). What do offenders think about CCTV?‚Äù Crime Prevention and 
Community Safety, 5(3), 17‚Äì25. 
Gill, M., Spriggs, A., Allen, J., Hemming, M., Jessiman, P. and Kara, D. (2005) Control room 
operation: findings from control room observations.  London: Home Office. 
Goold, B. (2004). CCTV & Policing. Oxford, UK: Oxford University Press. 
Haggerty, K. & Gozso, A. (2005). ‚ÄúSeeing Beyond the Ruins: Surveillance as a Response to 
Terrorist Threats. Canadian Journal of Sociology, 30(2), 169‚Äì187. 
Hier, S., Greenberg, J., Walby, K. & Lett, D. (2007).  ‚ÄúMedia, Communication & the 
Establishment of Public Camera Surveillance Programmes in Canada.‚Äù Media, Culture, and 
Society , 29(5), 727-751. 
Hyman, I. Boss, E., Matthew, S., Wise, B., McKenzie, M., Kira E., & Caggiano, J. (2009). Did 
you see the unicycling clown? Inattentional blindness while walking and talking on a cell 
phone.  Applied Cognitive Psychology , 24(5), 597‚Äì607. 
 Idrees, H., Warner, N., & Shah, M. (2014). Tracking in dense crowds using prominence and 
neighborhood motion concurrence. Image and Vision Computing , 32(1), 14-26. 
Javan Roshtkhari, M., & Levine, M. (2013). Online dominant and anomalous behavior detection 
in videos. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  
(pp. 2611-2618). 
Javed, O., Rasheed, Z., Alatas, O., & Shah, M. (2003). KNIGHT‚Ñ¢: a real time surveillance 
system for multiple and non-overlapping cameras. IEEE International Conference on 
Multimedia and Expo.  
Javed, O., Rasheed, Z., Shafique, K., & Shah, M. (2003). Tracking across multiple cameras with 
disjoint views. International Conference on Computer Vision. 
Johnston, W., Hawley, K., Plewe, S., Elliott, J., & DeWitt, M. (1990). Attention capture by novel 
stimuli. Journal of Experimental Psychology: General , 119(4), 397.  Keval, H. and sasse, M. 
19 (2010) ‚ÄòNot the usual suspects‚Äô: A study of factors reducing the effectiveness of CCTV. 
Sec
urity Journal 23(2): 134-154.  
Keval, H., & Sasse, M. (2010). ‚ÄúNot the Usual Suspects‚Äù: A study of factors reducing the 
effectiveness of CCTV. Security Journal , 23(2), 134-154. 
Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., & Serre, T. (2011). HMDB: a large video 
database for human motion recognition. In International Conference on Computer Vision  
(pp. 2556-2563).  
Kroener, I. (2014).  CCTV: A technology under the radar?  Burlington, VT: Ashgate. 
La Vigne, N., Lowry, S., Markman, J. and Dwyer, A.  (2011) Evaluating the use of public 
surveillance cameras for crime control and prevention.  Washington DC: US. Department of 
Justice, Office of Community Oriented Policing Services, Urban Institute Justice Policy 
Center.   
Levine, M., Taylor, P. J., & Best, R. (2011). Third parties, violence, and conflict resolution the 
role of group size and collective action in the microregulation of violence. Psychological 
Science . 
Lim, H., & Wilcox, P. (2016). Crime-reduction Effects of Open-street CCTV: Conditionality 
Considerations. Justice Quarterly , 1-30. 
Lohr, S. (2012). The age of big data. New York Times , 11. 
Mack, A. (2003).  Inattentional blindness: Looking without seeing. Current Directions in 
Psychological Science , 12(5), 180-184. 
Mack, A. & Rock, I. (1998). Inattentional Blindness .  Cambridge, MA: MIT Press. 
Marx, G. (1988). Undercover: Police Surveillance in America. Berkley: University of California 
Press. 
Mazaheri, A., Kalayeh, M., Idrees, H., & Shah, M. (2015). UCF-CRCV at TRECVID 2015: 
Semantic Indexing. NIST TRECVID. 
McCarthy, O. & O‚ÄôMahony, M. (2016). End user response to an event detection & route 
reconstruction security system prototype for use in airports and public transport hubs. 
20 Transportation Research Board of the National Academies.  Downloaded from 
ht
tp://amonline.trb.org/trb60693-2016-1.2807374/t001-1.2823436/254-1.2823593/16-5450-
1.2980693/16-5450-1.2993283?qr=1 
Memmert, D. (2006).  The effects of eye movement, age, and expertise on inattentional 
blindness. Consciousness and Cognition , 15(3), 620‚Äì627. doi :10.1016/j.concog.2006.01.001. 
PMID 16487725 . 
Moeller, K. (2016).  Temporal transaction patterns in an open-air cannabis market.  Police 
Practice and Research 17(1), 37-50. 
Most, S., Scholl, B., Clifford, E., & Simons, D. (2005). What you see is what you set: sustained 
inattentional blindness and the capture of awareness.  Psychological Review,  112(1), 217-
242. 
N√§sholm, E., Rohlfing, S., & Sauer, J. D. (2014). Pirate stealth or inattentional blindness? The 
effects of target relevance and sustained attention on security monitoring for experienced and 
na√Øve operators. PloS one, 9(1).   Downloaded from:  
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0086157 
Neisser, U. & Becklen, R. (1975).  Selective looking: attending to visually specified events. 
Cognitive Psychology , 7, 480-494. 
Neisser, U. (1979).  The control of information pickup in selective looking.  In A.D. Pick (Eds.) 
perception & its development: A tribute to Eleanor J. Gibson (pp. 201-219) Hillsdale, NJ. 
Erlbaum.   
Norris, C. & Armstrong, G.  (1999a). The Maximum Surveillance Society : The Rise of CCTV. 
Oxford, UK: Berg. 
Norris, C. & Armstrong, G.  (1999b). CCTV and the social structuring of surveillance.  In N. 
Tilley and K. Painter (Eds.), Surveillance of public space: CCTV, street lighting and crime 
prevention.  Crime Prevention studies (vol. 10, pp. 157-178).  Monsey, NY: Criminal Justice 
Press.   
21  Nguyen, A. Yosinski, J. & Clune. J. (2014).  Deep neural networks are easily fooled: High 
con
fidence predictions for unrecognizable images. In 2015 IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR)  (pp. 427-436). IEEE. 
Piza, E., Caplan, J. & Kennedy, L. (2014a). CCTV as a tool for early police intervention: 
Preliminary lessons from nine case studies. Security Journal.  doi:10.1057/sj.2014.17 
Downloaded from: http://link.springer.com/article/10.1057%2Fsj.2014.17 
 Piza, E., Caplan, J., & Kennedy, L. (2014b). Is the punishment more certain? An analysis of 
CCTV detections and enforcement. Justice Quarterly , 31(6), 1015-1043. 
Piza, E., Caplan, J., Kennedy, L. & Gilchrist, A.  (2015). The effects of merging proactive CCTV 
monitoring with directed police patrol: A randomized controlled trial.  Journal of 
Experimental Criminology , 11, 43-69. 
Piza, E. & Sytsma, V. (2016) exploring the defensive actions of drug sellers in open-air markets: 
A systematic social observation.  Journal of Research in Crime and Delinquency, 53(1), 36-
65 
Rodriguez, M. (2010). Cram: Compact representation of actions in movies. Computer Vision and 
Pattern Recognition (CVPR).  
Sampson, R. & Raudenbush, S. (1999).  Systematic social observation of public spaces: A new 
look at disorder in urban neighborhoods.  American Journal of Sociology 105:603-651. 
Sasse, A. (2010).  Not seeing the crime for the cameras? Communications of the ACM , 53, 22-25. 
Shah, M., Idees, H., & Surette, R. (2015).  Studying the impact of video analytics for pre, live, 
and post event analysis on outcomes of criminal justice.  Orlando, Fl: University of Central 
Florida Center for Research on Computer Vision.  Funded by U.S. department of Justice, 
NIJ-2015-R2-CX-K025. 
Shah, M., Javed, O., & Shafique, K. (2007). Automated visual surveillance in realistic 
scenarios. IEEE MultiMedia, 1, 30-39. 
Short, E. & Ditton, J. (1996).  Does closed circuit television prevent crime?  Monograph of the 
Scottish Office Central Records Unit, Edinburgh, Scotland. 
22 Soomro, K., Idrees, H., & Shah, M. (2016). Predicting the Where and What of actors and actions 
thr
ough Online Action Localization. IEEE Conference on Computer Vision and Pattern 
Recognition. 
St. Jean, LP. (2007)  Pockets of crime: Broken windows, collective efficacy, and the criminal 
point of view.  Chicago: University of Chicago Press. 
Staples W. (2014).  Everyday Surveillance .  New York: Rowman & Littlefield. 
Surette, R. (2005).  The thinking eye: Pros and cons of second generation CCTV surveillance 
systems.  Policing: An International Journal of Police strategies & Management , 28(1), 152-
173. 
Surette, R. (2006).  CCTV and citizen guardianship suppression: A questionable proposition. 
Police Quarterly , 9(1), 100-125. 
Sutton A. & Wilson, D.  (2004). Open-street CCTV in Australia: Politics and expansion. 
Surveillance and Society , 2(2/3), 310‚Äì322; 
Thomas, J. & Cook, K. (2006). A visual analytics agenda. IEEE computer graphics and 
applications , 26(1), 10-13. 
Tran, D., Bourdev, L., Fergus, R., Torresani, L., & Paluri, M. (2015). Learning spatiotemporal 
features with 3d convolutional networks. In IEEE International Conference on Computer 
Vision (ICCV) . 
Turk, M. & Pentland, A. (1991). Face recognition using eigenfaces. Computer Vision and 
Pattern Recognition, 1991. 
Vlahos, J. (2009).  Surveillance society: new high-tech cameras are watching you.  Popular 
Mechanics,  Oct. 1. 
Wang, D., Ding, W., Lo, H., Stepinski, T., Salazar, J., & Morabito, M. (2013). Crime hotspot 
mapping using the crime related factors‚Äîa spatial data mining approach. Applied 
intelligence , 39(4), 772-781. 
Welsh, B. & Farrington, D. (2002).  Crime prevention effects of closed circuit television: A 
systematic review .  Home Office research Study 252. London: Home Office. 
23 Welsh, B. & Farrington, D.  (2004). Evidence-based crime prevention: The effectiveness of 
CCTV
. Crime Prevention and Community Safety 6, 21-33. 
Welsh, B. & Farrington, D. (2009). Public area CCTV and crime prevention: An updated 
systematic review and meta ‚Äêanalysis. Justice Quarterly , 26(4), 716-745. 
Williams, D. (2007). Effective CCTV and the challenge of constructing legitimate suspicion 
using remote visual images.  Journal of Investigative Psychology and Offender Profiling , 4, 
97‚Äì107. 
Wolfe, J. (1994). Guided search 2.0: A revised model of visual search. Psychonomic Bulletin & 
Review , 1, 202-238. 
Ye, G., Liu, D., Wang, J., & Chang, S. (2013). Large-scale video hashing via structure learning. 
Proceedings of the IEEE International Conference on Computer Vision  (pp. 2272-2279). 
Yilmaz, A., Javed, O., & Shah, M. (2006). Object tracking: A survey. ACM computing surveys 
(CSUR) , 38(4), 13. 
Yu, C., Ward, M., Morabito, M., & Ding, W. (2011). Crime forecasting using data mining 
techniques. 2011 IEEE 11th International Conference on Data Mining Workshops  (pp. 779-
786). IEEE. 
Zavesky, E. & Chang, S. (2008). CuZero: Embracing the frontier of interactive visual search for 
informed users. Proceedings of the 1st ACM International conference on Multimedia 
Information Retrieval  (pp. 237-244). 
1 When and how these systems work in specific applications remains under debate (Ariel, 2016; 
Ari
el, Farrar & Sutherland, 2015; Williams, 2007). Indirect evidence suggests that offenders take 
into account the perceived level of surveillance and the likelihood of intervention when deciding 
24 whether to commit certain types of crimes, especially instrumental street crimes such as car 
break-ins.  This suggests that easily visible cameras with signage can deter certain offenders 
(Gill & Loveday, 2003; Allard, Wortley & Stewart, 2008; Short & Ditton, 1996; Welsh & 
Farrington, 2009).  Spontaneous crimes such as assaults appear to be less affected  and overall 
came
ras appear most effective in reducing crime when combined with other interventions (Lim 
& Wilcox 2016; Piza, Caplan & Kennedy 2014a, 2014b; Welsh & Farrington, 2004). 
2 Inattentional blindness is defined as the failure to  s ee highly visible objects directly looked at 
when cognitive attention is elsewhere (Mack, 2003, p. 180; Mack & Rock, 1998; see also 
Becklen & Cervone, 1983; Neisser, 1979; and Neisser & Becklen 1975).   ‚ÄúAttention capture‚Äù 
refers to the ability of novel stimuli to gain the focus of someone otherwise cognitively engaged 
(Johnston, Hawley, Plewe, Elliott & DeWitt, 1990; Most, Scholl, Clifford, & Simons, 2005; 
Wolfe, 1994).  The use of technology has been reported to effect both processes (Hyman, Boss, 
Matthew, Wise, McKenzie, Kira, & Caggiano, 2009, p. 605) and inattentional blindness has been 
found to be a common phenomenon associated with watching video streams (Most, Scholl, 
Clifford, & Simons, 2005). 
3A classic example is demonstrated by viewers tasked with counting passes failing to see a 
gor
illa walk through a group of people tossing a ball around. 
https://www.youtube.com/watch?v=vJG698U2Mvo
4Over the past decade, more sophisticated alternate approaches to tracking have been presented 
in t
he computer vision literature, including those that track single and multiple objects or persons 
across non-overlapping multiple camera field of views (called the ‚Äòhand-off‚Äô problem) and 
people in dense crowds (Assari, Idrees, & Shah, 2016; Idrees, Warner, & Shah, 2014; Javed, 
Rasheed, Alatas, & Shah, 2003; Javed, Rasheed, Shafique, & Shah, 2003). 
25 5The goal in this method (termed linear discriminative analysis or LDA) is to maximize the 
sep
aration of the set of images of one person from the set of images of different (but possibly 
similar looking on some characteristics) persons by using sets of image portraits.
6 Recent research has shown that training deep network s  containing large number of hidden 
layers significantly improves performance on computer vision tasks such as object detection, 
face identification, and action recognition.    However, deeper networks require larger quantities 
of training data compared to traditional machine learning algorithms and thus may be limited for 
public safety applications. 
Figure 1: A Digitized Letter 
The ori
ginal letter ‚Äòa‚Äô is rendered into a 14 by 12 array of 168 pixels with each pixel assigned 
a value representing a tone from ‚Äòwhite‚Äô scored 1.0 to black squares scored 0.0.  Gray squares 
are scored from light to dark (gray squares scored 0.5, light gray ones scored from .01 to .04, 
dark gray squares scored from 0.6 to .0.9).  The digitized photo of the original ‚Äòa‚Äô image 
results where the ‚Äúa‚Äù can be vaguely discerned in the pattern of 0.0 scored pixels.  For 
capturing color, multiple values, typically corresponding to red, green and blue, are stored for 
each pixel location.  Source:  
http://pippin.gimp.org/image_processing/images/sample_grid_a_square.png 
Figure 2: An Image Histogram 
The hi
stogram on the right is constructed from 
grayscale pixels from the image by quantizing them 
into 256 bins.  The significance of image histograms 
for computer vision is that histograms are often 
unique for different objects and correlate with their 
shape, texture and color and can be used for assigning 
labels to images.  
Figure 3: Positive Training Examples for Assault, Theft, Vandalism, and Robbery. 

Figure 4:  Public Safety Visual Analytics Workstation (PSVAW) 

Table 1:  Objects, Crimes, and Events of Interest to Law Enforcement for Computer Vision 
Algorithm Development and Field Testing. 
Interest Ope rational Definition
-- O
bjects -- 
Weapons handguns, rifles 
Police Cars marked law enforcement vehicles 
Police Officers uniformed law enforcement officers 
Emergency Vehicles fire trucks, ambulances (flashing emergency lights) 
Left Property abandoned bags, backpacks, etc., flagged after a set time limit. 
-- Crimes -- 
Criminal Mischief graffiti, vandalism (destroying property, tipping over objects, spray 
painting or other property damage) 
Theft removal of property (need to differentiate legal from illegal removal of 
property such as associated with property damage like breaking a car 
window) 
Robbery need to differentiate legal from illegal exchanges associated with force or 
weapon  
Burglary burglary from auto with window or other property damage; business 
burglaries constrained to ‚Äúpersons entering a business after closing‚Äù 
possibly flagging constrained from midnight to 6 AM time frame  
Drug Transactions need to differentiate legal from illegal exchanges associated with time of 
day or established illicit drug markets 
Batteries / Assaults shootings, stabbings, and fistfights  
-- Public Safety Events -- 
Car Crashes damage to moving vehicles 
Crowd Activity rioting, crowd density threshold reached, running, fighting, falling, 
property destruction 
Individual Activity running, falling, immobile (exceeds set time limit), blocking traffic, 
standing in roadway (exceeds set time limit) 
Injured Officer officer down (exceeds set time limit) 
Custody Events  arrests, mental health retentions 
Citizen / Police  citizen requests for assistance, officer information/identification requests 
Fire / Explosions set to minimum size limit.  
______________________________________________________________________________ "
"Are object detection assessment criteria ready for maritime computer
  vision?","['Dilip K. Prasad', 'Huixu Dong', 'Deepu Rajan', 'Chai Quek']",2018,http://arxiv.org/abs/1809.04659v2,"1
Are object detection assessment criteria ready
for maritime computer vision?
Dilip K. Prasad1;2;, Huixu Dong3, Deepu Rajan2, and Chai Quek2
Abstract ‚ÄîMaritime vessels equipped with visible and
infrared cameras can complement other conventional sen-
sors for object detection. However, application of computer
vision techniques in maritime domain received attention
only recently. The maritime environment offers its own
unique requirements and challenges. Assessment of the
quality of detections is a fundamental need in computer
vision. However, the conventional assessment metrics suitable
for usual object detection are deÔ¨Åcient in the maritime
setting. Thus, a large body of related work in computer
vision appears inapplicable to the maritime setting at the
Ô¨Årst sight. We discuss the problem of deÔ¨Åning assessment
metrics suitable for maritime computer vision. We consider
new bottom edge proximity metrics as assessment metrics
for maritime computer vision. These metrics indicate that
existing computer vision approaches are indeed promising
for maritime computer vision and can play a foundational
role in the emerging Ô¨Åeld of maritime computer vision.
I. I NTRODUCTION
Maritime vessels (MV) are equipped with sensors such
as radar, sonar and LIDAR for situational awareness. The
automatic identiÔ¨Åcation system (AIS) supports trafÔ¨Åc data
exchange over maritime communication channels, through
which each MV with on-board AIS declares its position,
speed, and intended path. The International Regulations
for Preventing Collisions at Sea 1972 (COLREGs) impose
that all cargo ships weighing more than 300 tonnes and
all passenger ships are equipped with AIS. There is
no such imposition on smaller MVs, including Ô¨Åshing
boats and small-medium sized cargo MVs. Such MVs are
invisible in trafÔ¨Åc data. Moreover, the AIS channel may be
inaccessible for several minutes to few hours at a time [1].
Cameras in the visible and infrared (IR) range now play
a complementary role by overcoming disadvantages of
traditional sensors like the minimum range associated with
radar and sonar [2]. Thus, computer vision (CV) techniques
should play an important role in detecting objects in
the maritime environment , especially in detecting small
and medium sized MVs that have weak radar or sonar
signatures and lack on-board AIS.
Maritime CV for object detection faces several chal-
lenges. Maritime video streams are characterized by scene
Ô¨Çatness , i.e. lack of landmarks and marked lanes as in
1Department of Computer Science, UiT The Arctic University of
Norway, Troms√∏ 9037, Norway, Email-dilipprasad@gmail.com
2School of Computer Science and Engineering, Nanyang Technolog-
ical University, Singapore 639798
3Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213(a) Physical distances vary non-linearly in image [4], [8]
(b) 10 Examples of maritime objects‚Äô appearance
Fig. 1: What is an acceptable detection of a maritime ves-
sel? (a) Collision avoidance requires accurate estimate of the
distance, which is related to the bottom edge of the vessel,
and the minimum span of a maritime object. (b) Green, blue,
and red boxes denote ground truth, acceptable detection, and
unacceptable detections, respectively.
roads. The maritime scene offers difÔ¨Åcult to model dy-
namic background features because of challenges such as
a semi-stochastic wave background, the sharp contrasts
of wakes, possibilities of occlusion of MVs, and weather
and illumination conditions such as rain, haze and glint
[3]. Further, planning the manoeuver and deceleration for
collision avoidance (CA) is challenging since the distance
and span of the MVs in the scene is related non-linearly to
the pixels along the y axis [4], [8], see Fig. 1(a). There
are other applications also, that face the same non-linearity
between the physical space and image space. For example,
a reviewer of this manuscript suggested terrestrial appli-
cations, where ‚Äúobstacle detection by automative vehicle
sensors (for automated braking for example) has the same
bias, since the Ô¨Çat world assumption is usually used in this
domain too.‚Äù An appropriate maritime CV solution has to
satisfy the following requirements:
detect and track MVs in the scene
determine MVs‚Äô accurate spans, positions and tracks
provide real-time results
perform in all weather and illuminationarXiv:1809.04659v2  [cs.CV]  17 Nov 2019
2
Detection and tracking of MVs falls under the ensemble
problem set of ‚Äòdetection and tracking in a dynamic
background‚Äô, which has been extensively studied in com-
puter vision. The existing CV solutions in this ensemble
can provide a Ô¨Årm foundation for developing dedicated
CV solutions for maritime object detection requirements.
We note that the above identiÔ¨Åed goals of maritime CV
comprise a broad topic and entail research for several
years to come. In this paper, we choose a very speciÔ¨Åc
problem within this broad scope and critical for the
entailing research. The speciÔ¨Åc problem considered in this
is paper as follows. Adoption of existing CV solutions
for maritime CV encounters a set back. We show that
traditional performance measures for object detection fail
in the maritime environment and we discuss the following
question. How do we assess the quality of detection for
maritime computer vision?
We show that assessment metrics such as intersection
over union (IOU, also called Jaccard index [5]) and
intersection over ground truth (IOG, also called sensitivity
[6]), most often used in object detection, are unsuitable
for maritime CV . They are deÔ¨Åcient in assessing the
accuracy of span and distance of detected MVs. Either
the detection method provides a very high IOU, say 90%,
or customized assessment metric is needed to meet the
requirements of maritime CV . The aim of this paper is
to design custom assessment metrics that provide good
assessment of the quality of detected objects while not
putting severe demands on detection algorithms.
We discuss two new assessment metrics customized for
maritime computer vision. We also study the performance
of existing background subtraction (BGS) algorithms and
regions with convolution neural network (R-CNN) features
using conventional and proposed assessment metrics. We
show that the conventional metrics indicate general unsuit-
ability of BGS algorithms for maritime CV whereas the
new metrics present hope of using them in maritime CV .
We expect that this exercise shall provide useful cursors
for developing maritime CV solutions.
The assessment requirements of maritime CV are dis-
cussed in section II. The deÔ¨Åciency of conventional met-
rics for maritime CV is discussed in section III. The
proposed bottom edge proximity metrics are presented
and compared with conventional metrics in section IV.
Experimental results of existing BGS algorithms and R-
CNN on a maritime dataset are presented in section V.
Section VI concludes this paper with a discussion on the
future outlook for maritime CV .
II. R EQUIREMENTS FOR MARITIME CV
Before discussing the suitability of conventional met-
rics, or lack thereof, we consider the fundamental question:
‚ÄòWhat is an acceptable detection of a maritime vessel?‚Äô.
It is important to accurately estimate the location of the
MV in a scene (given by the bottom edges of the MV)
and its minimum span (determined by the width of the
(c) Other example objects C-I from 5 different videos
(d) Comparison of pixel-based and BB based segmentations
Object IOU IOG
Pixel-based BB-based Pixel-based BB-based
A 0.27 0.64 0.29 0.83
B 0.55 0.85 0.64 0.85
C 0.68 0.65 0.95 0.96
D 0.45 0.60 0.49 0.97
E 0.39 0.22 0.87 1.00
F 0.39 0.89 0.40 0.99
G 0.36 0.42 0.47 0.91
H 0.15 0.47 0.15 0.48
I 0.09 0.10 0.09 0.10
Fig. 2: Pixel segmentations are more demanding than bounding
boxes, see subÔ¨Ågures (a-c) for qualitative examples. The same
metrics result into signiÔ¨Åcantly lower values when computed for
pixel segmentations, as noted in subÔ¨Ågure (d). The only exception
in the examples considered in (a-c) is shown in bold in (d).
In the subÔ¨Ågure (c), there are 3 panels for each object. The
top, middle, and bottom panels respectively show GT in image,
detected foreground pixels, and the overlap of DO and GT.
MV in pixels and its position in the image frame). See
Fig. 1(a) for illustration. Consider the example cases 1-10
shown in Fig. 1(b). Example 1 is close to ideal, where
the bounding box (BB) of the detected object (DO) is
almost the same as the BB of the ground truth (GT).
We restrict our discussion to bounding boxes because the
pixel-segmentations are signiÔ¨Åcantly more demanding than
bounding boxes. We illustrate this point using Fig. 2. Fig.
3
2(a) shows two objects A and B in an image and also the
foreground segmentation result obtained using a dynamic
background subtraction method. Their pixel segmentations
of GT and DO are shown in Fig. 2(b). Other examples
are shown in Fig. 2(c). Fig. 2(d) shows values of IOU and
IOG for pixel and BB segmentations. The small values of
IOU and IOG for the pixel segmentations of almost all
the objects indicate that assessing the pixel segmentations
is more demanding. Moreover, the pixel segmentations
are not particularly more informative than BBs about the
distance and span of the vessel anyway. Yet, at least for
one example, i.e. object E in Fig. 2(c,d), the IOU for
pixel based segmentation is larger than BB segmentation.
Therefore, the importance of pixel segmentations in accu-
rate detection of MVs cannot be discounted. It merits an
elaborate study, which we relegate to the future work.
Although there is a large variety of MVs, in general,
an MV is characterized by a hull and an optional super-
structure, i.e. all parts above the hull, including masts.
The existing CV solutions may detect hull and super-
structure separately due to two reasons. First, super-
structure is not an essential component and supervised
learning approaches may undertrain for vehicles with
super-structures. Second, stark differences in geometries,
color, and other image features of the hull and the super-
structure imply that the super-structure may appear as an
independent object. The hull or the super-structure may
even be left undetected, such as in the case of sailboats,
due to a lack of contrast between the background and
the super-structure. Consequently, the DO may appear as
shown in examples 2-4. For collision avoidance, accu-
rate detection of the hull is important, irrespective of
whether the super-structure is included in the DO with
the hull (example 1), detected independently (example 4),
or not detected at all (examples 2 and 3). Furthermore,
the physical distance between the MV and the sensor
is mapped non-linearly in an image along a direction
perpendicular to the horizon (see Fig. 1(a)). This means
that the line in image corresponding to horizon is at inÔ¨Ånity
while the bottom most pixel is only a few meters away
from the sensor. Thus, incorrect estimation of bottom
of hull may result in hugely incorrect estimation of the
physical distance . However, it is preferable to slightly
underestimate the distance between the sensor and an MV
for collision avoidance, rather than overestimate it. In this
sense, DOs in examples 2 and 3 of Fig. 1(b) are acceptable.
Current BGS solutions for object detection struggle with
the presence of wakes of maritime vessels [3]. Often wakes
are detected as part of the MVs, such as shown in examples
5-7. Similar to the logic of underestimating the distance
between the sensor and the detected MV , it is safer if the
estimated width is not lesser than the actual span. Thus,
horizontal wakes becoming a part of DO is acceptable,
though not preferable. However, large extension of the
DO in the vertical direction below the hull may result in
grossly incorrect estimate of distance, and is not preferred
Fig. 3: The notations relevant to the conventional metrics and
the proposed bottom edge proximity (BEP) metrics are shown
here.
Fig. 4: The current metrics are unsuitable for assessing detected
objects in maritime CV . For the same values of a, b, and c, one
DO may be preferred over others (a,b). Increasing IOU, Dice
Index, or IOG metrics need not indicate better detections (c).
(see example 7).
The condition of occlusion has a signiÔ¨Åcant implication
on collision avoidance. The extension of DO due to
occlusion in any direction may mean that the MV with
smaller pixel footprint is not detected (see examples 8-10).
Though the DOs for all these examples are not preferred,
the implications are much more severe for examples 9-
10, which involve a small MV (kayak) with no on-board
communication channel and poor detectability in radar
and sonar. These situations call for a close to perfect
overlap between the DO and the GT. However, even
between examples 9 and 10, example 10 is the least
preferred detection. In example 10, the DO leads to gross
underestimation of the location of large MV and missed
detection of a kayak that is much closer to sensor, much
agile, and invisible in other sensor streams.
4
Fig. 5: BEP is sensitive to the bottom edges of the DO and GT (a). X1is more strict than X2(b).Y1is more strict than Y2(c).
ThusBEP 1is more strict than BEP 2.
III. C ONVENTIONAL ASSESSMENT CRITERIA VERSUS
THE NEEDS OF MARITIME CV
Assessment of the quality of detection is usually per-
formed through similarity metrics, such as Jaccard index
[5] (also called IOU) or Dice index [7]. Their generalized
form is given by Twersky index [9], deÔ¨Åned as follows:
S =b
b +a +c(1)
wherea;b;c are the areas of (GT DO) ,(GT\DO) , and
(DO GT) , respectively (see Fig. 3(a)). The parameter 
emphasizes the allegiance of the overlapped region with
GT while the parameter emphasizes the allegiance of
the overlapped region with DO. Similarity metrics usually
employ symmetry with respect to GT and DO, i.e. =.
Dice index corresponds to == 0:5and widely used
IOU corresponds to == 1:0. A detection is assessed
as true positive if IOU >c0. Similar threshold is employed
if other similarity metrics are used. Usually in CV , IOU
>0.5 is considered sufÔ¨Åcient. We consider an additional
asymmetric metric with = 1,= 0, which we refer
to as intersection over ground truth (IOG). This metric
assesses the intersection area bwith respect to the area of
GT (a+b) only. Thus excess span detection due to wakes
(examples 5-7 in Fig. 1(b)) or excess detection in vertical
direction below the hull (example 3 in Fig. 1(b)) do not
affect the assessment negatively if the metric IOG is used.
The essential problem with the above metrics is that
two cases may have the same areas a;b;c , but one case
may be a preferred detection over another. See Fig. 4(a,b)
for examples. Also, the increasing value of the above
mentioned metrics need not imply better detection, as
shown in Fig. 4(c). New metrics that account speciÔ¨Åcally
for the importance of the bottom edge of the hull are
needed.
IV. P ROPOSED BOTTOM EDGE PROXIMITY CRITERIA
We consider two new criteria that speciÔ¨Åcally judge the
accuracy of detection of the bottom edge (BE) and the span
of the DO. We call them bottom edge proximity 1 (BEP 1
appears here for the Ô¨Årst time) and bottom edge proximity
2 (BEP 2, recently proposed in [10]). BEP 1is symmetricwith respect to DO and GT while BEP 2is biased towards
allegiance with GT. We use the notations in Fig. 3(b) for
the deÔ¨Ånitions of BEP 1and BEP 2presented next.
a) Bottom edge proximity 1 (BEP 1):We deÔ¨Åne
BEP 1=X1Y1where
X1=xb
xa+xb+xc;Y1= 1 yBE
min(yGT;yDO)(2)
The smaller the distance between the edges of the GT
and DO, the larger is Y1. See Fig. 5(a) for an illustration
of this point. However, if the DO is signiÔ¨Åcantly smaller
than GT,Y1becomes poorer. Thus, it indirectly embeds
the vertical size of DO in comparison with GT. This is
shown in Fig. 5(c).
b) Bottom edge proximity 2 (BEP 2):We deÔ¨Åne
BEP 2=X2Y2where
X2=xb
xa+xb;Y2= 1 yBE
yGT(3)
We note that BEP 1is stricter than BEP 2. This is
becauseX1is less tolerant to extended span of DO due
to wakes as well as occlusions, as shown in Fig. 5(b).
Further,Y1is sensitive to the size of DO if the DO is
smaller than the GT, as shown in Fig. 5(c).
For convenience, we refer to X1andX2asXmetrics.
Similarly, we refer to Y1andY2asYmetrics. An advan-
tage of BEP metrics is that the threshold(s) for assessing a
detection as a true positive can be chosen Ô¨Çexibly. Either
a single threshold c0can be used for the net BEP score,
or two thresholds x0andy0can be considered for Xand
Ymetrics independently, and a TP can be assessed if both
conditionsX >x 0andY >y 0are satisÔ¨Åed.
c) Qualitative comparison for examples in Fig. 1(b):
We perform a qualitative comparison of the metrics IOU,
Dice index, IOG, BEP 1, and BEP 2on the examples
in Fig. 1(b), which were used to study acceptable and
unacceptable detections for maritime CV . The results are
shown in Table I. We brieÔ¨Çy discuss the selection of the
thresholds (given in parentheses) for the metrics. Since
the threshold value of c0= 0:5is conventionally used in
object detection [11], we use this value for IOU. Similarly,
we usec0= 0:5as threshold for the Dice index and IOG
5
as well. Since X1andX2are 1-dimensional analogues
of the 2-dimensional IOU and IOG, we use a threshold
value ofx0=p
0:5. Lastly, we use threshold value of
y0= 0:75because the accuracy of bottom edge is critical
in collision avoidance.
As discussed before, conventional metrics that use a;b;c
shown in Fig. 4 are not suitable for assessing detections
in maritime CV . This is evident in Table I, where IOU,
Dice index, and IOG have successes for less than half
the number of examples. BEP 1performs better, getting 6
successes out of 10 examples. BEP 2performs the best,
getting success in all the 10 examples. We further study
theXandYmetrics, also provided in Table I. Notably,
X2is less strict in assessing TPs, assessing all DOs as
true positives. In BEP 2,Y2consequently plays the role
of suitable metric, providing correct assessment for all the
10 examples. Y1is only slightly poorer than Y2, providing
8 correct assessments out of 10. Thus, the role of bottom
edge in correct assessment is veriÔ¨Åed.
The general criteria of assessing the pixel-based seman-
tic segmentation are the same for maritime CV , where
distance and span of an MV are important considerations.
The bottom most pixels in semantic segmentations, which
also form the bottom edge in a bounding box, are the
most important determinant of the distance. The widest
span of the semantic segmentation, which also forms the
width of a bounding box, is the determinant of the span
of the vessel. Therefore, the concept of BEPs is generally
applicable to semantic segmentation as well.
V. E XPERIMENTS AND RESULTS
Detection of MVs in a maritime environment falls under
the ensemble problem set of ‚Äòdetection in dynamic back-
ground‚Äô. CV methods solve it by modeling and subtracting
the dynamic background, followed by segmentation of
the foreground [52], [53]. The dataset and the dynamic
background subtraction methods used here are described
below. We consider deep learning also for detection of
MVs. These details are presented, followed by quantitative
and qualitative results.
a) Dataset: We use on-shore (Ô¨Åxed camera) visible
range maritime videos from the maritime dataset, namely
Singapore maritime dataset, published with [3]. There are
34 high-deÔ¨Ånition videos taken from Canon 70D cameras,
Canon EF 70-300mm f/4-5.6 IS USM. The dataset has
been captured at different times, such as before sunrise,
at sunrise, at mid day, in the afternoon, in the evening,
and 2 hours after sunset. We excluded the videos taken
in haze and rain to avoid additional challenges. BBs of
objects in each frame of the video are provided along
with the dataset. Each BB is labeled with one of the
following class labels: boat, buoy, ferry, Ô¨Çying bird/plane,
kayak, sailboat, speed boat, vessel/ship, and others. We
have not included on-board videos for the reason we
explain next. The motion of the vessel on which camera
is mounted with respect to water and horizon presentsadditional challenges for dynamic background detection.
The static background methods that use only current frame
for background modeling are better candidates, but they
have been shown to present extremely poor performance
for maritime scenes [3].
b) Dynamic background subtraction (BGS) methods
tested: We tested 22 BGS methods from the BGS library
named bgslibrary [20], [46] and 14 BGS methods from the
low rank and sparse (LRS) tools library name lrslibrary
[47]. The methods in BGS library are implemented in
C++. The methods in LRS library are implemented in
Matlab. All the methods were executed on Intel i7 6500 U
@2.5 GHz desktop with 16 GB RAM and Linux platform.
Default parameters have been used for all the methods.
Parameter tuning for achieving the best performance for
each method is out of the scope of this work. Yet, we note
that Ô¨Åne tuning the control parameters for each method is
likely to have a positive effect on the quality of detections,
and is likely to impact all the metrics positively. All
detected BBs less than 20 pixels in any dimension are
rejected as obviously spurious detections. We group the 36
methods into six broad categories based on their central
concept. The groups and the methods in each of them are
listed in Table II. Among the 36 methods, only IMBS has
been developed speciÔ¨Åcally for maritime scenes.
c) Regions with convolution neural network (R-CNN)
features for detection using deep learning: We conducted
two experiments in deep learning. These experiments were
executed in Matlab on NVidia DGX-1 graphics server
and Linux platform. The standard procedure of applying
non-max suppression has not been used for the reason
explained next. Many overlapping objects may be present
in a maritime scene. Consider Fig. 6(b) for example.
The GT bounding box of the vessel A overlaps with
the GT bounding boxes of the vessels F, G, H, and I.
Even if the DOs corresponding to them might be accurate,
applying non-max suppression will result into lower recall
because it will suppress either the DO of object A and
other objects. First, we randomly selected 20 videos from
the dataset for training and trained R-CNN [48] with
AlexNet architecture. The results for this experiment were
extremely poor and are not reported here. We attribute
the poor performance to the challenging nature of the
maritime scene and consider that maritime scenes may
require camera and illumination speciÔ¨Åc training. In the
second experiment, we formed the training dataset using
every Ô¨Åfth frame of all the videos. The objective was to
test if R-CNN can detect the objects it has been trained for.
R-CNN trained on CIFAR-10 [54] performed poorly but
R-CNN trained on ImageNet [55] provided better results.
We note that R-CNN experiments may be considered to
have unfair advantage over the other methods tested in this
paper because the R-CNN experiments use training on a
subset of images drawn from the main set itself. We note
also that use of R-CNN here [48] is a Ô¨Årst attempt of deep
learning for maritime CV . Better suited approaches may
6
TABLE I: Qualitative comparison of metrics for examples in Fig. 1(b) is given here. The thresholds used for determining TPs are
given in parentheses. For BEPs, ( x0; y0) are given. The number of successes is the number of times a metric assesses the example
as acceptable for maritime CV (i.e. number of matches with the maritime CV row).
Example 1 2 3 4 5 6 7 8 9 10 Number of Successes
Maritime CV TP TP TP FP TP TP FP FP FP FP Not applicable
IOU (0.5) TP FP FP TP FP FP FP TP FP TP 3
Dice (0.5) TP FP FP TP TP FP TP TP TP TP 2
IOG (0.5) TP FP FP FP TP FP TP TP FP TP 4
BEP 1(0.7,0.75) TP TP TP FP FP FP FP TP TP FP 6
BEP 2(0.7,0.75) TP TP TP FP TP TP FP FP FP FP 10
X1(0.7) TP TP TP TP FP FP TP TP TP TP 3
X2(0.7) TP TP TP TP TP TP TP TP TP TP 5
Y1(0.75) TP TP TP FP TP TP FP TP TP FP 8
Y2(0.75) TP TP TP FP TP TP FP FP FP FP 10
TABLE II: List of background subtraction methods is presented here. The methods are grouped according to the central concept
behind them. The best results of each group appear in Table III. The number of methods in each group is indicated in fg.
Group Methods in the group
Spatio-temporal Ô¨Ålters (STF) - f4g Temporal mean (TM) [12], Prati‚Äôs median (PM) [13], adaptive median (AM) [14],  BGS [15]
Gaussian models (GM) - f8g Simple Gaussian (SG) [16], Gaussian average (GA) [17], Grimson‚Äôs Gaussian mixture model (GMM)
[18], Zivkovic‚Äôs adaptive GMM (AGMM) [19], mixture of Gaussians (MoG) [20], fuzzy Gaussian (FG)
[21], type-2 fuzzy GMM - uncertain mean (T2FUM) [22], type-2 fuzzy GMM - uncertain variance
(T2FUV) [22]
Kernel models (KM) - f2g Kernel density estimation (KDE) [23], VuMeter [24]
Self organizing maps (SOM) - f2g Adaptive self organizing maps (ASOM) [25], fuzzy ASOM (FASOM) [25]
Low rank and sparsity (LRS) -
f15gEigen-background (EB) [26], active subspace (AS) robust principal component analysis (RPCA) [27], fast
(F) principal component pursuit (PCP) [28], Reimanian robust (R2) PCP [29], MoG-RPCA [30], non-
convex (NC) RPCA [31], Grassman average [32], greedy semi-soft go decomposition (GreGoDec) [33],
orthogonal rank-one matrix pursuit (OR1MP) [34], Grassmannian rank-one update subspace estimation
(GROUSE) [35], low-rank matrix completion by Riemannian optimization (LRGeomCG) [36], non-
negative matrix factorization (NMF) with sparse matrix (LS2) [37], Deep semi NMF (DSNMF) [38],
alternating direction method of multipliers (ADMM) [39], robust orthonormal subspace learning (ROSL)
[40]
Texture, color, and regions (TCR)
-f5gTexture BGS (TBGS) [41], independent multimodal background subtraction (IMBS) [42], multicue [43],
local binary similarity segmenter (LOBSTER) [44], self-balanced sensitivity segmenter (SuBSENSE) [45]
be identiÔ¨Åed in the future. Some options include faster
R-CNN [49], long-term temporal convolution CNNs [50],
networks on convolutional feature maps CNN [51].
d) Qualitative examples: We consider four example
frames, each taken from a different video of the dataset.
The detection results of 10 BGS methods and R-CNN
are shown in Fig. 6. The selected BGS methods are the
ones that consistently outperform other methods in their
groups either is precision or in recall. These methods are
identiÔ¨Åed in Table III. All BGS methods are ineffective in
subtracting the background. In Fig. 6, all BGS methods
except SuBSENSE detect false positive objects in the
water background. This problem is more severe in frames
3 and 4, which show relatively more turbulent waters.
Consider fast moving objects in Fig. 6: E in frame 1,
A in frame 2, and D in frame 4. Most methods generate
phantom foreground for these objects, exceptions include
Prati‚Äôs median, SuBSENSE, and IMBS. Such phantoms
may result into one wider detection or multiple individual
detections, see KDE results for object A in frame 2 and ob-
ject E in frame 1 for respective examples. These examples
indicate a challenge not recognized in [3]. Dynamic BGS
should incorporate large variations in the speeds of the
vessels (both in the physical scales and the image scales)
for avoiding phantom detections of fast vessels.
Wakes result in wider BBs in most methods for object
D in frame 4. The detected spans of the fast movingobjects and the objects with wakes are larger than the
actual objects. For a fast moving object, information of
minimum span and bottom edge is critically important for
collision avoidance. It is acceptable, although not prefer-
able, to interpret a larger span than the actual span. Thus,
despite wider BBs, these detections are useful for collision
avoidance. The BB of SuBSENSE corresponding to object
A in frame 2 is comparatively less acceptable, since it
underestimates the span of the vessel. IOU (0.5) estimates
it as true positive, even though this detection indicates
deÔ¨Åciency of SuBSENSE for collision avoidance. Also,
note that fuzzy Gaussian BGS generates one signiÔ¨Åcantly
larger BB for each example frame, with the bottom edge
of BB much below a GT‚Äôs bottom edge. IOG detects it
as a true positive, even though such detections are clearly
deÔ¨Åcient for collision avoidance.
Now, consider object A in frame 1 and objects B-D in
frame 3. For these objects, several methods detect either
the super-structure or the hull. Or, they break down the
object into several smaller detections (note object A of
frame 1). While the detected hulls indicate acceptable
performance for collision avoidance, the detected super-
structures or portions of the objects are unacceptable.
BEPs are effective in assessing both these conditions
appropriately.
Frame 2 presents an example of several occluded objects
with small pixel foot prints. Different methods give varied
7
(a) Example frame 1 (b) Example frame 2
(c) Example frame 3 (d) Example frame 4
Fig. 6: Example results of CV methods for detection through dynamic BGS. The subtracted background appears white in the results
of the methods. Ground truths: yellow BBs. Detected objects (foreground segmentations obtained after BGS): blue or red BBs. Red
BBs: DOs referred in the text.
results, several of them being useful for an initial estimate.
This indicates potential for CV methods. However, sup-
pression of false positive detections in water background
is important for reasonable conclusion. At the same time,
situations such as example 9 from Fig. 1(b) also occur in
numerous places. See for example, the results of eigen-
background and KDE for the example frame 2. Even with
the BEP metrics, assessing them appropriately for collision
avoidance in maritime CV is an open problem.
The results of R-CNN for the four example frames
indicate that detections using R-CNN are better and less
affected by wake. Moreover, DOs typically span both
the hull and the super-structure. We note that the current
implementation detects the same objects that it has been
trained for, which is the reason for better quality of DOs.
This approach is suitable only where environment speciÔ¨Åc
training is feasible and practically useful.
e) Quantitative results: We assess the true positive
(TP) detections in all the frames of the all the videos in the
dataset. The precision for the entire dataset is computedas the ratio of the total number of TPs to the total number
of DOs. The recall is computed as the ratio of the total
number of TPs to the total number of GTs. The assessment
of TPs is performed using different assessment metrics and
different threshold values for all of them. For IOU, Dice
index, and IOG, we consider values 0.5, 0.7, and 0.9 for
the threshold c0. We note that IOU (0.5) is recommended
in the well-known Pascal challenge [11]. The threshold
x0for BEP 1and BEP 2is 1-dimensional analogue of c0
for IOU and IOG, respectively. Thus, we use three valuesp
0:5,p
0:7, andp
0:9forx0. We use three values 0.6,
0.75, and 0.9 for the threshold y0. We include the results
in which TPs are assessed using the Ymetrics alone.
The precision and recall values of the six BGS groups
identiÔ¨Åed in Table II and the R-CNN are given in Table
III. The precision and recall values are color coded for
easy visual interpretation.
TCR methods are more effective at background subtrac-
tion than the other methods (see results of SuBSENSE
in Fig. 6). So, false positive detections due to water
8
TABLE III: Precision and recall of CV methods for the maritime dataset. Best results for each group identiÔ¨Åed in Table II are
presented here. In each group, the methods that consistently give the best precision or recall for most assessment criteria are indicated
in the bottom row.
Legend
Precision Recall
0:10:20:30:40:50:10:20:30:40:5
parameters Precision Recall
c0orx0y0 STF GM KM SOM LRS TCR CNN STF GM KM SOM LRS TCR CNN
IOU
(c0)0.5  0.01 0.00 0.01 0.01 0.00 0.15 0.28 0.14 0.11 0.10 0.10 0.14 0.07 0.41
0.7  0.00 0.00 0.00 0.00 0.00 0.05 0.12 0.05 0.04 0.04 0.03 0.05 0.02 0.18
0.9  0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.01 0.01 0.01 0.00 0.00
Dice
(c0)0.5  0.01 0.01 0.01 0.01 0.00 0.25 0.35 0.26 0.20 0.19 0.18 0.25 0.11 0.51
0.7  0.00 0.00 0.00 0.01 0.00 0.14 0.25 0.12 0.09 0.08 0.08 0.11 0.07 0.37
0.9  0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.02 0.02 0.01 0.01 0.02 0.01 0.04
IOG
(c0)0.5  0.01 0.01 0.01 0.01 0.07 0.43 0.40 0.32 0.30 0.20 0.19 0.32 0.19 0.58
0.7  0.01 0.01 0.01 0.01 0.07 0.40 0.32 0.24 0.26 0.14 0.13 0.25 0.17 0.47
0.9  0.00 0.00 0.00 0.00 0.07 0.36 0.17 0.15 0.19 0.09 0.07 0.17 0.16 0.24
BEP 1
(x0;y0)p
0:5 0.6 0.01 0.01 0.01 0.01 0.00 0.18 0.26 0.15 0.12 0.13 0.10 0.15 0.06 0.38p
0:7 0.6 0.00 0.01 0.01 0.01 0.00 0.17 0.24 0.13 0.10 0.12 0.08 0.12 0.06 0.35p
0:9 0.6 0.00 0.00 0.00 0.00 0.00 0.13 0.16 0.08 0.07 0.08 0.06 0.08 0.04 0.23p
0:5 0.75 0.00 0.00 0.00 0.00 0.00 0.12 0.15 0.09 0.07 0.08 0.05 0.09 0.04 0.21p
0:7 0.75 0.00 0.00 0.00 0.00 0.00 0.11 0.14 0.08 0.06 0.07 0.05 0.07 0.04 0.20p
0:9 0.75 0.00 0.00 0.00 0.00 0.00 0.10 0.09 0.05 0.04 0.05 0.04 0.05 0.03 0.13p
0:5 0.9 0.00 0.00 0.00 0.00 0.00 0.04 0.02 0.03 0.03 0.03 0.02 0.03 0.01 0.03p
0:7 0.9 0.00 0.00 0.00 0.00 0.00 0.04 0.02 0.03 0.03 0.03 0.02 0.03 0.01 0.03p
0:9 0.9 0.00 0.00 0.00 0.00 0.00 0.04 0.01 0.02 0.02 0.03 0.02 0.02 0.01 0.02
BEP 2
(x0;y0)p
0:5 0.6 0.01 0.01 0.02 0.01 0.00 0.21 0.33 0.38 0.31 0.27 0.23 0.38 0.12 0.49p
0:7 0.6 0.01 0.01 0.01 0.01 0.00 0.21 0.31 0.35 0.28 0.25 0.21 0.32 0.12 0.45p
0:9 0.6 0.01 0.01 0.01 0.01 0.00 0.17 0.21 0.25 0.20 0.20 0.16 0.25 0.09 0.31p
0:5 0.75 0.01 0.01 0.01 0.01 0.00 0.16 0.26 0.33 0.26 0.23 0.19 0.33 0.10 0.38p
0:7 0.75 0.01 0.01 0.01 0.01 0.00 0.16 0.23 0.30 0.24 0.21 0.17 0.30 0.10 0.34p
0:9 0.75 0.01 0.01 0.01 0.01 0.00 0.13 0.15 0.23 0.18 0.18 0.14 0.23 0.08 0.23p
0:5 0.9 0.01 0.01 0.01 0.01 0.00 0.09 0.16 0.26 0.21 0.18 0.14 0.27 0.08 0.23p
0:7 0.9 0.01 0.01 0.01 0.01 0.00 0.09 0.14 0.24 0.20 0.17 0.14 0.25 0.08 0.20p
0:9 0.9 0.00 0.00 0.01 0.01 0.00 0.07 0.09 0.19 0.15 0.15 0.11 0.20 0.07 0.13
Y1(y0)  0.6 0.12 0.24 0.05 0.05 0.01 0.59 0.58 0.88 0.92 0.78 0.70 0.86 0.45 0.85
  0.75 0.09 0.17 0.04 0.04 0.01 0.53 0.55 0.81 0.87 0.72 0.63 0.80 0.37 0.81
  0.9 0.05 0.07 0.03 0.03 0.01 0.39 0.45 0.62 0.70 0.56 0.46 0.62 0.26 0.65
Y2(y0)  0.6 0.01 0.01 0.02 0.01 0.00 0.23 0.34 0.38 0.31 0.28 0.24 0.38 0.13 0.49
  0.75 0.01 0.01 0.01 0.01 0.00 0.17 0.24 0.30 0.24 0.22 0.18 0.30 0.10 0.35
  0.9 0.00 0.01 0.01 0.01 0.00 0.08 0.09 0.20 0.15 0.15 0.11 0.20 0.07 0.13
Consistently best
PM
GA
KDE
FASOM
EB
SuBSENSE
AlexNet
AM
FG
KDE
ASOM
EB
IMBS
AlexNet
background are very few, leading to better precision than
other methods. Also, precision values of SuBSENSE for
BEP 2metric are not poor considering that it was not
developed speciÔ¨Åcally for the maritime domain. On the
other hand, IMBS does not provide the best precision
or recall even though it was developed speciÔ¨Åcally for
the maritime domain. A reason could be that IMBS was
developed for high mounted cameras in urban maritime,
a setting different from the current dataset. The precision
and recall results for R-CNN are expectedly better than the
other approaches. However, noting that the R-CNN here
detects the objects it has been trained for, the precision and
recall should have been better. These clearly demonstrate
the challenging nature of maritime CV .
The several false positives in most BGS methods (see
Fig. 6) result in poor precision. Most methods have recall
better than precision, with the exception of TCR methods.
We also note that BEP 2values are more encouraging than
IOU, Dice Index, IOG, and BEP 2. The better suitabilityof BEP 2was established in Table I. Moreover, it is noted
in Table III that Y1is less selective about TPs. This puts
the responsibility on X1for improving the selectivity of
BEP 1. On the other hand, Y2is inherently more selective,
as demonstrated by lower precision and recall values than
Y1. This directly helps in making BEP 2selective.
We compare assessment metrics IOU(0.5) and
BEP 1(p
0:5;0:6), which correspond to most lenient
threshold values. Recall values for BEP 1(p
0:5;0:6)are
better than IOU(0.5) in each group. For the most strict
threshold values as well, recall values for BEP 1(p
0:9;0:9)
are better than IOU(0.9) in each group. The same can be
inferred from the comparison of IOG and BEP 2, barring
a few exceptions. Thus, although the conventional metrics
indicate dismal performance of CV methods for maritime,
the scene does not look so bleak when metrics designed
for maritime domain are used. This highlights the need
of both suitable metrics and dedicated CV solutions.
9
VI. D ISCUSSION
We evaluated the existing metrics for assessing the
quality of BB detections in the context of maritime CV .
The unique needs of maritime CV imply that the current
metrics are unsuitable. The proposed bottom edge prox-
imity metrics, custom designed for maritime CV , provide
a good starting point. However, there is a need to explore
more options for assessing detections in maritime CV .
Such assessment metrics would be strict in assessing the
location of the bottom edges and minimum span of the
BBs, suitable for assessing inaccurate detections due to
occlusion, and tolerant for BB degradation in presence of
wake or exclusion of super-structure in the detected BB.
It is worth considering if the conventional BB labeling of
GT is suitable for maritime CV . It should be explored if
the GT of each vessel should comprise of GTs for hull,
super-structure, and their union. An associated problem
is to design assessment of detected BBs for such GT.
Creating shape and pixel segmentations as ground truth
for large videos needs to be explored. Detections and their
assessment in the form of shape and pixel segmentations
can be explored for new maritime CV methods.
Our preliminary study of 36 background subtraction
methods and two R-CNN experiments shows a gap in CV
techniques for maritime applications. Appropriate model-
ing of maritime background can reduce false positives
and improve precision. Modeling wakes as background
as well may allow stricter assessment of span (larger
x0) and thus better assessment of occlusions as well.
Large range of speeds and sizes of maritime objects may
require innovative approaches for learning background
with adaptive time scales in local regions. Deep learning
also holds signiÔ¨Åcant promise. Our current experiments
assume the luxury of environment speciÔ¨Åc training. A
more generalizable deep learning framework for maritime
is needed for practical maritime computer vision.
We note that the maritime computer vision is in a
nascent stage at present. It is too early to decide on a
suitable metric. A better convergence on these topics will
emerge with further engagement of the CV community.
The engagement can be through new diverse maritime
datasets and maritime CV challenges similar to the PAS-
CAL challenge [11] with goal towards autonomous mar-
itime vehicle technology.
REFERENCES
[1] E. Tu, G. Zhang, L. Rachmawati, E. Rajabally, and G.-B. Huang,
‚ÄúExploiting AIS data for intelligent maritime navigation: A com-
prehensive survey from data to methodology,‚Äù IEEE Transactions
on Intelligent Transportation Systems , vol. PP, no. 99, pp. 1‚Äì24,
2017.
[2] D. Bloisi and L. Iocchi, ‚ÄúARGOS A video surveillance system for
boat trafÔ¨Åc monitoring in Venice,‚Äù International Journal of Pattern
Recognition and ArtiÔ¨Åcial Intelligence , vol. 23, no. 07, pp. 1477‚Äì
1502, 2009.
[3] D. K. Prasad, D. Rajan, L. Rachmawati, E. Rajabaly, and C. Quek,
‚ÄúVideo processing from electro-optical sensors for object detection
and tracking in maritime environment: a survey,‚Äù IEEE Transac-
tions on Intelligent Transportation Systems , vol. 18, no. 8, pp.
1993‚Äì2016, 2017.[4] P. J. Withagen, K. Schutte, A. M. V ossepoel, and M. G. Breuers,
‚ÄúAutomatic classiÔ¨Åcation of ships from infrared (FLIR) images,‚Äù
inSignal Processing, Sensor Fusion, and Target Recognition VIII ,
vol. 3720, 1999, pp. 180‚Äì188.
[5] M. Levandowsky and D. Winter, ‚ÄúDistance between sets,‚Äù Nature ,
vol. 234, article no. 5323, 1971.
[6] D. G. Altman and J. M. Bland, ‚ÄúDiagnostic tests. 1: Sensitivity and
speciÔ¨Åcity,‚Äù British Medical Journal , vol. 308, no. 5323, pp. 1552,
1994.
[7] L. R. Dice, ‚ÄúMeasures of the amount of ecologic association
between species,‚Äù Ecology , vol. 26, pp. 297‚Äì302, 1945.
[8] A. Cuzzocrea, E. Mumolo, and G. M. Grasso, ‚ÄúAdvanced pattern
recognition from complex environments: a classiÔ¨Åcation-based ap-
proach,‚Äù Soft Computing , pp. 1‚Äì16, 2017.
[9] A. Tversky, ‚ÄúFeatures of similarity,‚Äù Psychological Review , vol. 84,
no. 4, pp. 327‚Äì352, 1977.
[10] D. K. Prasad, C. K. Prasath, D. Rajan, L. Rachmawati, E. Raja-
bally, and C. Quek, ‚ÄúObject detection in maritime environment:
Performance evaluation of background subtraction methods,‚Äù IEEE
Transactions on Intelligent Transportation Systems ,vol. 22, no. 5,
pp. 1787‚Äì1802, 2019.
[11] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams,
J. Winn, and A. Zisserman, ‚ÄúThe pascal visual object classes
challenge: A retrospective,‚Äù International Journal of Computer
Vision , vol. 111, no. 1, pp. 98‚Äì136, 2015.
[12] A. H. Lai and N. H. Yung, ‚ÄúA fast and accurate scoreboard algo-
rithm for estimating stationary backgrounds in an image sequence,‚Äù
inIEEE International Symposium on Circuits and Systems , vol. 4,
1998, pp. 241‚Äì244.
[13] S. Calderara, R. Melli, A. Prati, and R. Cucchiara, ‚ÄúReliable
background suppression for complex scenes,‚Äù in ACM International
Workshop on Video Surveillance and Sensor Networks , 2006, pp.
211‚Äì214.
[14] N. J. McFarlane and C. P. SchoÔ¨Åeld, ‚ÄúSegmentation and tracking of
piglets in images,‚Äù Machine Vision and Applications , vol. 8, no. 3,
pp. 187‚Äì193, 1995.
[15] A. Manzanera and J. C. Richefeu, ‚ÄúA new motion detection algo-
rithm based on ‚Äìbackground estimation,‚Äù Pattern Recognition
Letters , vol. 28, no. 3, pp. 320‚Äì328, 2007.
[16] Y . Benezeth, P.-M. Jodoin, B. Emile, H. Laurent, and C. Rosen-
berger, ‚ÄúReview and evaluation of commonly-implemented back-
ground subtraction algorithms,‚Äù in International Conference on
Pattern Recognition , 2008, pp. 1‚Äì4.
[17] C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland,
‚ÄúPÔ¨Ånder: Real-time tracking of the human body,‚Äù IEEE Transac-
tions on Pattern Analysis and Machine Intelligence , vol. 19, no. 7,
pp. 780‚Äì785, 1997.
[18] C. Stauffer and W. E. L. Grimson, ‚ÄúAdaptive background mixture
models for real-time tracking,‚Äù in IEEE Conference on Computer
Vision and Pattern Recognition , vol. 2, 1999, pp. 246‚Äì252.
[19] Z. Zivkovic, ‚ÄúImproved adaptive gaussian mixture model for
background subtraction,‚Äù in International Conference on Pattern
Recognition , vol. 2, 2004, pp. 28‚Äì31.
[20] A. Sobral and A. Vacavant, ‚ÄúA comprehensive review of back-
ground subtraction algorithms evaluated with synthetic and real
videos,‚Äù Computer Vision and Image Understanding , vol. 122, pp.
4‚Äì21, 2014.
[21] M. H. Sigari, N. Mozayani, and H. Pourreza, ‚ÄúFuzzy running aver-
age and fuzzy background subtraction: concepts and application,‚Äù
International Journal of Computer Science and Network Security ,
vol. 8, no. 2, pp. 138‚Äì143, 2008.
[22] Z. Zhao, T. Bouwmans, X. Zhang, and Y . Fang, ‚ÄúA fuzzy back-
ground modeling approach for motion detection in dynamic back-
grounds,‚Äù in Multimedia and Signal Processing , 2012, pp. 177‚Äì185.
[23] A. Elgammal, D. Harwood, and L. Davis, ‚ÄúNon-parametric model
for background subtraction,‚Äù in European Conference on Computer
Vision , 2000, pp. 751‚Äì767.
[24] Y . Goya, T. Chateau, L. Malaterre, and L. Trassoudaine, ‚ÄúVehicle
trajectories evaluation by static video sensors,‚Äù in IEEE Intelligent
Transportation Systems Conference , 2006, pp. 864‚Äì869.
[25] L. Maddalena and A. Petrosino, ‚ÄúA fuzzy spatial coherence-based
approach to background/foreground separation for moving object
detection,‚Äù Neural Computing and Applications , vol. 19, no. 2, pp.
179‚Äì186, 2010.
10
[26] N. M. Oliver, B. Rosario, and A. P. Pentland, ‚ÄúA bayesian computer
vision system for modeling human interactions,‚Äù IEEE Transactions
on Pattern Analysis and Machine Intelligence , vol. 22, no. 8, pp.
831‚Äì843, 2000.
[27] G. Liu and S. Yan, ‚ÄúActive subspace: Toward scalable low-rank
learning,‚Äù Neural Computation , vol. 24, no. 12, pp. 3371‚Äì3394,
2012.
[28] P. Rodriguez and B. Wohlberg, ‚ÄúFast principal component pursuit
via alternating minimization,‚Äù in IEEE International Conference on
Image Processing , 2013, pp. 69‚Äì73.
[29] M. Hinterm ¬®uller and T. Wu, ‚ÄúRobust principal component pursuit
via inexact alternating minimization on matrix manifolds,‚Äù Journal
of Mathematical Imaging and Vision , vol. 51, no. 3, pp. 361‚Äì377,
2015.
[30] Q. Zhao, D. Meng, Z. Xu, W. Zuo, and L. Zhang, ‚ÄúRobust
principal component analysis with complex noise,‚Äù in International
Conference on Machine Learning , 2014, pp. 55‚Äì63.
[31] Z. Kang, C. Peng, and Q. Cheng, ‚ÄúRobust pca via nonconvex rank
approximation,‚Äù in IEEE International Conference on Data Mining ,
2015, pp. 211‚Äì220.
[32] S. Hauberg, A. Feragen, and M. J. Black, ‚ÄúGrassmann averages for
scalable robust pca,‚Äù in IEEE Conference on Computer Vision and
Pattern Recognition , 2014, pp. 3810‚Äì3817.
[33] T. Zhou and D. Tao, ‚ÄúGreedy bilateral sketch, completion &
smoothing,‚Äù in International Conference on ArtiÔ¨Åcial Intelligence
and Statistics , 2013.
[34] Z. Wang, M.-J. Lai, Z. Lu, W. Fan, H. Davulcu, and J. Ye, ‚ÄúOr-
thogonal rank-one matrix pursuit for low rank matrix completion,‚Äù
SIAM Journal on ScientiÔ¨Åc Computing , vol. 37, no. 1, pp. A488‚Äì
A514, 2015.
[35] L. Balzano, R. Nowak, and B. Recht, ‚ÄúOnline identiÔ¨Åcation and
tracking of subspaces from highly incomplete information,‚Äù in
Annual Allerton Conference on Communication, Control, and Com-
puting , 2010, pp. 704‚Äì711.
[36] B. Vandereycken, ‚ÄúLow-rank matrix completion by riemannian
optimization,‚Äù SIAM Journal on Optimization , vol. 23, no. 2, pp.
1214‚Äì1236, 2013.
[37] Y . Ji and J. Eisenstein, ‚ÄúDiscriminative improvements to distribu-
tional sentence similarity,‚Äù in Conference on Empirical Methods in
Natural Language Processing , 2013, pp. 891‚Äì896.
[38] G. Trigeorgis, K. Bousmalis, S. Zafeiriou, and B. W. Schuller,
‚ÄúA deep matrix factorization method for learning attribute repre-
sentations,‚Äù IEEE Transactions on Pattern Analysis and Machine
Intelligence , vol. 39, no. 3, pp. 417‚Äì429, 2017.
[39] S. Boyd, N. Parikh, E. Chu, B. Peleato, J. Eckstein et al. , ‚ÄúDis-
tributed optimization and statistical learning via the alternating di-
rection method of multipliers,‚Äù Foundations and Trends in Machine
learning , vol. 3, no. 1, pp. 1‚Äì122, 2011.
[40] X. Shu, F. Porikli, and N. Ahuja, ‚ÄúRobust orthonormal subspace
learning: EfÔ¨Åcient recovery of corrupted low-rank matrices,‚Äù in
IEEE Conference on Computer Vision and Pattern Recognition ,
2014, pp. 3874‚Äì3881.
[41] M. Heikkila and M. Pietikainen, ‚ÄúA texture-based method for
modeling the background and detecting moving objects,‚Äù IEEE
Transactions on Pattern Analysis and Machine Intelligence , vol. 28,
no. 4, pp. 657‚Äì662, 2006.
[42] D. Bloisi and L. Iocchi, ‚ÄúIndependent multimodal background
subtraction,‚Äù in International Conference on Computational Mod-
eling of Objects Presented in Images: Fundamentals, Methods and
Applications , 2012, pp. 39‚Äì44.
[43] S. Noh and M. Jeon, ‚ÄúA new framework for background subtraction
using multiple cues,‚Äù in Asian Conference on Computer Vision ,
2012, pp. 493‚Äì506.
[44] P.-L. St-Charles and G.-A. Bilodeau, ‚ÄúImproving background sub-
traction using local binary similarity patterns,‚Äù in IEEE Winter
Conference on Applications of Computer Vision , 2014, pp. 509‚Äì
515.
[45] P.-L. St-Charles, G.-A. Bilodeau, and R. Bergevin, ‚ÄúFlexible back-
ground subtraction with self-balanced local sensitivity,‚Äù in IEEE
Conference on Computer Vision and Pattern Recognition Work-
shops , 2014, pp. 408‚Äì413.
[46] A. Sobral, ‚ÄúBGSLibrary: An opencv c++ background subtraction
library,‚Äù 2013, pp. 1‚Äì16. [Online]. Available: https://github.com/
andrewssobral/bgslibrary[47] A. Sobral, T. Bouwmans, and E.-h. Zahzah, ‚ÄúLRSLibrary: Low-
rank and sparse tools for background modeling and subtraction in
videos,‚Äù in Robust Low-Rank and Sparse Matrix Decomposition:
Applications in Image and Video Processing . CRC Press, Taylor
and Francis Group.
[48] R. Girshick, J. Donahue, T. Darrell, and J. Malik, ‚ÄúRich feature hi-
erarchies for accurate object detection and semantic segmentation,‚Äù
inIEEE conference on Computer Vision and Pattern Recognition ,
2014, pp. 580‚Äì587.
[49] S. Ren, K. He, R. Girshick, and J. Sun, ‚ÄúFaster R-CNN: Towards
real-time object detection with region proposal networks,‚Äù in Ad-
vances in Neural Information Processing Systems , 2015, pp. 91‚Äì99.
[50] G. Varol, I. Laptev, and C. Schmid, ‚ÄúLong-term temporal con-
volutions for action recognition,‚Äù IEEE Transactions on Pattern
Analysis and Machine Intelligence , vol. PP, no. 99, pp. 1‚Äì1, 2017.
[51] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun, ‚ÄúObject detection
networks on convolutional feature maps,‚Äù IEEE Transactions on
Pattern Analysis and Machine Intelligence , vol. 39, no. 7, pp. 1476‚Äì
1481, 2017.
[52] T. Bouwmans, ‚ÄúTraditional and recent approaches in background
modeling for foreground detection: An overview,‚Äù Computer Sci-
ence Review , vol. 11, pp. 31-66,2014.
[53] T. Bouwmans, F. Porikli, B. H ¬®oferlin and A. Vacavant Background
modeling and foreground detection for video surveillance , CRC
Press, 2014.
[54] A. Krizhevsky and G. Hinton ‚ÄúLearning multiple layers of features
from tiny images,‚Äù, vol. 1, no. 4. Technical report , University of
Toronto, 2009.
[55] J. Deng, W. Dong, et. al, ‚ÄúImagenet: A large-scale hierarchical
image database,‚Äù in IEEE conference on Computer Vision and
Pattern Recognition , 2009, pp. 248‚Äì255.
Dilip K. Prasad received the B.Tech. and Ph.D.
degrees in computer science and engineering
from the Indian Institute of Technology (ISM),
Dhanbad, India, and Nanyang Technological
University, Singapore, in 2003 and 2013, re-
spectively. He is currently an associate profes-
sor at UiT The Arctic University of Norway.
His current research interests include image
processing, machine learning, and computer
vision.
Huixu Dong received the B.Sc degree in
mechatronics engineering from Harbin Insti-
tute of Technology in China, in 2013 and
Ph.D. degree at Robotics Research Centre of
Nanyang Technological University, Singapore,
in 2018. Currently, he is a post-doctoral fellow
in Robotics Institute of Carnegie Mellon Uni-
versity. His current research interests include
robotic perception and grasp in unstructured en-
vironments, computer vision and robot-oriented
artiÔ¨Åcial intelligence, the navigation of mobile
robot and optimal design of robotic gripper.
Deepu Rajan received the Bachelor of En-
gineering degree in electronics and commu-
nication engineering from the Birla Institute
of Technology, Ranchi, India, the M.S. de-
gree in electrical engineering from Clemson
University, Clemson, SC, USA, and the Ph.D.
degree from the Indian Institute of Technology,
Mumbai, India. He is an Associate Profes-
sor with the School of Computer Engineering,
Nanyang Technological University, Singapore.
His current research interests include image
processing, computer vision, and multimedia signal processing.
Chai Quek is with the School of Computer En-
gineering, Nanyang Technological University,
Singapore. He received B.Sc. and Ph.D. degrees
from Heriot-Watt University, Edinburgh, U.K.
His research interests include neurocognitive
informatics, biomedical engineering and com-
putational Ô¨Ånance. He has published over 250
international conference and journal papers. He
has been invited as a Program Committee Mem-
ber and reviewer for several conferences and
journals, including IEEE TNN, TEvC, etc."
BMVC 2019: Workshop on Interpretable and Explainable Machine Vision,['Alun Preece'],2019,http://arxiv.org/abs/1909.07245v1,
"Vision Transformers in Medical Computer Vision -- A Contemplative
  Retrospection","['Arshi Parvaiz', 'Muhammad Anwaar Khalid', 'Rukhsana Zafar', 'Huma Ameer', 'Muhammad Ali', 'Muhammad Moazam Fraz']",2022,http://arxiv.org/abs/2203.15269v1,"Vision Transformers in Medical Computer Vision - A
Contemplative Retrospection
Arshi Parvaiza, Muhammad Anwaar Khalida, Rukhsana Zafara, Huma Ameera,
Muhammad Aliaand Muhammad Moazam Fraza,‚àó
aSchool of Electrical Engineering and Computer Science,
National University of Sciences and Technology (NUST), Islamabad, 44000, Pakistan
ARTICLE INFO
Keywords :
Vision Transformers
Medical Image Analytics
Self Attention
Medical Computer Vision
Diagnostic Image Analysis
Literature SurveyABSTRACT
Recent escalation in the field of computer vision underpins a huddle of algorithms with the
magnificentpotentialtounraveltheinformationcontainedwithinimages.Thesecomputervision
algorithmsarebeingpracticedinmedicalimageanalysisandaretransfiguringtheperceptionand
interpretationofImagingdata.Amongthesealgorithms,VisionTransformers(ViTs)areevolved
as one of the most contemporary and dominant architectures that are being used in the field of
computervision.Theseareimmenselyutilizedbyaplentyofresearcherstoperformnewaswell
asformerexperiments.Here,inthisarticleweinvestigatetheintersectionofVisionTransformers
andMedicalimagesandprofferedanoverviewofvariousViTsbasedframeworksthatarebeing
usedbydifferentresearchersinordertodeciphertheobstaclesinMedicalComputerVision.We
surveyed the application of Vision transformers in different areas of medical computer vision
such as image-based disease classification, anatomical structure segmentation, registration,
region-based lesion Detection, captioning, report generation, reconstruction using multiple
medicalimagingmodalitiesthatgreatlyassistinmedicaldiagnosisandhencetreatmentprocess.
Along with this, we also demystify several imaging modalities used in Medical Computer
Vision. Moreover, to get more insight and deeper understanding, self-attention mechanism of
transformers is also explained briefly. Conclusively, we also put some light on available data
sets, adopted methodology, their performance measures, challenges and their solutions in form
of discussion. We hope that this review article will open future directions for researchers in
medical computer vision.
1. Introduction
Advances in medical imaging modalities have made them indispensable in clinical practice. The analysis of these
images by analysts is limited to human subjectivity, time constraints, and variation of interpretation, which leads to
delusion[1,2].Medicalimagescontainanampleinformationthatisthekeyformedicaldiagnosisandhencetreatment.
Thehealthcaredatacomprises90%ofimagingdata,soconsideredastheprimarysourceformedicalinterventionand
analysis. Multiple medical imaging modalities such as Computed Tomography (CT), ultrasound, X-ray radiography,
MR Imaging (MRI), and pathology are commonly used for medical imaging diagnostics. Several challenging factors
associated with medical imaging modalities such as expensive data acquisition [3], dense pixel resolution [4], lack of
standardimageacquisitiontechniquesintermsoftoolandscanningsettings[5],modality-specificartefacts[6],hugely
imbalanced data in negative and positive classes [7], sparse and noisy annotated datasets [8] are major hindrance in
translating AI based diagnosis into clinical practice.
Since its surge, deep learning has shown remarkable success in automatic image analyses of medical imaging
modalities. The advancements in deep learning have been flourished and perfected with time, revolving primarily
around one algorithm called Convolutional Neural Networks (CNN). CNNs are potentially the most popular deep
learning architecture for its distinguished capabilities to exploit the spatial and temporal relationship between the
features of images, which need to be deciphered for extracting meaningful information hidden in images [9, 10, 11].
It has achieved notable accomplishment in medical imaging applications [12, 13] such as, determining the presence
and then identifying the type of malignancy (Classification), locating the patient‚Äôs lesion (Detection), extracting the
desiredobject(organ)fromamedicalimage(Segmentation),placingseparateimagesinacommonframeofreference
moazam.fraz@seecs.edu.pk (M.M. Fraz)
ORCID(s):0000-0003-0495-463X (M.M. Fraz)
. : Page 1 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
for comparing or integrating the information they contain (Registration), synthesizing images for balancing dataset
(Generative Modeling) [14].
Despite that CNNs are very good at feature extraction tasks, they fail to encode the relative position of different
features. In a CNN, the deeper layers are limited to view at whatever the initial layers have passed to them. This way
theylosetheglobalcontextofthefeatures.Increasingthenumberoffiltersimprovestherepresentationcapacitybutat
the cost of computation [15]. Various architectural changes are suggested by researchers for an efficient solution over
thecourseoftimeandleadingtoattentionmechanisms[16].Usingattentionmechanism,theregionsoftheimageare
captured,towhichthe CNNshouldpayattention,andforwardedto thedeeperlayers.Researchershavedemonstrated
that replacing the convolutional layer with attention has improved performance [17, 18]. The breakthrough from
Transformer network [16] in Natural Language Processing (NLP) tasks has inspired researchers to leverage this
architecture for various computer vision tasks. Dosovitskiy et al. [19] proposed an adaptation to the transformer,
known as Vision Transformer (ViT) that can be applied directly to sequences of image patches for extracting fine-
grained features. In ViT, global attention is applied on 16x16 patches of the entire image, focusing on the global
salient features of the image, which resolve the long-range dependency among image content. It gets the best out
of the attention mechanism to incorporate global context in the image features without compromising computational
efficiency.
The potential of the vision transformers is further explored by many researchers for solving various problems.
However, in this survey we aim to highlight the contribution of vision transformers to circumvent the challenges in
automatic diagnostic of diseases using medical imaging modalities and their applications in medical computer vision
tasks. Our intended audience for this review are research practitioners from medical and interdisciplinary fields of
computer vision. For their assistance, we have described commonly used terminologies and their description in table
1.
This review is organized into seven sections. Section 1 briefly discusses the role of deep learning, the emergence
of the transformers, and the replacement of CNNs by transformers in medical computer vision. Section 2 discuss the
organization and papers selection methodology and distribution of the Review. Section 3 discusses different medical
imaging modalities and their application in the diagnostic and treatment of various diseases. Section 4 discusses the
emergence of vision transformers and lists all the publicly available datasets used by the reviewed paper in every
modality and deep learning task. Section 5 discusses visual recognition tasks to established the domain knowledge
for the audience of the interdisciplinary field. Section 6 gives the details about the reviewed techniques categorized
according to each deep learning task such as classification, segmentation, detection clinical report generation, and
Miscellaneous which also include image registration. Section 7 identifies research gaps in the review papers and
discusses the future directions for using transformers in medical computer vision.
1.1. Scope/Objective of the Review
The aim of writing this review paper is to highlight and discuss the contribution of Vision Transformers in
medical computer vision across different medical imaging modalities. For this purpose, we have searched out papers
from different top Conferences and Journals, excluding pre-prints, within the time span of three years from 2019-
2022. The results achieved and the adopted methodology of each paper is reviewed comprehensively. The distinct
categories that we reviewed belonging to the medical image analysis includes classification, detection, segmentation,
registration,clinicalreportgeneration,imageenhancement,imagereconstructionsandimagesynthesis.Theliterature
of these categories is further divide into different medical imaging modalities. Ultimately, in addition to accentuating
interesting techniques in the literature, we also put some light on research gaps and future directions. We hope this
review will bridge the gap between computer vision community and medical specialists to foster the future research
and development in medical computer vision This article is written keeping in mind the intended audience from the
interdisciplinary fields, medical and AI. Publicly available datasets and downloadable links are listed in the table 3.
1.2. Comparison with other Reviews
Although there exists some reviews on transformers already which enfolds a significant amount of work, yet we
feelthatthereisalotofroomforimprovement.Forexample,noreviewisprimarilyfocusedonapplicationsofvision
transformersinmedicaldomain.Tobridgethisgap,wecomeupwiththissurveyinwhichourpointofconvergenceis
to scrutinize the exertion of ViTs in medical computer vision. To initiate this process we collected a bunch of articles
addressingdifferenttransformersarchitectureandtheirutilizationonmultimodalmedicalimages.Weincludedalmost
80peerreviewedarticlesinoursurveyfromprestigiousplatformslikePubMed,Springer,IEEE,ScienceDirectwhich
. : Page 2 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Table 1
List of accronyms and abbreviations used in paper
Acronyms Words Acronyms Words
KID Kernel & Inception Distance MFTEMulti-Branch Features Transformation
and Extraction
AHA Align Hierarchical Attention MA Microaneurysms
MSAM Multi modal Spatial Attention Module CLAMClustering-Constrained-Attention
Multiple-Instance Learning
CAC Coronary Artery Calcium KFS Key Factor Sampling
EMVTEfficient Multi Scale Fusion Trans-
formerMSTGANetMulti Scale Transformer Global Atten-
tion Network
NSCF NonLocal Sparse Net Fusion MCAT Multimodal Co-Attention Transformer
TETRISTemplate Transformer Image Segmen-
tationFMNet Feature Mapping Sub-Network
RDLs Regionalized Dynamic Learners CRC Colorectal cancer
IDH Isocitrate Dehydrogenase MTTU Multi Task Transformer Unet
VITBISVision Transformer for Biomedical Im-
age SegmentationCIDErConsensus-based Image Description
Evaluation
DAFNet Disentangled Alligned and Fuse Net ASFT Adjacent Slices Feature Transformer
HYBRIDCTRM Hybrid Convolutional Transformer MTI Multi Text Indexer
VIF Visual Information Fidelity PCR Rpolymerase Chain Reaction
ABVS Automated Breast Volume Scanner PRCC Papillary Renal Cell Carcinoma
FID Frechet Inception Distance GSM Genitourinary syndrome of menopause
CEDT Cross Encoder-Decoder Transformer GLVE Global-Local Visual Extractor
makes our paper unique from other review articles. In addition to that we also explained different imaging modalities
used in medical computer comprehensively. Moreover, we have given a brief note on available medical data sets in
tabularform.Thedownloadablelinkstothesedatasetsarealsomentioninthesetables.Wealsodiscussedtheresultsof
state-of-the-artapproachesinawellstructuredtabularforminwhichwedescribedtheperformancemetricesalongwith
theirresultsontheavailabledatasets.Intheend,wehavealsopointedoutsomechallengesalongwiththeirinsightful
future directions. For comparative analysis of our review with Khan et al. [20] and Kai et al. [21] we visualized the
main points in Figure 1.
. : Page 3 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 1: Comparison with recent published reviews on Vision Transformers
2. Survey Methodology
In this section we will discuss the study selection criteria based on which articles are chosen for the review, and
distribution of the included articles according to venues (journals, conferences), medical imaging modalities, deep
learning tasks (classification, segmentation, detection etc.) and impact factors.
2.1. Papers Selection
We have demonstrated the details of the searched and included research papers in this review article through
PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses). Figure 2 shows the summary of
papers selection. We searched our papers on PubMed, Springer, IEEE Xplore, Science direct, and finally on google
scholar. In the result of search queries, we have found 11060 papers. Among which 3060 were duplicate and were
excluded from the study. We screened remaining 8000 and found 7,600 were not fulfilling the criteria of legitimacy
forthissurveyassomeofthemwasonlyaboutmedicalapplicationwithouttransformersandsomeofthemwasusing
transformer word in the different context than vision transformer. We further screened the remaining 400 articles and
excluded the preprints from our study. In the PRISMA we have shown the categorization of our included 80 papers
according to their application in medical domain. We have also demonstrated the distribution of the papers according
to the medical imaging modalities.
2.2. Data Extraction Methods
We searched different platforms such as PubMed, Springer, Science Direct, IEEE Xplore and google scholar for
extracting the research articles. We targeted top journals and conferences, in duration of last four years from 2019 to
2022. For extracting relevant papers for our study, we used different key words and combine them with the logical
operators ‚ÄòAND‚Äô, ‚ÄòOR‚Äô to get the better search results. The key words we used are:
. : Page 4 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 2: PRISMA - flow diagram of selected research articles for the review
‚Ä¢Computed tomography (CT) scans, Magnetic resonance images (MRI), Ultrasound, X-rays, Optical coher-
ence tomography (OCT), Fundus images, Positron emission tomography-Computed tomography (PET-CT),
Histopathology, Histology, WSI, Whole Slide Images
‚Ä¢Classification, Reconstruction, Segmentation, Registration, Detection, Report Generation, Enhancement
‚Ä¢Transformer, Vision Transformer
Weextractedthekeywordsfromallthearticlesonvisiontransformersincludedinourreviewandgeneratethetag
cloud,whichisshowninFigure3.ThetagcloudillustratesthetrendingtermsinViTapplicationsinmedicalcomputer
vision.Asourcentralfocusinthisreviewistorecapitulatetheapplicationofvisiontransformersonmedicalimaging
modalities, this word cloud mostly highlighting the applications (classification, segmentation, detection, denoising,
captioning), medical imaging modalities (X-rays, PET, OCT, Whole-slide, CT, Histopathological, Fundus), disease
(cancer,glaucoma,covid,diabetic,tumor,carcinoma),organs(retinal,chest,breast,pulmonary,brain)andotherdeep
learning related terms like transformer, vision, attention, encoder, decoder, multi-model.
. : Page 5 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 3: A visual depiction of most frequently used keywords in the reviewed articles
Table 2
Inclusion and exclusion criteria for papers selection
Inclusion criteria Exclusion criteria
Articles that address the medical computer
vision tasks such as registration, segmentation,
detection,classification, enactment, reconstruction
and report generation using medical imaging modalities
and vision transformer.Articles which are not using medical imaging modalities
and vision transformer.
Papers with proper evaluation metrics and detailed sum-
mary of proposed architecture including training parame-
ters.Articles that are not peer-reviewed.
Articles that are based on vision transformers. Articles that are survey papers.
The papers inclusion and exclusion criteria is given in table 2. Firstly, papers were selected on the basis of titles,
if it does not match the inclusion and exclusion criteria then we read the abstract, conclusion, and model diagram for
the final selection.
2.3. Papers Distribution
In this section we have shown the distribution of the published papers across journals, conferences, imaging
modalities, impact factors, and medical computer vision tasks. The purpose of this section is to give the bird‚Äôs eye
view of the published work, that how much literature is available in top journals and conferences, what is the impact
of the work, what imaging modalities are used and what is the progress of works across the years.
TheFigure4showsthedistributionofreviewedarticlesacrosstheyears.Itcanbeseeninthegraphthattheliterature
forvisiontransformershasgrownthroughouttheyearsfrom2019to2022astheapplicationsofvisiontransformersin
medicalimagingmodalitiesstartedgrowingfrom2019onward,withalargenumberofpublicationsintheyear2021,
in which more than 50 articles were published.
. : Page 6 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 4: A chronological distribution of vision transformers research publications in medical image analytic.
Figure5showsthecategorizationofresearcharticlesbasedonvisualrecognitiontasksusingvisiontransformers.
The recapitulation of the reviewed article is categorized based on the tasks such as classification, segmentation,
detection, report generation, registration, and miscellaneous. Miscellaneous further contains different tasks such as
reconstruction,enhancementandvisualNeuralVisualContentgeneration.Thegraphshowsthatmostofthereviewed
articles applied vision transformers on classification task which is 31%, segmentation 25%, Miscellaneous 19%,
Detection 16%, Report generation 7%, and Registration 2%.
Figure 5: Distribution of reviewed articles based on visual recognition tasks.
As the review paper is focusing on the application of vision transformers in medical imaging modalities. Figure 6
depictsthestatisticsofimagingmodalitiesusedinourreviewedarticles.Eighttypesofimagingmodalitiesareusedin
this survey paperexploiting vision transformers include X-rays imagingmodality which is 35%, CT Scans26%, MRI
Scans 13%, Histopathology Images 11%, OCT/Fundus Images 8%, PET 3%, Endoscopy 2%, and Microscopy 2%.
. : Page 7 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 6: Dispensation graph of medical imaging modalities among the articles that are reviewed.
Figure7showsthecountofvisiontransformerbasedmedicalimagingarticlestakenfromvarioustopjournalsand
conferences. Each bubble represents the number of a specific journal or a specific conference and number of articles
taken from these journals and conferences.
Figure 7: Bubble graph representing number of articles chosen from top ranked journals and conferences.
Figure 8 shows the distribution of vision transformer based reviewed papers across various journals of various
impact factors according to JCR year 2020. The bubble size shows the number of reviewed articles retrieved from
each journal. According to this figure the five papers are reviewed from IEEE Transaction on medical imaging with
. : Page 8 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
the impact factor of 10.048. The highest impact factor journal included in this survey paper is Nature Biomedical
engineering which is 25.671.
Figure 8: Visual representation of selected research publications from top tier journals along their impact factor.
3. A Delineation of Medical Imaging Modalities
Thissectionisaddedforassistingcomputervisionpractitionerstoestablishthebasicdomainknowledgeofmedical
imaging modalities and their application in the diagnostic and treatment of various diseases. Medical images differ
from natural images as they have specialized acquisition techniques. Physical phenomena such as electromagnetic
radiation, sound, light, nuclear magnetic resonance, and radioactivity are used for generating medical images of
externalorinternalorgansofhumanbody.Theseimagingtechniquescanbeappliedasnon-invasivemethodstoview
inside the human body, without any surgical intervention. Because of their importance in medical diagnostic a lot
of advancement has taken place in image acquisition devices called image modalities. These image modalities play
an important role in patients follow-up, regarding the growth of the already diagnosed disease state or undergoing
a treatment procedure as 90% of the health data comprises of images. These imaging modalities are very crucial in
public health and preventive measures as they help in establishing the accurate diagnosis. These medical images can
capturedifferentbodyregionssuchaseyes,chest,brain,heart,arms,andlegs.Therearedifferentmodalitiesofmedical
images such as computed tomography (CT), ultrasound, X-ray radiography, MR imaging (MRI), Positron emission
tomography‚Äìcomputed tomography (PET-CT), pathology fundus images and Optical coherence tomography (OCT).
The images acquired from these modalities are shown in Figure 9. Details about these image modalities are given in
the subsequent section.
. : Page 9 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
(a)
 (b)
 (c)
 (d)
(e)
 (f)
 (g)
 (h)
Figure 9: A catalogue of medical imaging modalities that vision transformers employed for assisted diagnosis. (a)Chest
X-rays that are widely used for COVID-19 or pneumonia detection. (b)Brain MRI scans that are being used for diagnosis of
aneurysms and tumors. (c)Brain CT scans that are employed to locate injuries, tumors, or clots leading to stroke. (d)OCT
images that are playing an important role in diagnosis of retinal diseases such as age-related macular degeneration (AMD)
anddiabeticeyediseases. (e)FundusimagescapturedtherearofeyeandareusedfordetectionandgradingofHypertensive
Retinopathy (f)Liver Ultrasound (g)Whole Slide Images (WSIs) that are being widely used in computational pathology
(h)PET-CT scans that are responsible for detection and diagnosis of cancer, determining the spread or recurrence of
cancer or metastasis
3.1. X-ray Imaging
AccordingtoNationalInstituteofHealth(NIH),US[22],X-raysimagesarecapturednon-invasivelyusingradiation
that is part of the electromagnetic spectrum. X-rays are mostly captured for diagnosing bone fracture [23], but chest
x-rays are also used for detecting pneumonia [24]. X-rays are also used by mammograms for breast cancer detection
[25]. Other most familiar uses of X-rays are for breast tumors [26], enlarged heart [27], blocked blood vessels [28],
conditions affecting your lungs [29] , infections [30], osteoporosis [31], arthritis [32], tooth decay [33].
3.2. Computed Tomography (CT) Scans
National Institute of Health (NIH), US [22] described computed tomography (CT) scan is a computerized x-ray
imagingtechniqueinwhichanarrowbeamofradiationisfocusedandthenquicklyrotatedaroundthebodytocapture
the detailed internal images, called tomographic images, of the body‚Äôs slice non-invasively. CT Scan produces 2-
dimesionalaswell3-dimensionalimagesofsliceofthebody.Onceseveralimagesaretakentheseimagesaredigitally
stacked together to form three-dimensional images. CT Scans are used for identifying the various organs/slices of
the body for example CT scan of the heart is used for detecting various types of heart disease or abnormalities [34].
CT Scans of the head, to locate injuries [35], tumors [36], clots leading to stroke, hemorrhage, and other conditions
[37].CTScansofthelungsisusedfordetectingcancer[38],tumorsexcessfluid,pulmonaryembolisms(bloodclots)
[39],lung infections [40] and emphysema or pneumonia [41].
3.3. Optical Coherence Tomography (OCT) & Fundus Images
AccordingtoAmericanAcademyofOphthalmology(AOA)[42],Opticalcoherencetomography(OCT)captures
invasivecross-sectionimagesoftheretinausinglightwaves.OCTcanbeusedtoexaminetheretina‚Äôsdistinctivelayers
whichhelpinmappingandmeasuringtheirthicknessandplayanimportantroleindiagnosisofretinaldiseasessuchas
. : Page 10 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
age-relatedmaculardegeneration(AMD)[43]anddiabeticeyedisease[44].OCTcanbe,further,helpfulindiagnosis
ofglaucoma[45],macularpucker[46],macularedema[47],centralserousretinopathy[48],diabeticretinopathy[49],
macular hole [50].
Another type of images, discussed by [51], that can be helpful in the diagnosis of age-related macular degeneration
(AMD) [52] are fundus images which capture the rear of the eye. It is 2D imaging modality and since glaucoma is a
‚Äú3D disease‚Äù, 3D image modality such as OCT is considered more efficient for diagnosis. Fundus images can also be
used for detection and grading of hypertensive retinopathy [53]
3.4. Magnetic Resonance Imaging (MRI)
MagneticResonanceImaging(MRI)modality,describedbytheNationalInstituteofHealth(NIH),US[22],capture
3d anatomical images noninvasively. MRI scanning does not use any radiation which make it an ultimate choice of
capturingwhenfrequentimagingisrequiredinthetreatmentprocessespeciallyinthebrain.MRIisparticularlysuitable
forcapturingthesofttissuesofthebody,butitismorecostlyascomparedtox-raysandCTscanning.MRIcanbeused
tocapturedifferentpartsofthebodyforexampleMRIareusedfordiagnosisofaneurysmsandtumors[54]aswellfor
differentiating between white matter and grey, in brain. MRI can further be used for spinal cord [55] and nerves[56],
muscles [57], and ligaments [58].
ThereisaspecializedMRIcalledfunctionalMagneticResonanceImaging(fMRI),whichisusedforobservingbrain
structure and locating the areas of the brain which are activated during cognitive tasks.[59]
3.5. Ultrasound
Radiologyinfo.org for patients [60] described ultrasound as an imaging modality that invasively create image of
organs, tissues, and other structures inside the body invasively by using sound waves without using any radiation.
Ultrasound can be used to internal organs within the body, noninvasively. For example, capturing the heart, eyes,
brain,thyroid,bloodvessels,breast,abdominalorgans,skin,andmuscles.Ultrasoundimagesarecapturedin2D,3D,
but it can also capture 4D images which is 3D in motion such as a heart beating [61] or blood flowing through blood
vessels [62].
3.6. Histopathology or Whole-Slide Imaging (WSI)
TheWhole-SlideImaging(WSI)referstocapturingthemicroscopictissuespecimensfromaglassslideofbiopsy
orsurgicalspecimenwhichresultsinhigh-resolutiondigitizedimages.Theseimagesarecapturedthrough,firsttaking
smallhigh-resolutionimagetilesorstripsandthenmontagingthemtocreateafullimageofahistologicalsection[63].
Specimens on glass slides transformed into high-resolution digital files can be efficiently stored, accessed, analyzed,
and shared with scientists from across the web using slide management technologies. Moreover, WSI is changing the
workflows of many laboratories. It is used in various disease diagnostics, prognostic and treatments such as survival
prediction [64],detection of tissue phenotypes [65], Automated grade classification [66, 67, 68], segmentation of
microvessels and nerves [69, 70], Multi-Organ Nuclei Segmentation [71].
3.7. Positron Emission Tomography ‚Äì Computed Tomography (PET-CT) Scans
According to Radiologyinfo.org [60], Positron Emission Tomography, also called PET imaging or a PET scan,
small amounts of radioactive material called radiotracers for capturing images. PET-CT scans can be used for cancer
detectionanddiagnosis[72],determiningspreadofthecancer,determiningtherecurrenceofcancer,metastasis[73],
evaluating brain abnormalities like tumor [36] and memory disorder [74], mapping normal human brain and heart
function.
4. Deep Neural Networks - Enhancing Representation Learning from CNNs to Vision
Transformers
The goal of this section is to bridge the gap between AI and healthcare analysts. It introduces the deep learning
concepts, techniques, and architectures that is found in the papers surveyed for this review article.
The progression of deep neural networks in computer vision has contributed to various fields of study, and
it primarily revolves around convolutional neural networks (CNN). For instance, while assessing medical images,
practitioners can recognize if there is an anomaly. Similarly, this mechanism can be taught to a computer via CNNs
to diagnose a disease or an anomaly while taking images as input, hence, giving vision to a computer.The model of a
. : Page 11 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
basic CNNis illustrated inFigure 10. Ittakes theimage input asa matrix ofpixel values, assignsweights to learnthe
variousdifferentiablefeatures[15].Itthenpassestheimagethroughmultiplelayersandusesmultiplefilterstocapture
thediscriminatedfeaturesfromtheimage.CNNsgenerallyconsistofthreekindsoflayers:convolutionlayers,pooling
layers,andfull-connectedlayers[75].ConvolutionlayersareresponsibleforlearningfeaturesandcapturingtheSpatial
andTemporaldependenciesbetweenthefeaturesbyapplicationofrelevantfilters.Thepoolinglayerisresponsiblefor
reducingthesizeoffeaturemapstocapturemoresemanticinformationthanspatialinformation.Inconvolutionallayer
filtersofsizeNxNwhereNisequalto1,3,5,7,oranyotheroddnumber.Thepoolinglayerusesawindowofsize2x2,
3x3,oranyotherdesiredsizetotakeaverageormaximumvalueinthatwindow.Beforethefullyconnectedlayer,the
output of the convolutional and pooling layer which is called feature map is flattened to make a fully connected layer
at a function such as softmax is applied to make a prediction and a loss function is used to calculate the error and the
is back propagated to update the values of learnable parameters.
Figure 10: A general framework of Convolutional Neural Networks (CNNs)
Depending on the application for example image classification, fully connected layers are added at the end of
the network. Stacking these layers on top of each other with a specific arrangement with the help of a differentiable
function is known as CNN architecture. In recent years several CNN architectures are developed with various such
arrangements:AlexNet[76],VGGNet[77],GoogleNet[78],ResNet[79],ResNeXt[80],SqueezeandExcitationNet
[81], DenseNet [82], and EfficientNet [83].
Convolutionalneuralnetworksareusedinvariousapplicationsinthecategoriesofimageclassification,detection,
and segmentation, etc. For example face detection [84], identification of emotions [85], Speech recognition, and
MachinetranslationusingCNNs[86],etc.ConsideringtheapplicationsofCNN,itcanbeinferredthattheycanenable
commendableresults[87].Theyareknowntobeablackbox,asthetrainingisaccordingtothetaskanddomain.One
majorlimitationistheunclarityofresultsi.e.thereasonforaparticularoutcome.Especiallyinthemedicaldomain,it
is imperative to know the cause of a specific outcome, otherwise, a wrong diagnosis can be a threat to human lives.
Onewaytotacklethisproblemhead-onistohavesuchamodelthatfocusesonrelevantpartsoftheimageandcan
be visualized by the doctors. To elucidate this issue, Attention models were proposed [16, 69]. The attention model
focusesonthepartsthatarerelevanttotheinputsequence.Moreover,amodelwasproposedknownasTransformers,
itusedtheconceptofAttentiontoenhancethetrainingspeed[19].Transformersconsistofmultipleblocksofidentical
encoders and decoders, which were composed of self-attention block and feed-forward networks. In addition, the
decoderconsistsofanextraattentionblock,whichfocusesontherelevantpartofthesequence.Theembeddedwords
oftheinputwerepassedtotheencodersequentiallyandwerepropagatedtoalltheencoders.Theoutcomesofthelast
encoderwerethenfedtothedecoders.Theperformanceofthetransfermodelswasstate-of-the-artinthetasksrelated
to natural language processing.
Inspiredbythetransformermodel,Dosovitskiyetal[19],appliedittotheimagesanditcanbeusedtoreplaceCNNs
. This model was called Vision Transformers (ViT) and its structure is illustrated in Figure 11. ViT model introduced
global attention, but not on the entire image, rather they divided the entire image into small image patches of 16x16.
They introduced simple numbers 1, 2, up to n as positional embeddings for specifying the positions of the patches.
. : Page 12 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
A lookup table was introduced which contained a learnable vector against each number representing the position of
the patch. These embeddings were passed to the network along with the patch. Each patch was unrolled to a linear
vector and projected linearly on embedding matrix. The final embedding along with positional embedding was then
fed into the transformer encoder. Along with embedding for patches, an extra embedding with number 0 is also fed
intothenetworkanditsoutputwasobtained.Thus,Visiontransformershavethecapabilityofmodelingglobalcontext
which assists in more accurate results. Lastly, in this review, medical images are considered as the input for vision
transformers.
Figure 11: Structural representation of Vision Transformers
4.1. Open Access Medical Imaging Datasets employed in ViT Applications
In table 3, we have summarized and structured open-accessed datasets in tabular form. The table includes
information regarding the tasks i.e. classification, segmentation, detection, report generations, and miscellaneous.
Furthermore, the respective image modalities and their applications are also included. Next, we have also compiled
thedescriptionandlinkstothecorrespondingdatasets.Hence,itwillassistresearcherstoidentifytheseresourcesthat
can be utilized against different applications.
. : Page 13 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Table 3: Publicly available datasets of medical imaging modalities used by researchers for assisted diagnosis
Tasks Modality Applications Datasets DescriptionDownload
Links
Classification CT ScansPulmonary Nod-
ule Characteriza-
tionLUNA16 [88]A commonly used dataset
for lung nodule identifica-
tion and false positive re-
duction.Download
LIDC-IDRI [89]One of the largest publicly
available lung cancer
screening datasets,
made up of diagnostic
and screening thoracic
computed tomography
(CT) images.Download
Emphysema
ClassificationComputed
Tomography
Emphysema
Database [90]A freely available dataset
that includes 115 high-
resolution CT (HRCT)
scans and 168 square
patchesthatweremanually
annotatedinasubsetofthe
slices.Download
COVID-CT [91]A freely accessible collec-
tionof CT-scanimagesex-
tracted from a number of
scholarly articles.Download
COVID-19
DetectionSars-CoV-2 [92]A multi-class CT scan
dataset for identification of
SARS-CoV-2 infection.Download
COVD19-CT-DB
[93]COVID19-CT-Database
consists of chest CT scans
that are annotated for the
existence of COVID-19.Download
COVID-CTset
[94]One of the largest open ac-
cess COVID-19 lung CT
datasetthatisavailableon-
line.Download
X-raysPneumonia Clas-
sificationChest X-ray Im-
ages [95]A Dataset of validated
OCT and Chest X-Ray
images.Download
. : Page 14 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Tuberculosis
Prognosis and
DetectionMontgomery
County (MC)
CXRSmall tuberculosis dataset
from USA.Download
Tuberculosis
Prognosis and
DetectionShenzhen datasetSmall tuberculosis dataset
from Shenzhen (China).Download
COVID Chest X-
ray datasetAn open access dataset of
chest X-ray and CT im-
ages of patients which are
positive or suspected of
COVID-19 or other viral
and bacterial Pneumonias.Download
ClassificationBIMCV COVID-
19+ [96]BIMCV-COVID19+
dataset is a large dataset
with chest X-ray images
CXR (CR, DX) and
computed tomography
(CT) imaging of COVID-
19patientsalongwiththeir
radiographic findings.Download
X-raysInterpretable
COVID-19
Detection
& Severity
QuantificationCOVID-19
Posterior-
Anterior Chest
Radiography
Images [97]This is a curated COVID-
19 Chest X-ray image
dataset that was created
by combining 15 publicly
accessible datasets.Download
Extensive
COVID-19
X-Ray and CT
Chest Images
[98]In this COVID-19 dataset,
both Non-COVID and
COVID cases are included
of both X-ray and CT
images.Download
COVIDx Dataset
[99]A database of chest X-ray
imagesforCOVID-19pos-
itive cases along with Nor-
mal and Viral Pneumonia
images.Download
MRI ScansMulti-Modal
Medical Image
ClassificationMRNet Dataset
[92]The MRNet dataset con-
sistsof1,370kneeMRIex-
ams performed at Stanford
UniversityMedicalCenter.Download
. : Page 15 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Fundus Im-
agesRetinal Image
Synthesis
and Disease
PredictionColorFundusIm-
ages [100]A database that has both
FFA image and color fun-
dus image in this DR grad-
ing system.Download
Histopatho-
logy ImagesColorectal
Histopathology
Image
ClassificationColorectal
Cancer Histology
Dataset [101]This data set represents a
collection of textures in
histological images of hu-
man colorectal cancer.Download
DetectionX-raysCOVID-19 Diag-
nosisCOVIDx [99]A database of chest X-ray
imagesforCOVID-19pos-
itive cases along with Nor-
mal and Viral Pneumonia
images.Download
COVIDGR-E
[102]It is built by adding 426
pneumonia images from
the ChestX-ray8 database
to the COVIDGR-1.0
datasetDownload
Fundus Im-
agesMicroaneurysms
DetectionIDRiD [103]This dataset consists of 81
images.Download
CT ScansCOV19-CT-DB
[93]COVID19-CT-Database
consists of chest CT scans
that are annotated for the
existence of COVID-19.Download
COVID-19 Diag-
nosisCOVIDx-CT-2A
[104]A benchmark dataset:
the largest comprising a
multinational cohort of
4,501patientsfromatleast
15 countries and contains
three classes ‚Äì COVID-19
Pneumonia, non-COVID-
19 Pneumonia, and
normal.Download
Histopathol-
ogy ImagesCancer DetectionThe Cancer
Genome Atlas
[105]The compendium includes
heat maps for 33 differ-
ent tumor types and three
platforms:geneexpression,
reverse-phase protein ar-
rays (RPPA), and miRNA
expression.Download
Segmentation iSeg-2017 [106]this datasets contains brain
MRI‚Äôs of 39 subjectsDownload
. : Page 16 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
MRI ScansBrainTissueSeg-
mentationBraTS-2020
[107]It contains 1756 MRI of
439 subjectsDownload
MRBrainS [108]It contains 20 Fully an-
notated multi sequence 3T
MRI BrainsDownload
UKBB [109] Download
Cardiac Segmen-
tationM&MS-2It contains 360 subjects
having 200 training and
160 testing imagesDownload
MRI Scans ERI [110]It contains LGE Data of
28 patients. the number of
segmented images are 358Download
Abdominal
SegmentationCHAOS [111]This Data set contains im-
ages for T2 Segmentation
of Liver and KidneysDownload
SegmentationX-raysTooth Root Seg-
mentationDRIVE [112]It include 40 color fundus
retinal images that are ran-
domly selectedDownload
Knee Segmenta-
tionOAI212Kneeimageswereseg-
mented randomly and the
training and testing split
was 100 and 112Download
Lung Segmenta-
tionJSRT [113]The database includes 154
conventional chest radio-
graphs with a lung nodule
(100 malignant and 54 be-
nign nodules)Download
CT ScansPediatric
SegmentationKiTS19 [114]ThisDatasetcontainsrenal
Tumor Segmentation Im-
ages of 210 AdultsDownload
Drusen Segmen-
tation from Reti-
nal OCT ImagesUSCD [115]This dataset contains 8616
retinal OCT B-scansDownload
. : Page 17 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
IU Chest X-ray
Collection [116]A public and open
source OpenI or Indiana
University Chest X-rays
database which contains
3955 medical reports with
7470 frontal and lateral
chest x-ray images.Download
Clinical
Report
GenerationX-raysMIMIC-CXR
[117]It is the recently released
largest dataset, consists of
377,110 chest X-rays im-
ages and 227,835 reports
from 64,588 patients.Download
PEIR GROSS
[118]It consists of publicly ac-
cessible 7442 teaching im-
ages, spread across 21 pre-
defined subcategories. The
vocabulary size of the to-
talimagecaptionsis4,452.
Each image on average
containsa12wordcaption.Download
MiscellaneousNIH-AAPM-
Mayo Clinic
LDCT Grand
Challenge [119]A public and open source
30 contrast-enhanced ab-
dominal CT patient scans.Download
PET-CT
ScansMedical Image
Enhancement
Kirby21 Dataset
(KKI01 to
KKI05) [120]A public and open source
dataset containing correla-
tion data for 20 subjects
from Kennedy KriegerDownload
MRI ImagesMedical Image
ReconstructionDIVerse 2K reso-
lution high qual-
ity (DIV2K) im-
agesdataset[121]It contains a total of 1000
2K resolution RGB im-
ages.Download
fastMRI Scans
[122]T1- and T2-weighted
images from 150 subjects
were analyzed (100 for
training, 10 for validation,
40 for testing).Download
. : Page 18 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Fluorescence
MicroscopyDenoising
of Celullar
Microscopy
Images for
Assisted
Augmented
MicroscopyFlywing, Planaria
and Tribolium
datasets [123]The training data 17,005
and 14,725 small cropped
patches of size 64 √ó64√ó16
for Planaria and Tribolium
datasets, while the testing
data are 20 testing images
ofsize1024 √ó1024√ó95and
6testingimagesofaverage
size around 700 √ó700√ó45
forthetwodatasets,respec-
tivelyDownload
5. Visual Recognition Tasks in Medical Images
Artificial intelligence and deep learning have played a vital role in assisting clinicians for better diagnosis. In
this context, the application of CNNs and ViTs on medical images assists the healthcare professional in disease
classification, lesion detection, segmenting the anatomical structures, automated report generation, denoising the
images,medicalimageregistration,andvariousothertasks.Thissectiongivesabriefoverviewoftheabove-mentioned
visual recognition tasks performed by the application of CNNs and ViTs on medical images.
5.1. Medical Image Classification
The practitioners give their diagnosis by analyzing the medical images, hence, determining the presence and type
of the disease. This conventional diagnosing way can be assisted with deep learning techniques. Figure 12 shows a
generalized classification network. Through these techniques, the ambiguity in diagnosis among different doctors can
bemitigatedandtheoutcomeswillbemoreaccurate.Thus,resultsachievedthroughCNNsarenotonlytimeefficient
butcanalsoassisthealthcareprofessionals.UsingCNNmodels,aninputimageisfedintothenetwork,whichisthen
assessed,analyzed,andinterpretedtodeterminethetargetandobjectofdifferentmodes[124].Thisprocessisknown
asimageclassification.Forexample,consideringthefigure5,thechestX-rayimagesaregivenasinput,andthemodel
classifies them into normal and pneumonia images. At present, image classification has various applications in the
medical domain such as; skin cancer [125], diabetic retinopathy [126], tuberculosis [127], etc.
ThesignificanceofimageclassificationusingCNNscanbedeterminedbytheaforementionedapplications.These
applications were achieved through various CNN architectures such as AlexNet [76], VGGNet [77], GoogleNet [78],
ResNet[79].Later,moreresource-efficientarchitectureswereproposedi.e.MobileNet[128],SqueezeandExcitation
Net[81],andEfficientNet[83],etc.ThroughtheseConvolutionalneuralnetworks,commendableresultswereachieved
in medical applications, however, in terms of resources, improvements are still required.
. : Page 19 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 12: Medical image classification pipeline in which Chest X-rays (CXRs) are fed into a Deep CNN architecture, which
assigns each image a class, such as pneumonia patient or healthy patient.
5.2. Lesions Detection
Classifyingimageswasindeedastepforwardtowardsautomateddiagnosis.Nevertheless,locatingtheobjectplays
animportantrolewhiledevelopingamorefunctioningapplication.Inthefieldofcomputervision,oneoftheunderlying
goals is to classify the object into a category and determine the location of the object in a given image, this technique
isknownasobjectdetection.Figure13figurativelyexplainstheageneraldetectionnetwork.Forinstance,iftheinput
image isan X-rayof ahand, the objectdetection modelwill not onlyclassify thecategory i.e. fracturedbone butalso
localize it by using bounding boxes. Considering a situation, where bones were fractured on multiple locations, here,
object detection will be a better technique to opt for as it will be more helpful to the diagnostician. The applications
of object detection involve; face detection [84], plant disease identification [129],weapon detection [130] , emotion
detection [85], etc.
Figure 13: A CNN architecture detecting a colony of tumorous cells given a histopathology image.
The task of object detection is composed of two types; two-stage networks and one-stage networks [124]. The
two-stage networks are based on region proposal algorithms such as R-CNN [131], Fast R-CNN [132] , and Faster-
RCNN [133]. The other technique is designed in a way that it works directly on images, examples include; YOLO
[134], SSD [135], etc. The two design types have a trade-off between accuracy and time efficiency. The two-stage
networks are capable of more accuracy, whilst, one-stage networks have more speed. Thus, it depends on the task at
hand and dataset, while choosing these networks for object detection. The limited datasets in the medical domain are
. : Page 20 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
the biggest constraint while training the models, therefore, researchers are working on models which can work with
limited datasets.
5.3. Anatomical Structure Segmentation
In the medical domain, there are numerous cases where it is difficult to distinguish between two different lesions
as there are minor differences. Since lesions can have different treatment strategies, determining them as separates is
vital. It is indeed a challenge to recognize such subtle differences, but it is not impossible. If the images are classified
pixel-wise, they can be localized as well, and result in a fine outline of the object. Such a technique is known as
Imagesegmentation.Forexample,ifanMRIimageisfedintothemodel,theoutcomewillnotjustclassifythetumor
type, its anatomical structure will also be highlighted as seen in Figure 14. There are various applications of image
segmentation, which includes ; Cardiovascular structure [136] , prostate cancer [137], blood vessel [138] etc.
Figure 14: A segmentation framework in which brain MRI scans are fed into a deep CNN architecture which is not just
classifying and locating the tumorous region but also highlighting the anatomical structure.
Fine-grainedsegmentationisadecisivestepinimage-guidedtreatmentandcomputer-aideddiagnosis.Thewidely
used architectures for image segmentation are; U-Net [139], DeepLab [140], Masked R-CNN [141] etc. The usage of
thesesegmentationmodelsdependsontheproblemthatisbeingsolved.Forinstance,multi-scaleobjectsintheimage,
deeplab,anditsvariousstructureswillbeawisechoice.Lastly,theconcernregardingimagesegmentationisthelack
of labeled data due to which researchers are considering more unsupervised approaches, however, it is still a work in
progress [142].
5.4. Clinical Report Generation
In the healthcare domain, while examining radiology images i.e. Chest X-rays, CT Scans, MRI, etc, the doctors
have to write detailed reports of the assessment. This conventional method of report writing is tedious, monotonous,
time-consuming,anderror-proneforradiologists[143].Immenseprogresshasbeenmadeindeeplearningtogenerate
medical reports automatically. Automatic report generation can assist clinical practitioners in quick and accurate
decision-making [144]. For example; if the CT scan of a brain is given as input, the output would be a complete
report such as, if the tumour exists, the location of the tumour, its size and other details. Figure 15 shows a general
workflow of clinical report generation. Medical report generation is an application of image caption in which these
models are applied to medical data. Image captioning refers to computers generating captions by giving images as
input.Itconsistsofmainlyanencoder-decoderwhereCNNisusedtoextractfeatures,andLSTMorRNNareusedto
generate the captions [145]. The initial work on its architectures include Show and Tell [146], Show Attend and Tell
[147],NeuralTalk[148],etc.SomeoftheStateoftheArtincludeDenseCap[149],SemStyle[150],Dense-CaptionNet
[151], etc. Lastly, as in supervised learning, a large amount of annotated data is required for these models to perform
well, in the future unsupervised learning could be a more powerful way to proceed with them [152].
. : Page 21 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 15: A visual representation of clinical report generation mechanism in which different types of image modalities
are independently diagnosed by Radiologist and AI model, and then compared to each other to provide a precise medical
report.
6. A Recapitulation on Vision Transformer Application to Medical Image Analytics for
Assisted Diagnosis
6.1. ViT in Image-based Disease Classification
Deep learning has recently come to the power in a variety of research domains. Convolutional Neural Networks
(CNNs)havebeenthemostdominantdeepneuralnetworksforautonomousmedicalimageanalysisapplicationssuch
as image classification during the last decade. These models, however, have shown poor performance in learning the
long-range information, due to their localized receptive field, which limits their capabilities for vision related tasks.
Transformer architecture, proposed by Vaswani et al. [16], is currently the most popular model in the field of natural
language processing (NLP). Getting inspiration from the success of self-attention based deep neural architectures,
Dosovitskiy et al. [19] introduced Vision Transformer (ViT) model for image classification based applications. In
thesemodels,theoveralltrainingprocessispredicatedondividingtheinputimageintopatchesandconsideringeach
embedded patch as a word in NLP. Self-attention modules are used in these models to learn the relationship between
the embedded patches.
In the following section, we will take a step forward in exploiting the potential applications of self-attention based
architectureslikeVisionTransformers(ViT)forthetaskofmedicalimageclassificationsuchasCOVID-19detection
and severity quantification, Emphysema classification, tuberculosis prognosis etc. The section is further divided into
medical imaging modalities, with a focus on contributions made by vision transformers to the respective medical
applications. The section ends with a tabular summary of the proposed approaches and their performance matrices.
6.1.1. Computed Tomography (CT) Scans:
PulmonaryNoduleCharacterization: Lungcancerisoneofthemostfrequentlyreportedcausesofcancer-related
morbiditiesandmortalities[153].Earlydetectionandtreatmentofmultiplepulmonarynoduleshasbecomeachallenge
in clinical practice as it is one of the most efficacious ways to reduce the number of fatalities associated with the
condition. Prior research [154, 155, 156] on detection and characterization of lung nodules focused on learning the
. : Page 22 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
associations between various nodules. In other words, they particularly use solitary nodule approaches on several
nodular patients, ignoring the relational/contextual information. To overcome this issue, Yang et al. [157] proposed a
MultipleInstanceLearning(MIL)strategy,whichempiricallyprovedtheutilityoflearningtherelationshipsbetween
nodules. It‚Äôs the first time researchers have looked into the relationships between several lung nodules and extracted
critical relational information between solitary-nodule voxels. Instead of using typical pooling-based aggregation in
multiple instance learning, they created Set Attention Transformers (SATs) based on self-attention to understand the
relationships between nodules. A 3D DenseNet is employed as the backbone to learn representations of voxels at the
solitary-nodule level. The SATs are then used to determine how several nodules from the same patient are related.
This data-driven methodology might aid in understanding of etiologic and biologic processes as well as metastasis
diagnosis of multiple pulmonary nodules and motivate clinical and biological research on this important topic.
Emphysema Classification: Chronic obstructive pulmonary disease (COPD) is a heterogeneous disorder with a
varietyofcharacteristics,includingsmallandlargerespiratoryinflammation,aswellasEmphysema,whichisthemost
common causeof progressive lungtissue loss. Emphysema,as characterized by thedestruction and persistentgrowth
ofthe alveoli,canbe classifiedautomaticallywhich canaidin determiningandquantifying lungdestructionpatterns.
In this regard, Convolutional Neural Networks (CNNs) serve an essential role, particularly in pulmonary CT image
classification, but transformers have yet to be explored. As a result, Wu et al. [158] conducted a thorough assessment
and extensive evaluation of the ViT model for Emphysema classification. First, large image patches (16 x 16) are
cropped from CT scans. After resizing, the patches are flattened and linearly embedded to create a sequence of patch
embeddings. The positional information is kept by concatenating the class embeddings with the patch embeddings.
To acquire the representation, the final embedding sequence is passed into the transformer encoder module. Finally,
the learnable class embedding is fed to a softmax layer for Emphysema classification. Despite the fact that this study
employed a vision transformer model to classify emphysema, unlike other techniques that use CNN models, it still
hasseverallimitations.Forexample,patch-basedclassificationmaynotbeasconvenientaspixel-basedsegmentation.
Furthermore, the architecture just uses the transformer encoder block, and the CNN‚Äôs benefit is not utilized. In short,
Emphysema quantification is difficult, and classification is merely the first step. Proposing more efficient networks
capableoflearningsemanticinformationofEmphysemabypartialoraccurateannotationsmaybeapressingneed.In
the near future, more research on the segmentation and quantification of Emphysema will be conducted.
COVID-19 Detection: The infectious Coronavirus (COVID-19) and lung disorders have been at the vanguard of
the research community as the pandemic has caused significant public health concerns throughout the world. Using
computer vision methods, several attempts [159, 160, 161] are being undertaken to create automated systems for
faster and more effective diagnosis of COVID-19. As per several research studies [162, 163], some radiographic
manifestations, such as broncho vascular thickening, Ground Glass Opacities (GGO), crazy-paving pattern, and
consolidation, have been found in chest CT images. However, with the rapidly growing number of patients in the
current situation, radiologists have a significant challenge in manually interpreting CT scans.
Ambita et al. [164] were the first to use a vision transformer to the task of COVID-19 detection from computed
tomography (CT) scans. They implement a variety of vision transformers (e.g., ViT-B 16, ViT-B 32, ViT-L 16, ViT-
L 32, and ViT-H 14) for image classification. They employed ResNet-based Self-Attention Generative Adversarial
Network (SAGAN-ResNet) as a data augmentation approach for synthetic image generation to alleviate the problem
ofinsufficientdata.Furthermore,theydemonstratedhowViToffersvisualizationsfortheimagesbyexhibitingwhich
sectionsoftheinputimagethemodelfocusesitsattentiononinthevariouslayers.Thismightbeusefulforradiologists
whenanalyzingCTscans.Improvementsmayalsobemadebyevaluatingtheproposedapproachondifferentdatasets
and customizing the architecture of transformers or GAN.
Zhang et al. [165] attempted to broaden the scope of vision transformers such that they may be used as a robust
feature learner for COVID-19 diagnosis in 3D CTs. Inspired by the success of Swin vision transformer and CT
classification work in [166, 167], their framework is made of two key stages: lung segmentation followed by image
classificationusingSwintransformerasabackbone.Inthefirststage,apre-trainedUnetisusedforlungsegmentation
in CT scans and produced a lung mask that restricted learning to certain lung regions. The features from each 2D
CT slice are then extracted using a Swin vision transformer, which are subsequently aggregated into 3D volume level
features using a max-pooling layer. However, it‚Äôs worth noting that the framework equipped with the backbone of
EfficientNetV2-M achieves a good speed-accuracy tradeoff according to the results on the validation dataset. This
implies that in future study, merely increasing the model size might result in an improvement in classification.
. : Page 23 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Abovereviewedstudiesareevidentthatvisiontransformers(ViT)isanovelandquicklyevolvingapproachthathas
demonstratedexcellentresultsusingCOVID-19datasets.Thanetal.[168]conductedapreliminarystudytoinvestigate
the efficacy of using ViT with different sized patches on CT scans of diseased lungs, COVID-19 infected lungs, and
normal lungs. A default positional embedding is used and the combination is then passed to a transformer encoder
which is composed of alternating layers of multi-headed self-attention and multi-layer perceptron (MLP) units. The
transformerencoder‚ÄôsoutputissentintoanMLPhead,whichoutputsthepredictedclass.Theproposedmethodology
issimplerandlessdemandingoncomputationalresourcesascomparedtoCNNs,however,pretrainingthetransformer
andaddingaconvolutionallayerinfrontofit(i.e.Convolutionalvisiontransformers)mayincreaseperformance.Aside
fromthat,otherhyper-parameterscanbemodifiedtoincreaseperformance,andexplainableartificialintelligence(XAI)
will be used in the future to explain how deep learning networks like ViT make decisions.
COVID-19 CT scans contain not only the local features, such as local crazy-paving and local hemangiectasis,
but also have global characteristics. Since it is characterized by the combination of both local and global features,
extracting image features with relatively complex features is a challenging classification problem of such medical
imagingmodalities.Fanetal.[169]proposedaparallelbi-branchnetwork(TransCNNNet)thatisessentiallybasedon
theTransformerandConvolutionalNeuralNetwork.Unliketheconventionalapproaches,thesizeoftheconvolutional
filter kernel is not changed to extract features at different scales; instead, they use the global receptive field of the
transformer network. A bi-directional feature fusion structure is then designed, which fuses the extracted features
fromtwobranchesbi-directionally,forminganetworkthathasthepotentialtoextractmorecomprehensiveandample
features.
6.1.2. X-ray or Radiographic Images:
Pneumonia Classification: Pneumonia is an infectious disease in which the alveoli in the lungs is to be filled with
fluid or pus, making it painful to breath as well as decreasing oxygen intake. A detailed inspection of chest X-ray
imagesbytheradiographerorradiotherapistisrequiredtodiagnosePneumonia.Asaresult,pneumoniadetectionisa
time-consuming process, and even a slight error can result in an excruciatingly painful outcome. Several researchers
haveexploredvariouscomputervisionapproachestodiagnosePneumonia,usingX-Rayimagesofhumanchests.Tuagi
etal[170],haveproposedavisiontransformer(ViT)modelfortheclassificationofpneumonia.Further,tovalidatethe
performanceoftheproposedmodel,itwasalsocomparedwiththeCNNmodelandVGG16.ItwasobservedthatViT
outperformedothertechniquesreachingthehighestaccuracyof0.96.Inaddition,itoutperformedothermodelsinterms
of computational cost as well. It is observed that the model still requires further experimentations to investigate the
robustnessofthemodelwithmoreheterogeneousdata.Therehasn‚Äôtbeenalotofworkdoneusingvisiontransformers
in the field of chest x-ray diagnosis. It may be useful in the diagnosis of other disorders, such as Covid-19 detection,
Cystic Fibrosis or Emphysema, Edema, Pleural thickening, Effusion, and even Cancer.
Tuberculosis Prognosis and Detection: Tuberculosis refers to an infection that affects the lungs and can travel
to other parts of the body. It can be diagnosed and assessed by referring to chest x-rays. If the infection is cured at
an earlier stage, it can save a person from further misery of painful treatment. For the classification of infected and
non-infected chest x-rays, a methodology was introduced by Duong et al. [171]. The authors have used EfficientNet
withvisiontransformersforthedetectionoftuberculosisusingchestX-raysimages.Theperformancemetricsinclude;
accuracy,precisionandrecall,f1-score,andareaunderthecurve.Thehighestaccuracyachievedwas97percentwith
the backbone of efficient net B1. Hence, the paper validates the robustness of vision transformer models used on a
heterogeneous dataset. However, it should be further evaluated on different baselines models.
Interpretable COVID-19 Detection & Severity Quantification: The novel coronavirus disease 2019 (COVID-
19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has become one of the deadliest
virusesofthecenturyasofApril2021,infectingover137millionpeoplewithover2.9milliondeathworldwide.Inthe
contextofunprecedentedCOVID-19pandemic,publichealthsystemshavebeenhitbyaslewofchallenges,including
scarcity of medical resources, that are pushing healthcare workers to face the threat of infection. Deep learning and
Computer Vision are commonly employed in numerous fields of medical imaging for the diagnosis of COVID-19
from radio-graphic images, X-rays or CT scans. Even though these techniques have yielded commendable outcomes,
thecostisalwaysconsideredasanimportantfactorforthesystemtobeapplicable.Theuseofacomputedtomography
(CT) scan for COVID-19 diagnosis offers great sensitivity and specificity [172], but it is a severe burden due to its
high cost and risk for cross-contamination in the radiology suite. In comparison of CT Imaging, X-rays have been
. : Page 24 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
widelyutilizedforCOVID-19screening,astheyrequirelessimagingtime,arelessexpensive,andX-rayscannersare
generally available even in remote regions [173].
Hence, many researchers have worked on diagnosing COVID-19 using Chest X-ray (CXR) images and through
theseautomatedmethods,counter-checkthePCRtestresults.Forinstance,inthestudypresentedbyParketal.[174],
vision transformers were utilized for both classification and quantification of the severity of COVID-19. Firstly, the
low-level features were extracted using state-of-the-art CNN architectures. Secondly, these extracted features were
given to the transformer model for classification and severity measurement. Thirdly, the severity was demonstrated
through heat maps which gives interpretability to the generated results. The robustness of the model was illustrated
throughvariousexperimentationsondifferentbaselinemodelsandtestedonexternaldatasets.Themodelwasevaluated
onmetricsincluding;AUC,sensitivity,specificity,andaccuracy.Thehighestaccuracyachievedontheexternaldataset
was above 84.8 percent.
Similarly, the study presented by Shome et al. [175] have proposed a pipeline based on Vision Transformers for
the classification of COVID-19. The model was also compared with other baseline models to validate the robustness
ofvisiontransformers.Ascomparedto[174],itwastrainedonalargerandmoreheterogeneousdatasetachievingthe
accuracy of 98 percent and AUC of 99 percent in the binary classification. Furthermore, for the multi-classification
which includes pneumonia x-rays as well, the model achieves the accuracy and AUC of 92 percent and 98 percent,
respectively. In addition, the model uses Grad-CAM for the visualization through heatmaps, for the explainability of
theoutcome.Althoughthemodelperformedbetterthan[174],themodeldoesnotquantifytheseverityoftheinfection.
Further, this research could be expanded to predict the rate at which the infection can spread.
Since the medical data in most domains is inadequate, it can be difficult to build robust models. Considering this
issue, Rahhal et al. [176] have put forth a methodology that performs well with small training data. To diagnose
covid-19 in both CT scans and X-ray images, the proposed method uses vision transformers as a backbone with the
employment of a Siamese encoder. The input image with a corresponding augmented image was fed into the siamese
encoder,itwasthenconnectedtotwoclassifiers.Further,theoutputoftheseclassifierswasfedintoafusionlayerfor
theoutcome,followedbyheatmapsondifferentlayerstointerprettheresults.Moreover,thesiameseencodercatersto
theissueregardingdeficientdata.Theproposedmodelhasachievedanaccuracyof94.2percentwhichiscommendable
in limited training data.
Untilnow,onlyafewCAD-basedapproachesfordetectingCOVID-19havebeendeveloped,buttheireffectiveness
has been hampered due to a number of factors. Taking an inspiration from Convolutional Block Attention Module
(CBAM), Nawshad et al. [177] proposed an attention-based Convolutional Module using ResNet32 as the backbone
architecture with a 97.69% accuracy. They also conducted a comparative assessment of a variety of deep learning
models, including VGG, ResNet, and Xception, for the successful detection of COVID-19 and viral pneumonia.
Utilizing the attention module with various CNN-based architectures produced much better results than using the
base CNN architectures.
Since it is believed that the newly developing pathogen would have similar low-level CXR features with existing
diseases,theuniqueconceptofproducinghigher-leveldiagnosesbyaggregatinglow-levelfeaturecorpusmaybeused
to swiftly construct a robust algorithm against it.
6.1.3. Magnetic Resonance Imaging (MRI):
Multi-Modal Medical Image Classification: The inadequacy of medical data is discussed in the aforementioned
papers, and it can be inferred that researchers are trying to overcome the issue by modeling various architectures.
For instance, Dai et al. [178], put forth a hybrid transformer model for the classification of multi-modal images. The
pipeline consists of a CNN to extract low-level features, and then transformers are utilized for the global context. The
model was applied on two different datasets: for classification of parotid gland tumors and classification of a knee
injury.ThehighestaccuracyachievedbytheParotidglandtumordatasetandkneeinjurydatasetwas88.9percentand
94.9 percent, respectively. Nevertheless, the study has not presented any means for interpretability of the model, as it
is essential, especially in the medical field.
6.1.4. OCT or Fundus Images:
RetinalImageSynthesisandDiseasePrediction: Forthediagnosisofretinalabnormality,FluoresceinAngiogra-
phy(FA)canbeused.Tocapturethevascularstructureoftheretina,afluidisinjectedintothebloodstream.However,
several reactions have been reported with the usage of FA. Another approach to diagnosing abnormality is by using
Fundus images. The vascular structure of the eye is not captured in these images. In the paper [179], the authors
. : Page 25 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
used fundus images and synthesize them to produce FA‚Äôs using a generative adversarial network(GAN). Next, these
imagesarethengiventoatransformermodeltoclassifyanormalandabnormalretina.Themodelachievedthehighest
accuracy, sensitivity, and specificity of 85.7, 83.3, and, 90.0, respectively. Moreover, this model needs to be validated
with a more heterogeneous dataset, along with applying this model for predicting other retinal diseases as well.
6.1.5. Histopathology Images:
Colorectal Histopathology Image Classification: Colorectal Cancer (CRC) is a type of cancer that begins in the
rectumorcolonandisdefinedbytheuncontrolledgrowthofaberrantcellswiththelatentabilitytoinvadeothertissues.
Despitethefactthatmanualinspectionofhistopathologyslidesisstillcrucialinexperimentalpractise,automaticimage
processing enables the quantitative and rapid analysis of malignant tissues. Early detection is crucial for identifying
theappropriatetreatmentapproachandincreasingthepatient‚Äôschancesofsurvival.Asaresult,automatedtechniques
are needed to save time and eliminate the risk of human error. Artificial intelligence has recently been put to use
in the diagnosis and prediction of several forms of cancer. Zeid et al. [180] used Vision Transformers to perform
a multiclass tissue classification of colorectal cancer, highlighting the potential of employing Transformers in the
histopathological image domain. First of all, a standard vision transformer model was proposed and it achieved an
accuracy of 93.3 percent. Since vision transformers demand more data, a hybrid approach combining CNN and
transformer was developed. Low-level features are extracted using the CNN model, and the embeddings are sent to
the transformer. This model is known as a Compact Convolutional transformer, and it achieved an accuracy of 94
percent.However,experimentationwithvariousdatasetsanddifferentformsofcancermayalsobedonetoimprovethe
model‚Äôsoverallperformance.Deeplearningalgorithmsarenowbecomingincreasinglyessentialfortheidentification
andclassificationofcolorectalhistopathologyimages.Existingtechniques,ontheotherhand,aremoreconcernedwith
end-to-end automatic classification using computers than with human-computer interaction. Hence, Chen et al. [181]
presented an IL-MCAM framework. It is based on interactive learning and attention techniques. Automatic learning
(AL) and interactive learning (IL) are two steps in the proposed IL-MCAM system (IL). To extract multichannel
features for classification in the AL stage, a multi-channel attention mechanism model with three separate attention
mechanism channels and convolutional neural networks is implemented. The proposed IL-MCAM system constantly
adds misclassified images to the training set in an interactive method during the IL stage, improving the MCAM
model‚Äôs classification ability. To handle different colorectal histopathological image classification tasks in the future,
permutation and combination can be used to identify the best model for the current task from attention mechanisms
anddeeplearningmodels.Moreover,attentionmechanismscanalsobeincludedinvariouslocationsofadeeplearning
model, in order to investigate the influence of convolutional layers on classification performance.
The table 4 given below summarized the performance gain by the reviewed articles of the classification category.
Table 4: List of datasets and performance measures employed by researchers for medical image classification.
Modality Publication Dataset Performance Measures
CT ScansYang et al. [157]LUNA16 [88] CPM Score = 0.916
LIDC-IDRI [89] Accuracy (%) = 93.17
Wu et al. [158]Accuracy (%) = 95.95
ComputedTomographyEm-
physema Database [90]Precision (%) = 98.0
Recall (%) = 97.1
Specificity (%) = 98.6
CT ScansAmbita et al. [164]COVID-CT [91]Accuracy (%) = 87.19
. : Page 26 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Positive Precision (%) =
89.11
Sensitivity (%) = 85.71
F1 Score = 0.8738
CT Scans
Sars-CoV-2 [92]Accuracy (%) = 95.41
Positive Precision (%) =
94.30
Sensitivity (%) = 98.03
F1 Score = 0.9613
Zhang et al. [165] COV19-CT-DB [93]Accuracy (%) = 94.3
Precision (%) = 93.7
Recall (%) = 93.8
F1 Score = 0.935
Macro F1 Score = 0.94
Than et al. [165] COVID-CTset [94]Accuracy (%) = 95.36
Senstivity (%) = 83.00
Li et al.[182]Private dataset from eight
different Hospital [182]Macro F1 (Score) = 0.97
Micro F1 (Score) = 0.98
X-ray ImagesTuagi et al. [170] Chest X-ray Images [95] Accuracy (%) = 96.45
Duong et al. [171]Montgomery County (MC)
CXR Images [183]
Accuracy (%) = 97.92
Shenzhen CXR Dataset
[183]
COVID-19 Dataset [184]
Park et al. [174] BIMCV COVID-19+ [96]AUC = 0.949
Accuracy (%) = 86.8
Sensitivity (%) = 90.2
. : Page 27 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Specificity (%) = 86.2
Shome et al. [175]Accuracy (%) = 92
COVID-19 Posterior-
Anterior Chest Radiography
Images [97]Precision (%) = 93
ExtensiveCOVID-19X-Ray
and CT Chest Images [98]Recall (%) = 89
F1 Score = 0.91
AUC = 0.98
Rahha et al. [176]Accuracy (%) = 94.62
Precision (%) = 96.77
Sars-CoV-2 [92] Recall (%) = 96.77
Specificity (%) = 99.65
F1 Score = 0.967
MRI Scans Dai et al. [178] MRNet Dataset [92]AUC-ROC = 0.976
Accuracy (%) = 91.8
Sensitivity (%) = 96.8
Specificity (%) = 72.8
OCT/Fundus Images Kamran et al. [179] Color Fundus Images [100] Accuracy (%) = 85.7
Sensitivity (%) = 83.3
Specificity (%) = 90.0
Histopathology Images Zeid et al. [180]Accuracy (%) = 93.3
ColorectalCancerHistology
Dataset [101]Precision (%) = 93.33
Recall (%) = 93.44
F1 Score = 0.933
. : Page 28 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
6.2. ViT in Region-based Lesion Detection
The proceeding section discusses the detection of anomalies in medical images using Vision Transformers, in
relation to the aforementioned modalities.
6.2.1. Computed Tomography (CT) Scans:
COVID-19Detection AframeworkforthedetectionofCovid-19wasproposedbyLiangetal.,withchestCTimages
asinput[185].TheframeworkwascomposedofaCNNmodelforfeatureextraction,andthentheSEattentionmodule
wasintegratedforthegenerationofattentionvectors.Next,thetransformermodelwasusedtodistinguishthefeatures
in the input. The study also proposed a method to resample the inputs, which also contributed to the efficiency of the
model. The highest f1-score was 88.21 percent which was a 10 percent improvement from the baseline model. The
dataset used, however, was small and imbalanced which doesnot validate the generalizability of the proposed model.
AnomalyDetection Inthemedicaldomain,variousmethodologiesareproposedforthedetectionofanomalies.The
authorsofthepaper[186],haveproposedatransformer-basedmodel,whichwasappliedonvariousimagesi.e.retinal
OCT, Head- CT Scans, and Brain- MRIs. The representation of the features was learned using autoencoders which
were based on transformers. In addition, to detect the anomalies in multiscale, a transformer model was proposed
with skip-connections, thus it reduced the usage of memory and cost of computation. The models were evaluated on
AUROC, achieving 93 %, 95.81 %, and 98.38% for the datasets related to Head-CT, Brain- MRI, and retinal OCT,
respectively.However,theproposedmodelstillrequiresafurtherreductionincomputationalcostsothatitcanbeused
in real-time.
6.2.2. X-ray or Radiographic Images:
COVID-19Diagnosis SinceX-raysarecomparativelycost-effectiveandafasterwayofdiagnosingthevirus,several
researchershaveproposedmethodsfordetectionusingchestx-rays.Inthepaper[187],anotherapproachisproposedto
detect covid-19 using chest X-rays. An adaptive attention network is used which consists of ResNet and an attention-
basedencoder.ResNetisusedtolearnthefeaturerepresentationsandtheAttentionmoduleisthenutilisedfordetection
of the infectious areas. The proposed model was compared with different CNN models on three different datasets.
The evaluation indicated that the proposed model performed significantly better. Moreover, the performance metrics
included Accuracy, sensitivity, precision, and F1 score. The highest accuracy achieved by the model was 98.5
Similarly, To capture the global context, the authors Kumar et al. have used vision transformers on both X-ray
images and CT images of the chest for the diagnosis of Covid-19 [188]. The data used was labelled as normal,
pneumonia and covid-19. Furthermore, to address the issue of scarce data, transfer learning is used followed by
explainability through visualisation of the infected areas. The proposed method was compared with other models
i.e InceptionV3, CoroNet, CovidNet, etc. The results were evaluated using the metrics; precision, recall, f1-score,
accuracy,andspecificity.TheproposedmodeloutperformedtheCNNmodelsreachingtheaccuracyof0.96and0.98
for CT scans and X-ray images, respectively. However, work on severity information requires attention in both [187]
[188].
Pulmonary Lesions Detection In the initial assessment of lung cancer, one of the most used techniques is chest
radiography. Since it is essential to diagnose cancer at an earlier stage, many methodologies have been proposed to
detect pulmonary lesions. The study is presented by [189], it has proposed two architectures; convolution networks
with attention feedback, and recurrent attention model with annotation feedback. The first method uses CNN to
learn the features, and generate saliency maps as the soft attention mechanism was incorporated. Next, a recurrent
attention model with attention feedback was proposed. The proposed architecture uses reinforcement learning for
better performance of the model. The architectures were evaluated through precision, recall, f1-score, and accuracy.
Thehighestaccuracyachievedwas85percentforclassification,and74percentforlocalization.Thus,thearchitectures
require improvement regarding the reduction of computation time and accuracy.
6.2.3. OCT or Fundus Images:
Microaneurysms Detection The early diagnosis of lesions in diabetic retinopathy can be done by the detection
of microaneurysms (MA). Since it is difficult to locate them because of their size, several methodologies have been
proposed. In this study [190], the proposed methodology for the detection of MAs comprises three stages. First, the
images are preprocessed to improve the quality. Second, a deep network is used with an attention mechanism for
detection. Third, the correspondence between Microaneaurysms and blood vessels is exploited for the final results.
. : Page 29 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
The performance metrics used for evaluation were precision, recall, sensitivity, and f1-score. The proposed method
outperformed prior proposed models with a sensitivity of 0.86. Nevertheless, the model was trained on the images
from one type of camera, which does not validate the generalizability of the proposed methodology.
Glaucoma Detection The authors of the paper [191], have proposed a methodology for the detection of a disease
knownasGlaucoma.Itcausesthelossofvisionandisirreversible.ThepaperhaspresentedaCNNmodelwhichwas
attention-based for the detection of the disease. Furthermore, due to the attention module, the localized features were
also visualized, giving results more explainability. The proposed architecture first locate the area and then classify
the disease. The detection was evaluated using the performance metrics; accuracy, sensitivity, specificity, AUC, and,
F2-score,withthehighestaccuracyachievedof96.2percent.However,inthenetwork,themodelsmayidentifyregions
with useless information which may hinder the performance of the model.
Further,Xuetal.havepresentedamodelwhichconsistsofanattentionmodulealongwithtransferlearningforthe
detection of glaucoma [192]. This work has contributed towards the discrimination of general and specific features.
Since the models are not able to identify the regions that may give no information, the proposed methodology can
extract the regions with more information. In addition, with the attention module, the regions can also be visualized.
The model was then evaluated on two different datasets, achieving the highest accuracy, sensitivity, specificity, and
AUCof85.77percent,84.9percent,86.9percent,and0.929,respectively.Lastly,thismethodcanbefurthervalidated,
by applying it to various other eye diseases.
6.2.4. Histopathology Images:
Cancer Detection Barret‚Äôs esophagus (BE) refers to the damaging of the swallowing tube that connects the mouth
to the stomach because of acid reflux [193]. Ultimately, it increases the risk of esophagus cancer i.e. adenocarcinoma
[194]. Moreover, patients that suffer from BE are at a higher risk of cancer. The detection of lesions at an early stage
can prevent the suffering of patients from cancer, with a better survival rate.In the paper[195], attention-based deep
neural networks were proposed for the detection of cancerous and precancerous esophagus tissues. The model uses
attention-based mechanisms to detect the cancerous tissues belonging to the classes; normal, BE-no-dysplasia, BE-
with-dysplasia, and adenocarcinoma. The mechanism does not require annotations for regions of interest, thus, it
dynamically identifies the ROIs. Hence, it is independent of the annotated bounding box and does not require a fixed
sizeofinputimages.Theproposedmethodwascomparedwiththeslidingwindowapproachbasedontheperformance
metrics; accuracy, recall, precision, and accuracy. The model outperformed the sliding window method in all classes
withanaverageaccuracyof0.83.However,themodelwastrainedonasmalldataset,hencetherobustnessofthemodel
still needs to be verified using more data.
The issues regarding whole-slide images in terms of detection, include poor adaptability of the model, explain-
ability, and resource-efficient model. The authors of the paper[196], have proposed a model known as clustering-
constrained-attention multiple-instance learning(CLAM). It was applied to detect three types of cancers; renal cell
carcinoma, non-small cell lung cancer, and breast cancer lymph node metastasis. The proposed method CLAM is a
weakly supervised algorithm, it uses an attention module to determine the regions, and classify the cancer type. In
addition, it also localized the affected regions with interpretability. The models were evaluated using AUC, hence, it
was greater than 0.95. On contrary to this data-efficient model, it considers various locations as independent, thus,
leading to a less context-aware model.
Next, another model was used for the detection of cancer leading to the prediction of survival prediction [197].
Theframeworkisamultimodalco-attentiontransformer(MCAT),thatlearnsthecorrespondencebetweenWSI‚Äôsand
genomic features. The attention module ensures interpretability along with the reduction of memory usage of image
bags. The model was applied to five different cancer datasets, and the results were compared with the state-of-the-art
models.
The table 5 given below summarized the performance gain by the reviewed articles of the detection category.
. : Page 30 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Table 5: List of datasets and performance measures employed by researchers for Region-based Lesion Detection.
Modality Publication Dataset Performance Measures
CT Scans Mondal et al. [188] COVIDx-CT-2A [104]Accuracy (%) = 98.1
Recall(%)=96
Precision(%)=96
Specificity(%)=98.8
F1(Score)=0.96
Liang et al. [185] COV19-CT-DB [93] Macro F1 (Score) = 88.21
Micro F1 (Score) = 0.98
X-raysLin et al. [187]COVIDx [99]Accuracy (%) = 95
sensitivity(%)=97
Precision(%)=98.98
Specificity(%)=99.47
F1(Score)=0.97
COVIDGR-E [102]Accuracy (%) = 89.53
sensitivity(%)=86.05
Precision(%)=83.15
Specificity(%)=91.28
F1(Score)=0.84
DLAI3 Accuracy (%) = 98.55
sensitivity(%)=98.63
Lin et al. [187] Precision(%)=98.63
Specificity(%)=99.90
F1(Score)=0.98
Precision (%) = 15
. : Page 31 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Pesce et al. [189]A dataset consisting of
745,479 chest x-ray exams
collected from the historical
archives of Guy‚Äôs and St.
Thomas‚Äô NHS Foundation
Trust in London from
January2005toMarch2016Sensitivity (%) = 65
Average Overlap (%) = 43
Fundus ImagesZhang et al. [190] IDRiD [103]Accuracy (%) = 94.3
Precision (%) = 87.2
Recall (%) = 81.0
F1 Score = 0.840
Sensitivity (%) = 86.8
Accuracy (%) = 96.2
Sensitivity (%) = 95.4
Li et al. [191]LAG Database (obtained
from Chinese Glaucoma
Study Alliance (CGSA) and
Beijing Tongren Hospital.)
[191]Specificity (%) = 96.7
AUC = 0.983
F2 Score = 0.954
Xu et al. [192]LAG Database (obtained
from Chinese Glaucoma
Study Alliance (CGSA) and
Beijing Tongren Hospital.)
[191]Accuracy (%) = 85.7
Sensitivity (%) = 84.9
Specificity (%) = 86.9
AUC = 0.929
Tomita et al. [195] Accuracy (%) = 83.0
Recall (%) = 60.0
. : Page 32 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Histopathology ImagesHistologicalimagesbetween
January 1, 2016, and
December 31, 2018, at
Dartmouth-Hitchcock
Medical Center (Lebanon,
New Hampshire) were
collected.Precision (%) = 62.0
F1 Score = 0.59
Chen [197]The Cancer Genome Atlas
[105]Concordance Index (c-
Index) = 0.653
6.3. ViT in Anatomical Structure Segmentation
Clear cut and detailed segmentation is a decisive step in image guided treatment and computer-aided diagnosis.
A great deal of image segmentation models have been proposed In the last 40 years from traditional models to deep
neural networks. But since the emergence of transformers, they have outperformed all the state of art segmentation
models. Transformers functions prominently in error free segmentation of medical images because of their capability
to model the global context. As the organs lay out over a wide receptive field, hence, transformers can easily encode
these organs by modeling the association of pixels that are distant spatially. Moreover, the background is dispersed in
medical scans, for that reason gaining the understanding of the global context between those pixels that relate to the
backgroundwillbebeneficialforthemodeltodotheunerringclassification.Belowwereviewedexperimentsthattried
to exploit ViT based models for a faultless segmentation. We divided these experiments in accordance with different
modalities used for medical imaging. In the end we give all the results obtained during these experiments on specific
datasets in tabular form.
6.3.1. Computed Tomography (CT) Scans:
Coronary Artery Segmentation A precise and correct segmentation of CAC is advantageous for early CVD
diagnosis. But as CAC has blurry and distorted boundaries, the task of segmentation is not very much satisfactory.
To address this issue Ning et al. [198] introduced an efficient multiscale vision Transformer for the segmentation of
coronaryarterycalciumandnameditasCAC-EMVT.Thisarchitectureutilizedthelocalaswellasglobalfeaturesand
thenusedthemcollectivelytomodelthelongandshort-termdependencies.Theirmodelwascomprisedofthreemain
modules,(a)KFS,KeyFactorSamplingmodulethattheyutilizedforextractingthekeyfactorsfromtheimage.These
key factors were made use for low-level reconstruction of highly structured features, (b) NSCF, a non local sparse
net fusion module, that was used to model the information of high level features of texture, (c) NMCA, a non local
multiscale context aggregation, that was used to get the dependencies of long range at different scales. Experimental
results showed that their model outperformed the state of the art methods at that that by giving a Dice similarity
co-efficient of 75.39% ¬±3.17.
Leeetal.[199]introducedanewconceptoftemplatetransformernetworksforsegmentationthroughshapepriors
(TETRIS), and performed coronary artery segmentation through their model. In this concept they used an end to
end trainable Spatial Transformer (STN)[200] to deform a shape template to complement the under laying region of
interest.TheyalsousedthisconceptofincorporatingthepriorstothestateoftheartCNNandU-Netusedforbinary
classification.TheexperimentalresultsofTETRISandU-Net[139]incorporatingtheprior,wereabletoproducesingly
connected components because they were given the prior information and gave to dice scores of 0.787 and 0.854
respectively. They also compared the U-Net[139] with FCN by giving the prior shape but FCN[201] didn‚Äôt perform
that well giving the Dice Score of 0.79 only.
Lung Tumor Segmentation PET-CT segmentation requires information from both PET and CT modality. Most of
the models get the segmentation information of these modalities separately. In a study, Fu et al. [202] established a
moduleMSAM,MultimodelSpatialAttentionModule,adeeplearningbasedframeworkforlungtumorsegmentation
. : Page 33 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
inPET-CT.MSAMwasanimpulsivemoduleanditwasabletohighlightthespatialareasorregionsthatarelinkedto
thetumorandcensoredthenormalregionsofinputspontaneously.ThismodulewasfollowedbyaCNNthatwasacting
asabackboneandthisCNNwasperformingthesegmentationtaskonthemapthatwasprovidedtoitasaninputfrom
the multi model spatial attention module. U-Net[139] was used as backbone for that purpose. They performed their
experimentsontwoclinicalPET-CTdatasetsofNSCLCandSTS.Theresultsoftheexperimentsgaveadicesimilarity
coefficient of 71.44% and 62.26% for NSCLC and STS datasets respectively. in order to refine this architecture, more
better procedures can be used to further improve the segmentation.
RenalTumorSegmentation Duetothediversificationthatispresentinsizeandpose,thetaskofsegmentationhas
become a strenuous task. Hence La et al. [203] proposed a network that was both size and pose invariant and they
tested their network for renal tumor segmentation on 2D CT Scan images. Their architecture was comprised of three
sequentialmodulesthatworkedtogetherinthetrainingprocess.Firstwastheregressionmodulethattheyusedtofind
the similarity matrix of input image to the ground truth. Second module was used to find the region of interest and
theynameditasdifferentiablemodule.Thethirdandlastmodulewasusedtoperformthesegmentationtaskandthey
usedU-Net[139]forthispurpose.TheyusedtheSpatialTransformer(STN)[200]intheirarchitecturetoautomatically
detect the bounding box which saved time. Results indicated that the training time was reduced by 8 hours and the
Dice Score for kidney almost remained same which was 88.01%, but in case of renal tumor, the score got better from
85.52% to 87.12%. one of the shortcomings of their model was that it was valid for only small set of data.
Aortic Valve Segmentation Basic CNN models for segmentation were performing good on 2D images and they
were struggling against 3D medical imaging. Hence Pak et al. [204] proposed a deep learning based architecture for
thesegmentationof3DCTScanimages.ThisnetworkwascomprisedofabaselineU-NetArchitecturethatperformed
thebasicsegmentationtaskandaSpatialTransformer[200]thatwasusedtoperformsomeaffinetransformation.The
use of only U-Net[139] was not sufficient for the segmentation tasks as it requires a lot of memory and also result in
decrease of accuracy. Hence they used a spatial Transformer (STN)[200] which reduced the size of input image by
performingsometransformationandhenceitresultedinbettercomputation.theyutilizetheirmodeltoperformaortic
valvesegmentation.Upontestingtheirmodelondifferentpatientsdata,theDiceScorecoefficienttheygetwas0.717.
Bone Segmentation In order to perform the segmentation of bone as well as the localization of the anatomical
landmarks of cone beamed computed tomography data simultaneously Lian et al. [205] proposed a network called
dynamictransformernetwork(DTNet).Theirmodelcontributedinthreeparts.Inthefirstpart,asynergicarchitecture
was made to accurately catch the global context and fine grained details of image for volume to volume prediction.
Secondly, by using the anatomic reliance between landmarks RDLs are made to collectively degenerate the large 3D
heatmapsofeverylandmarks.Thirdly,ATMsaremadeforthecompliantlearningofcontextspecificfeatureembedding
frommutualfeaturebases.WhiledoingtheexperimentsonCTscansofmandible,thesegmentationDSCcameoutto
be 93.95% with a std dev of 1.30 whereas for localization the RMSE was 1.95 ¬±0.43. these results were better than
U-Net and other models that were used for comparison.
Lesion Segmentation The early diagnosis of AIS provides valuable knowledge about the disease. But for a human
eyeitisburdensometodiscriminatedelicatechangesinpathology.Hence,Luoetal.Luoetal.[206]proposedanetwork
forthesegmentationofAcuteIschemicStroke(AIS)thatwasbasedonself-attention.Thismechanismhadanencoder
and a decoder. The encoder was comprised of a CNN as a backbone and a transformer. This encoder part picked the
globalcontextfeatures.ThedecoderpartconsistedofMultiHeadCrossAttention(MHCA)modulewhichupsampled
the feature maps that were coming from the encoder. These feature maps were connected via skip connections. The
backboneCNNusedwasRESNET-50.TheirexperimentalresultswerecomparedtoattentionU-Net[207],U-Net[139]
andTransUNet[208]buttheirmodeloutperformedthembygivingtheDiceSimilarityCoefficientofsegmentationof
lesion up to 73.58% which was better than all other compared models.
Segmentation of Organs Although transformers help in capturing the long term dependencies but when it comes
to the segmentation of 3D images, the dependencies face extreme computation. Hence to reduce some computations,
Xie et al. [209] presented CoTr, which is a combination of convolutional network and transformer. Instead of using
simpletransformer,theyintroduceddeformabletransfersforcatchingthelongrangedependencies(DeTrans).DeTrans
focusses on only a few key points, which greatly reduces the computational complexity, which also allows to process
multiscaleimages,whicharequiteimportanttoattainanaccuratesegmentation.theytestedtheirmodelonBCVdataset
. : Page 34 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
thatincludestheimagesof11differenthumanorgans.CoTrachievedaDiceScoreof85andHausdorffDistanceof4.01
onavgandthesemeasureswerebetterthanothermethodsthattheyusedtocomparetheirmodel.Furtherenhancement
in their model could be that it can be enhanced by extended it to operate on different modalities.
6.3.2. Magnetic Resonance Imaging (MRI) Scans:
Brain Tumor Segmentation Glioma segmentation and prediction of IDH genotyping is an important and difficult
taskduetosimilaritiespresentinintraandinter-tumor.ToaddressthisproblemChengetal.Chengetal.[210]proposed
an MRI based fully automated multi model that could predict IDH genotyping and Glioma segmentation at the same
time.Thepreexistingmethodswerenotabletoperformthebothtasksatthesametime,alsothesemethodsfacedthe
problemsofinterandintra-tumourheterogeneity.SotheyaddresstheseissuesbyusingajointCNNandTransformer
encoder. The transformer was used to extract the global features that were used for the glioma segmentation. It also
contained a multi-scale classifier, which was used for IDH genotyping. A multi-task loss was then used to balance
the segmentation and IDH genotyping and this loss collectively joined the classification loss and segmentation loss.
In the end they proposed an unpredictability aware pseudo-label-selection to make pseudo-labels for IDH on a large
unlabeledDataset.TheynamedtheirmodelasMTTU-Net.OnexperimentstheirmodelimprovedtheHD95andDice
score 1.69mm and 1.23% for glioma segmentation and 2.13% and 4.28% in case of AUC and accuracy respectively.
Sagar et al. [211] proposed ViTBIS, vision transformer for bio medical image segmentation, that was based on
encoder and decoder architecture. Both encoder and decoder had transformer inside them. The feauture map of input
image was split into three different convolutions before it was fed to the transformer. These convolutions were, 1X1,
3X3and5X5.Thesethreedifferentfeaturemapswereconcatenatedwiththehelpofconcatoperator,thenitwasfedto
thetransformerintheencoder.Thesetransformerhadtheattentionmechanisminsidethemthetransformersofencoder
and decoder were joined together via skip connections. The same architecture of multiscale was used in the decoder
as well. Before producing a segmentation mask after linear projection, different sizes were concatenated via concat
operator. Upon testing their architecture on a public dataset for brain tumor segmentation the DSC achieved was 0.86
which was better than other state of the art CNN and transformer networks.
Brain Tissue Segmentation In order to solve the problem of multi model medical image segmentation, Sun et al.
[212] presented a novel multi model architecture based on transformer and Convolutional Neural Network for multi
modelimagesegmentationandnameditasHybridCTrm,andusedthismodeltosegmentdifferentbraintissues.This
networkusedtwopathsfortheimageencoding,onepathwasfromtheCNNandtheotherpathwasfromTransformer.
Thentherepresentationofimagefrombothpathswerejoinedtogetherfordecodingandthesegmentationpurpose.The
CNN controlled the rapid convergence of gradient descent while extracting the local features, whereas the non local
features were extracted by the transformer. They used two strategies for the fusion, one was the single path strategy
and the other was the multiple path strategy and used both of these strategies in their experiments. Experiments were
carried out on two different datasets and by following both strategies. On MRBrainS dataset the DSC came out to be
82.98and83.47forsigleandmultiplepathstrategiesrespectivelywhereasontheiSeg-2017datasetthesescoreswere
86.75 and 87.16 which were better than the models they used to do the comparison like HyperDenseNet[213],
BrainStructureSegmentation Agooddealofdeeplearningarchitecturesusedtoperformthetaskofsegmentation
on medical images confront the problem of noise at the inference time and result in inaccurate result. To address this
problem Sinclair et al. [214] proposed a network, Atlas-ISTN, atlas image and spatial transformer network, that was
able to perform both registration as well as segmentation on 2D and 3D data of Brain structure. This network could
perform segmentation on numerous interest regions/ structures and to register the atlas label map to an in-between
segmentation(pixel-wise). This model was also able to do the fine tuning of the parameters at the inference time in
ordertoachievebetterpixelwisesegmentation,duetowhichtheeffectofnoiseintheimagealsoreduced.Thismodel
was then tested on three different datasets, two 3D and one 2D. the results were compared with U-Net and this model
was performing better than U-Net[139] giving a DSC of 0.888.
CardiacSegmentation Combiningthesharedinformationofanyorganfromdifferentmodalitiesisveryhelpfulfor
learningandmultimodalityprocessing.Inordertoschievethis,Chartsiasetal.[215]proposeddisentangled,alignand
fuse network, DAFNet, that was able to learn the information present in different modalities input, hence producing
a more precise segmentation mask. Anatomical factors from different inputs are combined and processed at the same
time.DAFNetcollectedtheinformationpresentindifferentmodalitiesdespiteofthefactthatfewlabels(supervised)are
. : Page 35 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
there or even no labels (unsupervised). Spatial Transformer was used to align the anatomical factors in case of image
misregistration. They evaluated their model by performing L2, T1 and Cardiac segmentation on different datasets.
Theirmodelwasabletoperformonbothsinglemodelandmultimodelinputsanditoutperformedothermodelswhen
it was trained on single modality input whether with few labels(semi supervised) or no labels (unsupervised).
Definingtherightventricle(RV)structureincardiacsegmentationisastretchingworktodobecauseofitscomplex
andmultiplexstructure.Henceitrequiresshortaxisaswellaslongaxisimages.InordertoaddressthisissueGaoetal.
[216] established a consistency based co training mechanism that used the geometric relationships between different
view CMR images for the segmentation. Along with this mechanism, they also used the U-Net[139] architecture in
ordertocapturesomelongrangedependencies.EvaluationofthemodelwasdoneontheM&MS-2challengedataset
and the Dice score came out to be 0.83 and 0.86 for short axis and long axis respectively.
ColerectalCancerSegmentation InastudydonetosegmentthecolorectalcancerregionSuietal.[217]established
a novel approach, based on transformer, that performed the segmentation as well as detection of colorectal cancer
region collectively. Their model was based on two pipelines, one for the detection and the other for the segmentation.
In the detection part, region proposals were generated. They utilize image level decision approach that was based
on auto encoders. Whereas in the segmentation part they used patches of the image as input and to make the final
mask prediction, class embeddings were used. They compared their model with the Faster CNN and Yolo-v3 for the
detection task and their model performed exceptionally well on the used dataset, giving an accuracy of 88.6% where
as the segmentation score came out to be 91.1% which was way better than U-Net[139] and FCN[201].
6.3.3. X-ray or Radiographic Images:
Breast Tumor Segmentation Correct and accurate segmentation of tumor in ABVS is a difficult task because the
sizeofimageishugeanditsqualityislow.InordertosegmenttumorfromtheABVSimages,Liuetal.[218]adopted
the use of both transformers and CNNs and named their model as 3D-UNet. They joined the attention module and
the U-Net[139] model. For further improvements in the performance they also made use of Atrous Spatial Pyramid
Pooling(ASPP)intheirmodel.ASPPcanhelpcatchtheinformationatmultiscales.Theycomparedtheirmodelwith
different3Dsegmentationmodelslike3DFCN[219],3DPSPNet[220]andrecordedtheDiceScoreCoefficient.Their
score recorded as 76.36 ¬±6.11, which was better than the networks that were used for the comparison.
AnatomicalStructureSegmentation Mostofthesegmentationnetworksworkonsupervisedlearningwhereexpert
labeled image is required as a label and this is an obstacle if there aren‚Äôt much experts available. Hence in order to
make an annotation efficient Lu et al. [221] introduced Contour Transformer Network, CTN, which is an annotation
efficient segmentation method for anatomical structures. They copied the human ability doing the segmentation of
anatomical structures with very less exemplars available. To achieve this, they proposed a semi supervised learning
mechanismthatutilizetheresemblanceofstructureandappearanceofthedesiredobjectbetweenunlabeledandlabeled
images. They made the segmentations of anatomy in the form of contour evolution process and model the behavior
by GCNs. They named their model as one shot anatomy segmentation model. On performing the segmentation on
four different anatomies, their model comprehensively performed better than u supervised learning mechanisms and
performed competitively against the supervised state of the art methods. Upon experiments the accuracy of one shot
model came out to be 96.58% which was almost 15% better than Braintorm, which is another one shot based model.
Whereas in comparison with non-learning based model, the accuracy was 16% improved. one shortcoming of their
networkisthattheirnetworkonlyperformedon2Ddata.henceextendingthisarchitecturetoworkon3Ddatawould
be an important step in the field of 3D segmentation.
Guide Wire Segmentation A study done by researchers tried to resolve the task of segmentation of guide wire
in X-ray fluoroscopy sequence. Zhang et al. [222] proposed a network that takes in account current frame as well
as the previous frame while taking input for the guide-wire segmentation. By considering both frames helped them
in obtaining the temporary information. Their network contained two parts, one was a CNN and the other was a
transformer. The CNN wasn‚Äôt able to capture the global features hence transformer came into play that can learn the
global features by using its attention mechanism. CNN and transformer lied in the encoder part whereas decoder
contained up sampling, concatenating operations and convolutions. They evaluated their model on datasets from
three different hospitals and measured the F1-score and compared their score with other state of the art models like
Frrnet[223], Parnet[224] and U-Net[139] and their model was outperforming all other models.
. : Page 36 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Tooth Root Segmentation An accurate and precise segmentation of the boundaries present in the roots of the tooth
isnecessarytoattainaperfectrootcanaltherapyassessment.Lietal.[225]introducedaGroupTransformerNetwork
(GTU-Net)inordertoachievesegmentationofrootboundaries.Theirmodel‚ÄôsstructurewassimilartotheU-Netbut
they used group of transformers in place of encoders and decoders. Also in order to incorporate the prior knowledge
theyusedFourierDescriptorLoss.Theirmodelachievedanaccuracyof96.31%andf1scoreof84.58%outperforming
other state of the art models.
6.3.4. OCT or Fundus Images:
Drusen Segmentation It is very crucial to diagnose the AMD at an early stage in retinal OCT images via Drusen
Segmentation.InordertoachieveanaccuratesegmentationWangetal.[226]proposedamultiscaletransformerglobal
attention network MsTGANet for the segmentation of drusen in retinal OCT images. Their model was composed of
a U-shaped architecture containing an encoder and decoder. To collect the non-local features at different scales with
ling term dependencies from multiple encoder layers, a novel multi scale transformer non local module is proposed
and used at encoder‚Äôs top. Another module, MsGCS was introduced to assist the model to join different semantic
knowledgebetweenencoderanddecoder.TheyalsointroducedasemisupervisedversionofMsTGANet.Thisversion
wascomprisedofpseudo-labeleddataaugmentationstrategy.Thismodelcanusedhugeamountofunlabeleddatain
order to increase the performance on segmentation, upon experiments the DSC came out to be 0.8692 with a std of
0.0052outperformingtheotherstateoftheartmodels.thismodelwastrainedonasmallerdataset,howeveritwillbe
better to collect a larger set of data in order to see its efficiency. Also, different semi-supervised learning approaches
can also be used to further improve its performance.
The table 6 given below summarized the performance gain by the reviewed articles of the segmentation category.
Table 6: A list of datasets and performance measures adopted by researchers for Segmentation.
Modality Publication Dataset Performance Measures
MRICheng et al.[210] BRaTS2020[107]Dice Score = 0.90
Hausdorff Distance = 4.4
AUC(%) = 91.04
Accuracy(%) = 90
Sensitivity(%) = 87.50
Specificity(%) = 92.11
Chartasis et al.[215]ERI[110] Dice Score = 0.82
CHAOS[111] Dice Score = 0.85
Sun et al.[212]MRBrainS[108] Dice Score = 0.83
iSeg2017[106] Dice Score = 0.87
Sinclair et al.[214] UKBB[109] Dice Score = 0.86
Hausdorff Distance = 7.2
Sagar et al.[211] BRaTS2019 Dice Score = 0.86
. : Page 37 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Hausdorff Distance = 7.1
Gao et al.[216] M&MS2Dice Score = 0.86
Hausdorff Distance = 9.6
X-ray ImagesLu et al.[221]OAIIOU(%) = 97.32
Hausdorff Distance = 6.0
JSRT[113]IOU(%) = 94.75
Hausdorff Distance = 12.1
Li et al. [225] DRIVE[112] Dice Score = 0.92
CT Scans La et al.[203] KiTS2019[114] Dice Score = 0.88
OCT/Fundus Images Wang et al.[226] USCD[115] Dice Score = 0.86
6.4. Clinical Report Generation
In this section, we briefly describe various transformer models to generate the medical reports and address the
preceding challenges associated with automatic clinical report generation.
6.4.1. Supervised Learning Based Approaches
Supervisedlearningreferstoatypeoflearningalgorithmsthatlearnunderthepresenceofasupervisor.Aninput
fromthetrainingsetispassedthroughthenetworkthentheoutputofthenetworkiscomparedtothedesiredoutputand
learningweightsareupdatedaccordingly.Followingstudieshaveemployedsupervisedlearningintheirmethodologies.
IncorporatingGlobalLevelFeatures Globallevelfeaturesareextractedfromtheentiremedicalimagei.e.encoded
features of both normal and disease regions in the image. Following studies have incorporated this notion into their
methodologies.Youetal.[144]proposedatransformer-basedarchitecture,AlignTransformer.Theyresolvedthedata
bias and long sequence modeling problems to generate a coherent medical report by delineating the normal and
abnormalregions.TheyusedResNet-50pre-trainedonImageNetandfine-tunedonCheXpertdatasettoextractvisual
features. Furthermore, they fed the extracted visual features into the pre-trained multi-label classification netwrok to
predict the disease tags. Align hierarchical attention as an encoder aligned the disease tags and visual regions by
learning the correlation and relationship between them. Moreover, they acquired the multi-grained disease-grounded
visual features from the aligned disease tags and visual regions to alleviate the data bias problem. Multi-grained
transformer as a decoder exploited the multi-grained disease grounded visual features to generate a proper medical
report. In automatic evaluation, they compared their experimental results with the previous state-of-the-art models
i.e.R2Gen,PPKED,andSentSAT+KGetc.andachievedcompetitiveresultsonIUX-rayandMIMIC-CXRdatasets.
In human evaluation, the results of their model were far better than that of R2Gen model. Similarly, Amjoud et al.
[227] also proposed a transformer-based deep learning model for generating long and detailed reports of chest x-ray
images. They used a pre-trained DenseNet-121 [82] instead of ResNet-50 [144] to avoid gradient vanishing problem
and redundant feature maps. They suppressed the last classification layer of the pre-trained model to extract global
andregionalfeaturesfrommedicalimages.Afterthat,theextractedfeatureswerefedasinputintotheencodertomap
themintoasequenceofcontinuousrepresentations.Theymodifiedthedecoderofthevanillatransformerbyaddinga
relational memory module to the normalization layer. Experiments demonstrated that their model generated detailed
findings reports for IU chest x-rays test images and outperformed the state-of-the-art models for BLUE-1, BLUE-2,
and ROUGE metrics with 0.479, 0.359, and 0.380 scores respectively. However, the model could not perform well
. : Page 38 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
forBLUE-3,BLUE-4,andMETEORmetrics.Also,theyusedasmallcorpusfortraining,asaresult,somesentences
were unseen during inference which lead to the scattering problem.
In another work, Pahwa et al. [143] leveraged the skip connections by proposing a transformer-based architecture
namedMEDSKIPbymodifyingahigh-resolutionnetwork(HRNet).Theymodified(HRNet)[228]forvisualfeatures
extraction by incorporating skip connections along with convolutional block attention modules (CBAM). First, they
extracted features representation from each down-sampled layer and after extracting crucial features using attention,
CBAMconcatenatedthem.CBAMconstitutedspatiallyandchannelattentionsub-modulesforinferringa1Dchannel
attention map and a 2D spatial attention map respectively. The proposed architecture also contained a memory-
driven transformer which constituted a standard encoder but the decoder contained a memory-driven conditional
normalization layer to incorporate relational memory. The decoder facilitated the learning from patterns in reports
and recorded key information of the generated process. Extensive experiments on two publicly available datasets
PEIR GROSS and IU chest x-rays showed that their proposed model had given the state-of-the-art results for BLEU,
METEOR, and ROGUE metrics.
Incorporating Global and Local Level Features A medical image contains both normal and disease regions. To
encode the disease regions of the image, previous studies encoded the complete image which lead to the encoding of
irrelevant visual content which is adverse for radiology report generation. Some diseases have strong correlation and
findingthosecorrelationisbeneficialforgeneratingreportforrarediseases.Variousstudiestriedtotakeadvantageof
thisfeaturebyconstructingcorrelationmatrixintheencodingstagebydata-drivenmethodologiesorexpertknowledge
but these studies failed to decode these correlations effectively while decoding.
To address these problems, Jia et al. [229] leveraged the transformer-based architecture and proposed a few-shot
radiology report generation model, namely TransGen. In the encoding stage, they introduced a semantic-aware visual
Learning(SVL)moduleinwhichtheyusedResNet101toidentifyandcapturethediseaseregionsofrarediseases.They
capturedthediseaseregionsfromtheimageitselfandthefeaturemapgeneratedattimestep(t-1)bylearningthetwo
masksrespectivelytorefinethevisualrepresentationofrarediseases.Theyadoptedaweightedsumofthesetwomasks
attimestepttolearnthevisualrepresentationsefficientlybyincorporatingbothglobalandlocallevelinformation.For
efficientdecodingofencodedcorrelationamongthediseases,thememoryaugmentedsemanticenhancementmodule
wasintroducedatthedecodingstage.Experimentsdemonstratedthattheirmodeloutperformedthestate-of-artmodels
on the MIMIC-CXR dataset but could not perform well for the IU X-ray dataset.
Similarly, Lee et al.[230] also incorporated both local and global level features by proposing Cross Encoder-
Decoder Transformer (CEDT) contained a Global-Local Visual Extractor. They used a convolution neural network
(e.g.ResNet101)asaglobalvisualextractortoencodethecompleteradiologyimageintoasequenceofpatchfeatures
toaccuratelycapturethefeaturesatthegloballeveli.e.bonestructureorsizeoftheorgan.However,whileincorporating
global-levelfeatures,itwasdifficulttoencodetheexactlocationandthesizeofthelesionarea.Toaddressthisproblem,
theycroppedthediseaseregionsoftheimagewiththelastlayeroftheCNNusingtheattention-guidedmaskinference
process and after resizing to the same size as the image, used them as input to the local feature extractor to extract
the local level features. Then, they concatenated the local visual features and global visual features and used them as
input to the CEDT. The standard transformer uses only the last layer information but they [230] also used low-level
features in addition to the high-level features by using the concept of [231]. They used multiple encoders to get the
all-level information from them and utilize the outputs of all encoders on each decoder using parallel multi-headed
attention.Theyaddedtheextractedfeaturesofeachencoderlayerwhichresultedinbettercaptioningthanthebaseline
modelR2Gen.Furthermore,theyalsoemployedMCLNandRMforrecordingandutilizingtheimportantinformation.
ExtensiveexperimentsdemonstratedthattheirmodeloutperformedthebaselinemodelforBLEU-1,BLEU-2,BLEU-
3,METEOR,andROUGE-LonIUX-raydataset.Theyalsoperformedexperimentwithpre-trainedGLVEbutitcould
not perform well for the BLEU-4 metric.
6.4.2. Reinforcement Learning Based Approach
Previous studies [143, 144, 227, 229, 230] have used supervised learning approaches to generate medical reports.
Supervised learning approaches are prone to exposure bias problems in language modeling methods. To address this
problem,Xiongetal.[232]proposedanovelhierarchicalneuralnetworkarchitectureusingreinforcementlearningto
generate a long coherent medical report. They incorporated the self-critical reinforcement learning method into the
detector, encoder, and captioning decoder. Previous studies used only top-down visual encoders, however, this was
the first study that incorporated a bottom-up visual detector as well to extract semantic rich features from the medical
. : Page 39 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
images.Forthispurpose,firstlytheyusedDenseNet-121,pre-trainedonchestX-ray14dataset,todetecttheregionof
interest(ROI)proposalsusingabottom-upattentionmechanism.TheregiondetectoroutputtedasetofROIproposals
alongwithclassifiedclassesandsomeassociatedattributes.Secondly,theyusedtop-downtransformervisualencoder
to extract further pixel-wise visual information from proposed ROI using pooling operations. Lastly, the transformer
captioningmoduleusedimprovedROIproposalsasinputfromthetransformervisualencoderandgenerateddescriptive
sentences for each proposed ROI by calculating reward directly using the CIDEr metric. Their proposed architecture
outperformedthestate-of-the-artmethodsfortheCIDErevaluationmetricontheIUX-raydatasetbutfortheBELU-1
metric, their model could not perform state-of-the-art. Their model over-fitted as they used only the findings portion
of the generated medical report. This problem can be resolved using a larger labelled dataset.
Thetable7givenbelowsummarizedtheperformancegainbythereviewedarticlesoftheclinicalreportgeneration.
Table 7: A list of datasets and performance measures adopted by researchers for Clinical Report Generation.
Modality Publication Dataset Performance Measures
X-ray ImagesYou et al. [144]IU X-ray [116]BLEU-1 = 0.484
BLEU-2 = 0.313
BLEU-3 = 0.225
BLEU-4=0.173
METERO=0.204
ROUGE-L=0.379
MIMIC-CXR [117]BLEU-1=0.378
BLEU-2=0.235
BLEU-3=0.156
BLEU-4=0.112
METERO=0.158
ROUGE-L = 0.283
BLEU-1=0.479
X-ray ImagesAmjoud et al.[227] IU X-ray [116]BLEU-2=0.359
ROUGE-L = 0.380
. : Page 40 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
X-raysPahwa et al. [143]IU X-ray [116]BLEU-1= 0.467
BLEU-2=0.297
BLEU-3=0.214
BLEU-4=0.162
METERO=0.187
ROUGE-L=0.355
PEIR GROSS [118]BLEU-1= 0.399
BLEU-2=0.278
BLEU-3=0.209
BLEU-4=0.148
METERO=0.176
ROUGE-L=0.414
Jia et al.[229]IU X-ray [116]BLEU-1=0.461
BLEU-2=0.285
BLEU-3=0.196
BLEU-4=0.145
ROUGE-L= 0.367
KA (%) = 0.367
MIMIC-CXR [117]BLEU-1=0.368
BLEU-2=0.243
BLEU-3=0.178
. : Page 41 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
BLEU-4=0.138
ROUGE-L= 0.338
BLEU-1=0.5064
IU X-ray [116]BLEU-2=0.3195
BLEU-3=0.2201
Lee et al.[230]BLEU-4=0.1924
ROUGE-L= 0.3802
X-ray Images
Xiong et al. [232] X-ray [116]BLEU-1=0.350
BLEU-2=0.234
BLEU-3=0.143
BLEU-4=0.096
CIDEr = 0.323
6.5. Miscellaneous ViT Applications in Medical Imaging
Tranformer-based architecture has also played a vital role in other applications of medical field i.e. in image
synthesis, denoising the low dose computed tomography, and positron emission tomography images, enhancing the
resolution of medical images etc.
6.5.1. functional Magnetic Resonance Imaging (fMRI) Scans:
Visualizing Regenerated Neural Visual Content In the past decades, few studies have been conducted to decode
thehumanbrainneuralactivitiesintonaturallanguagesentences.Themainpurposeofdecodingbrainneuralactivity
is basically to know the human brain‚Äôs perception of textual or visual content. In the past, most deep learning studies
focused on different task specific decodings, i.e. detection, classification, recognition etc., using functional magnetic
resonance imaging (fMRI) data. With the advancement in technology, several research has been done in language
decoding to decode the human brain semantics evoked by linguistic stimuli into natural language words or sentences.
Inspiringfromthesetasks,Zhangetal.[59]proposedahybridlanguagedecodingmodel:CNN-Transformertodecode
the visual stimuli evoked at multi-times by natural images into descriptive sentences. They exploited the concept of
neuralmachinetranslation(NMT)[17]butthedifferencewasinsourcesequencei.e.naturalimagesinNMTbutvisual
neuralactivitiesin[59].Toachievethistask,firstlytheyextractedmeaningfulsemanticlow-dimensionalfeaturesfrom
high-dimensionalvisualneuralactivities(low-levelrawfMRIdata)usingtwo-layeronedimensionalCNN.Secondly,
the encoder part of the transformer encoded the semantic features into multi-level abstract representation. Lastly, the
. : Page 42 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
decoder of the transformer decoded the multi-level representation into descriptive natural language sentences. They
comparedtheirmodelwithotherdecodingmodelsandachievedstate-of-the-artresultsforBLEU,CIDEr,andROUGE
metricswith0.17,0.66,and0.18scoresrespectively.Infuture,thistransformer-basedbraindecodingtechnologywill
beusefulforthepeoplewhoareunabletotransmittheirvisualperceptionintospeechandwillalsobeabreakthrough
for neuro-scientists in understanding and decoding the neural activities of human brain.
6.5.2. PET-CT Scans:
Medical Image Enhancement Computed tomography is a non-invasive imaging technique for medical diagnosis.
Since high exposure to X-rays radiation is deadly for humans and has become the main concern for medical
practitioners. To lessen this effect of X-rays radiation, it is used in less quantity in CT scans but it poses some serious
problems, i.e. less contrast, sharp features, corners, edges, and stronger noise, which affects the quality of CT scans.
Although low-dose computed tomography (LDCT) is mainstream in clinical applications, but the posed problems
causehindranceinaneffectiveclinicaldiagnosis.Manytraditionalmethods(iterativemethods)andconvolution-based
deep learning approaches were employed to acquire high quality LDCT images by deblurring and suppressing the
artifacts.Thehigh-frequencysub-bandofimagesarenoisyareaswhilethelow-frequencysub-bandarenoise-freeareas
containingmainimagecontent.Sinceconvolution-basedmethodsarelimitedtoextractingfeaturesfromthelocalareas
of images due to limited receptive fields. Therefore, transformers came into the scientific field and revolutionized the
world with their facts of capturing long-range dependencies between image regions.
Keepinginaccountalltheseobservations,Zhangetal.[233]proposedatransformer-basedarchitecturetodenoise
the LDCT images by decomposing them into high frequency (HF) and low-frequency parts. Hence, the noise was
only retained in the HF part and it also contained a plethora of image textures. To ensure the relationship between
HF and LF parts of the image, they denoised the noisy HF part with the assistance of the latent texture of the LF
part. For this purpose, they employed CNNs to extract corresponding texture and content features from the LF image
part.Furthermore,theyacquiredthehigh-levelfeaturesfromthetransformerusingthetexturefeaturesfromthenoisy
LF and embeddings from the HF part. They used a modified transformer with three encoders and three decoders.
Finally, they reconstructed the high-quality LDCT image piecewise by combining these high-level features with the
content features from the LF part of the image. ent features from the LF part of the image. Extensive experiments
demonstrated that their model outperformed all the baseline methods achieving 93.7% for VIT metrics, improved
structuresimilarityby12.3%,androotmeansquareerrorloweredby40.5%onMayolow-dosecomputedtomography
images dataset. Since convolution-based methods cannot capture global contextual information, Wang et al. [6] first
time proposed a convolution-free token-to-token vision transformer-based dilation network to denoise the LDCT
images.Theycapturedthenoisefrominputmedicalimagesbylearningdeepfeatures,afterthat,theyremovethenoisy
estimated residual images in order to clean them. Firstly, they used tokenization block to tokenize the feature map
patches into tokens. Secondly, they fed those tokens into transformer block, further, for enhancing the tokenization,
they applied tokenization in cascaded form in token-to-token block. They further enlarged the receptive field and
refinedthecontextualinformationusingdilationintokenizationprocedure.Theyperformeddilationusingreshaping,
soft split and cyclic shift to enhance the context. They compared their model with other state-of-the-art models and
their model outperformed for SSIM and RMSE metrics with 0.9144 and 8.7681 scores respectively, in denoising the
images. Without down-scaling tokenization of the image can be enhanced.
PET/MRIcanconcurrentlyprovideanatomicalandmorphologicalimaginginformationthataidsinclinicaldisease
diagnosis. PET acquires metabolic imaging information with the help of radio-tracers while MRI uses magnetic field
gradientsandradiowavestoacquireimagesofsoftbodytissues.Although,theseimagingmodalitieshaveapplications
indiseasediagnosis,i.e.cancer,tumor,andbraindiseases,butalsoposesomeseriousconcerns.Sincetimerequirement
for PET imaging acquisition is high and as a result, patient discomfort can affect the image quality i.e. low contrast-
to-noise ratio. The Information from MRI can assist in denoising the PET images using registration approach. Many
traditional deep learning and computer vision methods proposed to enhance the PET image quality using MRI but
duetodiscrepancyinmodalitiestheycouldnotextractcontextualandspatialinformationefficiently.Toaddressthese
problems. Zhang et al. [234] proposed a spatial adaptive and transformer fusion network (STFNet) for denoising low
count PET with MRI. They adapted dual path using the spatial-adaptive block to extract features. For the fusion of
high-levelfeatures,theymodifiedthetraditionaltransformerencoderandincorporatedglobalattentiontoformapixel-
to-pixelrelationshipbetweenMRIandPET.ThefusedfeaturemapwasusedasinputtothedecoderforPETdenoising.
Their model obtained promising results for on RMSE,PSNR,SSIM, and PCC metrics.
. : Page 43 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Similarly, Luo et al. [235] proposed 3D Transformer-GAN to build a standard dose PET image from the low-
dosePETimage.TheyleveragedtheCNN-Transformerarchitecturetoincorporatebothglobalandlocalinformation.
CNN-based-Encoderextractedenrichedspatialinformationfromtheinputmedicalimage.Moreover,thetransformer
captured the long-range dependency from the extracted features from the CNN-based-Encoder. The learned repre-
sentation from the transformer are incorporated into the CNN-based-Decoder to restore them and reconstructed the
standard PET image. Extensive experiments demonstrated that their model outperformed the state-of-the-art on real
human brain dataset.
6.5.3. Magnetic Resonance Imaging (MRI) Scans:
Medical Image Reconstruction MRI is a prevalent non-invasive imaging technique but its acquisition process is
slow.Consequently,thereisaneedtodevelopacceleratedMRImethods.Simultaneously,severalstudiesondeepneural
networkshavebeenconductedtodevelopstate-of-the-artmethodsforacceleratedMRI.Therefore,Korkmazetal.[3]
accelerated MRI by reconstructing full-sampled MRI images using unsupervised learning incorporating deep image
prior framework to alleviate the problem of under-sampled acquisition. They proposed generative vision transformer
basedunsupervisedMRIreconstructionarchitecturetoincreasethereceptivefield.Firstly,theyperformedgenerative
non-linear mapping over latent and noisy space to improve the invertibility of the model. Secondly, they used cross-
attention to improve the context, i.e. both global and local context, of image and latent features. They did not use
anypre-trainedmodel.Lastly,theyperformedinferenceoneachindividualsubjecttoincreasethegeneralization.They
performedextensiveexperimentsonacceleratedmulti-contrastbrainMRIdatasetandoutperformedtheconvolutional-
based generative models for PSNR and SSIM metrics.
Wang et al. [236] proposed a super-resolution approach to reconstruct the high resolution MRI scans from low
resolution scans. They proposed adjacent slices feature transformer (ASFT) network. Firstly, they incorporated extra
slices in the consecutive in-plane slices of an-isotropic 3D MRI images. Secondly, to harness the similarity between
theconsecutiveslicestheyintroducedamulti-branchfeaturestransformationandextraction(MFTE)block.Thirdly,to
enrichthetargethighresolutionsliceswiththeinformationfromthelowresolutionreferenceslicestheyfilteredoutthe
uselessinformationusingMFTEblock.Moreover,theyusedspatialadaptiveblocktorefinethefeaturesspatially.They
used channel attention to incorporate the multi-scale features and consequently, they enhanced the super resolution.
Their model achieved the state-of-the-art performance for super-resolution task.
Medical Image Synthesis Tissue morphology information acquired from multimodal medical images play an
important role in the clinical practice. However, it is not commonly used because of the expensive capturing of these
information. Generative models such as generative adversarial network (GAN) are in practice to artificially synthesis
theseimages.GANisaCNNbasedarchitecturethatshowslocalitybiasandspatialinvarianceacrossallthepositions.
Huetal.[237]introducedatransformerbaseddouble-scaledeeplearningarchitectureforcross-modalmedicalimage
synthesis to incorporate log-range dependencies Double-scale GAN showed efficient performance on benchmark IXI
MRI dataset.
MedicalImageRegistration Tissuedeformationinahighlydeformableanatomyisestimatedusingimageregistra-
tion.Diffeomorphicregistrationisoneoftheimageregistrationtechniquesthatpreservestheinvertibleandonetoone
mappingbetweenimages.Currentdeeplearningtechniqueslacktheabilitytohandlelongrangerelevancethuslimiting
the meaningful contextual correspondence between images. In the paper [238] a dual transformer network (DTN)
modelisproposedforthediffeomorphicregistrationofMRimages.DTN,usingself-attentionmechanisms,modelthe
relevance from both the separate and concatenated images embeddings, which facilitate contextual correspondence
between anatomies. DTN consists of learnable embedding module, relevance learning module and registration field
inferencemodule.Diffeomorphicregistrationfieldisestimatedusingmovingfixedandmovingimagesforone-to-one
mapping. DTN has two branches to learn the relevance based on the embeddings of separate one-channel images
andconcatenatedtwo-channelimages.FirsttheLow-levelimagefeaturefortheconcatenatedandseparateimagesare
extracted using CNN. Second, the image features, converted into sequences, are fed to DTN for feature enhancement,
basedonglobalcorrespondence.Concatenatedfeaturesformbothbranchesarethenusedtoinferthevelocityfieldand
registration filed. The deformation field is represented as the exponential of the velocity which ensure the invertible
mapping. Metric space is used to optimize the proposed DTN in an unsupervised manner.
. : Page 44 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
6.5.4. Endoscopy Images
MedicalImageReconstruction Endoscopeisaminimallyinvasivemedicalimagingmodality.Itassistsinmedical
diagnoses by acquiring accurate images of the internal organs of a patient‚Äôs body. However, the small imaging sensor
in the endoscope causes problems in acquiring the high magnified blood vessels images. Many traditional methods
incorporatinginterpolatedmethodsanddeeplearninghaveapplicationsinreconstructionofsuper-resolutionofsingle
images. Mainly convolution-based deep learning methods are there in reconstructing high-resolution images which
have problems in capturing the global context. Consequently, Gu et al. [239] leveraged transformers and proposed
hybrid architecture with the convolutional neural network to enhance the texture of blood vessels. Firstly, CNN
extracted the low-level features from low-resolution (LR) images and the transformer sampled the LR image into
threedifferentsamplestoextracttexturefromtheimage.Secondly,similaritywasexaminedbetweendifferentfeatures
extracted from transformer-based extractor and CNN-based extractor. Thirdly, they employed a texture migration
method to interchange the information between multi-scale features extracted from the transformer and CNN to
synthesizetheimage.Lastly,sub-pixelconvolutionoperationwasperformedonmigratedbasicimagestosynthesizea
high-resolution.TheirmodelacquiredpromisingqualitativeandquantitativeresultsthantheCNNbasedsingleimage
super-resolution methods.
6.5.5. Fluorescence Microscopy
Quantitative Characterization of Anatomical Structure using Combination of Markers in Bone Marrow
VasculatureandFetalLiverTissues Fluorescencemicroscopyisavariantoftraditionalmicroscopy,whichusesa
higherintensitylightsourcethatexcitesafluorescentspeciesinasampleofinterest.Itisusedfordifferentpurposessuch
asdetailedexaminationofanatomicallandmarks,cells,andcellularnetworks.AlvaroGomarizetal.[240]proposeda
MarkerSamplingandExcitenetworktoexploitthepotentialofattention-basednetworkonthefluorescencemicroscopy
datasets which are underexplored by the deep learning. The capability of the network is tested by the quantitative
characterization in various datasets of microvessels in fetal liver tissues and bone marrow vasculature in 3D confocal
microscopydatasets.ProposedmodelgivesaconvincingperformancewithF1Scoreof91.2%forsinusoidsand71.2%
for arteries in the liver vasculature dataset.
Denoising of Celullar Microscopy Images for Assisted Augmented Microscopy Deep learning has greatly
assisted augmented microscopy that enable high-quality microscopy images with using costly microscopy apparatus.
Zhengyang Wang et al. [241] proposed global voxel transformer networks (GVTNets) that uses attention operators
to address the limitations in already existing U-Net based neural. Instead of local operator that lack dynamic non-
local information aggregation they used attention operators that allow global receptive field during prediction. They
measured the performance of the model on three existing datasets for three different augmented microscopy tasks.
6.5.6. Histopathology Images
WholeSlideImagescontainrichcontentabouttheanatomicalandmorphologicalcharacteristics.Tobetterdepict
theimagecontent,pathologistsfrequentlyexaminethetagsassociatedwiththeseimages.Sincethistaggingprocessis
labour-intensive, consequently Li et al. [242] first time proposed a Patch Transformer based architecture to automate
the multi-tagging whole slide images process. They incorporated attention mechanism to extract global level features
from the patches of images. They employed multi-tag attention module to build tag on the basis of weight. Extensive
experiments demonstrated that their model outperformed on 4920 WSI for macro and micro F1 metric.
6.5.7. OCT or Fundus Images
Diabetes damages the retina and causes diabetic retinopathy. This disease can lead to vision loss, therefore, early
detectioniscrucial.Variousdeeplearningapproacheshaveautomatedtherecognitionofdiabeticretinopathygrading.
However, Wu et al. [158] proposed vision transformer based architecture to assist the ophthalmologist in recognising
the diabetic retinopathy grade. They divided the fundus images into patches, flatten them to generate sequence, and
thenconvertedthemintolinearandpositionalembeddings.Togeneratethefinalrepresentations,theyfedthepositional
embeddingsintomulti-headattentionlayers.TheirmodeloutperformedtheCNN-basedarchitecturewithanaccuracy
of 91.4%.
Thetable8givenbelowsummarizedtheperformancegainbythereviewedarticlesofthemiscellaneouscategory.
. : Page 45 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Table 8: A list of datasets and performance measures adopted by researchers for Miscellaneous Application in ViT
Modality Publication Dataset Performance Measures
CT ScansWang et al. [6]NIH-AAPM-Mayo Clinic
LDCT Grand Challenge
[119]RMSE = 8.7681
SSIM = 0.9144
RMSE = 21.199+
‚àí2.054
Zhang et al. [233]NIH-AAPM-Mayo
Clinic LDCT Grand
Challenge[119]SSIM = 0.933+
‚àí0.012
VIF = 0.144+
‚àí0.025
PET ScansZhang et al.[234] uPMR790RMSE = 0.0447
PSNR (db) = 27.5321
SSIM = 0.9291
PCC = 0.9899
Luo et al. [235]PSNR = 24.818
Clinical dataset which in-
cludes eight normal control
(NC) subjects.SSIM = 0.986
NMSE = 0.0212
PSNR = 25.249
Clinical dataset which in-
cludes eight mild cognitive
impairment (MCI) subjects.SSIM = 0.987
NMSE = 0.0231
Endoscopy Images
Gu et al. [239]DIVerse 2K resolution high
quality (DIV2K) images
dataset [121]
Set5 PSNR (db) = 31.94
. : Page 46 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
SSIM = 0.8935
Set14 PSNR (db) = 28.11
SSIM = 0.7842
B100 PSNR (db) = 27.54
SSIM = 0.7464
Urban100 PSNR (db) = 25.87
SSIM = 0.7844
Manga109 PSNR (db) = 30.09
SSIM = 0.9077
MRI ScansWang et al. [236]Kirby21 dataset (KKI01 to
KKI05) [120] PSNR = 40.19+
‚àí2.04
SSIM= 0.9882+
‚àí0.0034
Dice = 0.91
Tang et al. [238]T1-weighted images (of size
182√ó218√ó182)of102drug-
addicts and 10 healthy vol-
unteers.HD = 2.68
ASD = 0.59
IXI dataset
T1, R=4 PSNR = 32.55+
‚àí1.77
SSIM (%) = 94.58+
‚àí0.82
Kokmaz et al. [3] T1, R=8 PSNR = 30.28+
‚àí1.68
SSIM (%) = 91.64+
‚àí1.42
T2, R=4 PSNR = 32.71+
‚àí0.73
SSIM (%) = 87.66+
‚àí1.67
T2, R=8 PSNR = 29.90+
‚àí0.70
SSIM (%) = 84.03+
‚àí1.89
. : Page 47 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Hu et al. [237] IXI dataset
T1 PSNR = 34.91+
‚àí1.00
SSIM = 0.895+
‚àí0.07
T2 PSNR = 35.34+
‚àí0.95
SSIM (%) = 0.895+
‚àí0.07
fMRI ScansCIDEr = 0.741
Zhang et al. [59]fMRI experiments(the vi-
sual stimulus consisted of
2750 natural images from
ImageNet data)ROUGE-L= 0.2009
BLEU= 0.186
Fluorescence
MicroscopyGomariz et al. [240]3D confocal microscopy
datasetsF1 Score = 0.912+
‚àí3.9
Fundus Images Wu et al. [158]DiabeticRetinopathyDetec-
tion datasetAccuracy (%) = 91.4
Specificity = 0.977
Precision = 0.928
Sensitivity = 0.926
Quadratic weighted kappa
score = 0.935
AUC = 0.986
WSI Li et al. [242]4,920 WSIs provided by a
histopathology service com-
panyMcro F1 = 0.910
Micro F1 = 0.944
7. Discussion and Conclusion
Vision Transformers (ViT) are now one of the hottest topics in the discipline of computer vision because of its
exemplaryperformanceandtremendouspotentialcomparedwithCNNs.AlthoughCNNsarematuredenoughforthe
development of applications that can ensure an efficient and accurate diagnosis. However, in the medical field, where
an inaccurate output might endanger lives, the concept of attention in vision transformers has paved its way for more
precise outcomes. Since ViT models assess the global context of the image along with the interpretability through
. : Page 48 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
the attention module, their performance is more precise than CNNs. A variety of approaches have been proposed in
recentyears,asoutlinedandsummarizedinthisreview,toexploreandutilizethecompetencyofvisiontransformers.
Theseapproachesshowedexcellentperformanceonawiderangeofvisualrecognitiontasks,includingclassification,
lesionsdetection,anatomicalstructuresegmentation,andclinicalreportgeneration.Nevertheless,therealpotentialof
transformersforcomputervisionhasyettobefullyexplored,whichmeansthatsignificantchallengesarestillthereto
be resolved. The following section envisages these challenges as well as provides insights on future prospects.
7.1. Current Trends and Open Challenges
Althoughseveraltransformer-basedmodelshavebeenproposedbyresearcherstoaddressvisualrecognitiontasks,
thesemethodsaresimplytheinitialstepsinthisfieldandthereisstillconsiderableroomforimprovement.Forexample,
the transformer architecture in ViT [19], is based on the standard transformer for NLP [16], although an enhanced
versionparticularlytailoredforCVneedstobeexplored.Furthermore,itisnecessarytoemploytransformerstoother
medical domains such as orthodotics, medical report generation, and phenotype extraction etc., to discover the power
of vision transformers.
In this multidisciplinary study, we provide a comprehensive view of how vision transformers employed medical
imaging modalities in visual recognition tasks for assisted diagnosis. We have reviewed the research articles selected
from top tier journals and conferences in which researchers have proposed excellent frameworks that employed
vision transformers to accelerate the efficiency and performance of already proposed CNN based models. The
Figure 5 is a clear evidence that vision transformers are widely employed in classification and segmentation related
visual recognition tasks whereas their significance in registration related tasks have not been greatly explored. The
miscellaneous studies in this review include the work on enhancing the resolutions for better outcomes, denoising of
CT scans with a low dosage of X-ray radiation, image registration etc. Nevertheless, the undertaken studies are based
ondiscreteapplications,therefore,theyarenotcomparable.Thefieldofresearchisquiteopenatthisstageasbaselines
areavailable.Then,reportgenerationcoveredonly7%whichindicatesanareatoresearchuponasitsapplicationscan
assist both doctors and patients.
Similarly, in the imaging modalities, it can be observed in Figure 6 that 61% of the papers found, are related to
X-rays,andCTscans.Thevisiontransformermodelwasfirstproposedintheyear2020,andmostofthecorresponding
literature available was related to the diagnosis of coronavirus as it was prevailing in the same year. The transformer
models achieved admirable results in regards to COVID-19, however, other domains especially digital pathology,
retinal, breast, etc., require attention as not much literature is available. Also, the literature related to COVID-19 is
eitherclassifyingordetectingthevirus,thus,nostudieswerefoundthatwereperformingsegmentation.Eventhough
MRI produces images with better resolution than CT scans, it covered only 13% of the total modalities. The papers
aremostlyusedforimagesegmentationandhaveproducedrespectableoutcomes.Theseoutcomesarebasedonafew
studies targeting distinct diseases, hence, there is much more to explore.
Inaddition,itwasobservedinclinicalreportgeneration‚Äôsreviewthatamongvariousmodalities,thestudiesrelated
to x-ray images were only available. Thus, it means that the data required for report generation is only present in
correspondence to x-ray images. As X-rays are a cost-efficient method for examining various parts of the body, it
is comprehensible that due to more reports and X-ray images available, it was possible to collect enough data for
automaticreportgeneration.Sinceitassistsdoctorsinwritingmedicalreports,thecollectionofcorrespondingimages
and text should be collected to implement automated report generations in other domains.
Next, one of the major limitations regarding medical datasets is that not enough data is available to train a
transformermodel.Thesemodelsrequiredatainhugeamountstoperformwellandadapttogeneralizability.Another
observationisthenon-availabilityofabenchmarkasobservedintheFigure16.Sincealmostallthestudiesarebased
on different datasets, the performances of the models are not comparable. Even if the datasets are the same in some
studies, they are either used for different purposes or are combined with other datasets to form one large repository.
Now, as long as there is no standard dataset for experimenting with different proposed models, research can not take
its step forwards towards development using vision transformers.
. : Page 49 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 16: Long-tail graphical representation of datasets employed in literature.
7.2. Future Prospects:
In order to propel the development of vision transformers in medical domain, we propose various significant
future directions. One direction is that, several papers in this study have done a comparative analysis of CNNs and
ViT, thus, it can be inferred that transformer models have given better results in terms of efficiency, computational
cost, and accuracy. Since these models have achieve such outcomes where researchers are talking about CNN‚Äôs being
replaceable,visiontransformersrequiremoreresearchandimplementationastheyareunravelingapathtowardsmore
resource-efficient models.
The paper demonstrates the significance of using vision transformers on medical images. The future directions of
research involve working with more heterogeneous data sets, and more extensive comparisons with other models to
give validity to the proposed transformer models. Next, the studies, may they be related to any category or modality,
are using different datasets, hence there cannot be a comparative analysis in terms of the proposed vision transformer
models. Therefore, we should work on creating a benchmark dataset.
Similarly, new knowledge emerging in cancer biology and deep learning enabled us to step into this rapidly evolving
domain. Genomic profiling is prevalent; however, it is crucial to correlate cancer genomic and phenotypic data to
fully understand cancer behavior. Cancer phenotype information includes tumor morphology (e.g., histopathologic
diagnosis),laboratoryresults(e.g.,geneamplificationstatus),specifictumorbehaviors(e.g.,metastasis),andresponse
totreatment(e.g.,theeffectofachemotherapeuticagentontumorvolume).Visiontransformerscanalsobeappliedto
medical images in order extract phenotypic information in order to better diagnose cancer related disorders. Another
directionisrelatedtothelimitationregardingcoronaviruscasesbeingdetectedandclassified,butnotbeingsegmented.
The usage of segmenting the virus can help determine the rate of spread of the virus, that is why it should be worked
uponinthefuture.Furthermore,theliteratureforclinicalreportgenerationisrelatedtoX-raysimages,andasdiscussed
abovemostofthevisiontransformersusedforX-rayimagesarebeingusedtodetectcoronavirus.Asperourknowledge,
none of the literature is related to medical report generation for coronavirus, and considering the work that has been
done in this regard, research in medical image captioning can propel towards the application side and assist the
practitioners. Overall, since it covers only 7 % of the total tasks in this study, there is more to explore in terms of
X-raysandotherimagingmodalities.Althoughvisiontransformershaveachievedanothermilestoneforimprovedand
accuratediagnosisinthemedicaldomain,thereisstillroomforimprovementintermsofresourceefficiency.Inother
words, we still have to discover the undiscovered.
. : Page 50 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
7.3. Conclusion
Thepaperenvisagesthesummaryanalysisofvisiontransformermodelsusedinthemedicaldomain.Thisstudywill
serve researchers from multidisciplinary backgrounds. The visual recognition tasks considered in this paper include;
classification,detection,segmentation,andclinicalreportgenerations.Otherthanthat,somemiscellaneoustaskswere
also included such as image registration, image reconstruction, etc. Furthermore, medical imaging modalities such as
X-rays,CTScans,MRIs,etc.wereusedasinputtorecognizemedicalimagingthroughvariousmodels.Ourgoalisto
presentthestudyinawaythatapprehendsthestatusandfuturedirectionsofvisiontransformers.Hence,westructured
the information of datasets, categories, modalities, and their results in a tabular form which will assist researchers to
move forward in the medical field conveniently.
References
[1] Isabella Castiglioni, Leonardo Rundo, Marina Codari, Giovanni Di Leo, Christian Salvatore, Matteo Interlenghi, Francesca Gallivanone,
Andrea Cozzi, Natascha Claudia D‚ÄôAmico, and Francesco Sardanelli. Ai applications to medical images: From machine learning to deep
learning. Physica Medica , 83:9‚Äì24, 2021.
[2] S Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S Duncan, Bram Van Ginneken, Anant Madabhushi, Jerry L Prince, Daniel
Rueckert, and Ronald M Summers. A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with
progress highlights, and future promises. Proceedings of the IEEE , 2021.
[3] YilmazKorkmaz,MahmutYurt,SalmanUlHassanDar,Muzaffer√ñzbey,andTolgaCukur. Deepmrireconstructionwithgenerativevision
transformers. In International Workshop on Machine Learning for Medical Image Reconstruction , pages 54‚Äì64. Springer, 2021.
[4] James A Diao, Jason K Wang, Wan Fung Chui, Victoria Mountain, Sai Chowdary Gullapally, Ramprakash Srinivasan, Richard N Mitchell,
Benjamin Glass, Sara Hoffman, Sudha K Rao, et al. Human-interpretable image features derived from densely mapped cancer pathology
slides predict diverse molecular phenotypes. Nature communications , 12(1):1‚Äì15, 2021.
[5] GeorgiosAKaissis,MarcusRMakowski,DanielR√ºckert,andRickmerFBraren. Secure,privacy-preservingandfederatedmachinelearning
in medical imaging. Nature Machine Intelligence , 2(6):305‚Äì311, 2020.
[6] Dayang Wang, Zhan Wu, and Hengyong Yu. Ted-net: Convolution-free t2t vision transformer-based encoder-decoder dilation network for
low-dose ct denoising. In International Workshop on Machine Learning in Medical Imaging , pages 416‚Äì425. Springer, 2021.
[7] Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal of Big Data , 6(1):1‚Äì54, 2019.
[8] Nima Tajbakhsh, Laura Jeyaseelan, Qian Li, Jeffrey N Chiang, Zhihao Wu, and Xiaowei Ding. Embracing imperfect datasets: A review of
deep learning solutions for medical image segmentation. Medical Image Analysis , 63:101693, 2020.
[9] DandiYang, CristhianMartinez,LaraVisu√±a, HardevKhandhar,Chintan Bhatt,andJesusCarretero. Detectionand analysisofcovid-19 in
medical images using deep learning techniques. Scientific Reports , 11(1):1‚Äì13, 2021.
[10] Chen Li, Hao Chen, Xiaoyan Li, Ning Xu, Zhijie Hu, Dan Xue, Shouliang Qi, He Ma, Le Zhang, and Hongzan Sun. A review for cervical
histopathology image analysis using machine vision approaches. Artificial Intelligence Review , 53(7):4821‚Äì4862, 2020.
[11] Chaoran Yu and Ernest Johann Helwig. The role of ai technology in prediction, diagnosis and treatment of colorectal cancer. Artificial
Intelligence Review , 55(1):323‚Äì343, 2022.
[12] GhulamMurtaza,LiyanaShuib,AinuddinWahidAbdulWahab,GhulamMujtaba,HenryFridayNweke,MohammedAliAl-garadi,Fariha
Zulfiqar,GhulamRaza,andNorAnizaAzmi. Deeplearning-basedbreastcancerclassificationthroughmedicalimagingmodalities:stateof
the art and research challenges. Artificial Intelligence Review , 53(3):1655‚Äì1720, 2020.
[13] In√™s Domingues, Gis√®le Pereira, Pedro Martins, Hugo Duarte, Jo√£o Santos, and Pedro Henriques Abreu. Using deep learning techniques in
medical imaging: a systematic review of applications on ct and pet. Artificial Intelligence Review , 53(6):4093‚Äì4160, 2020.
[14] GeertLitjens,ThijsKooi,BabakEhteshamiBejnordi,ArnaudArindraAdiyosoSetio,FrancescoCiompi,MohsenGhafoorian,JeroenA.W.M.
van der Laak, Bram van Ginneken, and Clara I. S√°nchez. A survey on deep learning in medical image analysis. Medical Image Analysis ,
42:60‚Äì88, 2017.
[15] AsifullahKhan,AnabiaSohail,UmmeZahoora,andAqsaSaeedQureshi. Asurveyoftherecentarchitecturesofdeepconvolutionalneural
networks. Artificial intelligence review , 53(8):5455‚Äì5516, 2020.
[16] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,≈ÅukaszKaiser,andIlliaPolosukhin. Attention
is all you need. Advances in neural information processing systems , 30, 2017.
[17] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointlylearningtoalignandtranslate. arXivpreprint
arXiv:1409.0473 , 2014.
[18] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv
preprint arXiv:1508.04025 , 2015.
[19] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,Matthias
Minderer,GeorgHeigold,SylvainGelly,etal. Animageisworth16x16words:Transformersforimagerecognitionatscale. arXivpreprint
arXiv:2010.11929 , 2020.
[20] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision:
A survey. ACM Computing Surveys (CSUR) , 2021.
[21] KaiHan,YunheWang,HantingChen,XinghaoChen,JianyuanGuo,ZhenhuaLiu,YehuiTang,AnXiao,ChunjingXu,YixingXu,etal. A
survey on vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.
[22] NIH. National institutes of health, us. https://www.nibib.nih.gov/science-education/science-topics .
. : Page 51 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[23] Bin Guan, Guoshan Zhang, Jinkun Yao, Xinbo Wang, and Mengxuan Wang. Arm fracture detection in x-rays based on improved deep
convolutional neural network. Computers & Electrical Engineering , 81:106530, 2020.
[24] Amit Kumar Jaiswal, Prayag Tiwari, Sachin Kumar, Deepak Gupta, Ashish Khanna, and Joel J.P.C. Rodrigues. Identifying pneumonia in
chest x-rays: A deep learning approach. Measurement , 145:511‚Äì518, 2019.
[25] Naveed Chouhan, Asifullah Khan, Jehan Zeb Shah, Mazhar Hussnain, and Muhammad Waleed Khan. Deep convolutional neural network
and emotional learning based breast cancer detection using digital mammography. Computers in Biology and Medicine , 132:104318, 2021.
[26] Ademola Enitan Ilesanmi, Utairat Chaumrattanakul, and Stanislav S Makhanov. A method for segmentation of tumors in breast ultrasound
images using the variant enhanced deep learning. Biocybernetics and Biomedical Engineering , 41(2):802‚Äì818, 2021.
[27] Lifang Chen, Tengfei Mao, and Qian Zhang. Identifying cardiomegaly in chest x-rays using dual attention network. Applied Intelligence ,
pages 1‚Äì10, 2022.
[28] PearlMarySamuelandThanikaiselvanVeeramalai. Vsscnet:vesselspecificskipchainconvolutionalnetworkforbloodvesselsegmentation.
Computer methods and programs in biomedicine , 198:105769, 2021.
[29] AbdelbakiSouid,NizarSakli,andHediSakli. Classificationandpredictionsoflungdiseasesfromchestx-raysusingmobilenetv2. Applied
Sciences, 11(6):2751, 2021.
[30] Abdul Qayyum, Imran Razzak, M Tanveer, and Ajay Kumar. Depth-wise dense neural network for automatic covid19 infection detection
and diagnosis. Annals of operations research , pages 1‚Äì21, 2021.
[31] UM Prakash, Kottilingam Kottursamy, Korhan Cengiz, Utku Kose, and Bui Thanh Hung. 4x-expert systems for early prediction of
osteoporosis using multi-model algorithms. Measurement , 180:109543, 2021.
[32] NeslihanBayramoglu,MiikaTNieminen,andSimoSaarakkala. Machinelearningbasedtextureanalysisofpatellafromx-raysfordetecting
patellofemoral osteoarthritis. International journal of medical informatics , 157:104627, 2022.
[33] Atƒ±f Emre Y√ºksel, Sadullah G√ºltekin, Enis Simsar, ≈ûerife Damla √ñzdemir, Mustafa G√ºndoƒüar, Salih Barkƒ±n Tokg√∂z, and ƒ∞brahim Ethem
Hamamcƒ±. Dental enumeration and multiple treatment detection on panoramic x-rays using deep learning. Scientific reports , 11(1):1‚Äì10,
2021.
[34] Rima Arnaout, Lara Curran, Yili Zhao, Jami C Levine, Erin Chinn, and Anita J Moon-Grady. An ensemble of neural networks provides
expert-level prenatal detection of complex congenital heart disease. Nature medicine , 27(5):882‚Äì891, 2021.
[35] HanemEllethy,ShekharSChandra,andFatimaANasrallah. Thedetectionofmildtraumaticbraininjuryinpaediatricsusingartificialneural
networks. Computers in Biology and Medicine , 135:104614, 2021.
[36] Marcin Wo≈∫niak, Jakub Si≈Çka, and Micha≈Ç Wieczorek. Deep neural network correlation learning mechanism for ct brain tumor detection.
Neural Computing and Applications , pages 1‚Äì16, 2021.
[37] DhimanDas,KathyayiniSivasubramanian,PraveenbalajiRajendran,andManojitPramanik.Label-freehighframerateimagingofcirculating
blood clots using a dual modal ultrasound and photoacoustic system. Journal of Biophotonics , 14(3):e202000371, 2021.
[38] Jordan Chamberlin, Madison R Kocher, Jeffrey Waltz, Madalyn Snoddy, Natalie FC Stringer, Joseph Stephenson, Pooyan Sahbaee, Puneet
Sharma, Saikiran Rapaka, U Joseph Schoepf, et al. Automated detection of lung nodules and coronary artery calcium using artificial
intelligence on low-dose ct scans for lung cancer screening: accuracy and prognostic value. BMC medicine , 19(1):1‚Äì14, 2021.
[39] J Akilandeswari, G Jothi, A Naveenkumar, RS Sabeenian, P Iyyanar, and ME Paramasivam. Detecting pulmonary embolism using deep
neural networks. International Journal of Performability Engineering , 17(3), 2021.
[40] Adel Oulefki, Sos Agaian, Thaweesak Trongtirakul, and Azzeddine Kassah Laouar. Automatic covid-19 lung infected region segmentation
and measurement using ct-scans images. Pattern recognition , 114:107747, 2021.
[41] SumitaMondal,AnupKSadhu,andPranabKumarDutta. Adaptivelocalternarypatternonparameteroptimized-fasterregionconvolutional
neural network for pulmonary emphysema diagnosis. IEEE Access , 9:114135‚Äì114152, 2021.
[42] AAO. American academy of ophthalmology. https://www.aao.org/ .
[43] Gabriella Moraes, Dun Jack Fu, Marc Wilson, Hagar Khalid, Siegfried K Wagner, Edward Korot, Daniel Ferraz, Livia Faes, Christopher J
Kelly,TerrySpitz,etal. Quantitativeanalysisofoctforneovascularage-relatedmaculardegenerationusingdeeplearning. Ophthalmology ,
128(5):693‚Äì705, 2021.
[44] GahyungRyu,KyungminLee,DonggeunPark,SangHyunPark,andMinSagong. Adeeplearningmodelforidentifyingdiabeticretinopathy
using optical coherence tomography angiography. Scientific reports , 11(1):1‚Äì9, 2021.
[45] Shotaro Asano, Ryo Asaoka, Hiroshi Murata, Yohei Hashimoto, Atsuya Miki, Kazuhiko Mori, Yoko Ikeda, Takashi Kanamoto, Junkichi
Yamagami, and Kenji Inoue. Predicting the central 10 degrees visual field in glaucoma by applying a deep learning algorithm to optical
coherence tomography images. Scientific Reports , 11(1):1‚Äì10, 2021.
[46] Esther Parra-Mora, Alex Caza√±as-Gordon, Rui Proen√ßa, and Lu√≠s A da Silva Cruz. Epiretinal membrane detection in optical coherence
tomography retinal images using deep learning. IEEE Access , 9:99201‚Äì99219, 2021.
[47] Zhenhua Wang, Yuanfu Zhong, Mudi Yao, Yan Ma, Wenping Zhang, Chaopeng Li, Zhifu Tao, Qin Jiang, and Biao Yan. Automated
segmentation of macular edema for the diagnosis of ocular disease using deep learning method. Scientific Reports , 11(1):1‚Äì12, 2021.
[48] SyedAlEHassan,ShahzadAkbar,SaharGull,AmjadRehman,andHindAlaska. Deeplearning-basedautomaticdetectionofcentralserous
retinopathyusingopticalcoherencetomographicimages. In 20211stInternationalConferenceonArtificialIntelligenceandDataAnalytics
(CAIDA), pages 206‚Äì211. IEEE, 2021.
[49] M Kashif Yaqoob, Syed Farooq Ali, Irfan Kareem, and Muhammad Moazam Fraz. Feature-based optimized deep residual network
architecture for diabetic retinopathy detection. In 2020 IEEE 23rd International Multitopic Conference (INMIC) , pages 1‚Äì6. IEEE, 2020.
[50] Shumpei Obata, Yusuke Ichiyama, Masashi Kakinoki, Osamu Sawada, Yoshitsugu Saishin, Taku Ito, Mari Tomioka, and Masahito Ohji.
Predictionofpostoperativevisualacuityaftervitrectomyformacularholeusingdeeplearning‚Äìbasedartificialintelligence. Graefe‚ÄôsArchive
for Clinical and Experimental Ophthalmology , pages 1‚Äì11, 2021.
[51] MichaelAbr√†moffandChristineN.Kay. Chapter6-imageprocessing. InStephenJ.Ryan,SriniVasR.Sadda,DavidR.Hinton,AndrewP.
Schachat, SriniVas R. Sadda, C.P. Wilkinson, Peter Wiedemann, and Andrew P. Schachat, editors, Retina (Fifth Edition) , pages 151‚Äì176.
. : Page 52 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
W.B. Saunders, London, fifth edition edition, 2013.
[52] Quang TM Pham, Sangil Ahn, Jitae Shin, and Su Jeong Song. Generating future fundus images for early age-related macular degeneration
based on generative adversarial networks. Computer Methods and Programs in Biomedicine , page 106648, 2022.
[53] Sufian A Badawi, Muhammad Moazam Fraz, Muhammad Shehzad, Imran Mahmood, Sajid Javed, Emad Mosalam, and Ajay Kamath
Nileshwar. Detection and grading of hypertensive retinopathy using vessels tortuosity and arteriovenous ratio. Journal of Digital Imaging ,
pages 1‚Äì21, 2022.
[54] HariMohanRaiandKalyanChatterjee. 2dmriimageanalysisandbraintumordetectionusingdeeplearningcnnmodelleu-net. Multimedia
Tools and Applications , 80(28):36111‚Äì36141, 2021.
[55] ZamirMerali,JustinZWang,JetanHBadhiwala,ChristopherDWitiw,JeffersonRWilson,andMichaelGFehlings. Adeeplearningmodel
for detection of cervical spinal cord compression in mri scans. Scientific reports , 11(1):1‚Äì11, 2021.
[56] SaeedaNaz,AbidaAshraf,andAhmadZaib. Transferlearningusingfreezefeaturesforalzheimerneurologicaldisorderdetectionusingadni
dataset.Multimedia Systems , 28(1):85‚Äì94, 2022.
[57] Mei Yang, Yiming Zheng, Zhiying Xie, Zhaoxia Wang, Jiangxi Xiao, Jue Zhang, and Yun Yuan. A deep learning model for diagnosing
dystrophinopathies on thigh muscle mri images. BMC neurology , 21(1):1‚Äì9, 2021.
[58] Mazhar Javed Awan, Mohd Shafry Mohd Rahim, Naomie Salim, Mazin Abed Mohammed, Begonya Garcia-Zapirain, and Karrar Hameed
Abdulkareem. Efficient detection of knee anterior cruciate ligament from magnetic resonance imaging using deep learning approach.
Diagnostics , 11(1):105, 2021.
[59] Jiang Zhang, Chen Li, Ganwanming Liu, Min Min, Chong Wang, Jiyi Li, Yuting Wang, Hongmei Yan, Zhentao Zuo, Wei Huang, et al. A
cnn-transformerhybridapproachfordecodingvisualneuralactivityintotext. ComputerMethodsandProgramsinBiomedicine ,214:106586,
2022.
[60] Radiologyinfo.org. Radiologyinfo.org for patients. https://www.radiologyinfo.org/ .
[61] ZhenWang,GuangxuLi,JingjieZhou,andPhilipO.Ogunbona. Opticalflownetworksforheartbeatestimationin4dultrasoundimages. In
2021 7th International Conference on Computing and Artificial Intelligence , pages 127‚Äì131, 2021.
[62] Yuyu Guo, Lei Bi, Euijoon Ahn, Dagan Feng, Qian Wang, and Jinman Kim. A spatiotemporal volumetric interpolation network for 4d
dynamic medical image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4726‚Äì4735,
2020.
[63] Anil V Parwani. Whole Slide Imaging: Current Applications and Future Directions . Springer Nature, 2021.
[64] Muhammad Shaban, Syed Ali Khurram, Muhammad Moazam Fraz, Najah Alsubaie, Iqra Masood, Sajid Mushtaq, Mariam Hassan, Asif
Loya, and Nasir M Rajpoot. A novel digital score for abundance of tumour infiltrating lymphocytes predicts disease free survival in oral
squamous cell carcinoma. Scientific reports , 9(1):1‚Äì13, 2019.
[65] SajidJaved,ArifMahmood,MuhammadMoazamFraz,NavidAlemiKoohbanani,KsenijaBenes,Yee-WahTsang,KatherineHewitt,David
Epstein,DavidSnead,andNasirRajpoot.Cellularcommunitydetectionfortissuephenotypingincolorectalcancerhistologyimages. Medical
image analysis , 63:101696, 2020.
[66] RM Saad Bashir, Hanya Mahmood, Muhammad Shaban, Shan E Ahmed Raza, M Moazam Fraz, Syed Ali Khurram, and Nasir M Rajpoot.
Automated grade classification of oral epithelial dysplasia using morphometric analysis of histology images. In Medical Imaging 2020:
Digital Pathology , volume 11320, page 1132011. International Society for Optics and Photonics, 2020.
[67] Yanning Zhou, Simon Graham, Navid Alemi Koohbanani, Muhammad Shaban, Pheng-Ann Heng, and Nasir Rajpoot. Cgc-net: Cell graph
convolutional network for grading of colorectal cancer histology images. In Proceedings of the IEEE/CVF International Conference on
Computer Vision Workshops , pages 0‚Äì0, 2019.
[68] MuhammadShaban,RuqayyaAwan,MuhammadMoazamFraz,AyeshaAzam,Yee-WahTsang,DavidSnead,andNasirMRajpoot.Context-
aware convolutional neural network for grading of colorectal cancer histology images. IEEE Transactions on Medical Imaging , 2020.
[69] MMFraz,SAKhurram,SGraham,MShaban,MHassan,ALoya,andNMRajpoot.Fabnet:featureattention-basednetworkforsimultaneous
segmentationofmicrovesselsandnervesinroutinehistologyimagesoforalcancer. NeuralComputingandApplications ,pages1‚Äì14,2019.
[70] MMFraz,MuhammadShaban,SimonGraham,SyedAliKhurram,andNasirMRajpoot.Uncertaintydrivenpoolingnetworkformicrovessel
segmentation in routine histology images. In Computational pathology and ophthalmic medical image analysis , pages 156‚Äì164. Springer,
2018.
[71] SNRashid,MMFraz,andSJaved. Multiscaledilatedunetforsegmentationofmulti-organnucleiindigitalhistologyimages. In 2020IEEE
17thInternationalConferenceonSmartCommunities:ImprovingQualityofLifeUsingICT,IoTandAI(HONET) ,pages68‚Äì72.IEEE,2020.
[72] Moritz Schwyzer, Daniela A Ferraro, Urs J Muehlematter, Alessandra Curioni-Fontecedro, Martin W Huellner, Gustav K Von Schulthess,
Philipp A Kaufmann, Irene A Burger, and Michael Messerli. Automated detection of lung cancer at ultralow dose pet/ct by deep neural
networks‚Äìinitial results. Lung Cancer , 126:170‚Äì173, 2018.
[73] Benjamin H Kann, Sanjay Aneja, Gokoulakrichenane V Loganadane, Jacqueline R Kelly, Stephen M Smith, Roy H Decker, James B Yu,
HenrySPark,WendellGYarbrough,AjayMalhotra,etal.Pretreatmentidentificationofheadandneckcancernodalmetastasisandextranodal
extension using deep learning neural networks. Scientific reports , 8(1):1‚Äì11, 2018.
[74] Kobra Etminani, Amira Soliman, Anette Davidsson, Jose R Chang, Bego√±a Mart√≠nez-Sanchis, Stefan Byttner, Valle Camacho, Matteo
Bauckneht, Roxana Stegeran, Marcus Ressner, et al. A 3d deep learning model to predict the diagnosis of dementia with lewy bodies,
alzheimer‚Äôsdisease,andmildcognitiveimpairmentusingbrain18f-fdgpet. Europeanjournalofnuclearmedicineandmolecularimaging ,
49(2):563‚Äì584, 2022.
[75] RikiyaYamashita,MizuhoNishio,RichardKinhGianDo,andKaoriTogashi. Convolutionalneuralnetworks:anoverviewandapplication
in radiology. Insights into imaging , 9(4):611‚Äì629, 2018.
[76] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in
neural information processing systems , 25, 2012.
. : Page 53 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[77] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556 , 2014.
[78] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and
Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 1‚Äì9, 2015.
[79] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages 770‚Äì778, 2016.
[80] Saining Xie, Ross Girshick, Piotr Doll√°r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1492‚Äì1500, 2017.
[81] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 7132‚Äì7141, 2018.
[82] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition , pages 4700‚Äì4708, 2017.
[83] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on
machine learning , pages 6105‚Äì6114. PMLR, 2019.
[84] Hafiz Syed Ahmed Qasim, Muhammad Shahzad, and Muhammad Moazam Fraz. Deep learning for face detection: Recent advancements.
In2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2) , pages 1‚Äì6. IEEE, 2021.
[85] Muhammad Arslan Hashmi, Qaiser Riaz, Muhammad Zeeshan, Muhammad Shahzad, and Muhammad Moazam Fraz. Motion reveal
emotions: identifying emotions from human walk using chest mounted smartphone. IEEE Sensors Journal , 20(22):13511‚Äì13522, 2020.
[86] WeiWangandJianxunGang. Applicationofconvolutionalneuralnetworkinnaturallanguageprocessing. In 2018InternationalConference
on Information Systems and Computer Aided Education (ICISCAE) , pages 64‚Äì70. IEEE, 2018.
[87] Qi Zhan, Wenjin Wang, and Gerard de Haan. Analysis of cnn-based remote-ppg to understand limitations and sensitivities. Biomedical
optics express , 11(3):1268‚Äì1283, 2020.
[88] ArnaudArindraAdiyosoSetio,AlbertoTraverso,ThomasDeBel,MoiraSNBerens,CasVanDenBogaard,PiergiorgioCerello,HaoChen,
Qi Dou, Maria Evelina Fantacci, Bram Geurts, et al. Validation, comparison, and combination of algorithms for automatic detection of
pulmonary nodules in computed tomography images: the luna16 challenge. Medical image analysis , 42:1‚Äì13, 2017.
[89] Samuel G Armato III, Geoffrey McLennan, Luc Bidaut, Michael F McNitt-Gray, Charles R Meyer, Anthony P Reeves, Binsheng Zhao,
Denise R Aberle, Claudia I Henschke, Eric A Hoffman, et al. The lung image database consortium (lidc) and image database resource
initiative (idri): a completed reference database of lung nodules on ct scans. Medical physics , 38(2):915‚Äì931, 2011.
[90] LaugeSorensen,SaherBShaker,andMarleenDeBruijne. Quantitativeanalysisofpulmonaryemphysemausinglocalbinarypatterns. IEEE
transactions on medical imaging , 29(2):559‚Äì569, 2010.
[91] XingyiYang,XuehaiHe,JinyuZhao,YichenZhang,ShanghangZhang,andPengtaoXie. Covid-ct-dataset:actscandatasetaboutcovid-19.
arXiv preprint arXiv:2003.13865 , 2020.
[92] PlamenAngelovandEduardoAlmeidaSoares.Sars-cov-2ct-scandataset:Alargedatasetofrealpatientsctscansforsars-cov-2identification.
MedRxiv, 2020.
[93] DimitriosKollias,AnastasiosArsenos,LevonSoukissian,andStefanosKollias. Mia-cov19d:Covid-19detectionthrough3-dchestctimage
analysis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 537‚Äì544, 2021.
[94] Mohammad Rahimzadeh, Abolfazl Attar, and Seyed Mohammad Sakhaei. A fully automated deep learning-based network for detecting
covid-19 from a new and large lung ct scan dataset. Biomedical Signal Processing and Control , 68:102588, 2021.
[95] DanielKermany,KangZhang,andMichaelGoldbaum. Labeledopticalcoherencetomography(oct)andchestx-rayimagesforclassification.
Mendeley Data , 2, 2018.
[96] Maria de la Iglesia Vay√°, Jose Manuel Saborit, Joaquim Angel Montell, Antonio Pertusa, Aurelia Bustos, Miguel Cazorla, Joaquin Galant,
Xavier Barber, Domingo Orozco-Beltr√°n, Francisco Garc√≠a-Garc√≠a, et al. Bimcv covid-19+: a large annotated dataset of rx and ct images
from covid-19 patients. arXiv preprint arXiv:2006.01174 , 2020.
[97] Unais Sait, Gokul Lal KV, Sunny Prakash Prajapati, Rahul Bhaumik, Tarun Kumar, Sanjana Shivakumar, and Kriti Bhalla. Curated dataset
for covid-19 posterior-anterior chest radiography images (x-rays). Mendeley Data , 3, 2021.
[98] Fathi El-Shafai, Walid; Abd El-Samie. Extensive covid-19 x-ray and ct chest images dataset. Mendeley Data , 3, 2020.
[99] LindaWang,ZhongQiuLin,andAlexanderWong. Covid-net:Atailoreddeepconvolutionalneuralnetworkdesignfordetectionofcovid-19
cases from chest x-ray images. Scientific Reports , 10(1):1‚Äì12, 2020.
[100] Shirin Hajeb Mohammad Alipour, Hossein Rabbani, and Mohammad Reza Akhlaghi. Diabetic retinopathy grading by digital curvelet
transform. Computational and mathematical methods in medicine , 2012, 2012.
[101] JakobNikolasKather,FGZ√∂llner,FBianconi,SMMelchers,LRSchad,TGaiser,AMarx,andCAWeis. Collectionoftexturesincolorectal
cancer histology. Zenodo https://doi. org/10 , 5281, 2016.
[102] Siham Tabik, Anabel G√≥mez-R√≠os, Jos√© Luis Mart√≠n-Rodr√≠guez, Iv√°n Sevillano-Garc√≠a, Manuel Rey-Area, David Charte, Emilio Guirado,
Juan-Luis Su√°rez, Juli√°n Luengo, MA Valero-Gonz√°lez, et al. Covidgr dataset and covid-sdnet methodology for predicting covid-19 based
on chest x-ray images. IEEE journal of biomedical and health informatics , 24(12):3595‚Äì3605, 2020.
[103] Prasanna Porwal, Samiksha Pachade, Ravi Kamble, Manesh Kokare, Girish Deshmukh, Vivek Sahasrabuddhe, and Fabrice Meriaudeau.
Indian diabetic retinopathy image dataset (idrid): a database for diabetic retinopathy screening research. Data, 3(3):25, 2018.
[104] HaydenGunraj,AliSabri,DavidKoff,andAlexanderWong. Covid-netct-2:Enhanceddeepneuralnetworksfordetectionofcovid-19from
chest ct images through bigger, more diverse learning. arXiv preprint arXiv:2101.07433 , 2021.
[105] John N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Shaw, Brad A Ozenberger, Kyle Ellrott, Ilya Shmulevich, Chris Sander, and
Joshua M Stuart. The cancer genome atlas pan-cancer analysis project. Nature genetics , 45(10):1113‚Äì1120, 2013.
. : Page 54 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[106] Li Wang, Dong Nie, Guannan Li, √âlodie Puybareau, Jose Dolz, Qian Zhang, Fan Wang, Jing Xia, Zhengwang Wu, Jia-Wei Chen, et al.
Benchmark on automatic six-month-old infant brain segmentation algorithms: the iseg-2017 challenge. IEEE transactions on medical
imaging, 38(9):2219‚Äì2230, 2019.
[107] Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S Kirby, John B Freymann, Keyvan Farahani,
andChristosDavatzikos. Advancingthecancergenomeatlasgliomamricollectionswithexpertsegmentationlabelsandradiomicfeatures.
Scientific data , 4(1):1‚Äì13, 2017.
[108] Adri√´nne M Mendrik, Koen L Vincken, Hugo J Kuijf, Marcel Breeuwer, Willem H Bouvy, Jeroen De Bresser, Amir Alansary, Marleen
De Bruijne, Aaron Carass, Ayman El-Baz, et al. Mrbrains challenge: online evaluation framework for brain image segmentation in 3t mri
scans.Computational intelligence and neuroscience , 2015, 2015.
[109] Cathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul Downey, Paul Elliott, Jane Green, Martin
Landray, et al. Uk biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age.
PLoS medicine , 12(3):e1001779, 2015.
[110] Colin G Stirrat, Shirjel R Alam, Thomas J MacGillivray, Calum D Gray, Marc R Dweck, Jennifer Raftis, William SA Jenkins, William A
Wallace,RenzoPessotto,KelvinHHLim,etal.Ferumoxytol-enhancedmagneticresonanceimagingassessinginflammationaftermyocardial
infarction. Heart, 103(19):1528‚Äì1535, 2017.
[111] A Emre Kavur, N Sinem Gezer, Mustafa Barƒ±≈ü, Sinem Aslan, Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee,
Philipp Ernst, Sava≈ü √ñzkan, et al. Chaos challenge-combined (ct-mr) healthy abdominal organ segmentation. Medical Image Analysis ,
69:101950, 2021.
[112] JoesStaal,MichaelDAbr√†moff,MeindertNiemeijer,MaxAViergever,andBramVanGinneken. Ridge-basedvesselsegmentationincolor
images of the retina. IEEE transactions on medical imaging , 23(4):501‚Äì509, 2004.
[113] Junji Shiraishi, Shigehiko Katsuragawa, Junpei Ikezoe, Tsuneo Matsumoto, Takeshi Kobayashi, Ken-ichi Komatsu, Mitate Matsui, Hiroshi
Fujita,YoshieKodera,andKunioDoi.Developmentofadigitalimagedatabaseforchestradiographswithandwithoutalungnodule:receiver
operatingcharacteristicanalysisofradiologists‚Äôdetectionofpulmonarynodules. AmericanJournalofRoentgenology ,174(1):71‚Äì74,2000.
[114] Nicholas Heller, Fabian Isensee, Klaus H Maier-Hein, Xiaoshuai Hou, Chunmei Xie, Fengyi Li, Yang Nan, Guangrui Mu, Zhiyong Lin,
Miofei Han, et al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct imaging: Results of the kits19
challenge. Medical image analysis , 67:101821, 2021.
[115] Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang,
XiaokangWu,FangbingYan,etal. Identifyingmedicaldiagnosesandtreatablediseasesbyimage-baseddeeplearning. Cell,172(5):1122‚Äì
1131, 2018.
[116] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and
Clement J McDonald. Preparing a collection of radiology examinations for distribution and retrieval. Journal of the American Medical
Informatics Association , 23(2):304‚Äì310, 2016.
[117] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and
Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. Scientific data , 6(1):1‚Äì8,
2019.
[118] Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. arXiv preprint arXiv:1711.08195 , 2017.
[119] Baiyu Chen, Xinhui Duan, Zhicong Yu, Shuai Leng, Lifeng Yu, and Cynthia McCollough. Development and validation of an open data
format for ct projection data. Medical physics , 42(12):6964‚Äì6972, 2015.
[120] BennettALandman,AlanJHuang,AliyaGifford,DeeptiSVikram,IsselAnneLLim,JonathanADFarrell,JohnABogovic,JunHua,Min
Chen, Samson Jarso, et al. Multi-parametric neuroimaging reproducibility: a 3-t resource study. Neuroimage , 54(4):2854‚Äì2866, 2011.
[121] EirikurAgustssonandRaduTimofte. Ntire2017challengeonsingleimagesuper-resolution:Datasetandstudy. In ProceedingsoftheIEEE
conference on computer vision and pattern recognition workshops , pages 126‚Äì135, 2017.
[122] Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J Muckley, Aaron Defazio, Ruben Stern, Patricia
Johnson, Mary Bruno, et al. fastmri: An open dataset and benchmarks for accelerated mri. arXiv preprint arXiv:1811.08839 , 2018.
[123] Martin Weigert, Uwe Schmidt, Tobias Boothe, Andreas M√ºller, Alexandr Dibrov, Akanksha Jain, Benjamin Wilhelm, Deborah Schmidt,
Coleman Broaddus, Si√¢n Culley, et al. Content-aware image restoration: pushing the limits of fluorescence microscopy. Nature methods ,
15(12):1090‚Äì1097, 2018.
[124] Mingyu Kim, Jihye Yun, Yongwon Cho, Keewon Shin, Ryoungwoo Jang, Hyun-jin Bae, and Namkug Kim. Deep learning in medical
imaging. Neurospine , 16(4):657, 2019.
[125] Md Shahin Ali, Md Sipon Miah, Jahurul Haque, Md Mahbubur Rahman, and Md Khairul Islam. An enhanced technique of skin cancer
classificationusingdeepconvolutionalneuralnetworkwithtransferlearningmodels. MachineLearningwithApplications ,5:100036,2021.
[126] Jyostna Devi Bodapati, Nagur Shareef Shaik, and Veeranjaneyulu Naralasetti. Composite deep neural network with gated-attention
mechanism for diabetic retinopathy severity classification. Journal of Ambient Intelligence and Humanized Computing , 12(10):9825‚Äì9839,
2021.
[127] Vinayakumar Ravi, Harini Narasimhan, and Tuan D Pham. Efficientnet-based convolutional neural networks for tuberculosis classification.
InAdvances in Artificial Intelligence, Computation, and Data Science , pages 227‚Äì244. Springer, 2021.
[128] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.
Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 , 2017.
[129] Muhammad Sufyan Arshad, Usman Abdur Rehman, and Muhammad Moazam Fraz. Plant disease identification using transfer learning. In
2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2) , pages 1‚Äì5. IEEE, 2021.
[130] TufailSajjadShahHashmi,NazeefUlHaq,MuhammadMoazamFraz,andMuhammadShahzad. Applicationofdeeplearningforweapons
detection in surveillance videos. In 2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2) , pages
1‚Äì6. IEEE, 2021.
. : Page 55 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[131] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 580‚Äì587, 2014.
[132] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision , pages 1440‚Äì1448, 2015.
[133] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks.
Advances in neural information processing systems , 28, 2015.
[134] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings
of the IEEE conference on computer vision and pattern recognition , pages 779‚Äì788, 2016.
[135] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot
multibox detector. In European conference on computer vision , pages 21‚Äì37. Springer, 2016.
[136] Lohendran Baskaran, Subhi J Al‚ÄôAref, Gabriel Maliakal, Benjamin C Lee, Zhuoran Xu, Jeong W Choi, Sang-Eun Lee, Ji Min Sung, Fay Y
Lin, Simon Dunham, et al. Automatic segmentation of multiple cardiovascular structures from cardiac computed tomography angiography
images using deep learning. PloS one, 15(5):e0232573, 2020.
[137] Kunal Nagpal, Davis Foote, Yun Liu, Po-Hsuan Cameron Chen, Ellery Wulczyn, Fraser Tan, Niels Olson, Jenny L Smith, Arash
Mohtashamian, James H Wren, et al. Development and validation of a deep learning algorithm for improving gleason scoring of prostate
cancer.NPJ digital medicine , 2(1):1‚Äì10, 2019.
[138] Sufian A Badawi and Muhammad Moazam Fraz. Optimizing the trainable b-cosfire filter for retinal blood vessel segmentation. PeerJ,
6:e5855, 2018.
[139] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International
Conference on Medical image computing and computer-assisted intervention , pages 234‚Äì241. Springer, 2015.
[140] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation
withdeepconvolutionalnets,atrousconvolution,andfullyconnectedcrfs. IEEEtransactionsonpatternanalysisandmachineintelligence ,
40(4):834‚Äì848, 2017.
[141] Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on
computer vision , pages 2961‚Äì2969, 2017.
[142] Ruixin Yang and Yingyan Yu. Artificial convolutional neural network in object detection and semantic segmentation for medical imaging
analysis. Frontiers in Oncology , 11:573, 2021.
[143] EshaPahwa,DwijMehta,SanjeetKapadia,DevanshJain,andAchleshwarLuthra.Medskip:Medicalreportgenerationusingskipconnections
and integrated attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 3409‚Äì3415, 2021.
[144] Di You, Fenglin Liu, Shen Ge, Xiaoxia Xie, Jing Zhang, and Xian Wu. Aligntransformer: Hierarchical alignment of visual regions and
disease tags for medical report generation. In International Conference on Medical Image Computing and Computer-Assisted Intervention ,
pages 72‚Äì82. Springer, 2021.
[145] RunyiLi,ZizhouWang,andLeiZhang.Imagecaptionandmedicalreportgenerationbasedondeeplearning:areviewandalgorithmanalysis.
In2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI) , pages 373‚Äì379. IEEE, 2021.
[146] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of
the IEEE conference on computer vision and pattern recognition , pages 3156‚Äì3164, 2015.
[147] KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,AaronCourville,RuslanSalakhudinov,RichZemel,andYoshuaBengio. Show,attend
andtell:Neuralimagecaptiongenerationwithvisualattention. In Internationalconferenceonmachinelearning ,pages2048‚Äì2057.PMLR,
2015.
[148] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 7219‚Äì7228, 2018.
[149] JustinJohnson,AndrejKarpathy,andLiFei-Fei. Densecap:Fullyconvolutionallocalizationnetworksfordensecaptioning. In Proceedings
of the IEEE conference on computer vision and pattern recognition , pages 4565‚Äì4574, 2016.
[150] Alexander Mathews, Lexing Xie, and Xuming He. Semstyle: Learning to generate stylised image captions using unaligned text. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8591‚Äì8600, 2018.
[151] ImranKhurram,MMFraz,MuhammadShahzad,andNasirMRajpoot. Dense-captionnet:asentencegenerationarchitectureforfine-grained
description of image semantics. Cognitive Computation , 13(3):595‚Äì611, 2021.
[152] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. A comprehensive survey of deep learning for image
captioning. ACM Computing Surveys (CsUR) , 51(6):1‚Äì36, 2019.
[153] TomotakaSobue,NoriyukiMoriyama,MasahiroKaneko,MasahikoKusumoto,ToshiakiKobayashi,RyosukeTsuchiya,RyutaroKakinuma,
HironobuOhmatsu,KanjiNagai,HiroyukiNishiyama,etal.Screeningforlungcancerwithlow-dosehelicalcomputedtomography:anti-lung
cancer association project. Journal of clinical oncology , 20(4):911‚Äì920, 2002.
[154] WeiZhao,JianchengYang,YingliSun,ChengLi,WeilanWu,LiangJin,ZhimingYang,BingbingNi,PanGao,PeijunWang,etal. 3ddeep
learning from ct scans predicts tumor invasiveness of subcentimeter pulmonary adenocarcinomas. Cancer research , 78(24):6881‚Äì6889,
2018.
[155] Wei Zhao, Jiancheng Yang, Bingbing Ni, Dexi Bi, Yingli Sun, Mengdi Xu, Xiaoxia Zhu, Cheng Li, Liang Jin, Pan Gao, et al. Toward
automaticpredictionofegfrmutationstatusinpulmonaryadenocarcinomawith3ddeeplearning. Cancermedicine ,8(7):3532‚Äì3543,2019.
[156] Fangzhou Liao, Ming Liang, Zhe Li, Xiaolin Hu, and Sen Song. Evaluate the malignancy of pulmonary nodules using the 3-d deep leaky
noisy-or network. IEEE transactions on neural networks and learning systems , 30(11):3484‚Äì3495, 2019.
[157] JianchengYang,HaoranDeng,XiaoyangHuang,BingbingNi,andYiXu. Relationallearningbetweenmultiplepulmonarynodulesviadeep
set attention transformers. In 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI) , pages 1875‚Äì1878. IEEE, 2020.
[158] Yanan Wu, Shouliang Qi, Yu Sun, Shuyue Xia, Yudong Yao, and Wei Qian. A vision transformer for emphysema classification using ct
images.Physics in Medicine & Biology , 66(24):245016, 2021.
. : Page 56 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[159] Cuong Do and Lan Vu. An approach for recognizing COVID-19 cases using convolutional neural networks applied to CT scan images.
In Andrew G. Tescher and Touradj Ebrahimi, editors, Applications of Digital Image Processing XLIII , volume 11510, pages 719 ‚Äì 727.
International Society for Optics and Photonics, SPIE, 2020.
[160] HalgurdSMaghdid,ArasTAsaad,KayhanZrarGhafoor,AliSafaaSadiq,SeyedaliMirjalili,andMuhammadKhurramKhan. Diagnosing
covid-19 pneumonia from x-ray and ct images using deep learning and transfer learning algorithms. In Multimodal image exploitation and
learning 2021 , volume 11734, page 117340E. International Society for Optics and Photonics, 2021.
[161] Matteo Polsinelli, Luigi Cinque, and Giuseppe Placidi. A light cnn for detecting covid-19 from ct scans of the chest. Pattern recognition
letters, 140:95‚Äì100, 2020.
[162] LeiwenFu,BingyiWang,TanweiYuan,XiaotingChen,YunlongAo,ThomasFitzpatrick,PeiyangLi,YiguoZhou,Yi-fanLin,QibinDuan,
et al. Clinical characteristics of coronavirus disease 2019 (covid-19) in china: a systematic review and meta-analysis. Journal of Infection ,
80(6):656‚Äì665, 2020.
[163] FengPan,TianheYe,PengSun,ShanGui,BoLiang,LingliLi,DandanZheng,JiazhengWang,RichardLHesketh,LianYang,etal. Time
course of lung changes on chest ct during recovery from 2019 novel coronavirus (covid-19) pneumonia. Radiology , 2020.
[164] Ara Abigail E. Ambita, Eujene Nikka V. Boquio, and Prospero C. Naval. Covit-gan: Vision transformer forcovid-19 detection in ct scan
imageswith self-attention gan fordataaugmentation. In Igor Farka≈°, Paolo Masulli, Sebastian Otte, and Stefan Wermter, editors, Artificial
Neural Networks and Machine Learning ‚Äì ICANN 2021 , pages 587‚Äì598, Cham, 2021. Springer International Publishing.
[165] Lei Zhang and Yan Wen. A transformer-based framework for automatic covid19 diagnosis in chest cts. In 2021 IEEE/CVF International
Conference on Computer Vision Workshops (ICCVW) , pages 513‚Äì518, 2021.
[166] Xinggang Wang, Xianbo Deng, Qing Fu, Qiang Zhou, Jiapei Feng, Hui Ma, Wenyu Liu, and Chuansheng Zheng. A weakly-supervised
frameworkforcovid-19classificationandlesionlocalizationfromchestct. IEEEtransactionsonmedicalimaging ,39(8):2615‚Äì2625,2020.
[167] Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Qi Song, et al. Using artificial
intelligencetodetectcovid-19andcommunity-acquiredpneumoniabasedonpulmonaryct:evaluationofthediagnosticaccuracy. Radiology ,
296(2):E65‚ÄìE71, 2020.
[168] JoelCMThan,PunLiangThon,OmarMohdRijal,RosminahMKassim,AshariYunus,NorlizaMNoor,andPatrickThen.Preliminarystudy
on patch sizes in vision transformers (vit) for covid-19 and diseased lungs classification. In 2021 IEEE National Biomedical Engineering
Conference (NBEC) , pages 146‚Äì150. IEEE, 2021.
[169] Xiaole Fan, Xiufang Feng, Yunyun Dong, and Huichao Hou. Covid-19 ct image recognition algorithm based on transformer and cnn.
Displays, page 102150, 2022.
[170] Khushal Tyagi, Gaurav Pathak, Rahul Nijhawan, and Ankush Mittal. Detecting pneumonia using vision transformer and comparing with
other techniques. In 2021 5th International Conference on Electronics, Communication and Aerospace Technology (ICECA) , pages 12‚Äì16.
IEEE, 2021.
[171] LinhTDuong,NhiHLe,ToanBTran,VuongMNgo,andPhuongTNguyen. Detectionoftuberculosisfromchestx-rayimages:boosting
the performance with vision transformer and transfer learning. Expert Systems with Applications , 184:115519, 2021.
[172] AdamBernheim,XueyanMei,MingqianHuang,YangYang,ZahiAFayad,NingZhang,KaiyueDiao,BinLin,XiqiZhu,KunweiLi,etal.
Chest ct findings in coronavirus disease-19 (covid-19): relationship to duration of infection. Radiology , page 200463, 2020.
[173] Soumya Ranjan Nayak, Deepak Ranjan Nayak, Utkarsh Sinha, Vaibhav Arora, and Ram Bilas Pachori. Application of deep learning
techniques for detection of covid-19 cases using chest x-ray images: A comprehensive study. Biomedical Signal Processing and Control ,
64:102365, 2021.
[174] SangjoonPark,GwanghyunKim,YujinOh,JoonBeomSeo,SangMinLee,JinHwanKim,SungjunMoon,Jae-KwangLim,andJongChul
Ye. Multi-task vision transformer using low-level chest x-ray feature corpus for covid-19 diagnosis and severity quantification. Medical
Image Analysis , 75:102299, 2022.
[175] Debaditya Shome, T Kar, Sachi Nandan Mohanty, Prayag Tiwari, Khan Muhammad, Abdullah AlTameem, Yazhou Zhang, and Abdul
Khader Jilani Saudagar. Covid-transformer: Interpretable covid-19 detection using vision transformer for healthcare. International Journal
of Environmental Research and Public Health , 18(21):11086, 2021.
[176] Mohamad Mahmoud Al Rahhal, Yakoub Bazi, Rami M Jomaa, Ahmad AlShibli, Naif Alajlan, Mohamed Lamine Mekhalfi, and Farid
Melgani. Covid-19 detection in ct/x-ray imagery using vision transformers. Journal of Personalized Medicine , 12(2):310, 2022.
[177] Muhammad Aasharib Nawshad, Usama Aleem Shami, Sana Sajid, and Muhammad Moazam Fraz. Attention based residual network for
effectivedetectionofcovid-19andviralpneumonia. In 2021InternationalConferenceonDigitalFuturesandTransformativeTechnologies
(ICoDT2) , pages 1‚Äì7. IEEE, 2021.
[178] Yin Dai, Yifan Gao, and Fayu Liu. Transmed: Transformers advance multi-modal medical image classification. Diagnostics , 11(8):1384,
2021.
[179] Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli, Stewart Lee Zuckerbrod, and Salah A Baker. Vtgan: Semi-supervised
retinal image synthesis and disease prediction using vision transformers. In Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3235‚Äì3245, 2021.
[180] Magdy Abd-Elghany Zeid, Khaled El-Bahnasy, and SE Abo-Youssef. Multiclass colorectal cancer histology images classification using
vision transformers. In 2021 Tenth International Conference on Intelligent Computing and Information Systems (ICICIS) , pages 224‚Äì230.
IEEE, 2021.
[181] Haoyuan Chen, Chen Li, Xiaoyan Li, Md Mamunur Rahaman, Weiming Hu, Yixin Li, Wanli Liu, Changhao Sun, Hongzan Sun, Xinyu
Huang, et al. Il-mcam: An interactive learning and multi-channel attention mechanism-based weakly supervised colorectal histopathology
image classification approach. Computers in Biology and Medicine , page 105265, 2022.
[182] Jingxing Li, Zhanglei Yang, and Yifan Yu. A medical ai diagnosis platform based on vision transformer for coronavirus. In 2021 IEEE
International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI) , pages 246‚Äì
252. IEEE, 2021.
. : Page 57 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[183] Stefan Jaeger, Sema Candemir, Sameer Antani, Y√¨-Xi√°ng J W√°ng, Pu-Xuan Lu, and George Thoma. Two public chest x-ray datasets for
computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery , 4(6):475, 2014.
[184] Joseph Paul Cohen, Paul Morrison, Lan Dao, Karsten Roth, Tim Q Duong, and Marzyeh Ghassemi. Covid-19 image data collection:
Prospective predictions are the future. arXiv preprint arXiv:2006.11988 , 2020.
[185] Shuang Liang, Weicun Zhang, and Yu Gu. A hybrid and fast deep learning framework for covid-19 detection via 3d chest ct images. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 508‚Äì512, 2021.
[186] Liyang Chen, Zhiyuan You, Nian Zhang, Juntong Xi, and Xinyi Le. Utrad: Anomaly detection and localization with u-transformer. Neural
Networks, 147:53‚Äì62, 2022.
[187] Zhijie Lin, Zhaoshui He, Shengli Xie, Xu Wang, Ji Tan, Jun Lu, and Beihai Tan. Aanet: Adaptive attention network for covid-19 detection
from chest x-ray images. IEEE Transactions on Neural Networks and Learning Systems , 32(11):4781‚Äì4792, 2021.
[188] Arnab Kumar Mondal, Arnab Bhattacharjee, Parag Singla, and AP Prathosh. xvitcos: Explainable vision transformer based covid-19
screening using radiography. IEEE Journal of Translational Engineering in Health and Medicine , 10:1‚Äì10, 2021.
[189] EmanuelePesce,SamuelJosephWithey,Petros-PavlosYpsilantis,RobertBakewell,VickyGoh,andGiovanniMontana. Learningtodetect
chest radiographs containing pulmonary lesions using visual attention networks. Medical image analysis , 53:26‚Äì38, 2019.
[190] LizongZhang,ShuxinFeng,GuiduoDuan,YingLi,andGuisongLiu. Detectionofmicroaneurysmsinfundusimagesbasedonanattention
mechanism. Genes, 10(10):817, 2019.
[191] LiuLi,MaiXu,HanruoLiu,YangLi,XiaofeiWang,LaiJiang,ZulinWang,XiangFan,andNingliWang. Alarge-scaledatabaseandacnn
model for attention-based glaucoma detection. IEEE transactions on medical imaging , 39(2):413‚Äì424, 2019.
[192] Xi Xu, Yu Guan, Jianqiang Li, Zerui Ma, Li Zhang, and Li Li. Automatic glaucoma detection based on transfer induced attention network.
BioMedical Engineering OnLine , 20(1):1‚Äì19, 2021.
[193] Rodger C Haggitt. Barrett‚Äôs esophagus, dysplasia, and adenocarcinoma. Human pathology , 25(10):982‚Äì993, 1994.
[194] Christopher P Wild and Laura J Hardie. Reflux, barrett‚Äôs oesophagus and adenocarcinoma: burning questions. Nature Reviews Cancer ,
3(9):676‚Äì684, 2003.
[195] NaofumiTomita,BehnazAbdollahi,JasonWei,BingRen,AriefSuriawinata,andSaeedHassanpour. Attention-baseddeepneuralnetworks
for detection of cancerous and precancerous esophagus tissue on histopathological slides. JAMA network open , 2(11):e1914645‚Äìe1914645,
2019.
[196] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and Faisal Mahmood. Data-efficient and weakly
supervised computational pathology on whole-slide images. Nature biomedical engineering , 5(6):555‚Äì570, 2021.
[197] Richard J Chen, Ming Y Lu, Wei-Hung Weng, Tiffany Y Chen, Drew FK Williamson, Trevor Manz, Maha Shady, and Faisal Mahmood.
Multimodalco-attentiontransformerforsurvivalpredictioningigapixelwholeslideimages. In ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision , pages 4015‚Äì4025, 2021.
[198] YangNing,ShouyiZhang,XiaomingXi,JieGuo,PeideLiu,andCaimingZhang. Cac-emvt:Efficientcoronaryarterycalciumsegmentation
withmulti-scalevisiontransformers. In 2021IEEEInternationalConferenceonBioinformaticsandBiomedicine(BIBM) ,pages1462‚Äì1467.
IEEE, 2021.
[199] Matthew Chung Hai Lee, Kersten Petersen, Nick Pawlowski, Ben Glocker, and Michiel Schaap. Tetris: Template transformer networks for
image segmentation with shape priors. IEEE transactions on medical imaging , 38(11):2596‚Äì2606, 2019.
[200] MaxJaderberg,KarenSimonyan,AndrewZisserman,etal.Spatialtransformernetworks. Advancesinneuralinformationprocessingsystems ,
28, 2015.
[201] XiaomengLi,QiDou,HaoChen,Chi-WingFu,XiaojuanQi,DanielLBelav `y,GabrieleArmbrecht,DieterFelsenberg,GuoyanZheng,and
Pheng-AnnHeng. 3dmulti-scalefcnwithrandommodalityvoxeldropoutlearningforintervertebraldisclocalizationandsegmentationfrom
multi-modality mr images. Medical image analysis , 45:41‚Äì54, 2018.
[202] XiaohangFu,LeiBi,AshnilKumar,MichaelFulham,andJinmanKim. Multimodalspatialattentionmodulefortargetingmultimodalpet-ct
lung tumor segmentation. IEEE Journal of Biomedical and Health Informatics , 25(9):3507‚Äì3516, 2021.
[203] Giammarco La Barbera, Pietro Gori, Haithem Boussaid, Bruno Belucci, Alessandro Delmonte, Jeanne Goulin, Sabine Sarnacki, Laurence
Rouet, and Isabelle Bloch. Automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric
segmentation. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI) , pages 1773‚Äì1776. IEEE, 2021.
[204] Daniel H Pak, Andr√©s Caballero, Wei Sun, and James S Duncan. Efficient aortic valve multilabel segmentation using a spatial transformer
network. In 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI) , pages 1738‚Äì1742. IEEE, 2020.
[205] Chunfeng Lian, Fan Wang, Hannah H Deng, Li Wang, Deqiang Xiao, Tianshu Kuang, Hung-Ying Lin, Jaime Gateno, Steve GF Shen,
Pew-ThianYap,etal. Multi-taskdynamictransformernetworkforconcurrentbonesegmentationandlarge-scalelandmarklocalizationwith
dentalcbct. In InternationalConferenceonMedicalImageComputingandComputer-AssistedIntervention ,pages807‚Äì816.Springer,2020.
[206] Chun Luo, Jing Zhang, Xinglin Chen, Yinhao Tang, Xiechuan Weng, and Fan Xu. Ucatr: Based on cnn and transformer encoding and
cross-attention decoding for lesion segmentation of acute ischemic stroke in non-contrast computed tomography images. In 2021 43rd
Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) , pages 3565‚Äì3568. IEEE, 2021.
[207] OlivierPetit,NicolasThome,ClementRambour,LoicThemyr,TobyCollins,andLucSoler. U-nettransformer:Selfandcrossattentionfor
medical image segmentation. In International Workshop on Machine Learning in Medical Imaging , pages 267‚Äì276. Springer, 2021.
[208] JienengChen,YongyiLu,QihangYu,XiangdeLuo,EhsanAdeli,YanWang,LeLu,AlanLYuille,andYuyinZhou.Transunet:Transformers
make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 , 2021.
[209] Yutong Xie, Jianpeng Zhang, Chunhua Shen, and Yong Xia. Cotr: Efficiently bridging cnn and transformer for 3d medical image
segmentation. In Internationalconferenceonmedicalimagecomputingandcomputer-assistedintervention ,pages171‚Äì180.Springer,2021.
[210] Jianhong Cheng, Jin Liu, Hulin Kuang, and Jianxin Wang. A fully automated multimodal mri-based multi-task learning for glioma
segmentation and idh genotyping. IEEE Transactions on Medical Imaging , 2022.
. : Page 58 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[211] Abhinav Sagar. Vitbis: Vision transformer for biomedical image segmentation. In Clinical Image-Based Procedures, Distributed and
Collaborative Learning, Artificial Intelligence for Combating COVID-19 and Secure and Privacy-Preserving Machine Learning , pages
34‚Äì45. Springer, 2021.
[212] Qixuan Sun, Nianhua Fang, Zhuo Liu, Liang Zhao, Youpeng Wen, and Hongxiang Lin. Hybridctrm: Bridging cnn and transformer for
multimodal brain image segmentation. Journal of Healthcare Engineering , 2021, 2021.
[213] Jose Dolz, Karthik Gopinath, Jing Yuan, Herve Lombaert, Christian Desrosiers, and Ismail Ben Ayed. Hyperdense-net: a hyper-densely
connected cnn for multi-modal image segmentation. IEEE transactions on medical imaging , 38(5):1116‚Äì1126, 2018.
[214] MatthewSinclair,AndreasSchuh,KarlHahn,KerstenPetersen,YingBai,JamesBatten,MichielSchaap,andBenGlocker. Atlas-istn:joint
segmentation,registrationandatlasconstructionwithimage-and-spatialtransformernetworks. MedicalImageAnalysis ,page102383,2022.
[215] AgisilaosChartsias,GiorgosPapanastasiou,ChengjiaWang,ScottSemple,DavidENewby,RohanDharmakumar,andSotiriosATsaftaris.
Disentangle,alignandfuseformultimodalandsemi-supervisedimagesegmentation. IEEEtransactionsonmedicalimaging ,40(3):781‚Äì792,
2020.
[216] Zheyao Gao and Xiahai Zhuang. Consistency based co-segmentation for multi-view cardiac mri using vision transformer. In International
Workshop on Statistical Atlases and Computational Models of the Heart , pages 306‚Äì314. Springer, 2021.
[217] Dong Sui, Kang Zhang, Weifeng Liu, Jing Chen, Xiaoxuan Ma, and Zhaofeng Tian. Cst: A multitask learning framework for colorectal
cancer region mining based on transformer. BioMed Research International , 2021, 2021.
[218] YiyaoLiu,YiYang,WeiJiang,TianfuWang,andBaiyingLei. 3ddeepattentiveu-netwithtransformerforbreasttumorsegmentationfrom
automated breast volume scanner. In 2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society
(EMBC), pages 4011‚Äì4014. IEEE, 2021.
[219] HolgerRRoth,HirohisaOda,XiangrongZhou,NatsukiShimizu,YingYang,YuichiroHayashi,MasahiroOda,MichitakaFujiwara,Kazunari
Misawa, and Kensaku Mori. An application of cascaded 3d fully convolutional networks for medical image segmentation. Computerized
Medical Imaging and Graphics , 66:90‚Äì99, 2018.
[220] Xiliang Zhu, Zhaoyun Cheng, Sheng Wang, Xianjie Chen, and Guoqing Lu. Coronary angiography image segmentation based on pspnet.
Computer Methods and Programs in Biomedicine , 200:105897, 2021.
[221] Yuhang Lu, Kang Zheng, Weijian Li, Yirui Wang, Adam P Harrison, Chihung Lin, Song Wang, Jing Xiao, Le Lu, Chang-Fu Kuo, et al.
Contourtransformernetworkforone-shotsegmentationofanatomicalstructures. IEEEtransactionsonmedicalimaging ,40(10):2672‚Äì2684,
2020.
[222] GuifangZhang,Hon-ChengWong,ChengWang,JianjunZhu,LigongLu,andGaojunTeng.Atemporarytransformernetworkforguide-wire
segmentation. In 202114thInternationalCongressonImageandSignalProcessing,BioMedicalEngineeringandInformatics(CISP-BMEI) ,
pages 1‚Äì5. IEEE, 2021.
[223] Yan-JieZhou,Xiao-LiangXie,Zeng-GuangHou,Gui-BinBian,Shi-QiLiu,andXiao-HuZhou. Frr-net:Fastrecurrentresidualnetworksfor
real-time catheter segmentation and tracking in endovascular aneurysm repair. In 2020 IEEE 17th International Symposium on Biomedical
Imaging (ISBI) , pages 961‚Äì964. IEEE, 2020.
[224] Yan-Jie Zhou, Xiao-Liang Xie, Xiao-Hu Zhou, Shi-Qi Liu, Gui-Bin Bian, and Zeng-Guang Hou. Pyramid attention recurrent networks for
real-timeguidewiresegmentationandtrackinginintraoperativex-rayfluoroscopy. ComputerizedMedicalImagingandGraphics ,83:101734,
2020.
[225] Yunxiang Li, Shuai Wang, Jun Wang, Guodong Zeng, Wenjun Liu, Qianni Zhang, Qun Jin, and Yaqi Wang. Gt u-net: A u-net like group
transformer network for tooth root segmentation. In International Workshop on Machine Learning in Medical Imaging , pages 386‚Äì395.
Springer, 2021.
[226] MengWang,WeifangZhu,FeiShi,JinzhuSu,HaoyuChen,KaiYu,YiZhou,YuanyuanPeng,ZhongyueChen,andXinjianChen. Mstganet:
Automatic drusen segmentation from retinal oct images. IEEE Transactions on Medical Imaging , 2021.
[227] AyoubBenaliAmjoudandMustaphaAmrouch. Automaticgenerationofchestx-rayreportsusingatransformer-baseddeeplearningmodel.
In2021 Fifth International Conference On Intelligent Computing in Data Sciences (ICDS) , pages 1‚Äì5. IEEE, 2021.
[228] KeSun,BinXiao,DongLiu,andJingdongWang. Deephigh-resolutionrepresentationlearningforhumanposeestimation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5693‚Äì5703, 2019.
[229] Xing Jia, Yun Xiong, Jiawei Zhang, Yao Zhang, Blackley Suzanne, Yangyong Zhu, and Chunlei Tang. Radiology report generation for rare
diseases via few-shot transformer. In 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) , pages 1347‚Äì1352.
IEEE, 2021.
[230] Hojun Lee, Hyunjun Cho, Jieun Park, Jinyeong Chae, and Jihie Kim. Cross encoder-decoder transformer with global-local visual extractor
for medical image captioning. Sensors, 22(4):1429, 2022.
[231] MarcellaCornia,MatteoStefanini,LorenzoBaraldi,andRitaCucchiara. Meshed-memorytransformerforimagecaptioning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10578‚Äì10587, 2020.
[232] Yuxuan Xiong, Bo Du, and Pingkun Yan. Reinforced transformer for medical image captioning. In International Workshop on Machine
Learning in Medical Imaging , pages 673‚Äì680. Springer, 2019.
[233] Zhicheng Zhang, Lequan Yu, Xiaokun Liang, Wei Zhao, and Lei Xing. Transct: dual-path transformer for low dose computed tomography.
InInternational Conference on Medical Image Computing and Computer-Assisted Intervention , pages 55‚Äì64. Springer, 2021.
[234] Lipei Zhang, Zizheng Xiao, Chao Zhou, Jianmin Yuan, Qiang He, Yongfeng Yang, Xin Liu, Dong Liang, Hairong Zheng, Wei Fan, et al.
Spatialadaptiveandtransformerfusionnetwork(stfnet)forlow-countpetblinddenoisingwithmri. MedicalPhysics ,49(1):343‚Äì356,2022.
[235] Yanmei Luo, Yan Wang, Chen Zu, Bo Zhan, Xi Wu, Jiliu Zhou, Dinggang Shen, and Luping Zhou. 3d transformer-gan for high-quality pet
reconstruction. In International Conference on Medical Image Computing and Computer-Assisted Intervention , pages 276‚Äì285. Springer,
2021.
[236] LuluWang,HuazhengZhu,ZhongshiHe,YuanyuanJia,andJinglongDu. Adjacentslicesfeaturetransformernetworkforsingleanisotropic
3d brain mri image super-resolution. Biomedical Signal Processing and Control , 72:103339, 2022.
. : Page 59 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[237] ZebinHu,HaoLiu,ZhendongLi,andZekuanYu. Data-enabledintelligenceincomplexindustrialsystemscross-modeltransformermethod
for medical image synthesis. Complexity , 2021, 2021.
[238] KunTang,ZhiLi,LiliTian,LihuiWang,andYueminZhu. Admir‚Äìaffineanddeformablemedicalimageregistrationfordrug-addictedbrain
images.IEEE Access , 8:70960‚Äì70968, 2020.
[239] XiaogangGu,FeixiangZhou,RongfeiChen,XinzhenRen,andWenjuZhou. Endoscopicsingleimagesuper-resolutionbasedontransformer
and convolutional neural network. In Intelligent Life System Modelling, Image Processing and Analysis , pages 24‚Äì32. Springer, 2021.
[240] Alvaro Gomariz, Tiziano Portenier, Patrick M Helbling, Stephan Isringhausen, Ute Suessbier, C√©sar Nombela-Arrieta, and Orcun Goksel.
Modalityattentionandsamplingenablesdeeplearningwithheterogeneousmarkercombinationsinfluorescencemicroscopy. Naturemachine
intelligence , 3(9):799‚Äì811, 2021.
[241] Zhengyang Wang, Yaochen Xie, and Shuiwang Ji. Global voxel transformer networks for augmented microscopy. Nature Machine
Intelligence , 3(2):161‚Äì171, 2021.
[242] Weijian Li, Viet-Duy Nguyen, Haofu Liao, Matt Wilder, Ke Cheng, and Jiebo Luo. Patch transformer for multi-tagging whole slide
histopathology images. In International Conference on Medical Image Computing and Computer-Assisted Intervention , pages 532‚Äì540.
Springer, 2019.
. : Page 60 of 60"
"SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for
  Large-scale Vision-Language Models","['Youngjoon Yu', 'Sangyun Chung', 'Byung-Kwan Lee', 'Yong Man Ro']",2024,http://arxiv.org/abs/2408.12114v3,"/b‚ôÄtSPARK: Multi-Vision Sensor Perception and Reasoning Benchmark
for Large-scale Vision-Language Models
Youngjoon Yu‚Ä†, Sangyun Chung‚Ä†, Byung-Kwan Lee, and Yong Man Ro*
Integrated Vision Language Lab, KAIST, South Korea
{greatday, jelarum, leebk, ymro }@kaist.ac.kr
Abstract
Large-scale Vision-Language Models (LVLMs) have signif-
icantly advanced with text-aligned vision inputs. They have
made remarkable progress in computer vision tasks by align-
ing text modality with vision inputs. There are also endeav-
ors to incorporate multi-vision sensors beyond RGB, includ-
ing thermal, depth, and medical X-ray images. However, we
observe that current LVLMs view images taken from multi-
vision sensors as if they were in the same RGB domain with-
out considering the physical characteristics of multi-vision
sensors. They fail to convey the fundamental multi-vision
sensor information from the dataset and the correspond-
ing contextual knowledge properly. Consequently, alignment
between the information from the actual physical environ-
ment and the text is not achieved correctly, making it dif-
ficult to answer complex sensor-related questions that con-
sider the physical environment. In this paper, we aim to
establish a multi-vision Sensor Perception AndReasoning
benchmar Kcalled SPARK that can reduce the fundamen-
tal multi-vision sensor information gap between images and
multi-vision sensors. We generated 6,248 vision-language
test samples to investigate multi-vision sensory perception
and multi-vision sensory reasoning on physical sensor knowl-
edge proficiency across different formats, covering different
types of sensor-related questions. We utilized these samples
to assess ten leading LVLMs. The results showed that most
models displayed deficiencies in multi-vision sensory rea-
soning to varying extents. Codes and data are available at
https://github.com/top-yun/SPARK
Introduction
In recent days, Large-scale Vision-Language Models
(LVLMs) have achieved significant breakthroughs in ar-
eas such as visual dialogue (Koh, Salakhutdinov, and Fried
2023), video analysis (Ren et al. 2024), and document under-
standing (Ye et al. 2023), establishing themselves as critical
tools in the pursuit of artificial general intelligence (AGI).
These models function similarly to the human brain by
processing multimodal information and generating sophis-
ticated inferences. For instance, the latest LVLMs, like Ope-
nAI‚Äôs GPT-4o (OpenAI 2024), have exhibited exceptional
reasoning abilities that not only rival but in some cases ex-
ceed human performance.
*Corresponding author.‚Ä†Both authors are equally contributed.
Sensory Reasoning Performance Across Different LVLMs and Vision Sensors
Yes! the ribs are visible in this chest X-ray image Are there ribs visible in this 
image?
What could be the likely reason for capturing this image? 
[A] To measure the temperature of items. 
[B] To describe the lighting condition.
[C] To detect hazy, foggy atmosphere.
[D] To analyze the spatial arrangement of objects in a room .
[C] To detect hazy, foggy atmosphere.
XR image
Depth image
Multi -Vision SensorSensory Reasoning (Acc)LLAVA -v1.5 CogVLM InternVL2 TroL
Meteor IXC 2.5 QwenVL GPT4oFigure 1: The comparison of sensory reasoning performance
across different multi-vision sensors with respect to the re-
cent LVLMs. Note that, sensory reasoning performance sig-
nificantly drops across different multi-vision sensors.
One emerging concept in modern AI research gaining sig-
nificant attention is the development of large vision language
models (LVLMs) capable of handling a variety of multi-
modal inputs, surpassing the capabilities of previous large
language models (LLMs). LVLMs can process diverse forms
of data simultaneously, including images, videos, and text
(OpenAI 2024; OpenGVLab 2024; Zhang et al. 2024). This
ability also allows them to use multi-vision sensor data as
input, including thermal sensors, depth sensors, and medi-
cal imaging (Girdhar et al. 2023; Su et al. 2023). To fully
harness the potential of LVLMs, recent research has focused
on effectively integrating various multi-vision sensor data to
develop more sophisticated and practical AI systems for the
real world.arXiv:2408.12114v3  [cs.CV]  11 Oct 2024
However, despite the remarkable advancements in LVLM
models, significant challenges still remain in fully utilizing
multi-vision sensors. LVLMs often overlook the nuances of
the physical properties of individual vision sensors. Instead,
they tend to make judgments based on prior visual or lin-
guistic information from images they have learned using
low-level features in two-dimensional data. This results in
the models recognizing only superficial patterns in image
inputs, missing the underlying logical structures or contex-
tual understanding. When identifying specific objects in an
image input, a model might rely on patterns learned from
similar-looking images rather than considering the actual
physical properties of the multi-vision sensors used to cap-
ture the image. This can hinder accurate identification and
a deep understanding of the input images in fields where
the LVLM‚Äôs decision-making is crucial such as autonomous
driving (Mao et al. 2023; Xu et al. 2024), security sys-
tems (Shi et al. 2024), and medical image diagnosis (Bazi
et al. 2023).
We evaluate the behavior of the recent LVLMs using
multi-vision sensor images as input in Figure 1. The perfor-
mance of sensory reasoning, which we devised to assess the
understanding of fundamental knowledge of multi-vision
sensors in the real world, significantly drops across differ-
ent multi-vision sensors such as thermal infrared, depth,
and X-ray (XR) images. This highlights the challenges that
LVLMs face in accurately interpreting multi-vision sensor
data and making correct inferences based on the physical
properties of sensors. Additionally, from the interaction ex-
ample shown below in Figure 1, while the LVLM can ac-
curately identify the vision sensor used to capture the im-
age for a relatively simple question, it struggles with under-
standing the actual purpose or context of the image in the
sensor-related, more complicated questions. This indicates
that current LVLMs have difficulty in understanding the fun-
damental knowledge of physical vision sensors beyond what
the image looks like.
For example, as illustrated in Figure 1, when humans look
at a photograph of an X-ray medical image, they interpret it
deeply, drawing upon their knowledge base and their physi-
cal understanding of the human body beyond the X-ray im-
age itself. Despite never having seen their internal organs
and the structure of bones with the naked eye, humans can
comprehend the image through scientific contextual knowl-
edge and their inherent understanding of the physical world.
In contrast, current LVLMs try to understand the inside of
the human body based solely on the two-dimensional data
they have been trained on, revealing their limitations in
fully grasping the physical environment of the real world.
Therefore, establishing a comprehensive evaluation bench-
mark is necessary before LVLMs are implemented in crit-
ical and sensitive real-world applications. However, the as-
sessment of Large Vision-Language Models (LVLMs) has
significantly lagged behind their rapid development. Several
initiatives are striving to close this gap by introducing a vari-
ety of multimodal evaluation benchmarks. Notable examples
include MME (Fu et al. 2024), MMBench (Liu et al. 2024b),
LVLM-eHub (Xu et al. 2023), and SEED-Bench (Li et al.
2023a). These benchmarks aim to define key dimensions ofmultimodal capabilities and provide corresponding test sam-
ples. But, they cover a relatively narrow range of multimodal
tasks, primarily focusing on fundamental abilities such as vi-
sual recognition and OCR.
In this paper, to handle the aforementioned challenge, we
design the SPARK benchmark to evaluate multi-vision input
LVLMs on two fronts: multi-vision perception and multi-
vision reasoning. Multi-vision perception pertains to the in-
formation needed, which measures the LVLM‚Äôs effective-
ness in satisfying visual perception needs. Multi-vision rea-
soning measures the LVLM‚Äôs ability to base its responses on
fundamental information from the provided sensor knowl-
edge. To be specific, we generated 6,248 vision-language
test samples to investigate multi-vision sensory perception
and reasoning related to physical sensor knowledge profi-
ciency, covering 6 types of multi-vision sensory instruction
tasks across 2 different question-and-answer formats. We
used these samples to assess 10 leading large-scale vision
language models. The experiment results validate that most
LVLMs displayed deficiencies in sensory reasoning to vary-
ing extents.
In summary, the contributions of this work are as follows:
‚Ä¢ To the best of our knowledge, we first reveal the inca-
pability of current LVLMs, which suffer from limited
multi-vision sensory reasoning across different multi-
vision sensors due to an absence of fundamental under-
standing of sensors in the physical world.
‚Ä¢ We propose a novel benchmark, SPARK, to rigorously
test and evaluate the capabilities of LVLMs in under-
standing sensory knowledge, providing a comprehensive
framework for assessing their performance.
‚Ä¢ We evaluated a total of 10 state-of-the-art LVLMs using
our SPARK benchmark, which is designed to rigorously
assess the capability of the LVLMs in handling funda-
mental knowledge related to multi-vision sensors.
Related work
Large-scale Vison-Language Models. Recently, there has
been significant interest in visual language multimodal
learning. Visual language models such as LLA V A (Liu et al.
2023b, 2024a), CollaVO (Lee et al. 2024c), MoAI (Lee
et al. 2024d), TroL (Lee et al. 2024a), Meteor (Lee et al.
2024b), IXC2.5 (Zhang et al. 2024), and QwenVL (Bai
et al. 2023) have shown impressive performance in a variety
of downstream tasks. In addition, to obtain richer contex-
tual information, LVLMs have developed the capability to
handle multimodal inputs. Wang et al. introduces CogVLM,
an advanced visual language foundation multimodal model
that integrates a trainable visual expert module with a pre-
trained language model. InternVL2 (Chen et al. 2024) is an
open-source multimodal large language model that bridges
the gap between open-source and commercial models by
enhancing visual understanding, dynamic high-resolution
processing, and bilingual dataset quality. GPT4o (OpenAI
2024) possesses advanced multimodal capabilities, allow-
ing it to process and generate diverse multimodalities. This
enables the model to understand and create content that in-
tegrates visual and textual information, making it suitable
Position (Thermal)
[Y] Is the person standing to the leftof the dog ?
[N] Is the dog positioned behind the person?Existence (RGB)
[Y] Is there a flower on the man's suit in this image?
[N] Is there a hat being worn by anyone in this image?
General Description (XR)
[Y] Are the ribs visible in the image?
[N]Is there a distinct liver shape visible in the image?Counting (Depth)
[Y] Are there two bottles in this image?
[N] Are there two monitors in this image?
Multi
 -
vision 
Perception
Multi
 -
vision 
Reasoning
Sensory Reasoning (Thermal)
Q: What could be the likely reason for capturing this 
image? 
[A] To assess the environmental conditions of the area.
[B] To study the feeding habits of livestock.
[C] To evaluate the breeding patterns of the animal.
[D] To monitor the health and temperature of animal.Contextual Reasoning (Thermal)
Q: What might be the reason for the vehicles lined up in the image?
[A] They are waiting for a traffic signal .
[B] They are parked for the night.
[C] They are part of a parade.
[D] They are in a car wash.
SPARKFigure 2: In the proposed SPARK, we build the first benchmark for evaluating the abilities of LVLMs in multi-vision sensor
understanding, which covers four types of multi-vision perception tasks (Existence, Counting, Position, and General Descrip-
tion) and two types of multi-vision reasoning tasks (Contextual Reasoning and Sensory Reasoning).
for a wide range of applications that require various modal-
ities. Consequently, many LVLMs have emerged that take
multi-vision sensor images as input. Girdhar et al. presents
ImageBind, which creates a joint embedding space across
multi-vision sensors including depth and thermal sensor
data. PandaGPT (Su et al. 2023) is a LVLM that integrates
multimodal encoders and large language models to enable
multi-vision and auditory instruction-following capabilities,
performing complex tasks. However, relatively less attention
has been devoted to whether LVLMs truly understand the
physical meanings of multi-vision sensors used to capture
the input image.
Evaluation Benchmark for LVLMs. Numerous studies
have leveraged existing vision-language datasets to develop
benchmarks for assessing the reliability of LVLMs (Li and
Lu 2024). MME (Fu et al. 2024) includes 14 sub-tasks
based on publicly available images with manually created
annotations, evaluating both the recognition and perception
capabilities of LVLMs through yes/no question answering.
SEED-benchmark (Li et al. 2023a) designed to evaluate the
generative comprehension capabilities of multimodal LVLM
through human-annotated multi-choice questions across 12
evaluation dimensions. Other comparable benchmarks in-
clude LVLM-eHub (Xu et al. 2023), MM-Vet (Yu et al.
2023), and MMBench (Liu et al. 2024b). Additionally, there
are benchmarks aimed at assessing specific target propertiesof LVLMs. POPE (Li et al. 2023b) focuses on evaluating
object hallucination by asking yes/no questions about the
presence of objects in the input image. M-HalDetect (Gun-
jal, Yin, and Bas 2024) introduces hallucination tasks us-
ing human-annotated labels for sentence-level classification.
Unlike those previous evaluation benchmarks, the proposed
SPARK is designed to rigorously test and evaluate the ca-
pabilities of understanding the physical meaning of multi-
vision sensors.
Evaluation and Instruction Design
There are multiple formats available for evaluating the
multi-sensor perception and reasoning capabilities of
LVLM, each with distinct advantages and limitations. Free-
form questions (Yarom et al. 2024) offer flexibility and ease
of creation but demand labor-intensive human assessment
and present challenges in maintaining consistent scoring.
Similarity-based assessment are less resource-intensive but
can be significantly affected by biases present in the similar-
ity metrics. Yes-or-No questions (Fu et al. 2024) are straight-
forward and easier to assess, but they may oversimplify the
evaluation, failing to capture the full extent of LVLM‚Äôs com-
prehension of multi-vision reasoning ability.
First of all, to enable quantitative performance metrics for
multi-vision perception, the instruction design aims to elicit
‚Äúyes‚Äù or ‚Äúno‚Äù responses from the model. This binary re-
SPARKExistenceCounting
Sensory
ReasoningPosition
General
DescriptionContextual
Reasoning1423720Figure 3: Distribution of data sources of the SPARK bench-
mark. In SPARK, we demonstrate six core multi-vision sen-
sory tasks in the inner ring, and the outer ring displays the
number of samples for each specific task.
sponse format simplifies the evaluation process, allowing for
clear, objective performance measurement. As a result, each
instruction comprises two parts: a brief, targeted question
and an explanation corresponding to either ‚Äúyes‚Äù or ‚Äúno.‚Äù
This structure ensures that the LVLM‚Äôs comprehension can
be precisely assessed. For every test image, two instructions
are manually crafted, each posing a different question to the
model. These questions are designed to test different aspects
of the image‚Äôs content and context. The rationale behind this
approach is to ensure that the model‚Äôs answers are not based
on chance. When the LVLMs correctly answer both ques-
tions, it demonstrates an understanding of the image and its
related information, rather than merely guessing.
In addition, we also introduce a multi-vision sensor under-
standing evaluation design based on multi-choice questions.
This format presents questions with a set of predetermined
choices, allowing respondents to select the correct options.
The multi-choice question format is advantageous for sev-
eral reasons. First, it enables efficient grading and analysis of
responses, as answers can be objectively evaluated against a
fixed set of possible responses. Also, the multi-choice ques-
tion format allows for precise control over the difficulty level
of the questions. By varying the validity of each option, we
can create questions that test different levels of understand-
ing and comprehension. For example, including more plau-
sible but incorrect options can increase the difficulty, ensur-
ing that only models with a deeper understanding can con-
sistently choose the correct answer. This flexibility in ques-
tion design makes multi-choice questions a powerful tool
for assessing the nuanced capabilities of multi-vision sen-
sor systems. Furthermore, the Yes-or-No format can be seenas a specific case of multi-choice question, where the op-
tions are limited to ‚Äú(A) Yes‚Äù and ‚Äú(B) No.‚Äù This simplifica-
tion retains the benefits of the multi-choice question format
while providing a straightforward way to measure binary de-
cisions.
Using accuracy as the evaluation metric for both multi-
choice questions and Yes-or-No questions ensures consis-
tency in how we assess the model‚Äôs performance. Accuracy,
defined as the proportion of correctly answered questions,
provides a clear and intuitive measure of how well the model
understands the given inputs. The adoption of the multi-
choice question based evaluation design supports the de-
velopment of a more comprehensive evaluation framework.
The incorporation of both simple Yes-or-No questions and
more complex multi-choice questions ensures that the eval-
uation covers both basic and advanced aspects of LVLM‚Äôs
understanding.
Evaluation on Multi-vision Sensor Tasks
Our instruction dataset was collected according to two
multi-vision tasks: multi-vision perception and multi-vision
reasoning. As illustrated in Figure 2, first of all, multi-vision
perception focuses on the LVLM‚Äôs ability to accurately in-
terpret and identify objects, scenes, and relationships from
various multi-vision inputs. This involves tasks such as ob-
ject detection, image classification, scene recognition, and
relationship detection, where the model must process and
understand the content of images from multiple vision sen-
sors. The goal is to ensure that the model can consistently
recognize and categorize visual elements across different
contexts from different vision sensors. On the other hand,
multi-vision reasoning requires the model to not only per-
ceive but also make inferences based on the multi-vision
sensory data. This involves higher-order cognitive tasks such
as understanding relationships between objects, prediction
of intent of sensor use, and understanding sensor knowl-
edge. For instance, the model might need to infer the cause
of an event depicted in an image sequence or predict the
purpose of a captured image. Multi-vision reasoning tests
the LVLM‚Äôs capability to integrate multi-vision information
with contextual sensory knowledge, making logical deduc-
tions that go beyond mere perception.
Multi-vision Perception
Multi-vision perception is the foundational process by which
Large Vision-Language Models (LVLMs) analyze images
captured by various multi-vision sensors, including RGB,
thermal, depth, and X-ray images. This process involves rec-
ognizing and interpreting the fundamental elements within
each visual input based on cognitive science (Kahneman,
Treisman, and Gibbs 1992; Broadbent 2013).
‚Ä¢ Existence: LVLMs can identify and list common objects
present in the image, such as people, vehicles, animals,
furniture, and so on.
‚Ä¢ Count: LVLMs can count the number of identified ob-
jects or entities, providing a quantitative understanding
of the scene.
Models Vision Sensors Existence Count PositionGeneral
DescriptionMulti-vision
PerceptionContextual
ReasoningSensory
ReasoningMulti-vision
Reasoning
Open Source Large-scale Vision-Language Models
RGB 93.9 68.5 62.6 97.9 80.7 95.1 97.2 96.1Qwen-VL-ChatThermal 86.1 66.9 59.3 95.3 76.9 90.3 83.5 86.9(Bai et al. 2023)Depth 76.6 59.6 53.3 84.9 68.6 78.1 68.4 73.3
XR 68.0 71.3 55.1 74.1 67.1 81.8 74.7 78.3
RGB 94.2 75.5 59.8 96.9 81.6 88.7 94.8 91.8LLA V A-v1.5-7BThermal 93.3 76.1 62.4 95.1 81.7 85.5 51.0 68.2(Liu et al. 2023a)Depth 87.1 70.7 53.3 93.7 76.2 87.4 73.8 80.6
XR 74.2 57.4 67.4 72.3 67.8 62.1 50.7 56.4
RGB 96.5 73.4 61.4 97.2 82.1 98.0 97.2 97.6CogVLM-ChatThermal 94.9 76.1 64.6 96.2 82.9 96.2 59.0 77.6(Wang et al. 2023)Depth 94.9 76.1 64.5 96.5 83.0 90.1 71.7 80.9
XR 86.1 72.8 61.6 79.4 74.9 90.9 84.0 87.5
RGB 97.2 78.5 72.2 97.9 86.4 98.0 99.5 98.8Meteor-7BThermal 93.5 68.9 71.7 95.3 82.3 90.9 62.0 76.4(Lee et al. 2024b)Depth 83.5 65.9 62.2 91.6 75.8 89.5 77.3 83.4
XR 79.5 70.6 63.8 76.6 72.6 86.4 84.0 85.2
RGB 96.9 81.2 69.3 96.5 85.9 98.0 99.5 98.8TroL-7BThermal 93.9 72.8 68.1 92.8 81.9 94.1 65.5 79.8(Lee et al. 2024a)Depth 83.3 67.7 67.3 90.7 77.2 84.8 73.8 79.3
XR 82.8 69.1 71.0 78.7 75.4 83.3 84.0 83.7
RGB 96.5 76.9 69.3 98.6 85.3 98.6 99.5 99.1IXC2.5-VL-7BThermal 93.0 70.6 66.8 95.5 81.5 92.5 60.0 76.2(Zhang et al. 2024)Depth 86.1 59.9 59.4 93.3 74.7 90.6 74.3 82.4
XR 86.1 73.5 63.8 76.6 75.0 89.4 88.0 88.7
RGB 97.2 78.3 72.4 97.9 86.5 97.6 99.1 98.3InternVL2-8BThermal 90.5 75.8 61.1 93.7 80.3 94.6 61.5 78.1(OpenGVLab 2024)Depth 83.0 60.2 60.3 91.4 73.7 86.9 79.9 83.5
XR 92.7 77.9 71.7 84.9 81.8 89.4 82.7 86.0
Closed Source Large-scale Vision-Language Models
RGB 94.6 79.6 65.2 95.3 83.7 97.6 98.6 98.1Gemini 1.5 ProThermal 91.4 73.6 68.8 93.9 81.9 90.3 93.0 91.7(Team et al. 2024)Depth 87.8 73.7 62.6 94.2 79.6 78.0 88.4 83.2
XR 89.9 81.6 63.0 82.0 79.2 92.4 88.0 90.2
RGB 95.1 79.0 69.7 95.8 84.9 99.5 97.2 98.3Claude 3.5 SonnetThermal 92.1 79.2 62.9 95.0 82.3 94.1 85.0 89.6(Anthropic 2024)Depth 72.9 67.7 55.6 84.4 70.2 86.4 75.5 80.9
XR 83.2 76.5 74.6 83.5 79.5 93.9 82.7 88.3
RGB 96.9 80.9 71.4 97.4 86.7 98.5 98.6 98.6GPT-4oThermal 96.1 75.6 71.4 98.2 85.3 95.2 92.0 93.6(OpenAI 2024)Depth 87.6 77.3 71.0 94.4 82.6 95.8 85.8 90.8
XR 91.9 83.8 65.2 85.6 81.7 95.5 82.7 89.1
Table 1: Evaluation results of different models on SPARK benchmark. Accuracy is the metric. ‚ÄúMulti-vision Perception‚Äù shows
the average performance on four dimensions (Existence, Count, Position, and General Description) for evaluating visual per-
ception, and ‚ÄúMulti-vision Reasoning‚Äù shows the average performance on two dimensions (Contextual Reasoning and Sensory
Reasoning) for evaluating vision sensory understanding. LVLMs are sorted in ascending order of release date.
‚Ä¢ Position: LVLMs can determine the spatial arrangement
of objects within the image, noting their positions relative
to one another.
‚Ä¢ General Description: LVLMs are also equipped to gener-
ate nuanced descriptions of the overall scene depicted in
an image. They can articulate what is happening, identify
objects, and provide factual information that enhances
the understanding of the image itself.At the perception stage, LVLMs focus on extracting essen-
tial information directly from raw image data captured by
multi-vision sensors. This foundational perception is critical
for all subsequent reasoning tasks, serving as the foundation
upon which more complex interpretations are built.
Multi-vision Reasoning
Multi-vision reasoning is where LVLMs truly showcase
their advanced capabilities. Beyond simply perceiving im-
Vision Sensors RGB Thermal Depth XR
ModelsMulti-Vison
PerceptionMulti-Vision
ReasoningMulti-Vison
PerceptionMulti-Vision
ReasoningMulti-Vison
PerceptionMulti-Vision
ReasoningMulti-Vison
PerceptionMulti-Vision
ReasoningALL
Open Source Large-scale Vision-Language Models
LLaV A-v1.5-7B
(Liu et al. 2023a)81.6 91.8 81.7 68.2 76.2 80.6 67.8 56.4 75.6
Qwen-VL-Chat
(Bai et al. 2023)80.7 96.1 76.9 86.9 68.6 73.3 67.1 78.3 78.5
Meteor-7B
(Lee et al. 2024b)86.4 98.8 82.3 76.4 75.8 83.4 72.6 85.2 82.6
TroL-7B
(Lee et al. 2024a)85.9 98.8 81.9 79.8 77.2 79.3 75.4 83.7 82.8
IXC2.5-VL-7B
(Zhang et al. 2024)85.3 99.1 81.5 76.2 74.7 82.4 75.0 88.7 82.9
CogVLM-Chat
(Wang et al. 2023)82.1 97.6 82.9 77.6 83.0 80.9 74.9 87.5 83.3
InternVL2-8B
(OpenGVLab 2024)86.5 98.3 80.3 78.1 73.7 83.5 81.8 86.0 83.5
Closed Source Large-scale Vision-Language Models
Claude 3.5 Sonnet
(Anthropic 2024)84.9 98.3 82.3 89.6 70.2 80.9 79.5 88.3 84.3
Gemini 1.5 Pro
(Team et al. 2024)83.7 98.1 81.9 91.7 79.6 83.2 79.2 90.2 85.9
GPT-4o
(OpenAI 2024)86.7 98.6 85.3 93.6 82.6 90.8 81.7 89.1 88.5
Table 2: Leaderboards of 10 advanced leading LVLMs on proposed SPARK benchmark according to different multi-vision
sensors. Accuracy is the metric and the best accuracy is denoted in bold and underlined. LVLMs are sorted in ascending order
of overall accuracy (ALL).
ages, LVLMs can engage in logical reasoning to derive
deeper insights and make informed decisions. This distin-
guishes the recent LVLMs from traditional computer vision
models, which primarily focus on understanding and inter-
acting with the real world.
‚Ä¢ Contextual reasoning: LVLMs can utilize fundamen-
tal knowledge and contextual clues to make judgments
about a given scenario. This type of reasoning allows
LVLMs to refer to the underlying basis of physical sensor
knowledge and ensure that the reasoning process remains
consistent with the context provided by the image and the
associated information.
‚Ä¢ Sensory reasoning: A more complex reasoning ability
requires LVLMs to map 2D image data to the physical
meanings associated with different multi-vision sensors.
This process not only involves processing the raw data
from images but also integrates it with contextual infor-
mation about the underlying physical sensor knowledge
in the real world. By combining fundamental sensor in-
formation, LVLMs can derive conclusions that are both
accurate and contextually relevant. Sensory reasoning re-
quires a deep understanding of the knowledge underlying
the physical meaning of multi-vision sensor data. This
goes beyond surface-level image recognition, demanding
that LVLMs make sense of the sensor data in a way that
reflects real-world physics and usage scenarios.
Next, we integrate both visual and textual inputs into
GPT-4, guided by meticulously crafted prompts. These
prompts are specifically designed to align with various eval-
uation dimensions, ensuring that the generated questions areboth relevant and focused. To further enhance the quality
of the benchmark, we introduce an additional filtering step.
In the final stages of development, human annotators play a
crucial role, selecting the correct answers and categorizing
the questions according to their respective evaluation dimen-
sions.
Experiment
Implementation Details
Dataset Collection We collect six subsets for each multi-
sensor vision task type, together with 4k images and 6k
unique questions and answers. These instructions are built
from five public datasets: MS-COCO (Lin et al. 2015),
M3FD (Liu et al. 2022), Dog&People (Roboflow 2022),
RGB-D scene dataset (Cho et al. 2021), and UNIFESP X-
ray Body Part Classifier Competition dataset (Eduardo Fa-
rina 2022). The MS-COCO dataset is a commonly used ob-
ject detection dataset that contains RGB images with fine-
grained object bounding boxes, categories, and attribute an-
notations. We sampled 1.2k images from validation dataset.
Furthermore, for thermal sensor datasets, we sampled 1.2k
images from two different thermal datasets (M3FD and
Dog&People) in order to collect a thermal dataset covering
the widest possible range of diverse situations and objects.
Additionally, we sampled 1.2k images from RGB-D scene
dataset (Cho et al. 2021) for depth sensor because it covers
a variety of indoor and outdoor scenes. Finally, we sampled
0.4k images from the public X-ray body part dataset for the
XR sensor dataset because of the diversity of multiple hu-
man body parts. We described the overall distribution of data
sources of the SPARK benchmark in Figure 3.
Large Vision Language Models In our evaluation, we
selected 10 state-of-the-art (SOTA) Large Vision-Language
Models (LVLMs) that represent the leading edge of current
research. These models were chosen to provide a compre-
hensive assessment of the capabilities and performance of
both open-source and closed-source LVLMs across a vari-
ety of multi-vision sensor tasks on the SPARK benchmark.
‚Ä¢ Open source: CogVLM-Chat (Wang et al. 2023),
LLA V A-v1.5-7B (Liu et al. 2023b), InternVL2-
8B (OpenGVLab 2024), TroL-7B (Lee et al. 2024a),
Meteor-7B (Lee et al. 2024b), IXC2.5-VL-7B (Zhang
et al. 2024), Qwen-VL-Chat (Bai et al. 2023)
‚Ä¢ Closed source: GPT-4o (OpenAI 2024), Claude 3.5 Son-
net (Anthropic 2024), Gemini-Pro1.5 (Team et al. 2024)
Experiment Result
In this section, we conduct a comprehensive evaluation us-
ing the proposed SPARK benchmark, a rigorous framework
designed to assess the capabilities of Large Vision-Language
Models (LVLMs) in two target tasks: Multi-vision Percep-
tion and Multi-vision Reasoning. Multi-vision Perception
presents the averaged performance on four dimensions for
evaluating visual perception. Meanwhile, Multi-vision Rea-
soning demonstrates the averaged performance on two di-
mensions for evaluating the LVLMs‚Äô ability to understand
and reason about multi-vision sensory data.
As shown in Table 1, the evaluation revealed that perfor-
mance varies significantly depending on the type of multi-
vision sensor used to capture the input images. LVLMs gen-
erally perform well in simple Multi-vision perception tasks
such as generating general descriptions, but more complex
reasoning tasks like Multi-vision Reasoning reveal signif-
icant differences in model capabilities. Since they mainly
trained with general RGB images, the performance of multi-
vision perception and reasoning in RGB sensor is consis-
tently maintained at high levels. However, the performance
of LVLMs drops noticeably when dealing with images cap-
tured using thermal, depth, and X-ray(XR) sensors. This de-
cline is particularly evident in the Multi-vision Reasoning
task, especially in Sensory Reasoning.
Sensory Reasoning requires LVLMs to not only recognize
and describe images but also to understand the physical prin-
ciples underlying the sensor data. For example, interpreting
thermal data involves understanding heat signatures, while
depth data requires an understanding of the need for spa-
tial geometry beyond simple 2D interpretation. The experi-
ment demonstrates LVLMs‚Äô limited proficiency in interpret-
ing and mapping sensor data to its physical meaning.
Table 2 provides a clear comparison of the performance
of various LVLMs across different multi-vision sensors and
tasks. It highlights the strengths and weaknesses of each
model, particularly the advantage that closed-source models
have in maintaining high performance across more complex
reasoning tasks with diverse vision sensor types. Consider-
ing the overall accuracy score (ALL), GPT-4o excels in the
proposed SPARK benchmark.ModelVision
SensorSensor Reasoning
w/o Sensor Info.Sensor Reasoning
w/ Sensor Info.‚àÜ
LLA V A-v1.5-7B Thermal 51.0 81.0 +30.0
(Liu et al. 2023b) Depth 73.8 87.6 +13.8
Open source LVLM XR 50.7 54.0 +3.3
TroL-7B Thermal 65.5 97.0 +31.5
(Lee et al. 2024a) Depth 73.8 99.1 +25.3
Open source LVLM XR 84.0 84.0 -
InternVL2-8B Thermal 61.5 85.5 +24.0
(OpenGVLab 2024) Depth 79.9 99.6 +19.7
Open source LVLM XR 82.7 85.3 +2.6
Claude 3.5 Sonnet Thermal 85.0 96.5 +11.5
(Anthropic 2024) Depth 75.5 99.6 +24.1
Closed source LVLM XR 82.7 82.7 -
GPT-4o Thermal 92.0 94.0 +2.0
(OpenAI 2024) Depth 85.8 99.6 +13.8
Closed source LVLM XR 82.7 84.0 +1.3
Table 3: Ablation study on sensor reasoning performance
change whether the sensor information is given. We choose
three LVLMs from open source and two from closed source.
Ablation study
In the previous section, we observed that LVLMs frequently
struggle to accurately infer the purpose or context of an im-
age when the data is sourced from multi-vision sensors other
than RGB. However, as demonstrated in Figure 1, even when
the input image lacks explicit information about the sen-
sor type, LVLMs can still identify the sensor correctly. This
suggests that while LVLMs have already acquired sensor-
related knowledge through textual data, they face challenges
in mapping fundamental knowledge to real-world scenarios.
Thus, in Table 3, we conducted an ablation experiment
on data-centric enhancement by adding sensor information
as a text prompt at the beginning of the question (‚ÄúThis
is a{Thermal, Depth, X-Ray }image.‚Äù) and measured the
sensory reasoning performance change. The experiment
demonstrated that sensor information can enhance the rea-
soning capabilities of LVLMs, particularly for thermal and
depth images, while XR data showed the least impact. This
implies that LVLM models, including GPT-4o, are not fully
utilizing the knowledge they already possess to understand
multi-vision sensory data.
Conclusion
In this study, we focus on evaluating the ability of Large
Vision-Language Models (LVLMs) to understand and pro-
cess multi-vision sensory inputs. As LVLMs are increas-
ingly deployed in real-world applications, their ability to
accurately interpret and reason about data from diverse vi-
sion sensors has become crucial. To address this, we propose
an evaluation benchmark called SPARK, which generates
instruction tuning samples aimed at specific physical sen-
sor understanding in various question-and-answer formats.
Through extensive experiments, we assess the performance
of understanding sensory knowledge in the latest state-of-
the-art LVLMs handling multi-vision input. We believe this
approach, integrating a sensory knowledge annotated evalu-
ation benchmark paves the way for promising future appli-
cations of LVLMs.
References
Anthropic. 2024. Claude 3.5 sonnet. https://www.anthropic.
com/news/claude-3-5-sonnet.
Bai, J.; Bai, S.; Yang, S.; Wang, S.; Tan, S.; Wang, P.; Lin,
J.; Zhou, C.; and Zhou, J. 2023. Qwen-VL: A Versatile
Vision-Language Model for Understanding, Localization,
Text Reading, and Beyond. arXiv:2308.12966.
Bazi, Y .; Rahhal, M. M. A.; Bashmal, L.; and Zuair, M.
2023. Vision‚Äìlanguage model for visual question answer-
ing in medical imagery. Bioengineering , 10(3): 380.
Broadbent, D. E. 2013. Perception and communication . El-
sevier.
Chen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.;
Tong, W.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024. How Far
Are We to GPT-4V? Closing the Gap to Commercial Mul-
timodal Models with Open-Source Suites. arXiv preprint
arXiv:2404.16821 .
Cho, J.; Min, D.; Kim, Y .; and Sohn, K. 2021. DIML/CVL
RGB-D Dataset: 2M RGB-D Images of Natural Indoor and
Outdoor Scenes. arXiv:2110.11590.
Eduardo Farina, M. P., FelipeKitamura. 2022. UNIFESP X-
ray Body Part Classifier Competition.
Fu, C.; Chen, P.; Shen, Y .; Qin, Y .; Zhang, M.; Lin, X.;
Yang, J.; Zheng, X.; Li, K.; Sun, X.; Wu, Y .; and Ji, R. 2024.
MME: A Comprehensive Evaluation Benchmark for Multi-
modal Large Language Models. arXiv:2306.13394.
Girdhar, R.; El-Nouby, A.; Liu, Z.; Singh, M.; Alwala, K. V .;
Joulin, A.; and Misra, I. 2023. Imagebind: One embedding
space to bind them all. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
15180‚Äì15190.
Gunjal, A.; Yin, J.; and Bas, E. 2024. Detecting and Pre-
venting Hallucinations in Large Vision Language Models.
arXiv:2308.06394.
Kahneman, D.; Treisman, A.; and Gibbs, B. J. 1992. The
reviewing of object files: Object-specific integration of in-
formation. Cognitive psychology , 24(2): 175‚Äì219.
Koh, J. Y .; Salakhutdinov, R.; and Fried, D. 2023. Ground-
ing language models to images for multimodal inputs and
outputs. In International Conference on Machine Learning ,
17283‚Äì17300. PMLR.
Lee, B.-K.; Chung, S.; Kim, C. W.; Park, B.; and Ro, Y . M.
2024a. TroL: Traversal of Layers for Large Language and
Vision Models. arXiv:2406.12246.
Lee, B.-K.; Kim, C. W.; Park, B.; and Ro, Y . M. 2024b. Me-
teor: Mamba-based Traversal of Rationale for Large Lan-
guage and Vision Models. arXiv:2405.15574.
Lee, B.-K.; Park, B.; Kim, C. W.; and Ro, Y . M. 2024c.
CoLLaVO: Crayon Large Language and Vision mOdel.
arXiv:2402.11248.
Lee, B.-K.; Park, B.; Kim, C. W.; and Ro, Y . M. 2024d.
MoAI: Mixture of All Intelligence for Large Language and
Vision Models. arXiv:2403.07508.
Li, B.; Wang, R.; Wang, G.; Ge, Y .; Ge, Y .; and
Shan, Y . 2023a. Seed-bench: Benchmarking multimodalllms with generative comprehension. arXiv preprint
arXiv:2307.16125 .
Li, J.; and Lu, W. 2024. A Survey on Benchmarks
of Multimodal Large Language Models. arXiv preprint
arXiv:2408.08632 .
Li, Y .; Du, Y .; Zhou, K.; Wang, J.; Zhao, W. X.; and Wen, J.-
R. 2023b. Evaluating Object Hallucination in Large Vision-
Language Models. arXiv:2305.10355.
Lin, T.-Y .; Maire, M.; Belongie, S.; Bourdev, L.; Girshick,
R.; Hays, J.; Perona, P.; Ramanan, D.; Zitnick, C. L.; and
Doll¬¥ar, P. 2015. Microsoft COCO: Common Objects in Con-
text. arXiv:1405.0312.
Liu, H.; Li, C.; Li, Y .; and Lee, Y . J. 2023a. Improved Base-
lines with Visual Instruction Tuning.
Liu, H.; Li, C.; Li, Y .; Li, B.; Zhang, Y .; Shen, S.; and Lee,
Y . J. 2024a. LLaV A-NeXT: Improved reasoning, OCR, and
world knowledge.
Liu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023b. Visual Instruc-
tion Tuning.
Liu, J.; Fan, X.; Huang, Z.; Wu, G.; Liu, R.; Zhong, W.; and
Luo, Z. 2022. Target-aware Dual Adversarial Learning and a
Multi-scenario Multi-Modality Benchmark to Fuse Infrared
and Visible for Object Detection. arXiv:2203.16220.
Liu, Y .; Duan, H.; Zhang, Y .; Li, B.; Zhang, S.; Zhao, W.;
Yuan, Y .; Wang, J.; He, C.; Liu, Z.; Chen, K.; and Lin, D.
2024b. MMBench: Is Your Multi-modal Model an All-
around Player? arXiv:2307.06281.
Mao, J.; Qian, Y .; Zhao, H.; and Wang, Y . 2023. Gpt-
driver: Learning to drive with gpt. arXiv preprint
arXiv:2310.01415 .
OpenAI. 2024. Hello GPT-4o. https://openai.com/index/
hello-gpt-4o/.
OpenGVLab. 2024. InternVL2: Better than the
Best‚ÄîExpanding Performance Boundaries of Open-Source
Multimodal Models with the Progressive Scaling Strategy.
https://internvl.github.io/blog/2024-07-02-InternVL-2.0/.
Ren, S.; Yao, L.; Li, S.; Sun, X.; and Hou, L. 2024.
Timechat: A time-sensitive multimodal large language
model for long video understanding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 14313‚Äì14323.
Roboflow. 2022. thermal dogs and people x6ejw
Dataset. https://universe.roboflow.com/object-detection/
thermal-dogs-and-people-x6ejw. Visited on 2023-03-29.
Shi, Y .; Gao, Y .; Lai, Y .; Wang, H.; Feng, J.; He, L.; Wan,
J.; Chen, C.; Yu, Z.; and Cao, X. 2024. Shield: An eval-
uation benchmark for face spoofing and forgery detection
with multimodal large language models. arXiv preprint
arXiv:2402.04178 .
Su, Y .; Lan, T.; Li, H.; Xu, J.; Wang, Y .; and Cai, D. 2023.
Pandagpt: One model to instruction-follow them all. arXiv
preprint arXiv:2305.16355 .
Team, G.; Georgiev, P.; Lei, V . I.; Burnell, R.; Bai, L.;
Gulati, A.; Tanzer, G.; Vincent, D.; Pan, Z.; Wang, S.;
et al. 2024. Gemini 1.5: Unlocking multimodal understand-
ing across millions of tokens of context. arXiv preprint
arXiv:2403.05530 .
Wang, W.; Lv, Q.; Yu, W.; Hong, W.; Qi, J.; Wang, Y .; Ji,
J.; Yang, Z.; Zhao, L.; Song, X.; et al. 2023. Cogvlm: Vi-
sual expert for pretrained language models. arXiv preprint
arXiv:2311.03079 .
Xu, P.; Shao, W.; Zhang, K.; Gao, P.; Liu, S.; Lei, M.; Meng,
F.; Huang, S.; Qiao, Y .; and Luo, P. 2023. LVLM-eHub:
A Comprehensive Evaluation Benchmark for Large Vision-
Language Models. arXiv:2306.09265.
Xu, Z.; Zhang, Y .; Xie, E.; Zhao, Z.; Guo, Y .; Wong, K.-
Y . K.; Li, Z.; and Zhao, H. 2024. Drivegpt4: Interpretable
end-to-end autonomous driving via large language model.
IEEE Robotics and Automation Letters .
Yarom, M.; Bitton, Y .; Changpinyo, S.; Aharoni, R.; Herzig,
J.; Lang, O.; Ofek, E.; and Szpektor, I. 2024. What you see is
what you read? improving text-image alignment evaluation.
Advances in Neural Information Processing Systems , 36.
Ye, J.; Hu, A.; Xu, H.; Ye, Q.; Yan, M.; Dan, Y .; Zhao, C.;
Xu, G.; Li, C.; Tian, J.; et al. 2023. mplug-docowl: Modu-
larized multimodal large language model for document un-
derstanding. arXiv preprint arXiv:2307.02499 .
Yu, W.; Yang, Z.; Li, L.; Wang, J.; Lin, K.; Liu, Z.; Wang, X.;
and Wang, L. 2023. MM-Vet: Evaluating Large Multimodal
Models for Integrated Capabilities. arXiv:2308.02490.
Zhang, P.; Dong, X.; Zang, Y .; Cao, Y .; Qian, R.; Chen, L.;
Guo, Q.; Duan, H.; Wang, B.; Ouyang, L.; Zhang, S.; Zhang,
W.; Li, Y .; Gao, Y .; Sun, P.; Zhang, X.; Li, W.; Li, J.; Wang,
W.; Yan, H.; He, C.; Zhang, X.; Chen, K.; Dai, J.; Qiao, Y .;
Lin, D.; and Wang, J. 2024. InternLM-XComposer-2.5: A
Versatile Large Vision Language Model Supporting Long-
Contextual Input and Output. arXiv:2407.03320."
Adapting Computer Vision Algorithms for Omnidirectional Video,['Hannes Fassold'],2019,http://arxiv.org/abs/1907.09233v1,"Adapting Computer Vision Algorithms
for Omnidirectional Video
Hannes Fassold
JOANNEUM RESEARCH, DIGITAL - Institute for Information and Communication Technologies
Steyrergasse 17
Graz, Austria 8010
hannes.fassold@joanneum.at
ABSTRACT
Omnidirectional (360) video has got quite popular because it pro-
vides a highly immersive viewing experience. For computer vision
algorithms, it poses several challenges, like the special (equirectan-
gular) projection commonly employed and the huge image size. In
this work, we give a high-level overview of these challenges and
outline strategies how to adapt computer vision algorithm for the
speci/f_ics of omnidirectional video.
CCS CONCEPTS
‚Ä¢Human-centered computing !Virtual reality; ‚Ä¢Computing
methodologies!Scene understanding;
KEYWORDS
omnidirectional video, VR, deep learning, object detection
1 INTRODUCTION
Omnidirectional (360) video content recently got very popular
in the media industry as well as in robotics, because it allows the
viewer to experience the content in an immersive and interactive
way. Omnidirectional consumer video cameras like the Samsung
Gear 360 or the Ricoh /T_heta V have multiple lenses and capture
images which cover the whole viewing sphere, typically in 4K or Ul-
traHD resolution. Omnidirectional videos are typically consumed
with a head-mounted display (HMD), so that the user is free to
choose the area (viewport) within the sphere he is currently inter-
ested in. /T_he whole viewing sphere is encoded in one 2D image for
each timepoint, usually in equirectangular projection [ 1]. Coordi-
nates on the viewing sphere are usually given in a longitude-latidue
representation (see Figure 1 for the relation between the viewing
sphere and the 2D image). In the following, the longitude is always
denoted by œïand has the range¬ª 180 ;180¬ºdegrees. /T_he latitude is
always denoted by Œ∏and has the range¬ª 90;90¬ºdegrees.
Content captured with an omnidirectional camera poses several
challenges for computer vision algorithms. Firstly, the captured
images have a high resolution (usually UltraHD with 3,840 x 2,120
pixel) as they have to cover the whole viewing sphere. /T_his leads
to a high processing time of the algorithm unless some adaption
strategies (like spatial subsampling) are employed . Secondly, due to
the equirectangular projection which is commonly employed, the
areas of the sphere which are away from the equator are stretched
in the image and the le/f_t and right image border represent adja-
cent regions in the viewing sphere. When applying an algorithm
naively to the input image in equirectangular projection, it likely
will produce non-optimal results.
Figure 1: Coordinate systems employed for an omnidirec-
tional image. Image courtesy of [5].
/T_herefore, strategies are needed in order to adapt computer vi-
sion algorithms designed for content captured with a conventional
camera to the speci/f_ics of omnidirectional video. /T_hese strategies
can be grouped into two categories, which we will name viewport-
centric processing andimage-centric processing . In the following, we
will describe the two categories and outline the strategies which
can be employed for each category.
2 VIEWPORT-CENTRIC PROCESSING
In viewport-centric processing, the whole viewing sphere is split
up into a set overlapping viewports fœÑkgso that the full sphere is
covered. For each viewport œÑk, the respective viewport image /v.altkin
rectilinear projection is rendered from the omnidirectional image
I. /T_he computer vision algorithm is now applied to each viewport
image/v.altk, giving the result rk(e.g. a list of detected objects for
an object detector or the denoised image for an image restoration
algorithm). /T_he set of per-viewport results rkhave to be fused
now into a single result Rand back-projected into an image in
equirectangular projection.
For an object detector (like YoloV3 [ 3]), the fusion step usually in-
volves some sort of non-maximum suppression, in order to suppress
multiple detections occurring at the same region of the viewing
sphere. For an image restoration algorithm where the result is the
enhanced image, the fusion and back-projection step can be done
via some sort of image blending. Speci/f_ically, the blending can be
done with an accumulator image and weight image in a similar
fashion as described in [ 4] for the image warping algorithm. For the
generation of the overlapping viewports, the Vogel method [6] can
be employed for generating approximately uniformly distributed
points on the viewing sphere, which will serve as the center of the
respective viewport. In Figure 2 a decomposition of the viewingarXiv:1907.09233v1  [cs.CV]  22 Jul 2019
Figure 2: Visualization of 240 overlapping viewports em-
ployed in viewport-centric processing (back-projected to
equirectangular projection).
sphere into 240 overlapping quadratical viewports with a FOV of
24is visualized.
For viewport-centric processing, the computer vision algorithm
can be applied without major adaptions as the viewport image /v.altk
has been rendered in the usual rectilinear projection employed
in the pinhole camera model. On the other hand, the decompo-
sition into overlapping viewports means that signi/f_icantly more
pixels have to be processed compared with image-centric process-
ing. Of course, the viewport images have also to be rendered (and
back-projected) in a reasonable resolution which takes a certain
amount of time, even on the GPU. Furthermore, a good strategy
for the fusion step (which is task-speci/f_ic) has to be researched and
implemented.
3 IMAGE-CENTRIC PROCESSING
In image-centric processing, the input image from the omnidirec-
tional video is processed by the speci/f_ic computer vision algorithm
as a whole, without the intermediate step of rendering a set a
viewport images. /T_his means that the existing computer vision
algorithm has to be adapted to the peculiarities of omnidirectional
images, speci/f_ically the equirectangular projection commonly em-
ployed and the adjacency (in the viewing sphere) of the le/f_t and
right image border regions. Without these adaptions, the quality
of the algorithm may be signi/f_icantly worse or even completely
unusable.
Processing the input image as a whole is usually much more
eÔ¨Écient than viewport-centric processing, as runtime-intensive
steps like the viewport rendering and the /f_inal back-projection are
not necessary and less pixels have to be processed. Furthermore,
many computer vision algorithm nowadays are GPU-accelerated,
and GPU-acceleration is usually more eÔ¨Äective (higher speedup
factor compared to CPU) for larger images.
/T_he kind of adaption strategy which is employed depends heavily
on the components employed within the computer vision algorithm.
E.g. for the image blur measure proposed in [ 2], the global blur
magnitude is calculated from a statistical analysis of the widths of
vertical image edges. In order to adapt this algorithm properly for an
input image in equirectangular projection, one has to compensate
Figure 3: Result of object detector YoloV3 [3] for an omnidi-
rectional image.
for the stretching of vertical image edges near the top and bo/t_tom
border of the image (the poles of the viewing sphere). /T_his can
be done by calculating a distortion map with the local stretching
factors and multiply the measured edge width with a compensation
factor derived from the distortion map.
State of the art object detectors like YoloV3 [ 3] seem to be quite
robust against the stretching induced by the equirectangular pro-
jection, but are prone to multiple detections of the same object if it
appears partially at the le/f_t and right image border. A possible adap-
tion strategy for that case would be to merge these double detection
via some sort of non-maximum suppression. /T_he non-maximum
suppression must take into account of course that the longitude œï
is cyclic. An exemplary result of the YoloV3 object detector for an
image in equirectangular projection can be seen in Figure 3. /T_he
receptive /f_ield of the object detector was set to 896 448 pixel in
order to account for the 2 : 1 aspect ratio of the input image.
4 CONCLUSION
In this work, we described the challenges omnidirectional video
poses for computer vision algorithms designed for conventional
video. Approaches for adapting these algorithms like viewport-
centric processing or image-centric processing were outlined, and
the advantages / disadvantages of each approach were discussed.
ACKNOWLEDGMENTS
/T_his work has received funding from the European Union‚Äôs Hori-
zon 2020 research and innovation programme, grant n761934,
Hyper360 (‚ÄúEnriching 360 media with 3D storytelling and person-
alisation elements‚Äù).
REFERENCES
[1]H. Lee, Y. Tateyama, and T. Ogi. 2010. Realistic visual environment for immersive
projection display system. In 2010 16th International Conference on Virtual Systems
and Multimedia . 128‚Äì132. h/t_tps://doi.org/10.1109/VSMM.2010.5665954
[2]P. Marziliano, F. Dufaux, S. Winkler, and T. Ebrahimi. 2002. A no-reference
perceptual blur metric. In Proceedings. International Conference on Image Processing ,
Vol. 3. III‚ÄìIII. h/t_tps://doi.org/10.1109/ICIP.2002.1038902
[3]Joseph Redmon and Ali Farhadi. 2018. YOLOv3: An Incremental Improvement.
CoRR abs/1804.02767 (2018). arXiv:1804.02767 h/t_tp://arxiv.org/abs/1804.02767
[4]J. Rosner, H. Fassold, P. Schallauer, and W. Bailer. 2010. Fast GPU-based image
warping and inpainting for frame interpolation. International Conferences on
Computer Graphics,Vision and Mathematics (2010).
2
[5]Tatsuya Suzuki and Takao Yamanaka. 2018. Saliency Map Estimation for Omni-
Directional Image Considering Prior Distributions. In IEEE International Confer-
ence on Systems, Man, and Cybernetics, SMC 2018, Miyazaki, Japan, October 7-10,
2018. 2079‚Äì2084. h/t_tps://doi.org/10.1109/SMC.2018.00358
[6]R. Swinbank and J. Purser. 1999. Fibonacci grids. AMS 13th Conference on Numerical
Weather Prediction (1999).
3"
