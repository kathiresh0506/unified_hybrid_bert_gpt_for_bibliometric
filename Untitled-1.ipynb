{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc531bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.39.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: py2neo in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2021.2.4)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.4.1+cu118)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.26.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (2024.7.4)\n",
      "Requirement already satisfied: interchange~=2021.0.4 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (2021.0.4)\n",
      "Requirement already satisfied: monotonic in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (1.6)\n",
      "Requirement already satisfied: pansi>=2020.7.3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (2024.11.0)\n",
      "Requirement already satisfied: pygments>=2.0.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (2.18.0)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (1.16.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from py2neo) (2.2.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kathi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas sentence-transformers transformers faiss-cpu py2neo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d04647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from py2neo import Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4060f861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\kathi\\AppData\\Local\\Temp\\ipykernel_20752\\2770397488.py:2: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  df = pd.read_csv(\"unified_hybrid_bert_gpt_for_bibliometric\\papers_enriched.csv\")\n",
      "C:\\Users\\kathi\\AppData\\Local\\Temp\\ipykernel_20752\\2770397488.py:3: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  index = faiss.read_index(\"unified_hybrid_bert_gpt_for_bibliometric\\semantic_index.faiss\")\n",
      "C:\\Users\\kathi\\AppData\\Local\\Temp\\ipykernel_20752\\2770397488.py:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  with open(\"unified_hybrid_bert_gpt_for_bibliometric\\semantic_titles.pkl\", \"rb\") as f:\n",
      "c:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a52e393ed66427e97754ff72479ea1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kathi\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7daffd58684712a216017c660766ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b872222caa4ce88c053832eab55c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916ce32e334e4ca9a18a5d90167eb6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bdc8e5d5154e109f923ebd8dfb192f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79eb05a7fbc14cdfbc36b516ca4932a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load your data and FAISS index\n",
    "df = pd.read_csv(\"unified_hybrid_bert_gpt_for_bibliometric\\papers_enriched.csv\")\n",
    "index = faiss.read_index(\"unified_hybrid_bert_gpt_for_bibliometric\\semantic_index.faiss\")\n",
    "\n",
    "# Load titles\n",
    "with open(\"unified_hybrid_bert_gpt_for_bibliometric\\semantic_titles.pkl\", \"rb\") as f:\n",
    "    titles = pickle.load(f)\n",
    "\n",
    "# Load sentence embedding model\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba66a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_doc(query, k=1):\n",
    "    vec = embedder.encode([query])\n",
    "    D, I = index.search(np.array(vec), k)\n",
    "\n",
    "    results = []\n",
    "    for idx in I[0]:\n",
    "        title = df.iloc[idx][\"title\"]\n",
    "        abstract = df.iloc[idx][\"clean_abstract\"]\n",
    "        results.append({\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14144ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Neo4j\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"qwertyuiop\"))\n",
    "\n",
    "def get_graph_info(title):\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Paper {title: $title})\n",
    "    OPTIONAL MATCH (p)<-[:WROTE]-(a:Author)\n",
    "    OPTIONAL MATCH (p)-[:HAS_KEYWORD]->(k:Keyword)\n",
    "    OPTIONAL MATCH (p)-[:MENTIONS]->(e:Entity)\n",
    "    RETURN \n",
    "        collect(DISTINCT a.name) AS authors,\n",
    "        collect(DISTINCT k.name) AS keywords,\n",
    "        collect(DISTINCT e.name) AS entities\n",
    "    \"\"\"\n",
    "    result = graph.run(query, title=title).data()\n",
    "    if result:\n",
    "        return result[0]\n",
    "    return {\"authors\": [], \"keywords\": [], \"entities\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "240fed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def generate_summary_with_graph(doc, graph_data):\n",
    "    graph_context = \"\"\n",
    "    if graph_data[\"authors\"]:\n",
    "        graph_context += \"Authors: \" + \", \".join(graph_data[\"authors\"]) + \"\\n\"\n",
    "    if graph_data[\"keywords\"]:\n",
    "        graph_context += \"Keywords: \" + \", \".join(graph_data[\"keywords\"]) + \"\\n\"\n",
    "    if graph_data[\"entities\"]:\n",
    "        graph_context += \"Entities: \" + \", \".join(graph_data[\"entities\"]) + \"\\n\"\n",
    "\n",
    "    full_input = graph_context + \"\\n\" + doc[\"abstract\"]\n",
    "    input_trimmed = full_input[:2000]\n",
    "\n",
    "    summary = summarizer(input_trimmed, max_length=150, min_length=40, do_sample=False)\n",
    "    return summary[0][\"summary_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629865b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Title: A Survey of Large Language Models\n",
      "üìÑ Abstract: language is essentially a complex intricate system of human expressions governed by grammatical rules it poses a significant challenge to develop capable ai algorithms for comprehending and grasping a language as a major approach language modeling has been widely studied for language understanding and generation in the past two decades evolving from statistical language models to neural language models recently pretrained language models plms have been proposed by pretraining transformer models over largescale corpora showing strong capabilities in solving various nlp tasks since researchers have found that model scaling can lead to performance improvement they further study the scaling effect by increasing the model size to an even larger size interestingly when the parameter scale exceeds a certain level these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in smallscale language models to discriminate the difference in parameter scale the research community has coined the term large language models llm for the plms of significant size recently the research on llms has been largely advanced by both academia and industry and a remarkable progress is the launch of chatgpt which has attracted widespread attention from society the technical evolution of llms has been making an important impact on the entire ai community which would revolutionize the way how we develop and use ai algorithms in this survey we review the recent advances of llms by introducing the background key findings and mainstream techniques in particular we focus on four major aspects of llms namely pretraining adaptation tuning utilization and capacity evaluation besides we also summarize the available resources for developing llms and discuss the remaining issues for future directions\n",
      "\n",
      "üìä Graph Facts: {'authors': ['Wayne Xin Zhao', 'Kun Zhou', 'Junyi Li', 'Tianyi Tang', 'Xiaolei Wang', 'Yupeng Hou', 'Yingqian Min', 'Beichen Zhang', 'Junjie Zhang', 'Zican Dong', 'Yifan Du', 'Yang Chen', 'Yushuo Chen', 'Zhipeng Chen', 'Jinhao Jiang', 'Ruiyang Ren', 'Yifan Li', 'Xinyu Tang', 'Zikang Liu', 'Peiyu Liu', 'Jian‚ÄêYun Nie', 'Ji-Rong Wen'], 'keywords': ['language models', 'large language', 'language modeling', 'smallscale language', 'enlarged language'], 'entities': ['models', 'model', 'performance', 'complex', 'abilities', 'language', 'tasks', 'adaptation', 'level', 'chatgpt', 'research', 'evaluation', 'human', 'capabilities', 'llms', 'study', 'improvement', 'coined', 'findings', 'directions', 'expressions', 'parameter scale', 'discriminate', 'community', 'impact', 'enlarged', 'smallscale', 'comprehending', 'capacity', 'significant', 'largescale corpora', 'grasping', 'plms', 'grammatical rules', 'launch', 'neural language', 'survey', 'evolution', 'modeling', 'generation', 'nlp', 'decades', 'progress', 'evolving', 'significant size', 'pretraining', 'utilization', 'statistical language', 'mainstream techniques', 'technical', 'industry', 'researchers', 'society', 'academia', 'size', 'transformer models', 'scaling', 'scaling effect', 'resources', 'increasing', 'algorithms']}\n",
      "\n",
      "üß† Enriched Summary:\n",
      " language is essentially a complex intricate system of human expressions governed by grammatical rules it poses a significant challenge to develop capable ai algorithms for comprehending and grasping a language. Language modeling has been widely studied for language understanding and generation in the past two decades evolving from statistical language models to neural language models recently pretrained language models plms have been proposed by pretraining transformer models over largescale corpora.\n"
     ]
    }
   ],
   "source": [
    "query = input(\"üîç Enter a research topic: \")\n",
    "\n",
    "# Step 1: Retrieve paper\n",
    "top_doc = retrieve_top_doc(query, k=1)[0]\n",
    "print(\"\\nüìÑ Title:\", top_doc[\"title\"])\n",
    "print(\"üìÑ Abstract:\", top_doc[\"abstract\"])\n",
    "\n",
    "# Step 2: Get graph info\n",
    "facts = get_graph_info(top_doc[\"title\"])\n",
    "print(\"\\nüìä Graph Facts:\", facts)\n",
    "\n",
    "# Step 3: Generate enriched summary\n",
    "summary = generate_summary_with_graph(top_doc, facts)\n",
    "print(\"\\nüß† Enriched Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12d6e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7824574efa614e2b8c505b0d5c839646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'The model `mixtral-8x7b-32768` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 179\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    178\u001b[0m     user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müîç Enter your research topic: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 179\u001b[0m     \u001b[43mrun_all_and_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 169\u001b[0m, in \u001b[0;36mrun_all_and_display\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_all_and_display\u001b[39m(query):\n\u001b[1;32m--> 169\u001b[0m     \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     print_enriched_papers()\n\u001b[0;32m    171\u001b[0m     print_clusters()\n",
      "Cell \u001b[1;32mIn[4], line 164\u001b[0m, in \u001b[0;36mrun_all\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m    162\u001b[0m build_knowledge_graph()\n\u001b[0;32m    163\u001b[0m cluster_texts()\n\u001b[1;32m--> 164\u001b[0m \u001b[43msummarize_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m visualize_graph()\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Pipeline complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 104\u001b[0m, in \u001b[0;36msummarize_clusters\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m     text_blob \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(texts)[:\u001b[38;5;241m4000\u001b[39m]\n\u001b[0;32m    100\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead the following full research texts and provide a scholarly summary of their bibliometric insights \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincluding methods, findings, and trends:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text_blob\n\u001b[0;32m    103\u001b[0m     )\n\u001b[1;32m--> 104\u001b[0m     summaries[cluster] \u001b[38;5;241m=\u001b[39m \u001b[43mquery_groq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_summaries.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    107\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(summaries, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 86\u001b[0m, in \u001b[0;36msummarize_clusters.<locals>.query_groq\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery_groq\u001b[39m(prompt):\n\u001b[1;32m---> 86\u001b[0m     chat_completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmixtral-8x7b-32768\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a bibliometric research assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    922\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    923\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    924\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1227\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1236\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1237\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1238\u001b[0m     )\n\u001b[1;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\kathi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'The model `mixtral-8x7b-32768` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re, json, ast, torch, os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from keybert import KeyBERT\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from openai import OpenAI\n",
    "from pyvis.network import Network\n",
    "import webbrowser\n",
    "\n",
    "# === MODEL SETUP ===\n",
    "bert_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "kw_model = KeyBERT(model='bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()\n",
    "\n",
    "# === GROQ CLIENT SETUP ===\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",  # Replace with yours\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "# === NEO4J SETUP ===\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"qwertyuiop\"))\n",
    "\n",
    "# === TEXT CLEANING ===\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r\"10\\.\\d{4,9}/[-._;()/:a-z0-9]+\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def is_valid_keyword(k):\n",
    "    return k.isalpha() and len(k) > 2 and not k.startswith(\"http\")\n",
    "\n",
    "# === FETCHING ===\n",
    "def fetch_and_save_papers(query, max_results=10):\n",
    "    from fetch_arxiv import fetch_arxiv_papers\n",
    "    df = fetch_arxiv_papers(query, max_results=max_results)\n",
    "    df = df.dropna(subset=[\"full_text\"])\n",
    "    df.to_csv(\"papers3.csv\", index=False)\n",
    "\n",
    "# === ENRICH ===\n",
    "def enrich_papers():\n",
    "    df = pd.read_csv(\"papers3.csv\")\n",
    "    df[\"clean_text\"] = df[\"full_text\"].fillna(\"\").apply(clean_text)\n",
    "    df[\"keywords\"] = df[\"clean_text\"].apply(\n",
    "        lambda x: [kw[0] for kw in kw_model.extract_keywords(x, top_n=10) if is_valid_keyword(kw[0])] if x else []\n",
    "    )\n",
    "    df.to_csv(\"papers_enriched.csv\", index=False)\n",
    "\n",
    "# === GRAPH ===\n",
    "def build_knowledge_graph():\n",
    "    df = pd.read_csv(\"papers_enriched.csv\")\n",
    "    graph.delete_all()\n",
    "    for _, row in df.iterrows():\n",
    "        paper_node = Node(\"Paper\", title=row[\"title\"], year=row[\"year\"], doi=row[\"doi\"])\n",
    "        graph.create(paper_node)\n",
    "        for author in ast.literal_eval(row[\"authors\"]):\n",
    "            author_node = Node(\"Author\", name=author)\n",
    "            graph.merge(author_node, \"Author\", \"name\")\n",
    "            graph.create(Relationship(author_node, \"WROTE\", paper_node))\n",
    "        for keyword in ast.literal_eval(row[\"keywords\"]):\n",
    "            keyword_node = Node(\"Keyword\", name=keyword)\n",
    "            graph.merge(keyword_node, \"Keyword\", \"name\")\n",
    "            graph.create(Relationship(paper_node, \"HAS_KEYWORD\", keyword_node))\n",
    "\n",
    "# === CLUSTER ===\n",
    "def cluster_texts():\n",
    "    df = pd.read_csv(\"papers_enriched.csv\").dropna(subset=[\"clean_text\"])\n",
    "    embeddings = bert_encoder.encode(df[\"clean_text\"].tolist(), show_progress_bar=True)\n",
    "    gmm = GaussianMixture(n_components=5, random_state=42)\n",
    "    df[\"cluster\"] = gmm.fit_predict(embeddings)\n",
    "    df.to_csv(\"clustered_papers.csv\", index=False)\n",
    "\n",
    "# === SUMMARIZE (using new Groq-compatible SDK call) ===\n",
    "def summarize_clusters():\n",
    "    df = pd.read_csv(\"clustered_papers.csv\")\n",
    "    grouped = df.groupby(\"cluster\")[\"clean_text\"].apply(list).to_dict()\n",
    "\n",
    "    def query_groq(prompt):\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=\"mixtral-8x7b-32768\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a bibliometric research assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.5,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "\n",
    "    summaries = {}\n",
    "    for cluster, texts in grouped.items():\n",
    "        text_blob = \" \".join(texts)[:4000]\n",
    "        prompt = (\n",
    "            \"Read the following full research texts and provide a scholarly summary of their bibliometric insights \"\n",
    "            \"including methods, findings, and trends:\\n\" + text_blob\n",
    "        )\n",
    "        summaries[cluster] = query_groq(prompt)\n",
    "\n",
    "    with open(\"cluster_summaries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summaries, f, indent=2)\n",
    "\n",
    "# === VISUALIZE ===\n",
    "def visualize_graph():\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Paper)-[:HAS_KEYWORD]->(k:Keyword)\n",
    "    RETURN p.title AS title, k.name AS keyword LIMIT 100\n",
    "    \"\"\"\n",
    "    results = graph.run(query).data()\n",
    "    net = Network(notebook=False, cdn_resources='in_line')\n",
    "    for record in results:\n",
    "        net.add_node(record[\"title\"], label=record[\"title\"], shape=\"box\", color=\"lightblue\")\n",
    "        net.add_node(record[\"keyword\"], label=record[\"keyword\"], shape=\"dot\", color=\"orange\")\n",
    "        net.add_edge(record[\"title\"], record[\"keyword\"])\n",
    "    html_content = net.generate_html()\n",
    "    with open(\"graph.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_content)\n",
    "    print(\"\\n‚úÖ Interactive graph saved as 'graph.html'.\")\n",
    "\n",
    "# === DISPLAY FUNCTIONS ===\n",
    "def print_enriched_papers():\n",
    "    print(\"\\nüìÑ Top Enriched Papers with Keywords:\")\n",
    "    df = pd.read_csv(\"papers_enriched.csv\")\n",
    "    for idx, row in df.head(5).iterrows():\n",
    "        print(f\"\\nüìò {row['title']}\\nüóìÔ∏è Year: {row['year']}\")\n",
    "        keywords = eval(row[\"keywords\"]) if isinstance(row[\"keywords\"], str) else row[\"keywords\"]\n",
    "        print(f\"üîë Keywords: {', '.join(keywords)}\")\n",
    "\n",
    "def print_clusters():\n",
    "    print(\"\\nüß† Papers by Cluster:\")\n",
    "    df = pd.read_csv(\"clustered_papers.csv\")\n",
    "    for cluster in sorted(df['cluster'].unique()):\n",
    "        print(f\"\\n--- Cluster {cluster} ---\")\n",
    "        papers = df[df['cluster'] == cluster].head(3)\n",
    "        for _, row in papers.iterrows():\n",
    "            print(f\"‚Ä¢ {row['title']}\")\n",
    "\n",
    "def print_summaries():\n",
    "    print(\"\\nüìù Cluster Summaries:\")\n",
    "    with open(\"cluster_summaries.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        summaries = json.load(f)\n",
    "    for cid, summary in summaries.items():\n",
    "        print(f\"\\n--- üß© Cluster {cid} Summary ---\\n{summary.strip()}\\n\")\n",
    "\n",
    "def print_cluster_summaries():\n",
    "    print(\"\\nüìö Bibliometric Summaries by Cluster:\")\n",
    "    with open(\"cluster_summaries.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        summaries = json.load(f)\n",
    "        for cluster, summary in summaries.items():\n",
    "            print(f\"\\nüß† Cluster {cluster}:\\n{summary}\")\n",
    "\n",
    "# === PIPELINE WRAPPERS ===\n",
    "def run_all(query):\n",
    "    fetch_and_save_papers(query)\n",
    "    enrich_papers()\n",
    "    build_knowledge_graph()\n",
    "    cluster_texts()\n",
    "    summarize_clusters()\n",
    "    visualize_graph()\n",
    "    print(\"‚úÖ Pipeline complete.\")\n",
    "\n",
    "def run_all_and_display(query):\n",
    "    run_all(query)\n",
    "    print_enriched_papers()\n",
    "    print_clusters()\n",
    "    print_summaries()\n",
    "    print_cluster_summaries()\n",
    "    webbrowser.open(\"graph.html\")\n",
    "\n",
    "# === ENTRY POINT ===\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = input(\"üîç Enter your research topic: \")\n",
    "    run_all_and_display(user_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fe32af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
