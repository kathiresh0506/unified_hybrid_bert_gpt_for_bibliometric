title,authors,year,doi,full_text,clean_text,keywords
"Implications of Computer Vision Driven Assistive Technologies Towards
  Individuals with Visual Impairment","['Linda Wang', 'Alexander Wong']",2019,http://arxiv.org/abs/1905.07844v1,"Implications of Computer Vision Driven Assistive Technologies Towards
Individuals with Visual Impairment
Linda Wang and Alexander Wong
Waterloo Artiï¬cial Intelligence Institute
University of Waterloo
fly8wang,a28wong g@uwaterloo.ca
Abstract
Computer vision based technology is becoming ubiqui-
tous in society. One application area that has seen an in-
crease in computer vision is assistive technologies, speciï¬-
cally for those with visual impairment. Research has shown
the ability of computer vision models to achieve tasks such
provide scene captions, detect objects and recognize faces.
Although assisting individuals with visual impairment with
these tasks increases their independence and autonomy,
concerns over bias, privacy and potential usefulness arise.
This paper addresses the positive and negative implications
computer vision based assistive technologies have on indi-
viduals with visual impairment, as well as considerations
for computer vision researchers and developers in order to
mitigate the amount of negative implications.
1. Introduction
In recent years, the rise of deep learning has made previ-
ously unsolvable tasks possible. One particular area where
deep learning has made tremendous progress is computer
vision, such as in image recognition, object detection and
image understanding. As computer vision results are be-
coming more promising, larger issues regarding the use of
this technology need to be considered. An important area to
consider is assistive technologies for those with visual im-
pairments, as computer vision technologies have the poten-
tial to aid in tasks where previous solutions have struggled.
Although there are positive aspects of computer vision
applications, there are also negative aspects that should be
addressed. The use of black box artiï¬cial intelligence so-
lutions raises many concerns such as fairness and bias of
the models [13]. There are also ethical concerns related to
privacy protection as many computer vision models rely on
camera input. In addition, the exclusion of certain groups
during the development process may also lead to negative
aspects of the technology [15] and as a result, lead to low
adoption rates.
We thank NSERC, Canada Research Chairs program, and Microsoft.
Figure 1. Positive implications: computer vision-based devices al-
low blind individuals to navigate independently, recognize faces
and read text, which helps them overcome social barriers.
As AI is becoming more ubiquitous, it is crucial to ad-
dress issues related to the implications of AI, speciï¬cally
computer vision driven assistive technology towards indi-
viduals with visual impairment. The goal of the paper is
to review what implications computer vision has on assis-
tive technologies for individuals with visual impairment and
considerations for computer vision researchers. The paper
will be guided by the following questions:
What are the positive and negative aspects of using
computer vision in assistive technologies with respect
to the impact on the lives of individuals with visual
impairment?
What should researchers consider while conducting
computer vision research to reduce negative implica-
tions of AI-powered assistive technology on the lives
of individuals with visual impairment?
2. Positive Implications
Vision impairment and blindness cause a considerable
amount of economic and emotional burden for not only
the affected persons but also their caregivers and society at
large [11]. The recent rise in computer vision based assis-
tive technologies show the potential to reduce some burden
placed on the individuals, as well as on caregivers and so-
ciety. By assisting visually impaired individuals with tasks
1arXiv:1905.07844v1  [cs.CV]  20 May 2019
Negative Implications
Gender
Age Bias
Race/Ethnicity
Exploitation of personal
information
Obtrusiveness of camerasPrivacyTradeoff between autonomy
and privacy costs
Poor device evaluation
Age and condition dependentExclusion in
development
processInefï¬ciency in development
process
Table 1. Negative implications: bias in computer vision algo-
rithms, privacy concerns related to data collection and cameras,
and exclusion in development process.
they would otherwise need help in, as shown in Figure 1,
their level of independence and autonomy are increased.
Overcoming social barriers: One area assistive tech-
nologies have become an integral part in the lives of those
with visual impairment is overcoming barriers faced in ev-
eryday life. These individuals face adversity in all stages of
life. For instance, severely visually impaired young people
use their assistive technology as more than just a device to
overcome environmental barriers but also a means of com-
munication for peers in their school [18].
Face recognition and optical character recognition:
The ever growing presence of smartphones and advance-
ments in computer vision are transforming the accessibil-
ity of assistive technologies, allowing individuals to over-
come social barriers and have autonomy over when and how
they access information. Smartphone applications, such as
SeeingAI and Lookout, use auditory cues to assist users
in identifying scenes, recognizing faces, reading short text,
documents and currency [8, 6].
Navigation assistance: Individuals with visual impair-
ment also face difï¬culty localizing themselves in unknown
indoor and outdoor environments. Research projects are us-
ing cameras and sensors to give directions so these indi-
viduals can navigate outdoor and indoor environments in-
dependently. For instance, a prototype was developed for
guiding the visually impaired across streets in a straight line
using a wearable computer-based orientation and wayï¬nd-
ing aid [16]. For indoor navigation, Tian et al. developed a
proof of concept computer-vision based indoor wayï¬nding
aid that detects doors and elevators, as well as text on signs,
to ï¬nd different rooms [19].
3. Negative Implications
Although the advancement of technology is evident, only
a limited number of assistive technology solutions haveemerged to make a social or economic impact and im-
prove quality of life. Fundamental challenges, such as those
shown in Table 1, are still be to thoroughly addressed before
deploying into assistive technologies.
Bias: In machine learning, bias refers to statistics that
lead to a skew and as a result, brings an unjust outcome
for a population [13]. Bias often stems from training data
sample sets that are non-representative of the general popu-
lation. When algorithms are trained with biased data, they
are inherently bound to produce skewed results [5].
One of the biggest implications in applying AI systems
with bias is the potential for adversely impacting already
marginalized groups. In 2012, Klare et al. conducted a
study on the inï¬‚uence of gender, race/ethnicity and age
on the performance of six different face recognition algo-
rithms, three of which are commercial [10]. The results
found that there are lower matching accuracies for females
than males, Blacks compared to other race/ethnicities, and
18 to 30 year olds compared to other age groups.
In recent years, the low errors rates achieved by fa-
cial recognition models led to even more commercializa-
tion. However, studies have shown consistent bias in ar-
eas of gender, race and age from these commercial mod-
els. Buolamwini and Gebru evaluated bias present in
three commercial automated facial analysis algorithms from
IBM, Microsoft and Megvii with respect to phenotypic sub-
groups [5]. The results showed that there is a signiï¬cant
drop in performance of state of the art models when applied
to images of a particular gender and/or ethnicity group. For
instance, male subjects were more accurately classiï¬ed than
females and lighter subjects were more accurately classiï¬ed
than darker subjects. All three commercial classiï¬er per-
formed the worst on darker female subjects.
Raji and Buolamwini conducted a second audit of com-
mercial facial analysis models [14]. In this study, perfor-
mances from target companies, ones that were in the ï¬rst
audit, and non-target companies, Amazon and Kairos, are
presented. The results showed all targets had the greatest
reduction in error rates for female and darker faces. In terms
of non-target companies, the performance results were sim-
ilar to the ï¬rst audit, with the largest disparity gap between
black females and white males.
Although the awareness of disparity improved the facial
recognition models from target companies and produced a
lower error rate than non-target companies, the commercial-
ization of these models before evaluating biases and poten-
tial impacts on protected groups raises a concern.
Privacy: As shown in the Section 2, computer vision
based assistive technologies for the visually impaired allow
these individuals to gain independence and autonomy over
different aspects of their life. However, these devices also
pose privacy risks because of the vast amounts of personal
data stored. Although individuals with visual impairment
felt that smartphones help them communicate and achieve
greater independence, these devices create privacy risks be-
cause of the amount of personal data stored. As well, their
poor visual acuity makes it hard to safeguard their informa-
tion, such as if someone is around and eavesdropping [4].
Home-monitoring for older adults, who represent major-
ity of those with visual impairment [1], reliefs caregivers
burden and allows individuals with severe visual impair-
ment to live independently, but the devices for monitor-
ing also store personal data. Studies have found that older
adults are willing to have activity monitoring shared with
family members and doctors if the collected data is useful,
but expressed that the greatest concern is exploitation and
misuse of their personal health information [9].
Based on the studies, the greatest fear associated with the
collection of personal data is the concern that their collected
data could end up in the wrong hands and be misused. In
addition to the fear of personal information being exploited,
the use of cameras is obtrusive and found to elicit greater
fears than wearable solutions. In a comparison of four am-
bient intelligent systems, the camera-based behaviour and
emergency detection system was perceived with the great-
est fear and highest level of concern [9]. However, studies
have also shown that there is a tradeoff between gained au-
tonomy and privacy costs. Older adults with lower levels of
functioning are willing to accept video cameras and trade-
off the privacy lost if camera-based solution could prevent
transfer to a long term care facility [20].
The different perceptions of privacy over the use of data,
as well as the potential beneï¬ts of using cameras for home
monitoring, suggest that privacy is a complex topic. Under-
standing the variables that inï¬‚uence privacy concerns and
how these concerns can be mediated by potential beneï¬ts
are important when developing computer vision based as-
sistive technologies.
Exclusion in development process: The main goal of
assistive technologies is to improve the lives of end users.
However, when the design of form or function of the tech-
nology is poor, or when inequality exists between techno-
logical accessibility, the lives of those affected can be nega-
tively impacted, as well as perceptions of their abilities [12].
For instance, a device that has good design, usability and ac-
cessibility can be poorly evaluated. The userâ€™s lifestyle and
aspirations have to be taken into consideration to receive a
positive user evaluation [12].
The lifestyle and desired function of assistive technolo-
gies depend on age and level of adaption to their condition.
A predominant want for young disabled people is the sig-
niï¬cance of being ordinary [18]. No matter the degree of
visual impairment, all the participants expressed that inclu-
sion by peers and being ordinary is a big part in their daily
lives [18]. In addition to age, how the user has adapted to
their condition also impacts the desired functionality of the
Figure 2. Design considerations for computer vision researchers.
assistive technology. As users become more accustomed to
their condition, they may prefer to perform some activities
independently [16].
Not only does including users during the development
process point out which areas to focus on, but also saves
development time. When testing the usability of the indoor
wayï¬nding device on blind participants, the researchers
found that the participants were able to ï¬nd doors without
any problem since the participants use canes, and realized
that text localization and recognition were more useful for
indoor navigation [19]. By including the users earlier in
the development process could have identiï¬ed that locating
doors are not a problem and use the saved time to address
text localization and recognition.
4. Design Considerations for Researchers
Computer vision has the potential to impact peopleâ€™s
lives. However, just algorithmic advances to the accuracies
of computer vision models are insufï¬cient for assistive tech-
nologies, which interact with and around humans. Recently,
the term human-centered artiï¬cial intelligence is used to re-
fer to intelligent systems that are aware of the interaction
with humans and are designed with social responsibility in
mind [15]. As researchers, it is important to uphold soci-
etyâ€™s moral and legal obligations to treat citizens fairly, es-
pecially those in protected groups that face discrimination.
Figure 2 illustrates some considerations to reduce the nega-
tive implications mentioned in Section 3.
Bias mitigation: One method to uphold fairness is by
mitigating bias. For instance, researchers can use tools,
such as Googleâ€™s What-If tool [3] and IBM AI Fairness 360
kit [2], to analyze and identify unwanted bias in datasets and
ML models in order to mitigate such bias. For age, gender
and ethnicity, there are different methods to reduce the neg-
ative impacts of bias. Das et al. proposed a Multi-Task Con-
volution Neural Network that employs joint dynamic loss
weight adjustments to minimize bias when classifying gen-
der, age and race [7]. There are also methods to reduce bias
at the dataset level. Salimi et al. introduced a database re-
pair algorithm, which uses causal pre-processing to reduce
or eliminate sources of discrimination for fair ML [17].
Disability discrimination: Like age, gender and race,
disability status is also a protected characteristic. How-
ever, disability discrimination has not been explored in lit-
erature [21]. Similar to under-representation of age, gender
and racial groups in datasets, as shown in Section 3, there
is also potential for under-representation of individuals with
disabilities. Ways to mitigate disability bias have also not
been explored. Compared to gender, race and age, gather-
ing a balanced training dataset is not enough to address the
biased outcomes for those with a disability [21]. The many
different forms and degrees of disability makes it difï¬cult
for a machine learning model to ï¬nd patterns, form groups
and generalize. With the rise of machine learning based
assistive technologies, understanding and assessing the im-
pact towards people with disabilities is crucial, especially
since disability bias has not been widely explored.
Inclusion of end users: Taking into account where end
users will use the assistive technologies, as well as the needs
and goals, a task speciï¬c training set and appropriate model
architecture can allow computer vision based devices to be
perceived as useful, allowing individuals to gain indepen-
dence and autonomy. Based on the studies mentioned in
Section 3, users are willing to tradeoff privacy for more au-
tonomy. Thus, by including users in the development pro-
cess, the devices will be perceived as more useful, gaining
more adoption since users are willing to tradeoff privacy
concerns to have more independence and autonomy.
Diverse skill set: The ethical implications presented in
this paper are difï¬cult to address by just computer vision
researchers. Instead, a team with a diverse set of skills is
required to address both the positive and negative impli-
cations of an assistive technology. The underlying bias in
the models can cause protected groups to feel more iso-
lated. Researchers should be aware of the possible biases
the dataset and algorithm may have before the system be-
comes commercialized and interacts with people in every-
day context. In addition to bias, the use of cameras raise
privacy concerns over who has access to the data stored
and the amount of security measures taken to protect per-
sonal data. Before deployment, developers should ensure
that measures are in place to reduce the chances of data
exploitation. By understanding the needs and goals of in-
dividuals with visual impairment, designers can effectively
address these requirements in the design of the computer
vision system, resulting in a more useful device for the end
users.
References
[1] Blindness and vision impairment, Oct 2018. 3
[2] Introducing ai fairness 360, Sep 2018. 3
[3] The what-if tool, Sep 2018. 3
[4] T. Ahmed, R. Hoyle, K. Connelly, D. Crandall, and A. Ka-
padia. Privacy concerns and behaviors of people with vi-sual impairments. In Proceedings of the 33rd Annual ACM
Conference on Human Factors in Computing Systems , pages
3523â€“3532, 2015. 3
[5] J. Buolamwini and T. Gebru. Gender shades: Intersectional
accuracy disparities in commercial gender classiï¬cation. In
S. A. Friedler and C. Wilson, editors, Proceedings of the 1st
Conference on FAT , volume 81 of PMLR , pages 77â€“91, 23â€“
24 Feb 2018. 2
[6] P. Clary. Lookout: an app to help blind and visually impaired
people learn about their surroundings, May 2018. 2
[7] A. Das, A. Dantcheva, and F. Bremond. Mitigating Bias in
Gender, Age and Ethnicity Classiï¬cation: a Multi-Task Con-
volution Neural Network Approach. In ECCVW 2018 , Sept.
2018. 3
[8] S. Kelley. Seeing ai: Artiï¬cial intelligence for blind and
visually impaired users, 2019. 2
[9] F. Kirchbuchner, T. Grosse-Puppendahl, M. R. Hastall,
M. Distler, and A. Kuijper. Ambient intelligence from se-
nior citizensâ€™ perspectives. In Ambient Intelligence , pages
48â€“59, Cham, 2015. Springer International Publishing. 3
[10] B. F. Klare, M. J. Burge, J. C. Klontz, R. W. V order Bruegge,
and A. K. Jain. Face recognition performance: Role of de-
mographic information. IEEE Transactions on Information
Forensics and Security , 7(6):1789â€“1801, Dec 2012. 2
[11] J. K Â¨oberlein, K. Beifus, C. Schaffert, and R. P. Finger. The
economic burden of visual impairment and blindness: a sys-
tematic review. BMJ Open , 3(11), 2013. 1
[12] M. Leo, G. Medioni, M. Trivedi, T. Kanade, and G. Farinella.
Computer vision for assistive technologies. Computer Vision
and Image Understanding , 154:1 â€“ 15, 2017. 3
[13] K. Lloyd. Bias ampliï¬cation in artiï¬cial intelligence sys-
tems. CoRR , abs/1809.07842, 2018. 1, 2
[14] I. D. Raji and J. Buolamwini. Actionable auditing: Inves-
tigating the impact of publicly naming biased performance
results of commercial ai products. In Conference on AIES ,
2019. 2
[15] M. O. Riedl. Human-centered artiï¬cial intelligence and ma-
chine learning. CoRR , abs/1901.11184, 2019. 1, 3
[16] D. A. Ross. Implementing assistive technology on wear-
able computers. IEEE Intelligent Systems , 16(3):47â€“53, May
2001. 2, 3
[17] B. Salimi, L. Rodriguez, B. Howe, and D. Suciu. Capuchin:
Causal database repair for algorithmic fairness, 2019. 4
[18] S. S Â¨oderstr Â¨om and B. Ytterhus. The use and nonuse of as-
sistive technologies from the world of information and com-
munication technology by visually impaired young people: a
walk on the tightrope of peer inclusion. Disability & Society ,
25(3):303â€“315, 2010. 2, 3
[19] Y . Tian, X. Yang, C. Yi, and A. Arditi. Toward a computer
vision-based wayï¬nding aid for blind persons to access un-
familiar indoor environments. Machine vision and applica-
tions , 24:521â€“535, 04 2013. 2, 3
[20] D. Townsend, F. Knoefel, and R. Goubran. Privacy ver-
sus autonomy: A tradeoff model for smart home monitoring
technologies. In 2011 IEEE EMBC , pages 4749â€“4752, Aug
2011. 3
[21] S. Trewin. AI fairness for people with disabilities: Point of
view. CoRR , abs/1811.10670, 2018. 4","implications of computer vision driven assistive technologies towards
individuals with visual impairment
linda wang and alexander wong
waterloo articial intelligence institute
university of waterloo
fly8wanga28wong guwaterlooca
abstract
computer vision based technology is becoming ubiqui
tous in society one application area that has seen an in
crease in computer vision is assistive technologies speci
cally for those with visual impairment research has shown
the ability of computer vision models to achieve tasks such
provide scene captions detect objects and recognize faces
although assisting individuals with visual impairment with
these tasks increases their independence and autonomy
concerns over bias privacy and potential usefulness arise
this paper addresses the positive and negative implications
computer vision based assistive technologies have on indi
viduals with visual impairment as well as considerations
for computer vision researchers and developers in order to
mitigate the amount of negative implications
1 introduction
in recent years the rise of deep learning has made previ
ously unsolvable tasks possible one particular area where
deep learning has made tremendous progress is computer
vision such as in image recognition object detection and
image understanding as computer vision results are be
coming more promising larger issues regarding the use of
this technology need to be considered an important area to
consider is assistive technologies for those with visual im
pairments as computer vision technologies have the poten
tial to aid in tasks where previous solutions have struggled
although there are positive aspects of computer vision
applications there are also negative aspects that should be
addressed the use of black box articial intelligence so
lutions raises many concerns such as fairness and bias of
the models 13 there are also ethical concerns related to
privacy protection as many computer vision models rely on
camera input in addition the exclusion of certain groups
during the development process may also lead to negative
aspects of the technology 15 and as a result lead to low
adoption rates
we thank nserc canada research chairs program and microsoft
figure 1 positive implications computer visionbased devices al
low blind individuals to navigate independently recognize faces
and read text which helps them overcome social barriers
as ai is becoming more ubiquitous it is crucial to ad
dress issues related to the implications of ai specically
computer vision driven assistive technology towards indi
viduals with visual impairment the goal of the paper is
to review what implications computer vision has on assis
tive technologies for individuals with visual impairment and
considerations for computer vision researchers the paper
will be guided by the following questions
what are the positive and negative aspects of using
computer vision in assistive technologies with respect
to the impact on the lives of individuals with visual
impairment
what should researchers consider while conducting
computer vision research to reduce negative implica
tions of aipowered assistive technology on the lives
of individuals with visual impairment
2 positive implications
vision impairment and blindness cause a considerable
amount of economic and emotional burden for not only
the affected persons but also their caregivers and society at
large 11 the recent rise in computer vision based assis
tive technologies show the potential to reduce some burden
placed on the individuals as well as on caregivers and so
ciety by assisting visually impaired individuals with tasks
1arxiv190507844v1  cscv  20 may 2019
negative implications
gender
age bias
raceethnicity
exploitation of personal
information
obtrusiveness of camerasprivacytradeoff between autonomy
and privacy costs
poor device evaluation
age and condition dependentexclusion in
development
processinefciency in development
process
table 1 negative implications bias in computer vision algo
rithms privacy concerns related to data collection and cameras
and exclusion in development process
they would otherwise need help in as shown in figure 1
their level of independence and autonomy are increased
overcoming social barriers one area assistive tech
nologies have become an integral part in the lives of those
with visual impairment is overcoming barriers faced in ev
eryday life these individuals face adversity in all stages of
life for instance severely visually impaired young people
use their assistive technology as more than just a device to
overcome environmental barriers but also a means of com
munication for peers in their school 18
face recognition and optical character recognition
the ever growing presence of smartphones and advance
ments in computer vision are transforming the accessibil
ity of assistive technologies allowing individuals to over
come social barriers and have autonomy over when and how
they access information smartphone applications such as
seeingai and lookout use auditory cues to assist users
in identifying scenes recognizing faces reading short text
documents and currency 8 6
navigation assistance individuals with visual impair
ment also face difculty localizing themselves in unknown
indoor and outdoor environments research projects are us
ing cameras and sensors to give directions so these indi
viduals can navigate outdoor and indoor environments in
dependently for instance a prototype was developed for
guiding the visually impaired across streets in a straight line
using a wearable computerbased orientation and waynd
ing aid 16 for indoor navigation tian et al developed a
proof of concept computervision based indoor waynding
aid that detects doors and elevators as well as text on signs
to nd different rooms 19
3 negative implications
although the advancement of technology is evident only
a limited number of assistive technology solutions haveemerged to make a social or economic impact and im
prove quality of life fundamental challenges such as those
shown in table 1 are still be to thoroughly addressed before
deploying into assistive technologies
bias in machine learning bias refers to statistics that
lead to a skew and as a result brings an unjust outcome
for a population 13 bias often stems from training data
sample sets that are nonrepresentative of the general popu
lation when algorithms are trained with biased data they
are inherently bound to produce skewed results 5
one of the biggest implications in applying ai systems
with bias is the potential for adversely impacting already
marginalized groups in 2012 klare et al conducted a
study on the inuence of gender raceethnicity and age
on the performance of six different face recognition algo
rithms three of which are commercial 10 the results
found that there are lower matching accuracies for females
than males blacks compared to other raceethnicities and
18 to 30 year olds compared to other age groups
in recent years the low errors rates achieved by fa
cial recognition models led to even more commercializa
tion however studies have shown consistent bias in ar
eas of gender race and age from these commercial mod
els buolamwini and gebru evaluated bias present in
three commercial automated facial analysis algorithms from
ibm microsoft and megvii with respect to phenotypic sub
groups 5 the results showed that there is a signicant
drop in performance of state of the art models when applied
to images of a particular gender andor ethnicity group for
instance male subjects were more accurately classied than
females and lighter subjects were more accurately classied
than darker subjects all three commercial classier per
formed the worst on darker female subjects
raji and buolamwini conducted a second audit of com
mercial facial analysis models 14 in this study perfor
mances from target companies ones that were in the rst
audit and nontarget companies amazon and kairos are
presented the results showed all targets had the greatest
reduction in error rates for female and darker faces in terms
of nontarget companies the performance results were sim
ilar to the rst audit with the largest disparity gap between
black females and white males
although the awareness of disparity improved the facial
recognition models from target companies and produced a
lower error rate than nontarget companies the commercial
ization of these models before evaluating biases and poten
tial impacts on protected groups raises a concern
privacy as shown in the section 2 computer vision
based assistive technologies for the visually impaired allow
these individuals to gain independence and autonomy over
different aspects of their life however these devices also
pose privacy risks because of the vast amounts of personal
data stored although individuals with visual impairment
felt that smartphones help them communicate and achieve
greater independence these devices create privacy risks be
cause of the amount of personal data stored as well their
poor visual acuity makes it hard to safeguard their informa
tion such as if someone is around and eavesdropping 4
homemonitoring for older adults who represent major
ity of those with visual impairment 1 reliefs caregivers
burden and allows individuals with severe visual impair
ment to live independently but the devices for monitor
ing also store personal data studies have found that older
adults are willing to have activity monitoring shared with
family members and doctors if the collected data is useful
but expressed that the greatest concern is exploitation and
misuse of their personal health information 9
based on the studies the greatest fear associated with the
collection of personal data is the concern that their collected
data could end up in the wrong hands and be misused in
addition to the fear of personal information being exploited
the use of cameras is obtrusive and found to elicit greater
fears than wearable solutions in a comparison of four am
bient intelligent systems the camerabased behaviour and
emergency detection system was perceived with the great
est fear and highest level of concern 9 however studies
have also shown that there is a tradeoff between gained au
tonomy and privacy costs older adults with lower levels of
functioning are willing to accept video cameras and trade
off the privacy lost if camerabased solution could prevent
transfer to a long term care facility 20
the different perceptions of privacy over the use of data
as well as the potential benets of using cameras for home
monitoring suggest that privacy is a complex topic under
standing the variables that inuence privacy concerns and
how these concerns can be mediated by potential benets
are important when developing computer vision based as
sistive technologies
exclusion in development process the main goal of
assistive technologies is to improve the lives of end users
however when the design of form or function of the tech
nology is poor or when inequality exists between techno
logical accessibility the lives of those affected can be nega
tively impacted as well as perceptions of their abilities 12
for instance a device that has good design usability and ac
cessibility can be poorly evaluated the users lifestyle and
aspirations have to be taken into consideration to receive a
positive user evaluation 12
the lifestyle and desired function of assistive technolo
gies depend on age and level of adaption to their condition
a predominant want for young disabled people is the sig
nicance of being ordinary 18 no matter the degree of
visual impairment all the participants expressed that inclu
sion by peers and being ordinary is a big part in their daily
lives 18 in addition to age how the user has adapted to
their condition also impacts the desired functionality of the
figure 2 design considerations for computer vision researchers
assistive technology as users become more accustomed to
their condition they may prefer to perform some activities
independently 16
not only does including users during the development
process point out which areas to focus on but also saves
development time when testing the usability of the indoor
waynding device on blind participants the researchers
found that the participants were able to nd doors without
any problem since the participants use canes and realized
that text localization and recognition were more useful for
indoor navigation 19 by including the users earlier in
the development process could have identied that locating
doors are not a problem and use the saved time to address
text localization and recognition
4 design considerations for researchers
computer vision has the potential to impact peoples
lives however just algorithmic advances to the accuracies
of computer vision models are insufcient for assistive tech
nologies which interact with and around humans recently
the term humancentered articial intelligence is used to re
fer to intelligent systems that are aware of the interaction
with humans and are designed with social responsibility in
mind 15 as researchers it is important to uphold soci
etys moral and legal obligations to treat citizens fairly es
pecially those in protected groups that face discrimination
figure 2 illustrates some considerations to reduce the nega
tive implications mentioned in section 3
bias mitigation one method to uphold fairness is by
mitigating bias for instance researchers can use tools
such as googles whatif tool 3 and ibm ai fairness 360
kit 2 to analyze and identify unwanted bias in datasets and
ml models in order to mitigate such bias for age gender
and ethnicity there are different methods to reduce the neg
ative impacts of bias das et al proposed a multitask con
volution neural network that employs joint dynamic loss
weight adjustments to minimize bias when classifying gen
der age and race 7 there are also methods to reduce bias
at the dataset level salimi et al introduced a database re
pair algorithm which uses causal preprocessing to reduce
or eliminate sources of discrimination for fair ml 17
disability discrimination like age gender and race
disability status is also a protected characteristic how
ever disability discrimination has not been explored in lit
erature 21 similar to underrepresentation of age gender
and racial groups in datasets as shown in section 3 there
is also potential for underrepresentation of individuals with
disabilities ways to mitigate disability bias have also not
been explored compared to gender race and age gather
ing a balanced training dataset is not enough to address the
biased outcomes for those with a disability 21 the many
different forms and degrees of disability makes it difcult
for a machine learning model to nd patterns form groups
and generalize with the rise of machine learning based
assistive technologies understanding and assessing the im
pact towards people with disabilities is crucial especially
since disability bias has not been widely explored
inclusion of end users taking into account where end
users will use the assistive technologies as well as the needs
and goals a task specic training set and appropriate model
architecture can allow computer vision based devices to be
perceived as useful allowing individuals to gain indepen
dence and autonomy based on the studies mentioned in
section 3 users are willing to tradeoff privacy for more au
tonomy thus by including users in the development pro
cess the devices will be perceived as more useful gaining
more adoption since users are willing to tradeoff privacy
concerns to have more independence and autonomy
diverse skill set the ethical implications presented in
this paper are difcult to address by just computer vision
researchers instead a team with a diverse set of skills is
required to address both the positive and negative impli
cations of an assistive technology the underlying bias in
the models can cause protected groups to feel more iso
lated researchers should be aware of the possible biases
the dataset and algorithm may have before the system be
comes commercialized and interacts with people in every
day context in addition to bias the use of cameras raise
privacy concerns over who has access to the data stored
and the amount of security measures taken to protect per
sonal data before deployment developers should ensure
that measures are in place to reduce the chances of data
exploitation by understanding the needs and goals of in
dividuals with visual impairment designers can effectively
address these requirements in the design of the computer
vision system resulting in a more useful device for the end
users
references
1 blindness and vision impairment oct 2018 3
2 introducing ai fairness 360 sep 2018 3
3 the whatif tool sep 2018 3
4 t ahmed r hoyle k connelly d crandall and a ka
padia privacy concerns and behaviors of people with visual impairments in proceedings of the 33rd annual acm
conference on human factors in computing systems  pages
35233532 2015 3
5 j buolamwini and t gebru gender shades intersectional
accuracy disparities in commercial gender classication in
s a friedler and c wilson editors proceedings of the 1st
conference on fat  volume 81 of pmlr  pages 7791 23
24 feb 2018 2
6 p clary lookout an app to help blind and visually impaired
people learn about their surroundings may 2018 2
7 a das a dantcheva and f bremond mitigating bias in
gender age and ethnicity classication a multitask con
volution neural network approach in eccvw 2018  sept
2018 3
8 s kelley seeing ai articial intelligence for blind and
visually impaired users 2019 2
9 f kirchbuchner t grossepuppendahl m r hastall
m distler and a kuijper ambient intelligence from se
nior citizens perspectives in ambient intelligence  pages
4859 cham 2015 springer international publishing 3
10 b f klare m j burge j c klontz r w v order bruegge
and a k jain face recognition performance role of de
mographic information ieee transactions on information
forensics and security  7617891801 dec 2012 2
11 j k oberlein k beifus c schaffert and r p finger the
economic burden of visual impairment and blindness a sys
tematic review bmj open  311 2013 1
12 m leo g medioni m trivedi t kanade and g farinella
computer vision for assistive technologies computer vision
and image understanding  1541  15 2017 3
13 k lloyd bias amplication in articial intelligence sys
tems corr  abs180907842 2018 1 2
14 i d raji and j buolamwini actionable auditing inves
tigating the impact of publicly naming biased performance
results of commercial ai products in conference on aies 
2019 2
15 m o riedl humancentered articial intelligence and ma
chine learning corr  abs190111184 2019 1 3
16 d a ross implementing assistive technology on wear
able computers ieee intelligent systems  1634753 may
2001 2 3
17 b salimi l rodriguez b howe and d suciu capuchin
causal database repair for algorithmic fairness 2019 4
18 s s oderstr om and b ytterhus the use and nonuse of as
sistive technologies from the world of information and com
munication technology by visually impaired young people a
walk on the tightrope of peer inclusion disability  society 
253303315 2010 2 3
19 y  tian x yang c yi and a arditi toward a computer
visionbased waynding aid for blind persons to access un
familiar indoor environments machine vision and applica
tions  24521535 04 2013 2 3
20 d townsend f knoefel and r goubran privacy ver
sus autonomy a tradeoff model for smart home monitoring
technologies in 2011 ieee embc  pages 47494752 aug
2011 3
21 s trewin ai fairness for people with disabilities point of
view corr  abs181110670 2018 4","['1arxiv190507844v1', 'abs180907842', 'abs181110670', 'underrepresentation', 'obtrusiveness', 'homemonitoring', 'preprocessing', 'caregivers', 'eavesdropping', 'mitigating']"
Second Croatian Computer Vision Workshop (CCVW 2013),"['Sven LonÄariÄ‡', 'SiniÅ¡a Å egviÄ‡']",2013,http://arxiv.org/abs/1310.0319v3,,,[]
"Multiband NFC for High-Throughput Wireless Computer Vision Sensor
  Network","['F. Li', 'J. Du']",2017,http://arxiv.org/abs/1707.03720v1,"Multiband NFC for High-Throughput Wireless Computer Vision Sensor Network Fei Y. Li, Jason Y . Du 09212020027@fudan.edu.cn  Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR [1], non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput.     1. Computer Vision and Sensors  With emerging machine learning algorithms [1-5], computer vision tasks such as face recognition, object detection and simultaneous localization and mapping (SLAM) are now using more and more machine learning methods. In order to achieve good results, large amount of data is required to feed into algorithm. This not only requires energy-efficient computation [6-7], but energy-efficient data transfer is also necessary.   Point-to-point file exchange among smart devices has attracted peopleâ€™s attention today. The smart devices support high-definition photo/video capture (some smart phones even support 4K definition photo/video capture), and many people are actually using smart devices to replace camera. As 4K photos and videos have large file size (>10 MB per photo and >300 MB for video per minute), the most convenient way to share those photos with friends is by high-speed point-to-point link. In addition to file transfer, many computer vision application categories such as embedded systems and military applications also desire to have vision sensors coupled to computation unit wirelessly. Therefore, the connection/coupling between sensor and processor can be more flexible and more reliable. The distance between sensor and processor does not necessarily to be long, and near-field communication is a good match for this case (Fig. 1).        Fig. 1 Processor and vision sensor coupled by NFC  However, data rate of NFC is usually low. Currently several standards and products have been proposed for high-speed point-to-point communication. Bluetooth 4.0 has one HS mode, which uses ad-hoc point-to-point WiFi link to transfer data. The highest data rate can reach 25 Mbps. WiFi Alliance also proposed similar WiFI Direct technology, which supports data rate as high as 250 Mbps. However, the protocols of Bluetooth and WiFi both require a long time (can be as long as >10 seconds) to establish link (searching and pairing), and the user experience is not very smooth. In addition the bandwidth of WiFi and Bluetooth is not scalable due to spectrum regulation, and the data rate upgrade of Bluetooth/WiFi is much slower than the growth of 
Vision	Sensor	
Processor	
NFC	
computer vision requirements   2. Multiband RF Interconnect for NFC  High-speed NFC has the potential to solve above issues. Per FCC regulation, NFC must work in several pre-defined license-free industry, science and medic (ISM) bands, such as 900 MHz, 2.4 GHz, 5.8 GHz, etc [8]. Each frequency band only has limited bandwidth, so the conventional NFC which only uses one frequency band has limited data rate. On the other hand, NFC limits communication range to <3 cm, and therefore output power of TX can be low enough to pass spectrum regulation. This means that NFC can use broader bandwidth for data transfer. In addition, high speed NFC can use a simple protocol for link. By selecting frequency band properly, we can make sure there is no interference from other communications. There is only one TX-RX pair within communication range, and protocol can be simplified. This means the link establishment can be very quick, and user experience for data sharing will be very smooth. The diagram using high-speed NFC is shown in Fig. 2. A designed coupler is placed between sensor and processor, and the other part of device is shielded to reduce RF power leakage.  
 Fig. 2 Processor and vision sensor interface with coupler/shielding.  On the other hand, multiband RF signaling can be used to enhance data rate. By divide data stream into multiple sub-streams and each up-converted to one ISM band, multiple data streams can be transferred simultaneously. The multiple RF signals can be combined by power combiner [9].          Fig. 3 Multi-band NFC 
RF	signal	1	(data	stream1)	
Output	RF	signal	
RF	signal	2	(data	stream2)	
3. All-Digital Transmitter (ADTX) with FPGA  An all-digital transmitter design methodology has been proposed by Li et al. in [10]. In [10], transmitter is implemented by synthesis, therefore greatly reduces the turnaround time of transmitter design. It is an ideal solution to our prototyping of high-speed NFC.  We used the same transmitter architecture as in [10], using SDM and XOR mixing to convert analog signal into high-speed RF signal. The transmitter architecture is shown in Fig. 4.  fLO,IfLO,Qmixerbuffer	chainPAI-path	dataQ-path	dataÎ£-Î”1-bit	pulsesÎ£-Î”1-bit	pulsesn-bitn-bitADTX	chip	Fig. 4 ADTX architecture  Further, we used FPGA to fast implement ADTX circuit. Also, due to flexible reconfiguration of FPGA, we can repeatedly iterate our design until design goal is met.  4. Implementation Results  We implemented our high-speed NFC system, including one coupler and one FPGA-based ADTX. The coupler is shown in Fig. 5, and its coupling loss is around 10-20 dB for near-field (Fig. 6).  
  Fig. 5. Coupler design. 

 Fig. 6 Simulation results of coupling loss  The output spectrum of NFC system is captured by spectrum analyzer, as shown in Fig. 7. We used 250MHz and 400 MHz as carrier bands, and two peaks appear in spectrum analyzer.  
 Fig. 7 Spectrum of TX output  The demodulated waveform is shown in Fig. 8. QAM-16 is used for modulation, and it can be demodulated successfully. The data rate per band is 16 Mbps, and total data rate is 32 Mbps. The data rate is limited by carrier frequency, and carrier frequency is limited by speed of FPGA. With ASIC implementation, we expect carrier frequency as high as 6 GHz [10], as total data 

rate of greater than 300 Mbps can be achieved, which can meet the requirement of computer vision.  
 Fig. 8 Demodulated QAM-16 data at 450 MHz.  Conclusion In this work, we proposed a high-speed NFC system for computer vision applications. This system includes coupler and ADTX to achieve high speed data transfer, while achieves faster prototyping for non-contact data transfer. The data rate can be further boosted with ASIC implementation.  References [1] B. Kang et al., ""Hand Segmentation for Hand-Object Interaction from Depth map"", arXiv preprint arXiv:1603.02345, 2016. [2] P. Ballester and R. Araujo, ""On the performance of GoogLeNet and AlexNet applied to sketches"", in Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI Press, 2016. [3] Hongxiang Li et al., ""A convolutional neural network cascade for face detection"", in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015. [4] C. Szegedy, et al., ""Going deeper with convolutions"", in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015. [5] T. N. Sainath et al., ""Deep convolutional neural networks for large-scale speech tasks"", Neural Networks, pp.39-48, 2015. 

[6] Y.-H. Chen et al., ""Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks"", IEEE Journal of Solid-State Circuits vol. 52, no. 1, 2017. [7] C.-P. Lu, ""AI, Native Supercomputing and The Revival of Moore's Law"", arXiv preprint arXiv:1705.05983, 2017.  [8] Emission Mask/Analog Capability Requirements on Public Safety Channels [Online]. Available: https://www.fcc.gov [9] D. M. Pozar, ""Microwave Engineering"", third edition, 2005. [10] Y. L i e t a l., ""A Novel Fully Synthesizable All-Digital RF Transmitter for IoT Applications"", IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2017.  ","multiband nfc for highthroughput wireless computer vision sensor network fei y li jason y  du 09212020027fudaneducn  vision sensors lie in the heart of computer vision in many computer vision applications such as arvr 1 noncontacting nearfield communication nfc with high throughput is required to transfer information to algorithms in this work we proposed a novel nfc system which utilizes multiple frequency bands to achieve high throughput     1 computer vision and sensors  with emerging machine learning algorithms 15 computer vision tasks such as face recognition object detection and simultaneous localization and mapping slam are now using more and more machine learning methods in order to achieve good results large amount of data is required to feed into algorithm this not only requires energyefficient computation 67 but energyefficient data transfer is also necessary   pointtopoint file exchange among smart devices has attracted peoples attention today the smart devices support highdefinition photovideo capture some smart phones even support 4k definition photovideo capture and many people are actually using smart devices to replace camera as 4k photos and videos have large file size 10 mb per photo and 300 mb for video per minute the most convenient way to share those photos with friends is by highspeed pointtopoint link in addition to file transfer many computer vision application categories such as embedded systems and military applications also desire to have vision sensors coupled to computation unit wirelessly therefore the connectioncoupling between sensor and processor can be more flexible and more reliable the distance between sensor and processor does not necessarily to be long and nearfield communication is a good match for this case fig 1        fig 1 processor and vision sensor coupled by nfc  however data rate of nfc is usually low currently several standards and products have been proposed for highspeed pointtopoint communication bluetooth 40 has one hs mode which uses adhoc pointtopoint wifi link to transfer data the highest data rate can reach 25 mbps wifi alliance also proposed similar wifi direct technology which supports data rate as high as 250 mbps however the protocols of bluetooth and wifi both require a long time can be as long as 10 seconds to establish link searching and pairing and the user experience is not very smooth in addition the bandwidth of wifi and bluetooth is not scalable due to spectrum regulation and the data rate upgrade of bluetoothwifi is much slower than the growth of 
vision	sensor	
processor	
nfc	
computer vision requirements   2 multiband rf interconnect for nfc  highspeed nfc has the potential to solve above issues per fcc regulation nfc must work in several predefined licensefree industry science and medic ism bands such as 900 mhz 24 ghz 58 ghz etc 8 each frequency band only has limited bandwidth so the conventional nfc which only uses one frequency band has limited data rate on the other hand nfc limits communication range to 3 cm and therefore output power of tx can be low enough to pass spectrum regulation this means that nfc can use broader bandwidth for data transfer in addition high speed nfc can use a simple protocol for link by selecting frequency band properly we can make sure there is no interference from other communications there is only one txrx pair within communication range and protocol can be simplified this means the link establishment can be very quick and user experience for data sharing will be very smooth the diagram using highspeed nfc is shown in fig 2 a designed coupler is placed between sensor and processor and the other part of device is shielded to reduce rf power leakage  
 fig 2 processor and vision sensor interface with couplershielding  on the other hand multiband rf signaling can be used to enhance data rate by divide data stream into multiple substreams and each upconverted to one ism band multiple data streams can be transferred simultaneously the multiple rf signals can be combined by power combiner 9          fig 3 multiband nfc 
rf	signal	1	data	stream1	
output	rf	signal	
rf	signal	2	data	stream2	
3 alldigital transmitter adtx with fpga  an alldigital transmitter design methodology has been proposed by li et al in 10 in 10 transmitter is implemented by synthesis therefore greatly reduces the turnaround time of transmitter design it is an ideal solution to our prototyping of highspeed nfc  we used the same transmitter architecture as in 10 using sdm and xor mixing to convert analog signal into highspeed rf signal the transmitter architecture is shown in fig 4  floifloqmixerbuffer	chainpaipath	dataqpath	data1bit	pulses1bit	pulsesnbitnbitadtx	chip	fig 4 adtx architecture  further we used fpga to fast implement adtx circuit also due to flexible reconfiguration of fpga we can repeatedly iterate our design until design goal is met  4 implementation results  we implemented our highspeed nfc system including one coupler and one fpgabased adtx the coupler is shown in fig 5 and its coupling loss is around 1020 db for nearfield fig 6  
  fig 5 coupler design 

 fig 6 simulation results of coupling loss  the output spectrum of nfc system is captured by spectrum analyzer as shown in fig 7 we used 250mhz and 400 mhz as carrier bands and two peaks appear in spectrum analyzer  
 fig 7 spectrum of tx output  the demodulated waveform is shown in fig 8 qam16 is used for modulation and it can be demodulated successfully the data rate per band is 16 mbps and total data rate is 32 mbps the data rate is limited by carrier frequency and carrier frequency is limited by speed of fpga with asic implementation we expect carrier frequency as high as 6 ghz 10 as total data 

rate of greater than 300 mbps can be achieved which can meet the requirement of computer vision  
 fig 8 demodulated qam16 data at 450 mhz  conclusion in this work we proposed a highspeed nfc system for computer vision applications this system includes coupler and adtx to achieve high speed data transfer while achieves faster prototyping for noncontact data transfer the data rate can be further boosted with asic implementation  references 1 b kang et al hand segmentation for handobject interaction from depth map arxiv preprint arxiv160302345 2016 2 p ballester and r araujo on the performance of googlenet and alexnet applied to sketches in proceedings of the thirtieth aaai conference on artificial intelligence aaai press 2016 3 hongxiang li et al a convolutional neural network cascade for face detection in proceedings of the ieee conference on computer vision and pattern recognition 2015 4 c szegedy et al going deeper with convolutions in proceedings of the ieee conference on computer vision and pattern recognition 2015 5 t n sainath et al deep convolutional neural networks for largescale speech tasks neural networks pp3948 2015 

6 yh chen et al eyeriss an energyefficient reconfigurable accelerator for deep convolutional neural networks ieee journal of solidstate circuits vol 52 no 1 2017 7 cp lu ai native supercomputing and the revival of moores law arxiv preprint arxiv170505983 2017  8 emission maskanalog capability requirements on public safety channels online available httpswwwfccgov 9 d m pozar microwave engineering third edition 2005 10 y l i e t a l a novel fully synthesizable alldigital rf transmitter for iot applications ieee transactions on computeraided design of integrated circuits and systems 2017","['arxiv160302345', 'arxiv170505983', '09212020027fudaneducn', 'httpswwwfccgov', 'floifloqmixerbuffer', 'bluetoothwifi', 'supercomputing', 'energyefficient', 'connectioncoupling', 'highthroughput']"
Deep Learning vs. Traditional Computer Vision,"[""Niall O' Mahony"", 'Sean Campbell', 'Anderson Carvalho', 'Suman Harapanahalli', 'Gustavo Velasco-Hernandez', 'Lenka Krpalkova', 'Daniel Riordan', 'Joseph Walsh']",2019,http://arxiv.org/abs/1910.13796v1,"Deep Learning  vs. Traditional Computer Vision  
Niall Oâ€™ Mahony, Sean Campbell,  Anderson Carvalho, Suman Harapanahalli, 
Gustavo Velasco Hernandez, Lenka Krpalkova,  Daniel Riordan, Joseph Walsh  
IMaR Technology Gateway, Institute of Technology Tralee, Tralee, Ireland  
niall.omahony@research.ittralee.ie  
Abstract. Deep Learning  has pushed the limits of what was possible  in the 
domain of Digital Image Processing . However,  that is not  to say that the 
traditional computer vision techniques which had been undergoing progressive 
development in years prior to the rise of DL have become obsolete.  This paper 
will analyse  the benefits and drawbacks of each approach . The aim of th is paper 
is to promote a discussion on whether  knowledge of classical computer vision 
techniques should be maintained . The paper will also explore  how the two sides 
of computer vision can be combined . Several recent hybrid methodologies are 
reviewed  which have demonstrated the ability to improve computer vision 
performance  and to tackle problems not suited to Deep Learning . For example , 
combining traditional computer vision techniques with Deep Learning  has been  
popular  in emerging domains  such as Panoramic Vision and 3D vision for which  
Deep Learning models have  not yet  been fully optimised . 
Keywords:  Computer Vision , Deep Learning , Hybrid techniques . 
1 Introduction  
Deep Learning ( DL) is used in the domain of digital image processing to solve difficult 
problems ( e.g. image colourization , classification, segmentation and detection ). DL 
methods such as C onvolutiona l Neural Networks  (CNN s) mostly improve prediction 
performance using  big data and plentiful computin g resources  and have pushed the 
boundar ies of what was possible . Problems which were assumed to be unsolvable are 
now being solved with super -human accuracy . Image classification is a prime example 
of this. Since being reignited by Krizhevsky, Sutskever and Hinton in 2012 [1],  DL 
has dominated the domain ever since due to a substantial ly better performance  
compared to traditional methods  . 
Is DL making traditional Computer Vision (CV) techniques obsole te? Has DL 
superseded traditional computer vision? Is there still a need to study traditional CV 
techniques when DL seems to be so effective? These are all question s which have been 
brought up in the community  in recent years   [2], which this paper intends to address .  
Additionally , DL is not going t o solve all CV problems . There are some problems 
where  traditional techniques with global features  are a better solution . The advent of 
DL may open many  doors to do something with traditional techniques to overcome the 
many challenges DL brings  (e.g. compu ting power, time, accuracy, characteristics and 
quantity of inputs, and among others) . 
 
This paper will provide a comparison of  deep learning  to the more traditional hand -
crafted feature definition approaches which dominated CV prior to it. There has been 
so much progress in Deep Learning in recent years that it is impossible for this paper 
to capture the many facets and sub -domains of Deep Lea rning which are tackling the 
most pertinent problems in CV today. This paper will review traditional algorithmic 
approaches in CV, and more particularly, the applications in which they have been used 
as an adequate substitute for DL, to complement DL and to tackle problems DL cannot.  
The paper will then move on to review some of the recent activities in combining 
DL with  CV, with a focus on the state -of-the-art techniques for emerging technology 
such as  3D perception,  namely object registration, object det ection and semantic 
segmentation of 3D point clouds. Finally , developments and possible directions of 
getting the performance of 3D DL to the same heights as 2D DL are discussed along 
with an outlook on the impact the increased use of 3D will have on CV in  general.  
2 A Comparison of Deep Learning  and Traditional Computer 
Vision  
2.1 What is Deep Learning  
To gain a fundamental understanding of DL we need to consider the difference  between 
descriptive analysis and predictive analysis.  
Descriptive  analysis involves defining a comprehensible mathematical model which 
describes the phenomenon that we wish to observe. This entails collecting data about a 
process , forming hypotheses  on patterns in the data  and validating these hypotheses  
through comparing the out come of descriptive models we form with the real outcome 
[3]. Producing such models is precarious however because there is always a risk of un -
modelled variables that scientists and engineers neglect to include due to ignorance or 
failure to understand some complex, hidden or non -intuitive phenomena  [4].  
Predictive  analysis  involves the discovery of rules that underlie a phenomenon and 
form a predictive model which minimise the error between the actual and the predicted 
outcome considering all possible interfering factors [3]. Machine learning rejects the 
traditional programming paradigm where problem analysis is replaced by a training 
framework where the system is fed a large number of training patterns (sets of inputs 
for which the desired outputs are known) which it learns and uses t o compute new 
patterns [5].  
DL is a subset of machine learning . DL is based largely on Artificial Neural 
Networks (ANNs), a computing paradigm inspired by the functioning of the human 
brain. Like the human brain, it is composed of many computing cells or â€˜neuronsâ€™ that 
each perform  a simple operation and interact  with each other to make a decision  [6]. 
Deep Learning is all about learning  or â€˜credit assignmentâ€™ across many layers of a 
neural network accurately, efficiently and without supervision and is of recent interest 
due to enabling advancements in processing hardware [7]. Self-organisation and the 
exploitation of interactions between small units have proven to perform better than 
central control, particularly for complex non -linear process models in that better fault 
tolerance and adaptability to new data is achievable [7]. 
 
2.2 Advantages  of Deep Learning  
Rapid p rogression s in DL and improvements  in device capabilities including 
computing power, memory capacity, power consumption, image sensor resolution, and 
optics have improved the performance and cost -effectiveness of  further quickened  the 
spread  of vision -based application s. Compared to traditional CV techniques, DL 
enable s CV engineers to achieve  greater accuracy in tasks such as image  classification , 
semantic segmentation , object detection  and Simultaneous Localization and Mapping 
(SLAM) . Since neural networks used in DL are trained rather than programmed,  
applications using this approach often require less expert analysis and fine-tuning  and 
exploit  the tremendous amount o f video data available in todayâ€™s systems.  DL also 
provides superior flexibility  because CNN  models  and frameworks can be re -trained 
using a custom dataset  for any use case, contrary  to CV algorithms, which tend to be 
more domain -specific.  
 
Taking  the prob lem of object detection on a mobile robot  as an example, we can  
compare  the two types of algorithms for computer vision:  
 
The traditional  approach is  to use well -established  CV techniques such as  feature 
descriptors (SIFT, SURF, BRIEF, etc.) for object detection. Before the emergence of 
DL, a step called feature extraction  was carried out for tasks such as image  
classification . Features are small â€œinterestingâ€, descriptive or informative patches i n 
images. Several CV algorithms , such as  edge detection, corner detection  or threshold 
segmentation  may be involved in this step . As many features  as practicable are 
extracted from images and these features form a definition (known as a bag -of-words) 
of each object class . At the deployment stage , these definitions are search ed for in other 
images. If a significant number of features from one bag -of-words are in  another image, 
the image is classified as containing that specific object (i.e. chair, horse, etc .). 
The difficulty with this traditional approach is that it is necessary  to choose which 
features are important  in each given image.  As the number of classes to classify 
increases, feature extraction  becomes more and more cumbersome . It is up to the CV 
engineerâ€™s judgment and a long trial and error process to decide which features best 
describe  different classes of objects. Moreover, each feature  definition requires dealing 
with a plethora of parameters, all of which must  be fine -tuned by  the CV engineer . 
DL introduced the concept of end -to-end learning where the machine is  just given a 
dataset of images  which have been annotated with what classes of object are present in 
each image  [7].  Thereby a DL m odel is â€˜trainedâ€™ on the given data , where  neural 
networks discover  the underlying patterns in classes of images and automatically works 
out the most descriptive and salient features with respect to each specific class of object 
for each object. It has been well -established  that DNN s perform  far better than 
traditional algorithms, albeit with  trade -offs with respect to computing requirements  
and training time . With  all the state -of-the-art approaches in CV employing this 
methodology, the workflow of the CV engineer has changed dramatically  where the 
knowledge and expertise in extracting hand -crafted features has been replaced by 
knowledge and expertise in iterating thro ugh deep learning architectures  as depicted in 
Fig. 1.  
 
 
Fig. 1.  (a) Traditional Computer Vision workflow vs. (b) Deep Learning wo rkflow . Figure from [8]. 
 
The development of CNNs  has had a tremendous influence in the field of CV in recent 
years and is responsible for a big jump in the ability to recognize objects  [9]. This burst 
in progress has been enabled by a n increase in computing power, as well as an increase 
in the amount of data available for training neural networks.  The recent explosion in 
and wide -spread adoption of various deep -neural network architectures for CV is 
apparent in the fact that the semina l paper ImageNet Classification with Deep 
Convolutional Neural Networks has been cited over 3000 times [2].  
CNN s make use of kernels (also known as filters), to detect features (e.g. edges) 
throughout an image. A kernel is just a matrix of values, called weights, which  are 
trained to detect specific features. As their name indicates, the main idea behind the 
CNNs  is to spatially convolve the kernel on a given input image check if the feature it 
is meant to detect is present.  To provide a value representing how confident it is that a 
specific feature is present, a convolution operation is carried out by computing the  dot 
product of the kernel and the input area where kernel is overlapped (the area of the 
original image the kernel is looking at is known as the receptive field [10]). 
To facilitate the learning of kernel  weights , the convolution layerâ€™s output is   
summed  with a bias term and then fed to a non -linear activation function. Activation 
Functions are usually non -linear func tions like Sigmoid, TanH  and ReLU (Rectified 
Linear Unit). Depending on the nature of data and classification tasks, these activation 
functi ons are selected accordingly  [11]. For example,  ReLUs are known to have more 
biological representation  (neurons in th e brain either fire or they donâ€™t). As a result, it 
yields favourab le results for image recognition tasks  as it is less susceptible to  the 
vanishing gradient problem and it produces sparser, more efficient representations  [7].  

To speed up the training process and reduce the amount of memory consumed by 
the network, the convolutional layer is often followed by a pooling layer to remove 
redundancy present in the input feature. For example, max pooling moves a window 
over the input and  simply outputs the maximum value in that window effectively 
reducing  to the important pixels in an image  [7]. As shown in Fig. 2 , deep CNNs may 
have several pairs of convolutional and pooling layers . Finally, a  Fully Connected 
layer flattens the previous layer volume into a feature vector and then an output layer 
which computes the scores (confidence or probabilities) for th e output classes/features 
through a dense network. This output is then passed to a regression function such as 
Softmax  [12], for example, which  maps everything to a vector whose elements sum 
up to one  [7].  
 
Fig. 2.  Building blocks of a CNN. Figure from [13] 
But DL is still only a tool of CV For example, the most common neural network used 
in CV is the  CNN. But what is a convolution? Itâ€™s in fact a widely used image 
processing technique (e.g. see  Sobel edge detection ). The advantages of DL are clear,  
and it would be beyond the scope of this paper to review the state -of-the-art. DL is 
certainly not the panacea for all problems either, as we will see in following sections of 
this paper, there are problems and applications where the more conventional CV 
algorithms are more suitable . 
 
 
2.3 Advantages of Traditional  Computer Vision Techniques  
This section will detail how the t raditional feature -based  approaches such as those listed 
below have been shown to be useful in improving performance in CV tasks : 
â€¢ Scale Invariant Feature Transform (SIFT)  [14] 
â€¢ Speeded Up Robust Features (SURF) [15] 
â€¢ Features from Accelerated Segment Test (FAST)  [16] 
â€¢ Hough transforms  [17] 
â€¢ Geometric hashing  [18] 
 
Feature descriptors such as SIFT  and SURF  are generally combined with traditional 
machine learning classification algorithms such as Support Vector Machines and K -
Nearest Neighbours  to solve the aforementioned CV problems .  

DL is sometimes overkill  as often traditional CV techniques can solve a problem 
much more efficiently and in fewer lines of code than DL . Algorithms  like SIFT and 
even simple colour thresholding and pixel counting algorithms are not class -specific, 
that is, they are very general and perform the same for any image . In contrast , features 
learned from a deep neural net are specific  to your train ing dataset  which, if not well 
constructed, probably wonâ€™t perform well for images different from the training set . 
Therefore , SIFT  and other algorithms  are often used for applications such as  image 
stitching/3D mesh reconstruction  which  donâ€™t require specific class knowledge. These 
tasks have been shown to be achievable by training large datasets , however this requires 
a huge research effort and it is not practical to go through this effort for a closed  
application . One needs to practice common sense when it comes to choosing which 
route to take for a given CV application. For example, to classify two classes of product 
on an assembly line conveyor belt , one with red paint  and one with blue paint . A deep 
neural  net will work given that enough  data can be collected to train from. However,  
the same can be achieved  by using simple colour thresholding . Some  problems can be 
tackled with  simpler and faster techniques.  
What if a DNN perform s poorly outside o f the training data ? If the training dataset 
is limited,  then the machine may overfit to the training data and  not be able to generalize 
for the task  at hand . It would be too difficult to manually tweak the parameters of the 
model  because  a DNN  has million s of parameters inside of it  each with complex inter -
relationships . In this way, DL model s have been criticised to be a black box  in this way 
[5]. Traditional CV has full transparency and the one can  judge whether your solution 
will work outside of a training environment.  The CV engineer can  have insight s into a 
problem that they can transfer to their  algorithm  and if anything fails, the parameters 
can be tweaked to perform well for a wider range of images.  
Today, the traditional techniques are used when the problem can be simplified so 
that they can be deployed on low cost  microcontrollers or to limit the problem for deep 
learning techniques by highlighting certain features in data, augmenting data  [19] or 
aiding in dataset annotation   [20]. We will discuss later in this paper how many image 
transformatio n techniques can be used to improve your neural net training. Finally, 
there  are many more cha llenging problems in CV such as: Robotic s [21], augmented 
reality  [22], automatic panorama stit ching  [23], virtual reality  [24], 3D modelling  [24], 
motion estimation  [24], video stabilization  [21], motion capture  [24], video processing  
[21] and scene understanding  [25] which cannot simply be easily implemented in a 
differentiable manner with deep learning but benefit from solutions  using ""traditional"" 
techniques.  
3 Challenges for Traditional C omputer Vision  
3.1 Mixing Hand -Crafted Approaches  with DL for Better Performance  
There are clear trade -offs between traditional CV and deep learning -based 
approaches. Classic CV algorithms are well-established , transparent , and optimized for 
performance and power efficiency, while DL offers greater accuracy and versa tility at 
the cost of  large amounts of computing resources.   
Hybrid approaches combine traditional CV and deep learning  and offer the 
advantages traits of both methodologies. They are especially practical in high 
performance  systems which need to be implem ented quickly . For example, in a security 
camera, a CV algorithm can efficiently detect faces or other features [26] or moving 
objects  [27] in the scene. These detections can then be passed to a DNN for identity 
verification or object classification . The DNN need only be applied on a small patch of 
the image  saving significant computing resources  and training effort  compared to what 
would be required to process the entire frame.  
The fusion of Machine Learning metr ics and Deep Network have become very 
popular, due to the simple fact that it can generate better models.  Hybrid vision 
processing implementation s can introduce  performance advantage and â€˜ can deliver a 
130X -1,000X reduction in multiply -accumulate operation s and about 10X 
improvement in frame rates compared to a pure DL solution. Furthermore, the hybrid 
implementation uses about half of the memory bandwidth and requires significantly 
lower CPU resources â€™ [28].  
3.2 Overcoming the Challenges of Deep Learning  
There are also challenges introduced by DL. The latest DL approaches may achieve 
substantially better accuracy;  however  this jump comes at the cost of billions of 
additional math operations and an increased requirement for processing power.  DL 
requires a these  computing resources f or training and to a lesser extent for inference.  It 
is essential to have dedicated hardware (e.g. high -powered GPUs [29] and TPUs [30] 
for training and AI accelerated platforms such as VPUs for inference [31]) for 
developers of AI.  
Vision process ing results using DL are also dependent on image resolution. 
Achieving adequate performance in object classification, for example, requires high-
resolution  images or video â€“ with the consequent increase in the amount of data that 
needs to be processed, sto red, and transferred. Image resolution is especially important 
for applications in which it is necessary to detect and classify objects in the distance , 
e.g. in security camera  footage . The frame reduction techniques discussed previously 
such as using SIFT  features [26, 32]  or optical flow for moving objects [27] to first 
identify a region of interest are useful with respect to image resolution and also with 
respect to reducing t he time and data required for training . 
DL needs big data.  Often millions of data records are required. For example, 
PASCAL  VOC Dataset consists of 500K images with 20 object categories  [26][33], 
ImageNet consists of 1.5 million images with 1000 object categories  [34] and Microsoft 
Common Objects in Context (COCO) consists of 2.5 million images with 91 object 
categories  [35]. When big datasets  or high computing facility are unavailable,  
traditional methods will come into play.  
Training a DNN takes a very long time. Depending on computing hardware 
availability , training can take a matter of hours or days. Moreover, training for any 
given application often requires many iterations  as it entails trial and error with  different 
training parameters. The most common technique to reduce training time is tr ansfer 
learning [36]. With respect to traditional CV, the discrete Fourier transform is another 
CV technique which once experienced major popularity but now seems obscure. The 
algorithm can be used to speed up convolutions  as  demonstrated by [37, 38]  and hence 
may again become of major importance.   
However, it must be said that e asier , more domain -specific  tasks than general image 
classification will not re quire as much data (in the order of hundreds or thousands rather 
than millions) . This is still a considerable amount of data and CV techniques  are often 
used to boost training data  through  data augmentation or reduce the data down to a 
particular type of f eature through other pre -processing steps . 
Pre-processing entails transforming the data (usually with traditional CV techniques) 
to allow relationships/patterns to be more easily interpreted before training your model. 
Data augmentation is a common pre -processing task which is used when there is limited 
training data. It can involve performing random rotations, shifts, shears, etc. on the 
images in your training set to effectively increase the number of training images  [19]. 
Another approach is to highlight features of interest before passing the data to a CNN 
with CV -based methods such as background subtraction and segmentation [39]. 
3.3 Making Best Use of Edge Computing  
If algorithms and neural network inferences can be run at the edge, latency, costs, cloud 
storage and processing requirements, and bandwidth requirements are reduced 
compared to cloud -based implementations. Edge computing can also privacy and 
security requirements by avoiding transmission  of sensitive or identifiable data over the 
network.  
Hybrid or composite approaches involving conventional  CV and DL  take great 
advantage of the heterogeneous computing capabilities  available at the edge . A 
heterogeneous compute  architecture consists of a combination of CPUs, 
microcontroller coprocessors, Digital Signal Processors (DSPs), Field Programmable 
Gate Arrays (FPGAs) and AI accelerating devices [31] and can be power efficient by  
assigning different workloads to the most efficient compute engine. Test 
implementations show 10x latency reductions in object detection when DL inferences 
are executed on a DSP versus a CPU  [28].  
Several hybrids  of deep learning and hand -crafted features based approach es have 
demonstrated their benefits in edge applications. For example,  for facial -expression 
recognition , [41] propose a new feature lo ss to embed the information of hand -crafted 
features into the training process of network, which tries to reduce the difference 
between  hand -crafted features and features learned by  the deep neural network . The use 
of hybrid approaches has also been shown to be advantageous in incorporating data 
from other sensors on edge nodes. Such a  hybrid model where th e deep learning is 
assisted by additional sensor sources like synthetic aperture radar (SAR) imagery and 
elevation like synthetic aperture radar (S AR) imagery and elevation  is presented by 
[40]. In the context of 3D robot vision, [42] have shown that combining both linear 
subspace methods and deep convolutional prediction achieves improved performance 
along with several orders of magnitude faster runtime performance compared  to the 
state of the art.  
3.4 Problems Not Suited to Deep Learning  
There are many more changing problems in CV such as: Robotic, augmented reality, 
automatic panorama stitching, virtual reality, 3D modelling, motion stamation , video 
stabilization, motion captu re, video processing and scene understanding which cannot 
simply be easily implemented in a differentiable manner with deep learning but need 
to be solved using the other ""traditional"" techniques.  
DL excels at solving closed -end classification problems, in  which a wide range of 
potential signals must be mapped onto a limited number of categories, given that there 
is enough data available and the test set closely resembles the training set. However, 
deviations from these assumptions can cause problems and it  is critical to acknowledge 
the problems which DL is not good at. Marcus et al. present ten concerns for deep 
learning, and suggest that deep learning must be supplemented by other techniques if 
we are to reach artificial general intelligence  [43]. As well  as discussing the limitations 
of the training procedure and intense computing and data requirements as we do in our 
paper, key to their discussion is identifying problems where DL performs poorly and 
where it can be supplemented by other techniques .  
One such problem is the limited ability  of DL algorithms  to learn visual relations , 
i.e. identifying whether multiple objects in an image are the same or different. This 
limitation has been demonstrated by [43] who argue that feedback mechanisms 
including attention and perceptual grouping may be the key computational components 
to realising  abstract visual reasoning.   
It is also worth noting that ML models find it difficult to deal with priors, that is, not 
everythin g can be learnt from data,  so some priors must be injected into the models  
[44], [45]. Solutions that have to do with 3D CV need strong priors in order to work 
well, e.g. image -based 3D modelling requires smoothness, silhouette and illumination 
information [46].  
Below are  some emerging fields in CV where  DL faces new challenges and where  
classic CV will have a more prominent role.  
3.5 3D Vision  
3D vision systems are becoming increasingly accessible and as such there has been a 
lot of progress in the design of 3D Convolutional Neural Networks (3D CNNs). This 
emerging field is known as Geometric Deep Learning and has multiple applications 
such as video classification, computer graphics, vision and robotics. This paper will 
focus on 3DCNNs for processing data from 3D Vision System s. Wherein 2D 
convolutional layers the kernel has the  same depth so as to output a 2D matrix, the 
depth of a 3D convolutional kernel must be less than that of the 3D input volume so 
that the output of the convolution is also 3D and so preserve the spatial information.  
 
Fig. 3.  2DCNN vs. 3D CNN [47] 
 
The size of the input is much larger in terms of memory than conventional RGB images 
and the kernel m ust also be convolved through the input space in 3 dimensions  (see Fig. 
3). As a result, the computational complexity of 3D CNNs grows cubically with 
resolution.  Compared to 2D image processing, 3D CV is made even more difficult as 
the extra dimension introduces more u ncertainties, such as occlusions and different 
cameras angles as shown in Fig. 4 . 
 
 
 
Fig. 4.  3D object detection in point clouds is a challenging problem due to discrete sampling, noisy scans, 
occlusions and cluttered scenes. Figure from [48]. 
FFT based methods can optimise 3D CNNs reduce the amount of computation, at the 
cost of increased memory requirements however . Recent research h as seen the 

implementation of the Winograd Minimal Filtering Algorithm (WMFA) achieve a two -
fold speedup compared to cuDNN  (NVIDIAâ€™s language/API for programming on 
their graphics cards)  without increasing the required memory [49].  The next section 
will include some solutions with novel architectures and pre -processing steps to various 
3D data representations which have been proposed to overcome these challenges.  
Geometric Deep Learning (GDL) deals with the extension of DL techniques to 3D 
data. 3D data can be represented in a variety of different ways  which can be classified 
as Euclidean  or non-Euclidean [50].3D Euclidean -structured data has an underlying 
grid structure that allows for a global parametrization and having a common  system of 
coordinates  as in 2D images . This allows  existing 2D DL paradigms and 2DCNNs can 
be applied to 3D data .  3D Euclidean data is more suitable for analysing  simple rigid 
objects such as, chairs, planes, etc  e.g. with voxel -based approaches  [51]. On the other 
hand, 3D non -Euclidean data do not have the grid ded array structure where there is no 
global parametrization. Therefore, extending classical DL techniques to such 
representations is a challenging task  and has only recently been realized with 
architectures such as Point net [52].  
 Continuous shape information that is useful for recognition is often lost in their 
conversion to a voxel representation. W ith respect to tradit ional CV algorithms,  [53] 
propose a single dimensional feature that can be applied to vo xel CNNs. A novel 
rotation -invariant feature based on mean curvature that improves shape recognition for 
voxel CNNs  was proposed . The method was very suc cessful in that when it was applied 
to the state-of-the-art recent voxel CNN Octnet  architecture a 1% o verall accuracy 
increase on the ModelNet10 dataset  was achieved .  
 
3.6 SLAM  
Visual SLAM is a subset  of SLAM where a vision system is used instead of LiDAR for 
the registration of landmarks in a scene. Visual SLAM has the advantages of 
photogrammetry (rich visual data, low -cost, lightweight and low power consumption) 
without the associated heavy computat ional workload involved in post -processing. The 
visual SLAM problem consists of steps such as environment sensing, data matching, 
motion estimation, as well as location update and registration of new landmarks  [54]. 
Building a model of how visual objects appear in different conditions such as 3D 
rotation, scaling, lighting and extending from that representation using a strong form of 
transfer learning to achieve zero/ one shot  learning  is a challenging problem in this 
domain. Feature extraction and data representation  method s can be useful to reduce the 
amount of training examples needed for an ML model  [55].  
A two -step a pproach is commonly used in image based  localization ; place 
recognition followed by pose estimation . The former computes  a global descriptor for 
each of the images by aggregating local image descriptors, e.g. SIFT, using the bag -of-
words approac h. Each glo bal descriptor is stored in the database together with the 
camera pose of its associated image with respect to the 3D point cloud reference map. 
Similar global descriptor s are extracted from the query image and the closest global 
descriptor  in the database  can be retrieved via an efficient search. The camera pose of 
the closest global descriptor would give us a coarse localization of the query image with 
respect to the reference map. In pose estimation, the exact pose of the query image 
calculated more prec isely with algorithms such as the Perspective -n-Point (PnP) [13] 
and geometric verification [18] algorithms. [56] 
The success of image based  place recognition is largely attributed to the ability to 
extract image feature descriptors. Unfortunately, there is no algor ithm to extract local 
features similar to SIFT for LiDAR scans. A 3D scene is composed of 3D points and 
database images. One approach has associated e ach 3D point to a set of SIFT 
descriptors corresponding to the image features from which the point was tri angulated. 
These descriptors can then be averaged into a single SIFT descriptor that describes the 
appearance of that point  [57]. 
Another approach constructs multi -modal f eatures from RGB -D data rather than the 
depth processing. For the depth processing part, they adopt the well-known  
colourization method based on surface normals, since it has been proved to be effective 
and robust across tasks  [58]. Another alternative approach ut ilizing traditional CV 
techniques  present s the Force Histogram Decomposition (FHD), a graph -based 
hierarchical descriptor that allows the spatial relations and shape information between 
the pairwise structural subparts of objects  to be characterized . An ad vantage of this 
learning procedure is its compatibility with traditional bags -of-features frameworks, 
allowing for hybrid representations gathering structural and local features  [59]. 
 
3.7 360 camera s 
A 360 camera, also known as  an  omnidirectional or spherical  or panoramic  camera is 
a camera with a 360-degree  field of view  in the horizontal plane, or with a visual field 
that covers (approximately) the entire sphere. Omnidirectional cameras are important 
in applications suc h as robotics  where large visual field coverage is needed . A 360 
camera can replace multiple monocular cameras  and eliminate blind spots which 
obviously advantageous in omnidirectional Unmanned Ground Vehicles (UGVs) and 
Unmanned Aerial Vehicles (UAVs).  Thanks to the imaging characteristic of spherical 
cameras, each image captures the 360â—¦ panorama  of the scene, eliminating the 
limitation on available steering choices. One of the major challenges with spherical 
images is the heavy barrel distortion due to t he ultra-wide -angle fisheye lens, which 
complicates the implementation of conventional human vision inspired methods such 
as lane detection and trajectory tracking. Additional pre-processing  steps such as prior 
calibration and deworming  are often required.  An alternative approach which has been 
presented by [60], who circumvent  these pre-processing  steps by formulating 
navigation as a classification problem on finding the optimal potential path orientation 
directly based on the raw, uncalibrated spherical images.   
Panorama stitching is an other open research problem  in this area.  A real -time 
stitchi ng methodology [61]  uses a group of deformable meshes  and the final image  and 
combine the inputs using a robust pixel -shader. Another approach  [62], combine the 
accuracy provided by geometric reasoning (lines and vanishing points) with the higher 
level of data abstraction and pattern recognition achieved by DL techniques (edge and 
normal maps)  to extract structural and generate layout hypotheses for indoor scenes . In 
sparsely structured scenes , feature -based image alignment methods often fail due to 
shortage  of distinct image features. Instead, direct image alignment methods, such as 
those based on phase correlation, can be applied. Correlation -based image alignment 
techniques based on discriminative correlation filters (DCF)  have been investigat ed 
by [23] who show that the proposed DCF -based methods outper form phase 
correlation -based approaches on these datasets.  
3.8 Dataset Annotation  and Augmentation  
There are arguments against the combination of CV and DL and they summarize to the 
conclusion that we need to re -evaluate our methods from rule -based to data -driven. 
Traditionally, from the perspective of signal processing, we know the operational 
connotations of CV algorithms such as SIFT and SURF method s, but DL leads such 
meaning nowhere, all you need is more data. This can be seen as a huge step forward,  
but may be  also a backward move. Some of the pros and cons of each side of this debate 
have been discussed already in this paper, however, if future -methods are to be purely 
data-driven then focus should be placed on more intelligent methods for dataset 
creation.  
 The fundamental problem of current research is that there is no longer enough data 
for advanced algorithms or models for special applications . Coupling custom datasets 
and DL models  will be the future theme to many research papers. So many researchersâ€™ 
outputs consist of not only algorithms or architectures, but also datasets or methods  to 
amass data. Dataset annotation is a major bottleneck  in the DL workflow which requires 
many hours of manual labelling. Nowhere is this more problematic than in  semantic 
segmentation  applications where every pixel needs to be annotated accurately. There 
are many useful tools available to semi -automate the process  as reviewed by [20], many  
of which take advantage of algorith mic approaches  such as ORB features   [55], polygon 
morphing [63], semi -automatic Area of Interest (AOI) fitting  [55] and all of the above 
[63]. 
The easiest and most common method to overcome limited datasets and reduce 
overfitting of deep learning models for image classification is to artificially enlarge the 
dataset using label -preserving transformations . This process is known as dataset 
augmentatio n and it involves the artificial generation of extra training data from the 
available ones, for example, by cropping, scaling, or rotating images [64]. It is desirable 
for data augmentation procedures to require very little computation  and to be 
implementable within the DL training pipeline so that the transformed images do not 
need to be stor ed on disk . Traditional algorithmic approaches that have been employed 
for dataset augmentation include Principle Component Analysis (PCA) [1], adding 
noise, interpolating or extrapolating  between samples in a feature space [65] and 
modelling  the visual context surrounding objects from segmentation annotations [66]. 
Conclusion  
A lot of the CV techniques invented in the past 20 years have become irrelevant in 
recent years because of DL. However, knowledge is never obsolete and there is always 
something worth learning from e ach generation of innovation. That knowledge can give 
you more intuitions and tools to use especially when you wish to deal with 3D CV 
problems for example. Knowing only DL for CV will dramatically limit the kind of 
solutions in a CV engin eerâ€™s arsenal.  
In this paper  we have laid down m any arguments for why traditional CV techniques 
are still very much useful  even in the age of DL . We have compared and contrasted 
traditional CV and DL for typical  applications and d iscussed how s ometimes traditional 
CV can be considered as an alternative in situations  where DL  is overkill for a specific 
task.  
The paper also highlight ed some areas where traditional CV techniques remain 
relevant  such as being utilized in hybrid approaches to improv e performance . DL 
innovations are driving exciting breakthroughs for the IoT  (Internet of Things) , as well 
as hybrid techniques that combine the technologies with traditional algorithms. 
Additionally, we reviewed how traditional CV techniques can actually improve DL 
performance in a wide range of applications from reducing training time, processing 
and data requirements to being applied in emerging fields such as SLAM, Panoramic -
stitching, Geometric Deep Learning and 3D vision where DL  is not yet well 
established . 
The digital image processing domain has undergone some very dramatic changes 
recently and in a very short period . So much so it has led us to question whether the CV 
techniques that were in vogue prior to the AI explosion are still relevant. This paper 
hopefully highlight  some cases where traditional CV techniques are useful and that 
there is something still to gain from the years of effort put in to their develop ment even 
in the age of data -driven intelligence.   
References  
1.  Krizhevsky A, Sutskever I, Hinton GE (2012) ImageNet Classification with Deep 
Convolutional Neural Networks. NIPSâ€™12 Proc 25th Int Conf Neural Inf Process Syst 
1:1097 â€“1105  
2.  Nash W, Drummond T, Birbilis N (2018) A Review of Deep Learning in the Study of 
Materials Degradation. npj Mater Degrad 2:37. https://doi.org/10.1038/s41529 -018-
0058 -x 
3.  Bonaccorso G (2018) Machine Learning Algorithms Popular Algorithms for Data 
Science and Machine Learning, 2nd Editi on. Packt Publishing Ltd  
4.  Mahony NO, Murphy T, Panduru K, et al (2017) Improving controller performance in 
a powder blending process using predictive control. In: 2017 28th Irish Signals and 
Systems Conference (ISSC). IEEE, pp 1 â€“6 
5.  Oâ€™Mahony N, Murphy  T, Panduru K, et al (2017) Real -time monitoring of powder blend 
composition using near infrared spectroscopy. In: 2017 Eleventh International 
Conference on Sensing Technology (ICST). IEEE, pp 1 â€“6 
6.  Oâ€™ Mahony N, Murphy T, Panduru K, et al (2016) Adaptive  process control and sensor 
fusion for process analytical technology. In: 2016 27th Irish Signals and Systems 
Conference (ISSC). IEEE, pp 1 â€“6 
7.  Koehn P, Koehn P (1994) Combining Genetic Algorithms and Neural Networks: The 
Encoding Problem  
8.  Wang J, Ma Y, Zhang L, Gao RX (2018) Deep learning for smart manufacturing: 
Methods and applications. J Manuf Syst 48:144 â€“156. 
https://doi.org/10.1016/J.JMSY.2018.01.003  
9.  Voulodimos A, Doulamis N, Doulamis A, Protopapadakis E (2018) Deep Learning for 
Computer Visi on: A Brief Review. Comput Intell Neurosci 2018:1 â€“13. 
https://doi.org/10.1155/2018/7068349  
10.  Dumoulin V, Visin F, Box GEP (2018) A guide to convolution arithmetic for deep 
learning. arXiv Prepr arXiv arXiv160307285v2  
11.  Hayou S, Doucet A, Rousseau J ( 2018) On The Selection of Initialization and Activation 
Function for Deep Neural Networks. arXiv Prepr arXiv 180508266v2  
12.  Horiguchi S, Ikami D, Aizawa K (2017) Significance of Softmax -based Features in 
Comparison to Distance Metric Learning -based Featu res 
13.  Adit Deshpande A Beginnerâ€™s Guide To Understanding Convolutional Neural Networks 
â€“ Adit Deshpande â€“ CS Undergrad at UCLA ('19). https://adeshpande3.github.io/A -
Beginner%27s -Guide -To-Understanding -Convolutional -Neural -Networks/. Accessed 
19 Jul 201 8 
14.  Karami E, Shehata M, Smith A (2017) Image Identification Using SIFT Algorithm: 
Performance Analysis against Different Image Deformations  
15.  Bay H, Tuytelaars T, Van Gool L (2006) SURF: Speeded Up Robust Features. Springer, 
Berlin, Heidelberg, pp 4 04â€“417 
16.  Rosten E, Drummond T (2006) Machine Learning for High -Speed Corner Detection. 
Springer, Berlin, Heidelberg, pp 430 â€“443 
17.  Goldenshluger A, Zeevi A (2004) The Hough Transform Estimator. 32:. 
https://doi.org/10.1214/009053604000000760  
18.  Tsai FCD (1994) Geometric hashing with line features. Pattern Recognit 27:377 â€“389. 
https://doi.org/10.1016/0031 -3203(94)90115 -5 
19.  Wang J, Perez L The Effectiveness of Data Augmentation in Image Classification using 
Deep Learning  
20.  SchÃ¶ning J, Faion P, He idemann G (2016) Pixel -wise Ground Truth Annotation in 
Videos - An Semi -automatic Approach for Pixel -wise and Semantic Object Annotation. 
In: Proceedings of the 5th International Conference on Pattern Recognition Applications 
and Methods. SCITEPRESS - Science and and Technology Publications, pp 690 â€“697 
21.  Zhang X, Lee J -Y, Sunkavalli K, Wang Z (2017) Photometric Stabilization for Fast -
forward Videos  
22.  Alhaija HA, Mustikovela SK, Mescheder L, et al (2017) Augmented Reality Meets 
Computer Visionâ€¯: Effici ent Data Generation for Urban Driving Scenes  
23.  Meneghetti G, Danelljan M, Felsberg M, Nordberg K (2015) Image Alignment for 
Panorama Stitching in Sparsely Structured Environments. Springer, Cham, pp 428 â€“439 
24.  Alldieck T, Kassubeck M, Magnor M (2017) Optical Flow -based 3D Human Motion 
Estimation from Monocular Video  
25.  Zheng B, Zhao Y, Yu J, et al (2015) Scene Understanding by Reasoning Stability and 
Safety. Int J Comput Vis 112:221 â€“238. https://doi.org/10.1007/s11263 -014-0795 -4 
26.  Zheng L, Yang Y,  Tian Q SIFT Meets CNN: A Decade Survey of Instance Retrieval  
27.  AlDahoul N, Md Sabri AQ, Mansoor AM (2018) Real -Time Human Detection for 
Aerial Captured Video Sequences via Deep Models. Comput Intell Neurosci 2018:1 â€“
14. https://doi.org/10.1155/2018/1639 561 
28.  Conventional computer vision coupled with deep learning makes AI better | Network 
World. https://www.networkworld.com/article/3239146/internet -of-
things/conventional -computer -vision -coupled -with-deep -learning -makes -ai-
better.html. Accessed 12 Sep 2018  
29.  Bahrampour S, Ramakrishnan N, Schott L, Shah M (2015) Comparative Study of Deep 
Learning  Software Frameworks  
30.  (2017) An in -depth look at Googleâ€™s first Tensor Processing Unit (TPU) | Google Cloud 
Big Data and Machine Learning Blog | Google Cloud Platform. 
https://cloud.google.com/blog/big -data/2017/05/an -in-depth -look-at-googles -first-
tensor-processing -unit-tpu. Accessed 11 Jan 2018  
31.  Vision Processing Unit | Machine Vision Technology | Movidius. 
https://www.movidius.com/solutions/vision -processing -unit. Accessed 11 Jan 2018  
32.  Ng H -W, Nguyen D, Vonikakis V, Winkler S Deep Learning for Emotion Recognition 
on Small Datasets Using Transfer Learning. https://doi.org/10.1145/2818346.2830593  
33.  Pepik B, Stark M, Gehler P, Schiele B (2012) Teaching 3D geometry to deformable part 
models. In: Proceedings of the IEEE Computer Socie ty Conference on Computer Vision 
and Pattern Recognition  
34.  Russakovsky O, Deng J, Su H, et al (2015) ImageNet Large Scale Visual Recognition 
Challenge. Int J Comput Vis 115:211 â€“252. https://doi.org/10.1007/s11263 -015-0816 -y 
35.  Lin T -Y, Maire M, Belong ie S, et al (2014) Microsoft COCO: Common Objects in 
Context  
36.  CS231n Convolutional Neural Networks for Visual Recognition. 
http://cs231n.github.io/transfer -learning/. Accessed 9 Mar 2018  
37.  Highlander TC Efficient Training of Small Kernel Convolution al Neural Networks 
using Fast Fourier Transform  
38.  Highlander T, Rodriguez A (2016) Very Efficient Training of Convolutional Neural 
Networks using Fast Fourier Transform and Overlap -and-Add 
39.  Li F, Wang C, Liu X, et al (2018) A Composite Model of Woun d Segmentation Based 
on Traditional Methods and Deep Neural Networks. Comput Intell Neurosci 2018:1 â€“
12. https://doi.org/10.1155/2018/4149103  
40.  Nijhawan R, Das J, Raman B (2018) A hybrid of deep learning and hand -crafted features 
based approach for snow cover mapping. Int J Remote Sens 1 â€“15. 
https://doi.org/10.1080/01431161.2018.1519277  
41.  Zeng G, Zhou J, Jia X, et al (2018) Hand -Crafted Feature Guided Deep Learning for 
Facial Expression Recognition. In: 2018 13th IEEE International Conference on 
Automa tic Face & Gesture Recognition (FG 2018). IEEE, pp 423 â€“430 
42.  Burchfiel B, Konidaris G Hybrid Bayesian Eigenobjects: Combining Linear Subspace 
and Deep Network Methods for 3D Robot Vision  
43.  Marcus G, thank Christina I, Chollet F, et al Deep Learning: A Critical Appraisal  
44.  Nalisnick E, Smyth P (2018) Learning Priors for Invariance. 366 â€“375 
45.  Diligenti M, Roychowdhury S, Gori M (2017) Integrating Prior Knowledge into Deep 
Learning. In: 2017 16th IEEE International Conference on Machine Learning an d 
Applications (ICMLA). IEEE, pp 920 â€“923 
46.  Zhu H, Nie Y, Yue T, Cao X (2017) The role of prior in image based 3D modeling: a 
survey. Front Comput Sci 11:175 â€“191. https://doi.org/10.1007/s11704 -016-5520 -8 
47.  Tran D, Bourdev L, Fergus R, et al (2015) Le arning Spatiotemporal Features with 3D 
Convolutional Networks. arXiv Prepr arXiv 14120767  
48.  Pang G, Neumann U (2016) 3D point cloud object detection with multi -view 
convolutional neural network. In: 2016 23rd International Conference on Pattern 
Recognit ion (ICPR). IEEE, pp 585 â€“590 
49.  Lan Q, Wang Z, Wen M, et al (2017) High Performance Implementation of 3D 
Convolutional Neural Networks on a GPU. Comput Intell Neurosci 2017:1 â€“8. 
https://doi.org/10.1155/2017/8348671  
50.  Ahmed E, Saint A, Shabayek AER, et  al (2018) Deep Learning Advances on Different 
3D Data Representations: A Survey. arXiv Prepr arXiv 180801462  
51.  Zhou Y, Tuzel O (2017) VoxelNet: End -to-End Learning for Point Cloud Based 3D 
Object Detection. arXiv Prepr arXiv 171106396  
52.  Qi CR, Yi L,  Su H, Guibas LJ (2017) PointNet++: Deep Hierarchical Feature Learning 
on Point Sets in a Metric Space. arXiv Prepr arXiv170602413v1  
53.  Braeger S, Foroosh H (2018) Curvature Augmented Deep Learning for 3D Object 
Recognition. In: 2018 25th IEEE Internatio nal Conference on Image Processing (ICIP). 
IEEE, pp 3648 â€“3652  
54.  Niall Oâ€™ Mahony (Institute of Technology Tralee), Sean Campbell (Institute of 
Technology Tralee), Lenka Krpalkova (Institute of Technology Tralee), et al (2018) 
Deep Learning for Visual Nav igation of Unmanned Ground Vehicles; A review  
55.  Karami E, Prasad S, Shehata M Image Matching Using SIFT, SURF, BRIEF and ORB: 
Performance Comparison for Distorted Images  
56.  Angelina M, Gim U, Lee H PointNetVLAD: Deep Point Cloud Based Retrieval for 
Large -Scale Place Recognition  
57.  Camposeco F, Cohen A, Pollefeys M, Sattler T Hybrid scene Compression for Visual 
Localization  
58.  Loghmani MR, Planamente M, Caputo B, Vincze M Recurrent Convolutional Fusion 
for RGB -D Object Recognition  
59.  ClÃ©ment M, Kurtz C, Wendling L (2018) Learning spatial relations and shapes for 
structural object description and scene recognition. Pattern Recognit 84:197 â€“210. 
https://doi.org/10.1016/J.PATCOG.2018.06.017  
60.  Ran L, Zhang Y, Zhang Q, et al (2017) Con volutional Neural Network -Based Robot 
Navigation Using Uncalibrated Spherical Images. Sensors 17:1341. 
https://doi.org/10.3390/s17061341  
61.  Silva RMA, FeijÃ³ B, Gomes PB, et al (2016) Real time 360Â° video stitching and 
streaming. In: ACM SIGGRAPH 2016 Pos ters on - SIGGRAPH â€™16. ACM Press, New 
York, New York, USA, pp 1 â€“2 
62.  Fernandez -Labrador C, Perez -Yus A, Lopez -Nicolas G, Guerrero JJ Layouts from 
Panoramic Images with Geometry and Deep Learning  
63.  SchÃ¶ning J, Faion P, Heidemann G (2016) Pixel -wise Gr ound Truth Annotation in 
Videos - An Semi -automatic Approach for Pixel -wise and Semantic Object Annotation. 
In: Proceedings of the 5th International Conference on Pattern Recognition Applications 
and Methods. SCITEPRESS - Science and and Technology Publica tions, pp 690 â€“697 
64.  Ioannidou A, Chatzilari E, Nikolopoulos S, Kompatsiaris I (2017) Deep Learning 
Advances in Computer Vision with 3D Data. ACM Comput Surv 50:1 â€“38. 
https://doi.org/10.1145/3042064  
65.  Devries T, Taylor GW (2017) Dataset Augmentation i n Feature Space. arXiv Prepr 
arXiv 170205538v1  
66.  Dvornik N, Mairal J, Schmid C Modeling Visual Context is Key to Augmenting Object 
Detection Datasets  
 ","deep learning  vs traditional computer vision  
niall o mahony sean campbell  anderson carvalho suman harapanahalli 
gustavo velasco hernandez lenka krpalkova  daniel riordan joseph walsh  
imar technology gateway institute of technology tralee tralee ireland  
niallomahonyresearchittraleeie  
abstract deep learning  has pushed the limits of what was possible  in the 
domain of digital image processing  however  that is not  to say that the 
traditional computer vision techniques which had been undergoing progressive 
development in years prior to the rise of dl have become obsolete  this paper 
will analyse  the benefits and drawbacks of each approach  the aim of th is paper 
is to promote a discussion on whether  knowledge of classical computer vision 
techniques should be maintained  the paper will also explore  how the two sides 
of computer vision can be combined  several recent hybrid methodologies are 
reviewed  which have demonstrated the ability to improve computer vision 
performance  and to tackle problems not suited to deep learning  for example  
combining traditional computer vision techniques with deep learning  has been  
popular  in emerging domains  such as panoramic vision and 3d vision for which  
deep learning models have  not yet  been fully optimised  
keywords  computer vision  deep learning  hybrid techniques  
1 introduction  
deep learning  dl is used in the domain of digital image processing to solve difficult 
problems  eg image colourization  classification segmentation and detection  dl 
methods such as c onvolutiona l neural networks  cnn s mostly improve prediction 
performance using  big data and plentiful computin g resources  and have pushed the 
boundar ies of what was possible  problems which were assumed to be unsolvable are 
now being solved with super human accuracy  image classification is a prime example 
of this since being reignited by krizhevsky sutskever and hinton in 2012 1  dl 
has dominated the domain ever since due to a substantial ly better performance  
compared to traditional methods   
is dl making traditional computer vision cv techniques obsole te has dl 
superseded traditional computer vision is there still a need to study traditional cv 
techniques when dl seems to be so effective these are all question s which have been 
brought up in the community  in recent years   2 which this paper intends to address   
additionally  dl is not going t o solve all cv problems  there are some problems 
where  traditional techniques with global features  are a better solution  the advent of 
dl may open many  doors to do something with traditional techniques to overcome the 
many challenges dl brings  eg compu ting power time accuracy characteristics and 
quantity of inputs and among others  
 
this paper will provide a comparison of  deep learning  to the more traditional hand 
crafted feature definition approaches which dominated cv prior to it there has been 
so much progress in deep learning in recent years that it is impossible for this paper 
to capture the many facets and sub domains of deep lea rning which are tackling the 
most pertinent problems in cv today this paper will review traditional algorithmic 
approaches in cv and more particularly the applications in which they have been used 
as an adequate substitute for dl to complement dl and to tackle problems dl cannot  
the paper will then move on to review some of the recent activities in combining 
dl with  cv with a focus on the state oftheart techniques for emerging technology 
such as  3d perception  namely object registration object det ection and semantic 
segmentation of 3d point clouds finally  developments and possible directions of 
getting the performance of 3d dl to the same heights as 2d dl are discussed along 
with an outlook on the impact the increased use of 3d will have on cv in  general  
2 a comparison of deep learning  and traditional computer 
vision  
21 what is deep learning  
to gain a fundamental understanding of dl we need to consider the difference  between 
descriptive analysis and predictive analysis  
descriptive  analysis involves defining a comprehensible mathematical model which 
describes the phenomenon that we wish to observe this entails collecting data about a 
process  forming hypotheses  on patterns in the data  and validating these hypotheses  
through comparing the out come of descriptive models we form with the real outcome 
3 producing such models is precarious however because there is always a risk of un 
modelled variables that scientists and engineers neglect to include due to ignorance or 
failure to understand some complex hidden or non intuitive phenomena  4  
predictive  analysis  involves the discovery of rules that underlie a phenomenon and 
form a predictive model which minimise the error between the actual and the predicted 
outcome considering all possible interfering factors 3 machine learning rejects the 
traditional programming paradigm where problem analysis is replaced by a training 
framework where the system is fed a large number of training patterns sets of inputs 
for which the desired outputs are known which it learns and uses t o compute new 
patterns 5  
dl is a subset of machine learning  dl is based largely on artificial neural 
networks anns a computing paradigm inspired by the functioning of the human 
brain like the human brain it is composed of many computing cells or neurons that 
each perform  a simple operation and interact  with each other to make a decision  6 
deep learning is all about learning  or credit assignment across many layers of a 
neural network accurately efficiently and without supervision and is of recent interest 
due to enabling advancements in processing hardware 7 selforganisation and the 
exploitation of interactions between small units have proven to perform better than 
central control particularly for complex non linear process models in that better fault 
tolerance and adaptability to new data is achievable 7 
 
22 advantages  of deep learning  
rapid p rogression s in dl and improvements  in device capabilities including 
computing power memory capacity power consumption image sensor resolution and 
optics have improved the performance and cost effectiveness of  further quickened  the 
spread  of vision based application s compared to traditional cv techniques dl 
enable s cv engineers to achieve  greater accuracy in tasks such as image  classification  
semantic segmentation  object detection  and simultaneous localization and mapping 
slam  since neural networks used in dl are trained rather than programmed  
applications using this approach often require less expert analysis and finetuning  and 
exploit  the tremendous amount o f video data available in todays systems  dl also 
provides superior flexibility  because cnn  models  and frameworks can be re trained 
using a custom dataset  for any use case contrary  to cv algorithms which tend to be 
more domain specific  
 
taking  the prob lem of object detection on a mobile robot  as an example we can  
compare  the two types of algorithms for computer vision  
 
the traditional  approach is  to use well established  cv techniques such as  feature 
descriptors sift surf brief etc for object detection before the emergence of 
dl a step called feature extraction  was carried out for tasks such as image  
classification  features are small interesting descriptive or informative patches i n 
images several cv algorithms  such as  edge detection corner detection  or threshold 
segmentation  may be involved in this step  as many features  as practicable are 
extracted from images and these features form a definition known as a bag ofwords 
of each object class  at the deployment stage  these definitions are search ed for in other 
images if a significant number of features from one bag ofwords are in  another image 
the image is classified as containing that specific object ie chair horse etc  
the difficulty with this traditional approach is that it is necessary  to choose which 
features are important  in each given image  as the number of classes to classify 
increases feature extraction  becomes more and more cumbersome  it is up to the cv 
engineers judgment and a long trial and error process to decide which features best 
describe  different classes of objects moreover each feature  definition requires dealing 
with a plethora of parameters all of which must  be fine tuned by  the cv engineer  
dl introduced the concept of end toend learning where the machine is  just given a 
dataset of images  which have been annotated with what classes of object are present in 
each image  7  thereby a dl m odel is trained on the given data  where  neural 
networks discover  the underlying patterns in classes of images and automatically works 
out the most descriptive and salient features with respect to each specific class of object 
for each object it has been well established  that dnn s perform  far better than 
traditional algorithms albeit with  trade offs with respect to computing requirements  
and training time  with  all the state oftheart approaches in cv employing this 
methodology the workflow of the cv engineer has changed dramatically  where the 
knowledge and expertise in extracting hand crafted features has been replaced by 
knowledge and expertise in iterating thro ugh deep learning architectures  as depicted in 
fig 1  
 
 
fig 1  a traditional computer vision workflow vs b deep learning wo rkflow  figure from 8 
 
the development of cnns  has had a tremendous influence in the field of cv in recent 
years and is responsible for a big jump in the ability to recognize objects  9 this burst 
in progress has been enabled by a n increase in computing power as well as an increase 
in the amount of data available for training neural networks  the recent explosion in 
and wide spread adoption of various deep neural network architectures for cv is 
apparent in the fact that the semina l paper imagenet classification with deep 
convolutional neural networks has been cited over 3000 times 2  
cnn s make use of kernels also known as filters to detect features eg edges 
throughout an image a kernel is just a matrix of values called weights which  are 
trained to detect specific features as their name indicates the main idea behind the 
cnns  is to spatially convolve the kernel on a given input image check if the feature it 
is meant to detect is present  to provide a value representing how confident it is that a 
specific feature is present a convolution operation is carried out by computing the  dot 
product of the kernel and the input area where kernel is overlapped the area of the 
original image the kernel is looking at is known as the receptive field 10 
to facilitate the learning of kernel  weights  the convolution layers output is   
summed  with a bias term and then fed to a non linear activation function activation 
functions are usually non linear func tions like sigmoid tanh  and relu rectified 
linear unit depending on the nature of data and classification tasks these activation 
functi ons are selected accordingly  11 for example  relus are known to have more 
biological representation  neurons in th e brain either fire or they dont as a result it 
yields favourab le results for image recognition tasks  as it is less susceptible to  the 
vanishing gradient problem and it produces sparser more efficient representations  7  

to speed up the training process and reduce the amount of memory consumed by 
the network the convolutional layer is often followed by a pooling layer to remove 
redundancy present in the input feature for example max pooling moves a window 
over the input and  simply outputs the maximum value in that window effectively 
reducing  to the important pixels in an image  7 as shown in fig 2  deep cnns may 
have several pairs of convolutional and pooling layers  finally a  fully connected 
layer flattens the previous layer volume into a feature vector and then an output layer 
which computes the scores confidence or probabilities for th e output classesfeatures 
through a dense network this output is then passed to a regression function such as 
softmax  12 for example which  maps everything to a vector whose elements sum 
up to one  7  
 
fig 2  building blocks of a cnn figure from 13 
but dl is still only a tool of cv for example the most common neural network used 
in cv is the  cnn but what is a convolution its in fact a widely used image 
processing technique eg see  sobel edge detection  the advantages of dl are clear  
and it would be beyond the scope of this paper to review the state oftheart dl is 
certainly not the panacea for all problems either as we will see in following sections of 
this paper there are problems and applications where the more conventional cv 
algorithms are more suitable  
 
 
23 advantages of traditional  computer vision techniques  
this section will detail how the t raditional feature based  approaches such as those listed 
below have been shown to be useful in improving performance in cv tasks  
 scale invariant feature transform sift  14 
 speeded up robust features surf 15 
 features from accelerated segment test fast  16 
 hough transforms  17 
 geometric hashing  18 
 
feature descriptors such as sift  and surf  are generally combined with traditional 
machine learning classification algorithms such as support vector machines and k 
nearest neighbours  to solve the aforementioned cv problems   

dl is sometimes overkill  as often traditional cv techniques can solve a problem 
much more efficiently and in fewer lines of code than dl  algorithms  like sift and 
even simple colour thresholding and pixel counting algorithms are not class specific 
that is they are very general and perform the same for any image  in contrast  features 
learned from a deep neural net are specific  to your train ing dataset  which if not well 
constructed probably wont perform well for images different from the training set  
therefore  sift  and other algorithms  are often used for applications such as  image 
stitching3d mesh reconstruction  which  dont require specific class knowledge these 
tasks have been shown to be achievable by training large datasets  however this requires 
a huge research effort and it is not practical to go through this effort for a closed  
application  one needs to practice common sense when it comes to choosing which 
route to take for a given cv application for example to classify two classes of product 
on an assembly line conveyor belt  one with red paint  and one with blue paint  a deep 
neural  net will work given that enough  data can be collected to train from however  
the same can be achieved  by using simple colour thresholding  some  problems can be 
tackled with  simpler and faster techniques  
what if a dnn perform s poorly outside o f the training data  if the training dataset 
is limited  then the machine may overfit to the training data and  not be able to generalize 
for the task  at hand  it would be too difficult to manually tweak the parameters of the 
model  because  a dnn  has million s of parameters inside of it  each with complex inter 
relationships  in this way dl model s have been criticised to be a black box  in this way 
5 traditional cv has full transparency and the one can  judge whether your solution 
will work outside of a training environment  the cv engineer can  have insight s into a 
problem that they can transfer to their  algorithm  and if anything fails the parameters 
can be tweaked to perform well for a wider range of images  
today the traditional techniques are used when the problem can be simplified so 
that they can be deployed on low cost  microcontrollers or to limit the problem for deep 
learning techniques by highlighting certain features in data augmenting data  19 or 
aiding in dataset annotation   20 we will discuss later in this paper how many image 
transformatio n techniques can be used to improve your neural net training finally 
there  are many more cha llenging problems in cv such as robotic s 21 augmented 
reality  22 automatic panorama stit ching  23 virtual reality  24 3d modelling  24 
motion estimation  24 video stabilization  21 motion capture  24 video processing  
21 and scene understanding  25 which cannot simply be easily implemented in a 
differentiable manner with deep learning but benefit from solutions  using traditional 
techniques  
3 challenges for traditional c omputer vision  
31 mixing hand crafted approaches  with dl for better performance  
there are clear trade offs between traditional cv and deep learning based 
approaches classic cv algorithms are wellestablished  transparent  and optimized for 
performance and power efficiency while dl offers greater accuracy and versa tility at 
the cost of  large amounts of computing resources   
hybrid approaches combine traditional cv and deep learning  and offer the 
advantages traits of both methodologies they are especially practical in high 
performance  systems which need to be implem ented quickly  for example in a security 
camera a cv algorithm can efficiently detect faces or other features 26 or moving 
objects  27 in the scene these detections can then be passed to a dnn for identity 
verification or object classification  the dnn need only be applied on a small patch of 
the image  saving significant computing resources  and training effort  compared to what 
would be required to process the entire frame  
the fusion of machine learning metr ics and deep network have become very 
popular due to the simple fact that it can generate better models  hybrid vision 
processing implementation s can introduce  performance advantage and  can deliver a 
130x 1000x reduction in multiply accumulate operation s and about 10x 
improvement in frame rates compared to a pure dl solution furthermore the hybrid 
implementation uses about half of the memory bandwidth and requires significantly 
lower cpu resources  28  
32 overcoming the challenges of deep learning  
there are also challenges introduced by dl the latest dl approaches may achieve 
substantially better accuracy  however  this jump comes at the cost of billions of 
additional math operations and an increased requirement for processing power  dl 
requires a these  computing resources f or training and to a lesser extent for inference  it 
is essential to have dedicated hardware eg high powered gpus 29 and tpus 30 
for training and ai accelerated platforms such as vpus for inference 31 for 
developers of ai  
vision process ing results using dl are also dependent on image resolution 
achieving adequate performance in object classification for example requires high
resolution  images or video  with the consequent increase in the amount of data that 
needs to be processed sto red and transferred image resolution is especially important 
for applications in which it is necessary to detect and classify objects in the distance  
eg in security camera  footage  the frame reduction techniques discussed previously 
such as using sift  features 26 32  or optical flow for moving objects 27 to first 
identify a region of interest are useful with respect to image resolution and also with 
respect to reducing t he time and data required for training  
dl needs big data  often millions of data records are required for example 
pascal  voc dataset consists of 500k images with 20 object categories  2633 
imagenet consists of 15 million images with 1000 object categories  34 and microsoft 
common objects in context coco consists of 25 million images with 91 object 
categories  35 when big datasets  or high computing facility are unavailable  
traditional methods will come into play  
training a dnn takes a very long time depending on computing hardware 
availability  training can take a matter of hours or days moreover training for any 
given application often requires many iterations  as it entails trial and error with  different 
training parameters the most common technique to reduce training time is tr ansfer 
learning 36 with respect to traditional cv the discrete fourier transform is another 
cv technique which once experienced major popularity but now seems obscure the 
algorithm can be used to speed up convolutions  as  demonstrated by 37 38  and hence 
may again become of major importance   
however it must be said that e asier  more domain specific  tasks than general image 
classification will not re quire as much data in the order of hundreds or thousands rather 
than millions  this is still a considerable amount of data and cv techniques  are often 
used to boost training data  through  data augmentation or reduce the data down to a 
particular type of f eature through other pre processing steps  
preprocessing entails transforming the data usually with traditional cv techniques 
to allow relationshipspatterns to be more easily interpreted before training your model 
data augmentation is a common pre processing task which is used when there is limited 
training data it can involve performing random rotations shifts shears etc on the 
images in your training set to effectively increase the number of training images  19 
another approach is to highlight features of interest before passing the data to a cnn 
with cv based methods such as background subtraction and segmentation 39 
33 making best use of edge computing  
if algorithms and neural network inferences can be run at the edge latency costs cloud 
storage and processing requirements and bandwidth requirements are reduced 
compared to cloud based implementations edge computing can also privacy and 
security requirements by avoiding transmission  of sensitive or identifiable data over the 
network  
hybrid or composite approaches involving conventional  cv and dl  take great 
advantage of the heterogeneous computing capabilities  available at the edge  a 
heterogeneous compute  architecture consists of a combination of cpus 
microcontroller coprocessors digital signal processors dsps field programmable 
gate arrays fpgas and ai accelerating devices 31 and can be power efficient by  
assigning different workloads to the most efficient compute engine test 
implementations show 10x latency reductions in object detection when dl inferences 
are executed on a dsp versus a cpu  28  
several hybrids  of deep learning and hand crafted features based approach es have 
demonstrated their benefits in edge applications for example  for facial expression 
recognition  41 propose a new feature lo ss to embed the information of hand crafted 
features into the training process of network which tries to reduce the difference 
between  hand crafted features and features learned by  the deep neural network  the use 
of hybrid approaches has also been shown to be advantageous in incorporating data 
from other sensors on edge nodes such a  hybrid model where th e deep learning is 
assisted by additional sensor sources like synthetic aperture radar sar imagery and 
elevation like synthetic aperture radar s ar imagery and elevation  is presented by 
40 in the context of 3d robot vision 42 have shown that combining both linear 
subspace methods and deep convolutional prediction achieves improved performance 
along with several orders of magnitude faster runtime performance compared  to the 
state of the art  
34 problems not suited to deep learning  
there are many more changing problems in cv such as robotic augmented reality 
automatic panorama stitching virtual reality 3d modelling motion stamation  video 
stabilization motion captu re video processing and scene understanding which cannot 
simply be easily implemented in a differentiable manner with deep learning but need 
to be solved using the other traditional techniques  
dl excels at solving closed end classification problems in  which a wide range of 
potential signals must be mapped onto a limited number of categories given that there 
is enough data available and the test set closely resembles the training set however 
deviations from these assumptions can cause problems and it  is critical to acknowledge 
the problems which dl is not good at marcus et al present ten concerns for deep 
learning and suggest that deep learning must be supplemented by other techniques if 
we are to reach artificial general intelligence  43 as well  as discussing the limitations 
of the training procedure and intense computing and data requirements as we do in our 
paper key to their discussion is identifying problems where dl performs poorly and 
where it can be supplemented by other techniques   
one such problem is the limited ability  of dl algorithms  to learn visual relations  
ie identifying whether multiple objects in an image are the same or different this 
limitation has been demonstrated by 43 who argue that feedback mechanisms 
including attention and perceptual grouping may be the key computational components 
to realising  abstract visual reasoning   
it is also worth noting that ml models find it difficult to deal with priors that is not 
everythin g can be learnt from data  so some priors must be injected into the models  
44 45 solutions that have to do with 3d cv need strong priors in order to work 
well eg image based 3d modelling requires smoothness silhouette and illumination 
information 46  
below are  some emerging fields in cv where  dl faces new challenges and where  
classic cv will have a more prominent role  
35 3d vision  
3d vision systems are becoming increasingly accessible and as such there has been a 
lot of progress in the design of 3d convolutional neural networks 3d cnns this 
emerging field is known as geometric deep learning and has multiple applications 
such as video classification computer graphics vision and robotics this paper will 
focus on 3dcnns for processing data from 3d vision system s wherein 2d 
convolutional layers the kernel has the  same depth so as to output a 2d matrix the 
depth of a 3d convolutional kernel must be less than that of the 3d input volume so 
that the output of the convolution is also 3d and so preserve the spatial information  
 
fig 3  2dcnn vs 3d cnn 47 
 
the size of the input is much larger in terms of memory than conventional rgb images 
and the kernel m ust also be convolved through the input space in 3 dimensions  see fig 
3 as a result the computational complexity of 3d cnns grows cubically with 
resolution  compared to 2d image processing 3d cv is made even more difficult as 
the extra dimension introduces more u ncertainties such as occlusions and different 
cameras angles as shown in fig 4  
 
 
 
fig 4  3d object detection in point clouds is a challenging problem due to discrete sampling noisy scans 
occlusions and cluttered scenes figure from 48 
fft based methods can optimise 3d cnns reduce the amount of computation at the 
cost of increased memory requirements however  recent research h as seen the 

implementation of the winograd minimal filtering algorithm wmfa achieve a two 
fold speedup compared to cudnn  nvidias languageapi for programming on 
their graphics cards  without increasing the required memory 49  the next section 
will include some solutions with novel architectures and pre processing steps to various 
3d data representations which have been proposed to overcome these challenges  
geometric deep learning gdl deals with the extension of dl techniques to 3d 
data 3d data can be represented in a variety of different ways  which can be classified 
as euclidean  or noneuclidean 503d euclidean structured data has an underlying 
grid structure that allows for a global parametrization and having a common  system of 
coordinates  as in 2d images  this allows  existing 2d dl paradigms and 2dcnns can 
be applied to 3d data   3d euclidean data is more suitable for analysing  simple rigid 
objects such as chairs planes etc  eg with voxel based approaches  51 on the other 
hand 3d non euclidean data do not have the grid ded array structure where there is no 
global parametrization therefore extending classical dl techniques to such 
representations is a challenging task  and has only recently been realized with 
architectures such as point net 52  
 continuous shape information that is useful for recognition is often lost in their 
conversion to a voxel representation w ith respect to tradit ional cv algorithms  53 
propose a single dimensional feature that can be applied to vo xel cnns a novel 
rotation invariant feature based on mean curvature that improves shape recognition for 
voxel cnns  was proposed  the method was very suc cessful in that when it was applied 
to the stateoftheart recent voxel cnn octnet  architecture a 1 o verall accuracy 
increase on the modelnet10 dataset  was achieved   
 
36 slam  
visual slam is a subset  of slam where a vision system is used instead of lidar for 
the registration of landmarks in a scene visual slam has the advantages of 
photogrammetry rich visual data low cost lightweight and low power consumption 
without the associated heavy computat ional workload involved in post processing the 
visual slam problem consists of steps such as environment sensing data matching 
motion estimation as well as location update and registration of new landmarks  54 
building a model of how visual objects appear in different conditions such as 3d 
rotation scaling lighting and extending from that representation using a strong form of 
transfer learning to achieve zero one shot  learning  is a challenging problem in this 
domain feature extraction and data representation  method s can be useful to reduce the 
amount of training examples needed for an ml model  55  
a two step a pproach is commonly used in image based  localization  place 
recognition followed by pose estimation  the former computes  a global descriptor for 
each of the images by aggregating local image descriptors eg sift using the bag of
words approac h each glo bal descriptor is stored in the database together with the 
camera pose of its associated image with respect to the 3d point cloud reference map 
similar global descriptor s are extracted from the query image and the closest global 
descriptor  in the database  can be retrieved via an efficient search the camera pose of 
the closest global descriptor would give us a coarse localization of the query image with 
respect to the reference map in pose estimation the exact pose of the query image 
calculated more prec isely with algorithms such as the perspective npoint pnp 13 
and geometric verification 18 algorithms 56 
the success of image based  place recognition is largely attributed to the ability to 
extract image feature descriptors unfortunately there is no algor ithm to extract local 
features similar to sift for lidar scans a 3d scene is composed of 3d points and 
database images one approach has associated e ach 3d point to a set of sift 
descriptors corresponding to the image features from which the point was tri angulated 
these descriptors can then be averaged into a single sift descriptor that describes the 
appearance of that point  57 
another approach constructs multi modal f eatures from rgb d data rather than the 
depth processing for the depth processing part they adopt the wellknown  
colourization method based on surface normals since it has been proved to be effective 
and robust across tasks  58 another alternative approach ut ilizing traditional cv 
techniques  present s the force histogram decomposition fhd a graph based 
hierarchical descriptor that allows the spatial relations and shape information between 
the pairwise structural subparts of objects  to be characterized  an ad vantage of this 
learning procedure is its compatibility with traditional bags offeatures frameworks 
allowing for hybrid representations gathering structural and local features  59 
 
37 360 camera s 
a 360 camera also known as  an  omnidirectional or spherical  or panoramic  camera is 
a camera with a 360degree  field of view  in the horizontal plane or with a visual field 
that covers approximately the entire sphere omnidirectional cameras are important 
in applications suc h as robotics  where large visual field coverage is needed  a 360 
camera can replace multiple monocular cameras  and eliminate blind spots which 
obviously advantageous in omnidirectional unmanned ground vehicles ugvs and 
unmanned aerial vehicles uavs  thanks to the imaging characteristic of spherical 
cameras each image captures the 360 panorama  of the scene eliminating the 
limitation on available steering choices one of the major challenges with spherical 
images is the heavy barrel distortion due to t he ultrawide angle fisheye lens which 
complicates the implementation of conventional human vision inspired methods such 
as lane detection and trajectory tracking additional preprocessing  steps such as prior 
calibration and deworming  are often required  an alternative approach which has been 
presented by 60 who circumvent  these preprocessing  steps by formulating 
navigation as a classification problem on finding the optimal potential path orientation 
directly based on the raw uncalibrated spherical images   
panorama stitching is an other open research problem  in this area  a real time 
stitchi ng methodology 61  uses a group of deformable meshes  and the final image  and 
combine the inputs using a robust pixel shader another approach  62 combine the 
accuracy provided by geometric reasoning lines and vanishing points with the higher 
level of data abstraction and pattern recognition achieved by dl techniques edge and 
normal maps  to extract structural and generate layout hypotheses for indoor scenes  in 
sparsely structured scenes  feature based image alignment methods often fail due to 
shortage  of distinct image features instead direct image alignment methods such as 
those based on phase correlation can be applied correlation based image alignment 
techniques based on discriminative correlation filters dcf  have been investigat ed 
by 23 who show that the proposed dcf based methods outper form phase 
correlation based approaches on these datasets  
38 dataset annotation  and augmentation  
there are arguments against the combination of cv and dl and they summarize to the 
conclusion that we need to re evaluate our methods from rule based to data driven 
traditionally from the perspective of signal processing we know the operational 
connotations of cv algorithms such as sift and surf method s but dl leads such 
meaning nowhere all you need is more data this can be seen as a huge step forward  
but may be  also a backward move some of the pros and cons of each side of this debate 
have been discussed already in this paper however if future methods are to be purely 
datadriven then focus should be placed on more intelligent methods for dataset 
creation  
 the fundamental problem of current research is that there is no longer enough data 
for advanced algorithms or models for special applications  coupling custom datasets 
and dl models  will be the future theme to many research papers so many researchers 
outputs consist of not only algorithms or architectures but also datasets or methods  to 
amass data dataset annotation is a major bottleneck  in the dl workflow which requires 
many hours of manual labelling nowhere is this more problematic than in  semantic 
segmentation  applications where every pixel needs to be annotated accurately there 
are many useful tools available to semi automate the process  as reviewed by 20 many  
of which take advantage of algorith mic approaches  such as orb features   55 polygon 
morphing 63 semi automatic area of interest aoi fitting  55 and all of the above 
63 
the easiest and most common method to overcome limited datasets and reduce 
overfitting of deep learning models for image classification is to artificially enlarge the 
dataset using label preserving transformations  this process is known as dataset 
augmentatio n and it involves the artificial generation of extra training data from the 
available ones for example by cropping scaling or rotating images 64 it is desirable 
for data augmentation procedures to require very little computation  and to be 
implementable within the dl training pipeline so that the transformed images do not 
need to be stor ed on disk  traditional algorithmic approaches that have been employed 
for dataset augmentation include principle component analysis pca 1 adding 
noise interpolating or extrapolating  between samples in a feature space 65 and 
modelling  the visual context surrounding objects from segmentation annotations 66 
conclusion  
a lot of the cv techniques invented in the past 20 years have become irrelevant in 
recent years because of dl however knowledge is never obsolete and there is always 
something worth learning from e ach generation of innovation that knowledge can give 
you more intuitions and tools to use especially when you wish to deal with 3d cv 
problems for example knowing only dl for cv will dramatically limit the kind of 
solutions in a cv engin eers arsenal  
in this paper  we have laid down m any arguments for why traditional cv techniques 
are still very much useful  even in the age of dl  we have compared and contrasted 
traditional cv and dl for typical  applications and d iscussed how s ometimes traditional 
cv can be considered as an alternative in situations  where dl  is overkill for a specific 
task  
the paper also highlight ed some areas where traditional cv techniques remain 
relevant  such as being utilized in hybrid approaches to improv e performance  dl 
innovations are driving exciting breakthroughs for the iot  internet of things  as well 
as hybrid techniques that combine the technologies with traditional algorithms 
additionally we reviewed how traditional cv techniques can actually improve dl 
performance in a wide range of applications from reducing training time processing 
and data requirements to being applied in emerging fields such as slam panoramic 
stitching geometric deep learning and 3d vision where dl  is not yet well 
established  
the digital image processing domain has undergone some very dramatic changes 
recently and in a very short period  so much so it has led us to question whether the cv 
techniques that were in vogue prior to the ai explosion are still relevant this paper 
hopefully highlight  some cases where traditional cv techniques are useful and that 
there is something still to gain from the years of effort put in to their develop ment even 
in the age of data driven intelligence   
references  
1  krizhevsky a sutskever i hinton ge 2012 imagenet classification with deep 
convolutional neural networks nips12 proc 25th int conf neural inf process syst 
11097 1105  
2  nash w drummond t birbilis n 2018 a review of deep learning in the study of 
materials degradation npj mater degrad 237 httpsdoiorg101038s41529 018
0058 x 
3  bonaccorso g 2018 machine learning algorithms popular algorithms for data 
science and machine learning 2nd editi on packt publishing ltd  
4  mahony no murphy t panduru k et al 2017 improving controller performance in 
a powder blending process using predictive control in 2017 28th irish signals and 
systems conference issc ieee pp 1 6 
5  omahony n murphy  t panduru k et al 2017 real time monitoring of powder blend 
composition using near infrared spectroscopy in 2017 eleventh international 
conference on sensing technology icst ieee pp 1 6 
6  o mahony n murphy t panduru k et al 2016 adaptive  process control and sensor 
fusion for process analytical technology in 2016 27th irish signals and systems 
conference issc ieee pp 1 6 
7  koehn p koehn p 1994 combining genetic algorithms and neural networks the 
encoding problem  
8  wang j ma y zhang l gao rx 2018 deep learning for smart manufacturing 
methods and applications j manuf syst 48144 156 
httpsdoiorg101016jjmsy201801003  
9  voulodimos a doulamis n doulamis a protopapadakis e 2018 deep learning for 
computer visi on a brief review comput intell neurosci 20181 13 
httpsdoiorg10115520187068349  
10  dumoulin v visin f box gep 2018 a guide to convolution arithmetic for deep 
learning arxiv prepr arxiv arxiv160307285v2  
11  hayou s doucet a rousseau j  2018 on the selection of initialization and activation 
function for deep neural networks arxiv prepr arxiv 180508266v2  
12  horiguchi s ikami d aizawa k 2017 significance of softmax based features in 
comparison to distance metric learning based featu res 
13  adit deshpande a beginners guide to understanding convolutional neural networks 
 adit deshpande  cs undergrad at ucla 19 httpsadeshpande3githubioa 
beginner27s guide tounderstanding convolutional neural networks accessed 
19 jul 201 8 
14  karami e shehata m smith a 2017 image identification using sift algorithm 
performance analysis against different image deformations  
15  bay h tuytelaars t van gool l 2006 surf speeded up robust features springer 
berlin heidelberg pp 4 04417 
16  rosten e drummond t 2006 machine learning for high speed corner detection 
springer berlin heidelberg pp 430 443 
17  goldenshluger a zeevi a 2004 the hough transform estimator 32 
httpsdoiorg101214009053604000000760  
18  tsai fcd 1994 geometric hashing with line features pattern recognit 27377 389 
httpsdoiorg1010160031 32039490115 5 
19  wang j perez l the effectiveness of data augmentation in image classification using 
deep learning  
20  schning j faion p he idemann g 2016 pixel wise ground truth annotation in 
videos  an semi automatic approach for pixel wise and semantic object annotation 
in proceedings of the 5th international conference on pattern recognition applications 
and methods scitepress  science and and technology publications pp 690 697 
21  zhang x lee j y sunkavalli k wang z 2017 photometric stabilization for fast 
forward videos  
22  alhaija ha mustikovela sk mescheder l et al 2017 augmented reality meets 
computer visionâ€¯ effici ent data generation for urban driving scenes  
23  meneghetti g danelljan m felsberg m nordberg k 2015 image alignment for 
panorama stitching in sparsely structured environments springer cham pp 428 439 
24  alldieck t kassubeck m magnor m 2017 optical flow based 3d human motion 
estimation from monocular video  
25  zheng b zhao y yu j et al 2015 scene understanding by reasoning stability and 
safety int j comput vis 112221 238 httpsdoiorg101007s11263 0140795 4 
26  zheng l yang y  tian q sift meets cnn a decade survey of instance retrieval  
27  aldahoul n md sabri aq mansoor am 2018 real time human detection for 
aerial captured video sequences via deep models comput intell neurosci 20181 
14 httpsdoiorg10115520181639 561 
28  conventional computer vision coupled with deep learning makes ai better  network 
world httpswwwnetworkworldcomarticle3239146internet of
thingsconventional computer vision coupled withdeep learning makes ai
betterhtml accessed 12 sep 2018  
29  bahrampour s ramakrishnan n schott l shah m 2015 comparative study of deep 
learning  software frameworks  
30  2017 an in depth look at googles first tensor processing unit tpu  google cloud 
big data and machine learning blog  google cloud platform 
httpscloudgooglecomblogbig data201705an indepth lookatgoogles first
tensorprocessing unittpu accessed 11 jan 2018  
31  vision processing unit  machine vision technology  movidius 
httpswwwmovidiuscomsolutionsvision processing unit accessed 11 jan 2018  
32  ng h w nguyen d vonikakis v winkler s deep learning for emotion recognition 
on small datasets using transfer learning httpsdoiorg10114528183462830593  
33  pepik b stark m gehler p schiele b 2012 teaching 3d geometry to deformable part 
models in proceedings of the ieee computer socie ty conference on computer vision 
and pattern recognition  
34  russakovsky o deng j su h et al 2015 imagenet large scale visual recognition 
challenge int j comput vis 115211 252 httpsdoiorg101007s11263 0150816 y 
35  lin t y maire m belong ie s et al 2014 microsoft coco common objects in 
context  
36  cs231n convolutional neural networks for visual recognition 
httpcs231ngithubiotransfer learning accessed 9 mar 2018  
37  highlander tc efficient training of small kernel convolution al neural networks 
using fast fourier transform  
38  highlander t rodriguez a 2016 very efficient training of convolutional neural 
networks using fast fourier transform and overlap andadd 
39  li f wang c liu x et al 2018 a composite model of woun d segmentation based 
on traditional methods and deep neural networks comput intell neurosci 20181 
12 httpsdoiorg10115520184149103  
40  nijhawan r das j raman b 2018 a hybrid of deep learning and hand crafted features 
based approach for snow cover mapping int j remote sens 1 15 
httpsdoiorg1010800143116120181519277  
41  zeng g zhou j jia x et al 2018 hand crafted feature guided deep learning for 
facial expression recognition in 2018 13th ieee international conference on 
automa tic face  gesture recognition fg 2018 ieee pp 423 430 
42  burchfiel b konidaris g hybrid bayesian eigenobjects combining linear subspace 
and deep network methods for 3d robot vision  
43  marcus g thank christina i chollet f et al deep learning a critical appraisal  
44  nalisnick e smyth p 2018 learning priors for invariance 366 375 
45  diligenti m roychowdhury s gori m 2017 integrating prior knowledge into deep 
learning in 2017 16th ieee international conference on machine learning an d 
applications icmla ieee pp 920 923 
46  zhu h nie y yue t cao x 2017 the role of prior in image based 3d modeling a 
survey front comput sci 11175 191 httpsdoiorg101007s11704 0165520 8 
47  tran d bourdev l fergus r et al 2015 le arning spatiotemporal features with 3d 
convolutional networks arxiv prepr arxiv 14120767  
48  pang g neumann u 2016 3d point cloud object detection with multi view 
convolutional neural network in 2016 23rd international conference on pattern 
recognit ion icpr ieee pp 585 590 
49  lan q wang z wen m et al 2017 high performance implementation of 3d 
convolutional neural networks on a gpu comput intell neurosci 20171 8 
httpsdoiorg10115520178348671  
50  ahmed e saint a shabayek aer et  al 2018 deep learning advances on different 
3d data representations a survey arxiv prepr arxiv 180801462  
51  zhou y tuzel o 2017 voxelnet end toend learning for point cloud based 3d 
object detection arxiv prepr arxiv 171106396  
52  qi cr yi l  su h guibas lj 2017 pointnet deep hierarchical feature learning 
on point sets in a metric space arxiv prepr arxiv170602413v1  
53  braeger s foroosh h 2018 curvature augmented deep learning for 3d object 
recognition in 2018 25th ieee internatio nal conference on image processing icip 
ieee pp 3648 3652  
54  niall o mahony institute of technology tralee sean campbell institute of 
technology tralee lenka krpalkova institute of technology tralee et al 2018 
deep learning for visual nav igation of unmanned ground vehicles a review  
55  karami e prasad s shehata m image matching using sift surf brief and orb 
performance comparison for distorted images  
56  angelina m gim u lee h pointnetvlad deep point cloud based retrieval for 
large scale place recognition  
57  camposeco f cohen a pollefeys m sattler t hybrid scene compression for visual 
localization  
58  loghmani mr planamente m caputo b vincze m recurrent convolutional fusion 
for rgb d object recognition  
59  clment m kurtz c wendling l 2018 learning spatial relations and shapes for 
structural object description and scene recognition pattern recognit 84197 210 
httpsdoiorg101016jpatcog201806017  
60  ran l zhang y zhang q et al 2017 con volutional neural network based robot 
navigation using uncalibrated spherical images sensors 171341 
httpsdoiorg103390s17061341  
61  silva rma feij b gomes pb et al 2016 real time 360 video stitching and 
streaming in acm siggraph 2016 pos ters on  siggraph 16 acm press new 
york new york usa pp 1 2 
62  fernandez labrador c perez yus a lopez nicolas g guerrero jj layouts from 
panoramic images with geometry and deep learning  
63  schning j faion p heidemann g 2016 pixel wise gr ound truth annotation in 
videos  an semi automatic approach for pixel wise and semantic object annotation 
in proceedings of the 5th international conference on pattern recognition applications 
and methods scitepress  science and and technology publica tions pp 690 697 
64  ioannidou a chatzilari e nikolopoulos s kompatsiaris i 2017 deep learning 
advances in computer vision with 3d data acm comput surv 501 38 
httpsdoiorg1011453042064  
65  devries t taylor gw 2017 dataset augmentation i n feature space arxiv prepr 
arxiv 170205538v1  
66  dvornik n mairal j schmid c modeling visual context is key to augmenting object 
detection datasets","['arxiv160307285v2', 'httpsdoiorg101214009053604000000760', 'arxiv170602413v1', 'httpsdoiorg10115520187068349', 'httpsdoiorg101038s41529', 'httpsdoiorg103390s17061341', 'httpsdoiorg101007s11704', 'httpsdoiorg1010800143116120181519277', 'httpsdoiorg101007s11263', 'httpswwwmovidiuscomsolutionsvision']"
Enhancing camera surveillance using computer vision: a research note,"['Haroon Idrees', 'Mubarak Shah', 'Ray Surette']",2018,http://arxiv.org/abs/1808.03998v1," 
   
 
 
Enhancing camera surveillance  
using computer vision: a research note 
 
Haroon Idrees and Mubarak Shah 
Center for Research in Computer Vision,  
University of Central Florida,  
Orlando, Florida, USA, and  
 
Ray Surette  
Department of Criminal Justice,  
University of Central Florida,  
Orlando, Florida, USA  
1 Enhancing Camera Surveillance using Computer Vision: A Research Note 
 Int
roduction  
Police Surveillance of Public Spaces.  Historically called the â€œstake-outâ€, police 
surveillance has a long history and evidence gained from surveillance has been an important part 
of investigations for nearly two centuries (Marx, 1988).  Similarly, the use of visual technology 
by police began in the nineteenth century with the photographing of inmates and evolved to 
include crime scene photographs as standard police procedures (Buckland, 2001; Norris & 
Armstrong, 1999a). While both practices became law enforcement mainstays, police surveillance 
and visual evidence remained separate realms well into the twentieth century (Norris & 
Armstrong, 1999a).   Surveillance cameras operated by law enforcement are therefore a 
relatively recent phenomenon and their marriage has moved surveillance from a human based 
activity to a heavily technological one. 
As evolved, police surveillance has two goals: proactively deterring offenders and aiding 
in investigations.   Initially, the investigative goal dominated and surveillance was aimed at 
solving crimes, not preventing them.  But as cameras became less expensive and more pervasive, 
deterrence and risk reduction became important (Kroener, 2014).  Current camera surveillance 
projects aim to provide some combination of retrospective crime scene analysis, deterrence of 
future crimes, and facilitation of real-time intervention and force deployment (Haggerty & 
Gozso, 2005). 
It is unknown how many public space surveillance cameras are operated by law 
enforcement agencies but a 2014 U.S. estimate was about 30 million (Staples, 2014, p. 71 citing 
Vlahos, 2009).  Despite their limitations surveillance cameras have emerged as a popular law 
enforcement choice to address crime and security concerns and much of the gap between what 
2 was promised and what was delivered has been linked to their rapid adoption (Surette, 2005).1
The n
umber of cameras installed quickly outpaced the capacity to monitor them and thus to 
effectively respond to what was visually captured (Piza, Caplan & Kennedy, 2014a, 2014b; Gill 
et al, 2005; Keval & Sasse, 2010).  It is apparent that a weak link in the information chain from 
camera to police response is the human monitor tasked with watching the screens (Surette, 
2005). 
Humans as Camera Monitors: A Poor Match.  Humans are not particularly good as 
camera monitors (Hier, Greenberg, Walby, & Lett, 2007; NÃ¤sholm, Rohlfing, & Sauer, 2014; 
Sutton & Wilson, 2004).  Surveillance camera monitors are most frequently tasked with general 
camera monitoring.  They sit at a desk before a bank of monitor screens and conduct on-going 
non-specific assessment of live video feeds. The review of a surveillance cameraâ€™s archived 
video is also sometimes required to determine if an event of interest was recorded.   In this 
second task, monitors are asked to search for a specific event, person, or object.  Again, the 
human monitor is often asked to watch hours of video.  The monitoring difficulty is further 
compounded in that many criminal activities have subtle precursors that are easily overlooked 
when humans are tasked with monitoring multiple cameras (La Vigne et al, 2011; Piza Caplan & 
Kennedy, 2014a; Piza & Sytsma, 2016).  Humans quickly become image swamped, missing 
more than they observe even when vigilant (Boksem, Meijman & Lorist, 2005; Faber, Maurits & 
Lorist, 2012; Gill et al 2005;  Sasse, 2010; Surette, 2005). 
The deficiency of human monitors occurs because perception failure occurs when there is 
little visual change present in long video stretches.  The monitorâ€™s attention shifts from visual 
review to other non-visual tasks such as conversing or daydreaming resulting in â€œinattentional 
blindnessâ€ (Johnston, Hawley, Plewe, Elliott, & DeWitt, 1990; Sasse, 2010).  In these instances, 
3 monitors have their eyes open and are looking at a video stream but their minds are cognitively 
el
sewhere, the visual images failing to reach psychological â€œattention captureâ€ levels necessary 
for effective monitoring.2  Significant amounts of time and video can pass, the images passing in
pl
ain view but unseen (Driver, 1998). 
Relevant for specific event searches,  perceptual blindness more often occurs when a 
monitorâ€™s cognitive attention is focused on finding one type of activity to the exclusion of other 
significant events (Bredemeier & Simons, 2012; Fougnie & Marois, 2007; Most, Scholl, Clifford 
& Simons, 2005).  In this situation, unexpected and even bizarre events are more likely to fail to 
capture the attention of monitors.  Important for noting anomalies, such as unexpected crimes, 
when a monitor is looking intently for a particular element in a video stream, failure to see other 
things of interest increases (see for example Piza, Caplan, & Kennedy, 2014a, p. 10).   The more 
different the unexpected event is from what is being looked for, the more likely it is to be missed 
(Memmert, 2006).3  Hence, different but serious crimes than one being searched for are less
li
kely to be noticed, the opposite of the case in general monitoring tasks where the lack of visual 
change contributes to monitor error.  In addition to these cognitive barriers, a number of 
surveillance barriers that reduce potential deterrent effects from surveillance cameras have been 
described.  In addition to defensive actions taken by offenders (Piza & Sytsma, 2016) and 
camera related contextual factors (Lim & Wilcox, 2016), two additional noted barriers are high 
camera to operator ratios (Piza, Caplan, Kennedy, & Gilchrist, 2015; Piza, Caplan & Kennedy, 
2014a, 2014b) and poor integration into agency practices (La Vigne, Lowry, Markman, & 
Dwyer, 2011; Piza, Caplan & Kennedy 2014b; Piza, Caplan, Kennedy, & Gilchrist, 2015). 
The cumulative result is that â€œa high camera-to-operator ratio has the predictable result of 
crime occurring within sight of a camera going undetected and the detection of criminal events 
4 by CCTV operators as rareâ€ (Piza, Caplan, & Kennedy, 2014a, p. 1019-1020 citing Norris & 
Arm
strong, 1999a, 1999b).  Faced with significant competition for attention, the camera systems 
currently are â€˜hit or missâ€™ tools regarding the detection of on-going incidents and expensive time 
and human capital consuming drudgery-laden search platforms for finding useful investigative 
evidence.  Additionally criticized for using sworn personnel as monitors and for instances of 
monitor abuses such as voyeurism and profiling, for police agencies the need for an alternative to 
human monitors is apparent (Bredemeier & Simons, 2012; Surette, 2005).  Computer vision 
app
lications can potentially address these issues and increase the deterrent impact of cameras and 
their organizational benefits.  However, the lack of computer vision use in law enforcement is 
exacerbated by a lack of computer vision software development designed with law enforcement 
needs in mind and the absence of field trials to justify agency costs for upgrading to computer 
vision enhanced camera networks. 
Computer Vision as Solution.  An emerging approach to the shortfalls of human 
monitored camera systems is computer vision (also known as machine vision).  A literature 
search reveals little current use of computer vision capabilities by law enforcement agencies 
although calls for its incorporation and discussions of potential applications have been forwarded 
(see Baldwin & Baird, 2001; Barett, Todd, Miller & Blythe, 2005; Piza, Caplan, & Kennedy, 
2014b; Shah, Javed & Shafique, 2007; Thomas & Cook, 2006).  The bulk of law enforcement 
computer applications have concentrated not on computer vision but on data-mining coupled 
with crime mapping to identify crime â€˜hot spotsâ€™ (Lohr, 2012; Wang, Ding, Lo, Stepinski, 
Salazar & Morabito, 2013; Yu, Ward, Morabito & Ding, 2011).  The common current computer 
vision uses are facial recognition applications and license plate readers. While computer vision 
as a public safety tool remains under-explored, the recent coupling of surveillance cameras to 
5 fast, inexpensive computers have made computer vision solutions feasible.  The primary benefit 
th
at computer vision offers law enforcement agencies is the substitution of automated analysis of 
camera video streams for human monitors.  With computer vision, the human in a computer 
vision enhanced security camera network assumes a supervisory assessment and response 
decision role. 
The first step in understanding computer vision involves comprehending digitization of a 
visual image into a grid of pixels where each pixel is assigned a numerical value representing its 
color.  This initial process generates for an image (or in the case of a video, each frame) a two-
dimensional grid of numbers which mathematically renders the original image as digits, hence 
the term â€˜digital photoâ€™. A simplified example is provided in Figure 1. These assigned numbers 
are the foundation for all subsequent manipulation, analysis, interpretation, labeling and other 
higher-level computer vision capabilities.  When a digital photo is opened for viewing, the 
process is reversed by a photo processing program which uses each pixelâ€™s value to instruct the 
viewing device (a computer, smart phone, or digital camera) on how to color a corresponding 
screen pixel â€“ converting numbers back to colored pixels and reconstructing the picture in a form 
that humans can see.  This â€œpicture to number to pictureâ€ process makes computer vision 
possible. 
Figure 1 about here 
The key to computer vision is the analysis made possible by the pixel values when the 
state of the image is not visual but numerical.   The art of computer vision moves quickly from 
input that looks loosely analogous to an â€˜imageâ€™ (for example, the numbers assigned to each 
pixel in Figure 1) to working with data and outputs that do not appear to correspond to the 
6 original image in any straightforward fashion as the photo and histogram in Figure 2 
dem
onstrate. 
Figure 2 about here 
In essence, computer vision involves determining what a quantitative analysis of pixel 
values can tell about an image.  From its numerical foundation the core tasks of computer vision 
proceed and in turn allow the development of common computer vision applications such as 
locating people and places in images, face recognition, and image stabilization. 
An example of a core computer vision task with direct criminal justice applications is the 
tracking of objects across a set of video frames (Yilmaz, Javed, & Shah, 2006).  The 
mathematical representations of the tracked objects are derived from determining key points 
(unique sets of pixels in an image) so that a â€œprobability distribution functionâ€ (PDF) can be 
generated. A PDF is analogous to a visual fingerprint without being as individually unique. 
Thus, an object will usually have a similar PDF that can be tracked across video frames.  The 
objects tracked can be automatically set by a computer vision object detector or can be assigned 
manually by a human placing an outline around a region of interest within a video frame.  Recent 
tracking methods can lose and reacquire objects as they move into and out of camera fields of 
view (Comaniciu, Ramesh, & Meer, 2000) and work well when the tracked object is 
substantially different from its background.  Multiple objects that are similar in appearance or 
that cross in front or behind one another are more difficult to track.4
Anot
her useful computer vision task is the assignment of a name to objects and actions. 
Not only is it useful to be able to name objects in a picture but it is an important goal to 
determine whether a human is present and to determine who they are and what they are doing, a 
crucial public safety surveillance task. For a computer vision program to be able to recognize and 
7 name objects, a set of images are initially used to train â€˜classifiersâ€™, computer vision sub-routines 
tha
t assign labels to images.  Once developed, classifiers can be used to answer queries about 
unlabeled images such as: Is there a handgun in this video?  The classifier training process for a 
previously unknown object proceeds after manual annotation during which training data is 
created by humans who assign labels to a set of representative images of the object (positive 
visual examples as shown in Figure 3) as well as those which do not contain the object (negative 
examples). The training images allow the computer vision program to mathematically 
differentiate among objects. For example, providing examples of â€œweaponsâ€ and â€œnot weaponsâ€ 
trains a classifier that can then better calculate the probability of a weapon being in an image. 
Such an approach is termed â€˜supervisedâ€™ as it assumes availability of annotated training data in 
contrast to â€˜semi-supervisedâ€™ and â€˜unsupervisedâ€™ approaches that require partially-annotated or 
no training data, respectively. In general, the performance of a particular computer vision task is 
proportional to amount of human-labeled data available. 
Figure 3 about here 
The now common task of matching a particular individualâ€™s face with a face in an image 
database is also useful (Turk & Kentland, 1991).  To accurately match a face, the computer 
vision program must consider â€œbetween class variationâ€ (different people who share features 
such as blue eyes) and â€œwithin class variationâ€ (the same person who looks differently due to 
differences in image aspects (frontal face compared with profile for example).  As with object 
and action labelling, face recognition is set as a probabilistic outcome with some threshold level 
of image similarity needed to be reached before a match is declared.  A recent improvement 
works to maximize the distinction between the faces of different persons and takes into account 
8 differences (i.e., normalizes within class variation) observed across multiple images of the same 
per
son.5   With these and other capabilities under development, potential computer vision
sol
utions for police surveillance camera tasks are now within reach. 
Computer Vision Applications in Policing.   General law enforcement surveillance needs 
that computer vision can address fall under two umbrellas.  The first set revolves around the need 
for automated real-time, live video stream analysis.  The second involves the need for post-hoc 
searches of archived video files. Computer vision based automated identification of public safety 
events of interest addresses the first task and query-based searches of video files addresses the 
second. 
Related to the first task, live real-time video analysis involves the need to rapidly identify 
and correctly respond to ongoing incidents.  This capability is important because effectiveness of 
a surveillance system in reducing crime has been linked to real-time intervention. Unless 
surveillance results in someone showing up to address an observed problem, camera deterrent 
effects wane (Ariel, 2016; Gill, 2003; Goold, 2004; Piza et al 2014a, 2014b; Welsh & Farrington, 
2002). The development of live event analysis is conducted along two computer vision paths: 
action and event detection of pre-identified events of interest and detection of anomalous new, 
unanticipated but potentially noteworthy events.  For real-time detection of activities, it is 
imperative that the system analyzes the surveillance video as it is captured and classifies actions 
and events as they appear.  Of particular interest to public safety monitors are many activities 
which occur infrequently and are precursors to criminal activity (for example, â€˜car hoppingâ€™ 
where a person pulls on car door handles as they walk along a street would be a precursor to theft 
from vehicles).  These activities are more difficult to program because first they are rare and 
therefore have a limited number of examples available for analysis and second, they can be 
9 ambiguous and difficult to define mathematically.  Hence, a murderous assault will likely occur 
onl
y once in the lifetime of a cameraâ€™s view-shed but it is crucial that it be noted by a computer 
vision program and that it be distinguished from one person giving another a vigorous friendly 
hug.  Humans quickly distinguish the two activities; however, computer vision programs must be 
quantitatively trained to do so. To be useful, anomaly models also must continuously update and 
incorporate environmental changes, for instance changes in weather, crowd density, or lightning 
conditions at different times of the day. 
Computer vision can also reduce the immense amount of time currently spent reviewing 
and searching videos.  Even when it is known that a video contains specific images such as 
weapons, the minutes or seconds of interest are often buried within hours of output.  A computer 
vision solution to this issue is query-based searches.  To be useful, query-based searches require 
search options that permits retrieval of objects with particular properties such as a person with 
specific height, weight, race, gender, or appearance; or â€˜objectsâ€™ such as an item a person was 
carrying like an umbrella or back-pack.  The ability to submit an object and attribute-based 
search would significantly reduce the number of irrelevant video clips that an investigator must 
review.  Independent of specific query-linked searches, it is also useful to have computer vision 
based video summarization programs for the distillation of videos into shortened but accurate 
summaries.  An eight-hour video can typically be reduced to an edited â€˜change onlyâ€™ video 
lasting minutes (Chen, Wang & Wang, 2009; Evangelopoulos et al, 2009; Gao, Wang, Yong & 
Gu, 2009). 
In another potential use of computer vision, recent criminal justice research has used 
camera footage to study pre-crime visual cues.  For example, Piza, Caplan, & Kennedy (2014b) 
and Levine, Taylor and Best (2011) used video footage to examine violence precursors and 
10 Moeller (2016) and Piza and Sytsma (2016) searched for correlates of illegal drug sales. 
Com
puter vision has the potential to significantly aid these research efforts and increase the use 
of surveillance videos as a data source. A number of prior research efforts have employed 
surveillance video as data (see Piza & Sytsma 2016; Piza, Caplan, & Kennedy, 2014a; Sampson 
& Raudenbush 1999; St. Jean, 2007) but their usefulness has been limited by heavy processing 
and time demands.  Despite having a number of years of video, Piza and Sytsma (2016) had to 
limit analysis to a single year and 62 incidents due to processing workload; in their study, each 
minute of video equaled 20 minutes of transcription time.  Lastly, as implied by Piza & Sytsma 
(2016) and Moeller (2016) a set of criminological theories and concepts such as routine 
activities, environmental crime, crime displacement, and hotspot analysis could benefit from the 
exploration of computer vision generated data. 
On-going Research.  A National Institute of Justice funded study is underway to address 
three research questions associated with police use of computer vision (Shah, Idees, & Surette, 
2015).  In this study computer vision analytics for a large surveillance camera network is being 
developed and their integration into a Public Safety Visual Analytics Workstation (PSVAW) 
within a municipal police department will be field tested.  The law enforcement targeted 
computer vision analytics under development include the retrieval of objects, concepts and 
events (Mazaheri, Kalayeh, Idrees, & Shah, 2015); the localization of actions in long untrimmed 
videos (Soomro, Idrees, & Shah, 2016); the interactive detection of anomalies without annotated 
training examples (Zavesky & Chang, 2008); and multiple methods for video summarization 
(Rodriguez, 2010).  The research questions addressed are: what is the accuracy and speed of the 
analytics; what is the organizational fit of computer vision in a police department; and what is 
the impact of a computer vision capability on a municipal criminal justice system? 
11 Research Question 1: How well do the computer vision algorithms work in the lab? 
Com
puter vision algorithms are being evaluated on standard pre-curated, annotated datasets 
which are partitioned for training and testing. For many computer vision tasks, prior algorithm 
accuracy has been high, above 90 percent for easy action recognition datasets.  However, for 
challenging datasets accuracy drops to around 60 percent (Kuehne, Jhuang, Garrote, Poggio, & 
Serre, 2011), a level that would generate numerous false hits in police applications.  The goal is 
to produce computer vision algorithms that are sensitive enough to not miss significant events 
but also do not swamp human reviewers with large number of erroneously flagged video clips. A 
second programming goal is to achieve significant reduction in storage and computational cost 
over large-scale surveillance video archives.  The practical impact for a law enforcement agency 
would be significant gains in search speed and the ability to search thousands of hours of video 
data (Ye, Liu, Wang, & Chang, 2013). 
A computer vision based method for detection of static concepts and dynamic events is 
also being developed for object detection such as weapons, police officers, police vehicles; and 
complex event detection like assaults, thefts, and car crashes.  Both use features from deep neural 
networks for processing images and video frames.6  In the static concept search, a human can
que
ry a single concept such as â€˜police officerâ€™ and the system will return a sorted list of video 
clips in which the concept â€˜police officerâ€™ appears. The complex event detection categorizes 
video into broad categorical classes of behaviors beyond a brief appearance of single objects. 
Thus, more challenging activities can be dealt with and video clips can be robustly classified into 
events such as â€˜robberiesâ€™ and â€˜assaultsâ€™. 
Regarding the need for real-time video analysis, computer vision software for live online 
abnormality detection is additionally being created (Javan, Roshtkhari, & Levine, 2013). The 
12 quantitative problem amounts to finding patterns in the digital data that significantly deviate 
fro
m behaviors previously identified empirically. The detection of abnormal behaviors is a 
difficult task.  First, the quantitative definition of a normal versus abnormal visual pattern is not 
well defined.  Second, normal behavior evolves over time and may change significantly as time 
passes (for instance, many people walking during daylight versus few people walking during 
nighttime differ visually but both may be normal activity when it comes to crime detection (Lim 
and Wilcox, 2016; Moeller, 2016).   Third, because abnormal events are rare it is difficult to 
obtain enough examples to train classifiers. 
To cope with these challenges, an online dictionary learning approach to detect 
abnormalities is being pursued which divides long videos into small non-overlapping meaningful 
clips. Since these segments are computed based on appearance and motion information, many 
will contain tracked vehicles and people, which are then compared with existing elements in a 
dictionary thereby permitting the detection of abnormalities (Tran, Bourdev, Fergus, Torresani, 
& Paluri, 2015). If the flagged anomaly is deemed a normal event, it is added to the dictionary. 
This allows the computer program to interactively update and recognize a â€œnew normalâ€ such as 
when a crowded day time street becomes a sparsely populated nighttime scene.  The anomaly 
detection process flags anomalous events from an unsupervised approach so that labeled training 
data is not required. The normalcy models will also be unique for each camera, since abnormal 
behavior may vary by camera across a network. 
In addition to detection, a computer vision benefit is the ability to automatically 
summarize video files and screen out irrelevant information (McCarthy & Oâ€™Mahony, 2015). 
One approach is to use computer vision to identify a small set of suspicious video clips in real 
time from multiple camera feeds or from a large video archive. A promising approach being 
13 pursued is built on semantic indexing (SIN) which uses ideas from deep learning and foreground 
obje
ct detection (Shah, Idees, & Surette, 2015).  A temporal action localization (finding an action 
in long videos) approach automatically decomposes an action into several sub-actions, models 
each sub-action on appearance and duration into distinct steps, and detects sub-actions in an 
original untrimmed video.  An action event usually consists of a sequence of sub-actions/sub-
events in a specific order. For example, a robbery action can be decomposed into person A 
approaching person B, person A producing a weapon and gesturing at person B, person B 
holding hands aloft, handing over wallet or phone, and the two separating.  The approach for 
localization automatically discovers the number of sub-actions for each action/event from a set 
of training videos, registering the point in time the action begins and ends in a video. Once 
identified, these segments can be flagged for human monitor review and deployment decisions. 
A second video summarization approach renders a new video that highlights interesting 
activities in the original video and skims through redundant information to save viewing time. 
Along these lines, a hierarchical video summarization method is being created which will first 
identify small video regions termed supervoxels (regions with similar appearance and coherent 
motion) based on information such as color and motion.  Next, high-level objects of interest such 
as moving humans or vehicles will be incorporated.  These information sources will be combined 
and the defined video segments matched with previously detected and labeled objects . By 
detecting interesting regions as well as objects, analysis of human and object interactions are 
possible (e.g. a theft involving a person in a car or a fight involving multiple people). 
Research Question 2: Does computer vision work in the field?   If a large automated 
camera system results in event swamping from the flagging of numerous events for review or has 
no significant impact on daily agency operations, computer visionâ€™s promise will be unmet. To 
14 address this issue, a set of events of interest to law enforcement and the design and installation of 
a co
mputer vision workstation in a municipal police department will be evaluated.  Table 1 lists 
18 objects, events, and interactions of interest to law enforcement that computer vision 
algorithms are being developed to detect. 
Table 1 about here 
Some of the events of interest are rare and have proven difficult to locate sufficient 
numbers of training examples from police surveillance cameras.  In addition, some events occur 
in conjunction with other crimes or are ambiguous. These events seldom happen without other 
confounding criminal activity or they are hard to identify by annotators (e.g. injured officer and 
custody events).  Thus, it is difficult to train detectors for such less straightforward events to flag 
them reliably.  Correspondingly, anomaly detection in the real-world assumes greater 
importance. 
In terms of agency impact, the key computer vision field component will be a â€œPublic 
Safety Visual Analytics Workstation (PSVAW â€“ see Figure 4).  The PSVAW will have multiple 
capabilities ranging from detection and localizing objects in camera feeds, labeling actions and 
events associated with training data, and allowing query based searches for specific events in 
videos.  It will also be programmed to flag pre-trained criminal and new non-trained abnormal 
events.   Using human monitor feedback, the PSVAW will refine the retrieval parameters and 
improve its search results over time. After repeating a number of iterations, the PSVAW will 
create an inductive model to detect new activity of interest in real-time so that an initial anomaly 
will over time become a computer vision trained, recognized, and labeled event. 
Figure 4 about here 
15 Research Question 3.  What are computer vision impacts on a criminal justice system? 
The   p
resence of surveillance cameras has been forwarded as both possibly increasing the 
reporting of events to the police or suppressing citizen guardianship levels (Surette, 2006). 
Besides issues of loss of privacy, costs, and effectiveness, because computer vision surveillance 
cameras are expected to catch events that humans would miss, more people may be arrested as 
the criminal justice net becomes wider and finer (Surette, 2005). 
The system-wide impact of a computer vision enhanced camera network will be assessed 
along two dimensions: its impact on the policing of a community through time series analysis of 
â€˜all reported crimeâ€™ and â€˜calls for serviceâ€™ data, and the local criminal justice systemâ€™s use of 
computer vision generated video for investigations and evidence. Utilizing measures of time 
spent by human monitors on video requests and processing, flagged events and response times, 
and use of camera video for investigations and case evidence the system-wide impact of 
computer vision capabilities will be evaluated. 
 Conclusion.  Computer vision has the potential to address a wide set of problems 
associated with current public space camera surveillance systems from inappropriate use such as 
profiling and voyeurism to inherent unintended errors by human monitors.  An automated 
camera monitor system will not view what it has not been programmed to view and when 
appropriately programmed will reduce the surveillance gaze from falling on unsuitable subjects. 
Computer vision systems can also greatly reduce the two sources of error and ineffectiveness in 
the use of public space surveillance cameras.  A computer algorithm will not become bored or 
distracted during real-time monitoring so events of interest are less likely to go unseen. 
Simultaneously, an algorithm will not be so focused on a search for a specific event that other 
16 important events go unnoticed.  These benefits are currently being developed and tested in 
com
puter vision lab settings.
Computer vision should not be expected to be a panacea  f or law enforcementâ€™s 
surveillance needs and software gaps remain such as algorithms confidently misidentifying 
images (Nguyen, Yosinski & Clune, 2014).  Additional shortfalls are due to object size (number 
of pixels) presenting detection errors in labeling small objects such as guns and tracking can be 
hampered by the occlusion of people and objects. Hence, following computer vision 
developments from the lab to the field will be an important step.  The promise of computer 
vision is that the automation of monitoring can upgrade the current reality of a poorly utilized 
technology expenditure to a reliable public safety tool.  To already budget conscious, low-on-
manpower agencies a field evaluated computer vision capability stands as potentially invaluable. 
References 
Alla
rd, T., Wortley, R. & Stewart, A. (2008). The effect of CCTV on prisoner misbehavior. The 
Prison Journal 88(3), 404â€“422. 
Ariel, B. (2016).  Do police body cameras really work: IEEE Spectrum .  Posted May 4, 2016. 
Downloaded May 24, 2016 from http://spectrum.ieee.org/consumer-electronics/portable-
devices/do-police-body-cameras-really-work 
Ariel, B., Farrar, W., & Sutherland, A. (2015). The effect of police body-worn cameras on use of 
force and citizensâ€™ complaints against the police: a randomized controlled trial. Journal of 
quantitative criminology , 31(3), 509-535. 
Assari, S. M., Idrees, H., & Shah M. (2016). Human re-identification in crowd videos using 
personal, social and environmental constraints.  European Conference on Computer Vision 
(ECCV). 
17 Baldwin, D. & Baird, J. (2001).  Discerning intentions in dynamic human action. Tre nds in 
Cognitive Sciences, 5, 171-178. 
Barrett, H., Todd, P., Miller, G., & Blythe, P. (2005).  Accurate judgments of intention from 
motion cues alone: A cross-cultural study. Evolution and Human Behavior, 26, 313-331. 
Becklen, R. & Cervone, D. (1983).  Selective looking and the noticing of unexpected events. 
Memory & Cognition, 11, 601-608. 
Boksem, M., Meijman, T. & Lorist, F. (2005). Effects of mental fatigue on attention: An ERP 
study. Cognitive Brain Research, 25(1), 107-116. 
Bredemeier, K & Simons, D. (2012). Working memory and inattentional blindness. 
Psychological Bulletin Review , 19, 239-244. 
Buckland, G.  (2001). Shots in the dark: True crime pictures.  NY: Little Brown & Co. 
Chen B., Wang J. & Wang J. (2009). A novel video summarization based on mining the story-
structure & semantic relations among concept entities. Multimedia, IEEE Transactions on, 
11(2), 295-312. 
Comaniciu, D., Ramesh, V., & Meer, P. (2000). Real-time tracking of non-rigid objects using 
mean shift. Computer Vision and Pattern Recognition, 2, 142-149. 
Driver, J. (1998). The neuropsychology of spatial attention. In H. Pashler (Ed.), Attention. (pp. 
297-340). London: Taylor Francis. 
Evangelopoulos, G., Zlatintsi, A., Skoumas, G., Rapantzikos, K., Potamianos, A., Maragos, P., & 
Avrithis, Y. (2009). Video event detection and summarization using audio, visual and text 
saliency. Acoustics, Speech and Signal Processing, ICASSP IEEE International Conference. 
Faber, L., Maurits, N., & Lorist, M. (2012). Mental fatigue affects visual selective attention. 
PloS one , 7(10), e48073. 
Fougnie, D. & Marois, R. (2007). Executive working memory load induces inattentional 
blindness.  Psychonomic Bulletin and Review, 14(1), 142â€“147. 
Gao, Y., Wang, D., Yong, J., & Gu, H. (2009). Dynamic video summarization using two level 
redundancy detection. Multimedia Tools and Applications , 42(2), 233-250. 
18 Gill, M. (2003). CCTV. Leicester, UK: Perpetuity Press; 
Gil
l, M. & Loveday, K. (2003). What do offenders think about CCTV?â€ Crime Prevention and 
Community Safety, 5(3), 17â€“25. 
Gill, M., Spriggs, A., Allen, J., Hemming, M., Jessiman, P. and Kara, D. (2005) Control room 
operation: findings from control room observations.  London: Home Office. 
Goold, B. (2004). CCTV & Policing. Oxford, UK: Oxford University Press. 
Haggerty, K. & Gozso, A. (2005). â€œSeeing Beyond the Ruins: Surveillance as a Response to 
Terrorist Threats. Canadian Journal of Sociology, 30(2), 169â€“187. 
Hier, S., Greenberg, J., Walby, K. & Lett, D. (2007).  â€œMedia, Communication & the 
Establishment of Public Camera Surveillance Programmes in Canada.â€ Media, Culture, and 
Society , 29(5), 727-751. 
Hyman, I. Boss, E., Matthew, S., Wise, B., McKenzie, M., Kira E., & Caggiano, J. (2009). Did 
you see the unicycling clown? Inattentional blindness while walking and talking on a cell 
phone.  Applied Cognitive Psychology , 24(5), 597â€“607. 
 Idrees, H., Warner, N., & Shah, M. (2014). Tracking in dense crowds using prominence and 
neighborhood motion concurrence. Image and Vision Computing , 32(1), 14-26. 
Javan Roshtkhari, M., & Levine, M. (2013). Online dominant and anomalous behavior detection 
in videos. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition  
(pp. 2611-2618). 
Javed, O., Rasheed, Z., Alatas, O., & Shah, M. (2003). KNIGHTâ„¢: a real time surveillance 
system for multiple and non-overlapping cameras. IEEE International Conference on 
Multimedia and Expo.  
Javed, O., Rasheed, Z., Shafique, K., & Shah, M. (2003). Tracking across multiple cameras with 
disjoint views. International Conference on Computer Vision. 
Johnston, W., Hawley, K., Plewe, S., Elliott, J., & DeWitt, M. (1990). Attention capture by novel 
stimuli. Journal of Experimental Psychology: General , 119(4), 397.  Keval, H. and sasse, M. 
19 (2010) â€˜Not the usual suspectsâ€™: A study of factors reducing the effectiveness of CCTV. 
Sec
urity Journal 23(2): 134-154.  
Keval, H., & Sasse, M. (2010). â€œNot the Usual Suspectsâ€: A study of factors reducing the 
effectiveness of CCTV. Security Journal , 23(2), 134-154. 
Kuehne, H., Jhuang, H., Garrote, E., Poggio, T., & Serre, T. (2011). HMDB: a large video 
database for human motion recognition. In International Conference on Computer Vision  
(pp. 2556-2563).  
Kroener, I. (2014).  CCTV: A technology under the radar?  Burlington, VT: Ashgate. 
La Vigne, N., Lowry, S., Markman, J. and Dwyer, A.  (2011) Evaluating the use of public 
surveillance cameras for crime control and prevention.  Washington DC: US. Department of 
Justice, Office of Community Oriented Policing Services, Urban Institute Justice Policy 
Center.   
Levine, M., Taylor, P. J., & Best, R. (2011). Third parties, violence, and conflict resolution the 
role of group size and collective action in the microregulation of violence. Psychological 
Science . 
Lim, H., & Wilcox, P. (2016). Crime-reduction Effects of Open-street CCTV: Conditionality 
Considerations. Justice Quarterly , 1-30. 
Lohr, S. (2012). The age of big data. New York Times , 11. 
Mack, A. (2003).  Inattentional blindness: Looking without seeing. Current Directions in 
Psychological Science , 12(5), 180-184. 
Mack, A. & Rock, I. (1998). Inattentional Blindness .  Cambridge, MA: MIT Press. 
Marx, G. (1988). Undercover: Police Surveillance in America. Berkley: University of California 
Press. 
Mazaheri, A., Kalayeh, M., Idrees, H., & Shah, M. (2015). UCF-CRCV at TRECVID 2015: 
Semantic Indexing. NIST TRECVID. 
McCarthy, O. & Oâ€™Mahony, M. (2016). End user response to an event detection & route 
reconstruction security system prototype for use in airports and public transport hubs. 
20 Transportation Research Board of the National Academies.  Downloaded from 
ht
tp://amonline.trb.org/trb60693-2016-1.2807374/t001-1.2823436/254-1.2823593/16-5450-
1.2980693/16-5450-1.2993283?qr=1 
Memmert, D. (2006).  The effects of eye movement, age, and expertise on inattentional 
blindness. Consciousness and Cognition , 15(3), 620â€“627. doi :10.1016/j.concog.2006.01.001. 
PMID 16487725 . 
Moeller, K. (2016).  Temporal transaction patterns in an open-air cannabis market.  Police 
Practice and Research 17(1), 37-50. 
Most, S., Scholl, B., Clifford, E., & Simons, D. (2005). What you see is what you set: sustained 
inattentional blindness and the capture of awareness.  Psychological Review,  112(1), 217-
242. 
NÃ¤sholm, E., Rohlfing, S., & Sauer, J. D. (2014). Pirate stealth or inattentional blindness? The 
effects of target relevance and sustained attention on security monitoring for experienced and 
naÃ¯ve operators. PloS one, 9(1).   Downloaded from:  
http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0086157 
Neisser, U. & Becklen, R. (1975).  Selective looking: attending to visually specified events. 
Cognitive Psychology , 7, 480-494. 
Neisser, U. (1979).  The control of information pickup in selective looking.  In A.D. Pick (Eds.) 
perception & its development: A tribute to Eleanor J. Gibson (pp. 201-219) Hillsdale, NJ. 
Erlbaum.   
Norris, C. & Armstrong, G.  (1999a). The Maximum Surveillance Society : The Rise of CCTV. 
Oxford, UK: Berg. 
Norris, C. & Armstrong, G.  (1999b). CCTV and the social structuring of surveillance.  In N. 
Tilley and K. Painter (Eds.), Surveillance of public space: CCTV, street lighting and crime 
prevention.  Crime Prevention studies (vol. 10, pp. 157-178).  Monsey, NY: Criminal Justice 
Press.   
21  Nguyen, A. Yosinski, J. & Clune. J. (2014).  Deep neural networks are easily fooled: High 
con
fidence predictions for unrecognizable images. In 2015 IEEE Conference on Computer 
Vision and Pattern Recognition (CVPR)  (pp. 427-436). IEEE. 
Piza, E., Caplan, J. & Kennedy, L. (2014a). CCTV as a tool for early police intervention: 
Preliminary lessons from nine case studies. Security Journal.  doi:10.1057/sj.2014.17 
Downloaded from: http://link.springer.com/article/10.1057%2Fsj.2014.17 
 Piza, E., Caplan, J., & Kennedy, L. (2014b). Is the punishment more certain? An analysis of 
CCTV detections and enforcement. Justice Quarterly , 31(6), 1015-1043. 
Piza, E., Caplan, J., Kennedy, L. & Gilchrist, A.  (2015). The effects of merging proactive CCTV 
monitoring with directed police patrol: A randomized controlled trial.  Journal of 
Experimental Criminology , 11, 43-69. 
Piza, E. & Sytsma, V. (2016) exploring the defensive actions of drug sellers in open-air markets: 
A systematic social observation.  Journal of Research in Crime and Delinquency, 53(1), 36-
65 
Rodriguez, M. (2010). Cram: Compact representation of actions in movies. Computer Vision and 
Pattern Recognition (CVPR).  
Sampson, R. & Raudenbush, S. (1999).  Systematic social observation of public spaces: A new 
look at disorder in urban neighborhoods.  American Journal of Sociology 105:603-651. 
Sasse, A. (2010).  Not seeing the crime for the cameras? Communications of the ACM , 53, 22-25. 
Shah, M., Idees, H., & Surette, R. (2015).  Studying the impact of video analytics for pre, live, 
and post event analysis on outcomes of criminal justice.  Orlando, Fl: University of Central 
Florida Center for Research on Computer Vision.  Funded by U.S. department of Justice, 
NIJ-2015-R2-CX-K025. 
Shah, M., Javed, O., & Shafique, K. (2007). Automated visual surveillance in realistic 
scenarios. IEEE MultiMedia, 1, 30-39. 
Short, E. & Ditton, J. (1996).  Does closed circuit television prevent crime?  Monograph of the 
Scottish Office Central Records Unit, Edinburgh, Scotland. 
22 Soomro, K., Idrees, H., & Shah, M. (2016). Predicting the Where and What of actors and actions 
thr
ough Online Action Localization. IEEE Conference on Computer Vision and Pattern 
Recognition. 
St. Jean, LP. (2007)  Pockets of crime: Broken windows, collective efficacy, and the criminal 
point of view.  Chicago: University of Chicago Press. 
Staples W. (2014).  Everyday Surveillance .  New York: Rowman & Littlefield. 
Surette, R. (2005).  The thinking eye: Pros and cons of second generation CCTV surveillance 
systems.  Policing: An International Journal of Police strategies & Management , 28(1), 152-
173. 
Surette, R. (2006).  CCTV and citizen guardianship suppression: A questionable proposition. 
Police Quarterly , 9(1), 100-125. 
Sutton A. & Wilson, D.  (2004). Open-street CCTV in Australia: Politics and expansion. 
Surveillance and Society , 2(2/3), 310â€“322; 
Thomas, J. & Cook, K. (2006). A visual analytics agenda. IEEE computer graphics and 
applications , 26(1), 10-13. 
Tran, D., Bourdev, L., Fergus, R., Torresani, L., & Paluri, M. (2015). Learning spatiotemporal 
features with 3d convolutional networks. In IEEE International Conference on Computer 
Vision (ICCV) . 
Turk, M. & Pentland, A. (1991). Face recognition using eigenfaces. Computer Vision and 
Pattern Recognition, 1991. 
Vlahos, J. (2009).  Surveillance society: new high-tech cameras are watching you.  Popular 
Mechanics,  Oct. 1. 
Wang, D., Ding, W., Lo, H., Stepinski, T., Salazar, J., & Morabito, M. (2013). Crime hotspot 
mapping using the crime related factorsâ€”a spatial data mining approach. Applied 
intelligence , 39(4), 772-781. 
Welsh, B. & Farrington, D. (2002).  Crime prevention effects of closed circuit television: A 
systematic review .  Home Office research Study 252. London: Home Office. 
23 Welsh, B. & Farrington, D.  (2004). Evidence-based crime prevention: The effectiveness of 
CCTV
. Crime Prevention and Community Safety 6, 21-33. 
Welsh, B. & Farrington, D. (2009). Public area CCTV and crime prevention: An updated 
systematic review and meta â€analysis. Justice Quarterly , 26(4), 716-745. 
Williams, D. (2007). Effective CCTV and the challenge of constructing legitimate suspicion 
using remote visual images.  Journal of Investigative Psychology and Offender Profiling , 4, 
97â€“107. 
Wolfe, J. (1994). Guided search 2.0: A revised model of visual search. Psychonomic Bulletin & 
Review , 1, 202-238. 
Ye, G., Liu, D., Wang, J., & Chang, S. (2013). Large-scale video hashing via structure learning. 
Proceedings of the IEEE International Conference on Computer Vision  (pp. 2272-2279). 
Yilmaz, A., Javed, O., & Shah, M. (2006). Object tracking: A survey. ACM computing surveys 
(CSUR) , 38(4), 13. 
Yu, C., Ward, M., Morabito, M., & Ding, W. (2011). Crime forecasting using data mining 
techniques. 2011 IEEE 11th International Conference on Data Mining Workshops  (pp. 779-
786). IEEE. 
Zavesky, E. & Chang, S. (2008). CuZero: Embracing the frontier of interactive visual search for 
informed users. Proceedings of the 1st ACM International conference on Multimedia 
Information Retrieval  (pp. 237-244). 
1 When and how these systems work in specific applications remains under debate (Ariel, 2016; 
Ari
el, Farrar & Sutherland, 2015; Williams, 2007). Indirect evidence suggests that offenders take 
into account the perceived level of surveillance and the likelihood of intervention when deciding 
24 whether to commit certain types of crimes, especially instrumental street crimes such as car 
break-ins.  This suggests that easily visible cameras with signage can deter certain offenders 
(Gill & Loveday, 2003; Allard, Wortley & Stewart, 2008; Short & Ditton, 1996; Welsh & 
Farrington, 2009).  Spontaneous crimes such as assaults appear to be less affected  and overall 
came
ras appear most effective in reducing crime when combined with other interventions (Lim 
& Wilcox 2016; Piza, Caplan & Kennedy 2014a, 2014b; Welsh & Farrington, 2004). 
2 Inattentional blindness is defined as the failure to  s ee highly visible objects directly looked at 
when cognitive attention is elsewhere (Mack, 2003, p. 180; Mack & Rock, 1998; see also 
Becklen & Cervone, 1983; Neisser, 1979; and Neisser & Becklen 1975).   â€œAttention captureâ€ 
refers to the ability of novel stimuli to gain the focus of someone otherwise cognitively engaged 
(Johnston, Hawley, Plewe, Elliott & DeWitt, 1990; Most, Scholl, Clifford, & Simons, 2005; 
Wolfe, 1994).  The use of technology has been reported to effect both processes (Hyman, Boss, 
Matthew, Wise, McKenzie, Kira, & Caggiano, 2009, p. 605) and inattentional blindness has been 
found to be a common phenomenon associated with watching video streams (Most, Scholl, 
Clifford, & Simons, 2005). 
3A classic example is demonstrated by viewers tasked with counting passes failing to see a 
gor
illa walk through a group of people tossing a ball around. 
https://www.youtube.com/watch?v=vJG698U2Mvo
4Over the past decade, more sophisticated alternate approaches to tracking have been presented 
in t
he computer vision literature, including those that track single and multiple objects or persons 
across non-overlapping multiple camera field of views (called the â€˜hand-offâ€™ problem) and 
people in dense crowds (Assari, Idrees, & Shah, 2016; Idrees, Warner, & Shah, 2014; Javed, 
Rasheed, Alatas, & Shah, 2003; Javed, Rasheed, Shafique, & Shah, 2003). 
25 5The goal in this method (termed linear discriminative analysis or LDA) is to maximize the 
sep
aration of the set of images of one person from the set of images of different (but possibly 
similar looking on some characteristics) persons by using sets of image portraits.
6 Recent research has shown that training deep network s  containing large number of hidden 
layers significantly improves performance on computer vision tasks such as object detection, 
face identification, and action recognition.    However, deeper networks require larger quantities 
of training data compared to traditional machine learning algorithms and thus may be limited for 
public safety applications. 
Figure 1: A Digitized Letter 
The ori
ginal letter â€˜aâ€™ is rendered into a 14 by 12 array of 168 pixels with each pixel assigned 
a value representing a tone from â€˜whiteâ€™ scored 1.0 to black squares scored 0.0.  Gray squares 
are scored from light to dark (gray squares scored 0.5, light gray ones scored from .01 to .04, 
dark gray squares scored from 0.6 to .0.9).  The digitized photo of the original â€˜aâ€™ image 
results where the â€œaâ€ can be vaguely discerned in the pattern of 0.0 scored pixels.  For 
capturing color, multiple values, typically corresponding to red, green and blue, are stored for 
each pixel location.  Source:  
http://pippin.gimp.org/image_processing/images/sample_grid_a_square.png 
Figure 2: An Image Histogram 
The hi
stogram on the right is constructed from 
grayscale pixels from the image by quantizing them 
into 256 bins.  The significance of image histograms 
for computer vision is that histograms are often 
unique for different objects and correlate with their 
shape, texture and color and can be used for assigning 
labels to images.  
Figure 3: Positive Training Examples for Assault, Theft, Vandalism, and Robbery. 

Figure 4:  Public Safety Visual Analytics Workstation (PSVAW) 

Table 1:  Objects, Crimes, and Events of Interest to Law Enforcement for Computer Vision 
Algorithm Development and Field Testing. 
Interest Ope rational Definition
-- O
bjects -- 
Weapons handguns, rifles 
Police Cars marked law enforcement vehicles 
Police Officers uniformed law enforcement officers 
Emergency Vehicles fire trucks, ambulances (flashing emergency lights) 
Left Property abandoned bags, backpacks, etc., flagged after a set time limit. 
-- Crimes -- 
Criminal Mischief graffiti, vandalism (destroying property, tipping over objects, spray 
painting or other property damage) 
Theft removal of property (need to differentiate legal from illegal removal of 
property such as associated with property damage like breaking a car 
window) 
Robbery need to differentiate legal from illegal exchanges associated with force or 
weapon  
Burglary burglary from auto with window or other property damage; business 
burglaries constrained to â€œpersons entering a business after closingâ€ 
possibly flagging constrained from midnight to 6 AM time frame  
Drug Transactions need to differentiate legal from illegal exchanges associated with time of 
day or established illicit drug markets 
Batteries / Assaults shootings, stabbings, and fistfights  
-- Public Safety Events -- 
Car Crashes damage to moving vehicles 
Crowd Activity rioting, crowd density threshold reached, running, fighting, falling, 
property destruction 
Individual Activity running, falling, immobile (exceeds set time limit), blocking traffic, 
standing in roadway (exceeds set time limit) 
Injured Officer officer down (exceeds set time limit) 
Custody Events  arrests, mental health retentions 
Citizen / Police  citizen requests for assistance, officer information/identification requests 
Fire / Explosions set to minimum size limit.  
______________________________________________________________________________ ","enhancing camera surveillance  
using computer vision a research note 
 
haroon idrees and mubarak shah 
center for research in computer vision  
university of central florida  
orlando florida usa and  
 
ray surette  
department of criminal justice  
university of central florida  
orlando florida usa  
1 enhancing camera surveillance using computer vision a research note 
 int
roduction  
police surveillance of public spaces  historically called the stakeout police 
surveillance has a long history and evidence gained from surveillance has been an important part 
of investigations for nearly two centuries marx 1988  similarly the use of visual technology 
by police began in the nineteenth century with the photographing of inmates and evolved to 
include crime scene photographs as standard police procedures buckland 2001 norris  
armstrong 1999a while both practices became law enforcement mainstays police surveillance 
and visual evidence remained separate realms well into the twentieth century norris  
armstrong 1999a   surveillance cameras operated by law enforcement are therefore a 
relatively recent phenomenon and their marriage has moved surveillance from a human based 
activity to a heavily technological one 
as evolved police surveillance has two goals proactively deterring offenders and aiding 
in investigations   initially the investigative goal dominated and surveillance was aimed at 
solving crimes not preventing them  but as cameras became less expensive and more pervasive 
deterrence and risk reduction became important kroener 2014  current camera surveillance 
projects aim to provide some combination of retrospective crime scene analysis deterrence of 
future crimes and facilitation of realtime intervention and force deployment haggerty  
gozso 2005 
it is unknown how many public space surveillance cameras are operated by law 
enforcement agencies but a 2014 us estimate was about 30 million staples 2014 p 71 citing 
vlahos 2009  despite their limitations surveillance cameras have emerged as a popular law 
enforcement choice to address crime and security concerns and much of the gap between what 
2 was promised and what was delivered has been linked to their rapid adoption surette 20051
the n
umber of cameras installed quickly outpaced the capacity to monitor them and thus to 
effectively respond to what was visually captured piza caplan  kennedy 2014a 2014b gill 
et al 2005 keval  sasse 2010  it is apparent that a weak link in the information chain from 
camera to police response is the human monitor tasked with watching the screens surette 
2005 
humans as camera monitors a poor match  humans are not particularly good as 
camera monitors hier greenberg walby  lett 2007 nsholm rohlfing  sauer 2014 
sutton  wilson 2004  surveillance camera monitors are most frequently tasked with general 
camera monitoring  they sit at a desk before a bank of monitor screens and conduct ongoing 
nonspecific assessment of live video feeds the review of a surveillance cameras archived 
video is also sometimes required to determine if an event of interest was recorded   in this 
second task monitors are asked to search for a specific event person or object  again the 
human monitor is often asked to watch hours of video  the monitoring difficulty is further 
compounded in that many criminal activities have subtle precursors that are easily overlooked 
when humans are tasked with monitoring multiple cameras la vigne et al 2011 piza caplan  
kennedy 2014a piza  sytsma 2016  humans quickly become image swamped missing 
more than they observe even when vigilant boksem meijman  lorist 2005 faber maurits  
lorist 2012 gill et al 2005  sasse 2010 surette 2005 
the deficiency of human monitors occurs because perception failure occurs when there is 
little visual change present in long video stretches  the monitors attention shifts from visual 
review to other nonvisual tasks such as conversing or daydreaming resulting in inattentional 
blindness johnston hawley plewe elliott  dewitt 1990 sasse 2010  in these instances 
3 monitors have their eyes open and are looking at a video stream but their minds are cognitively 
el
sewhere the visual images failing to reach psychological attention capture levels necessary 
for effective monitoring2  significant amounts of time and video can pass the images passing in
pl
ain view but unseen driver 1998 
relevant for specific event searches  perceptual blindness more often occurs when a 
monitors cognitive attention is focused on finding one type of activity to the exclusion of other 
significant events bredemeier  simons 2012 fougnie  marois 2007 most scholl clifford 
 simons 2005  in this situation unexpected and even bizarre events are more likely to fail to 
capture the attention of monitors  important for noting anomalies such as unexpected crimes 
when a monitor is looking intently for a particular element in a video stream failure to see other 
things of interest increases see for example piza caplan  kennedy 2014a p 10   the more 
different the unexpected event is from what is being looked for the more likely it is to be missed 
memmert 20063  hence different but serious crimes than one being searched for are less
li
kely to be noticed the opposite of the case in general monitoring tasks where the lack of visual 
change contributes to monitor error  in addition to these cognitive barriers a number of 
surveillance barriers that reduce potential deterrent effects from surveillance cameras have been 
described  in addition to defensive actions taken by offenders piza  sytsma 2016 and 
camera related contextual factors lim  wilcox 2016 two additional noted barriers are high 
camera to operator ratios piza caplan kennedy  gilchrist 2015 piza caplan  kennedy 
2014a 2014b and poor integration into agency practices la vigne lowry markman  
dwyer 2011 piza caplan  kennedy 2014b piza caplan kennedy  gilchrist 2015 
the cumulative result is that a high cameratooperator ratio has the predictable result of 
crime occurring within sight of a camera going undetected and the detection of criminal events 
4 by cctv operators as rare piza caplan  kennedy 2014a p 10191020 citing norris  
arm
strong 1999a 1999b  faced with significant competition for attention the camera systems 
currently are hit or miss tools regarding the detection of ongoing incidents and expensive time 
and human capital consuming drudgeryladen search platforms for finding useful investigative 
evidence  additionally criticized for using sworn personnel as monitors and for instances of 
monitor abuses such as voyeurism and profiling for police agencies the need for an alternative to 
human monitors is apparent bredemeier  simons 2012 surette 2005  computer vision 
app
lications can potentially address these issues and increase the deterrent impact of cameras and 
their organizational benefits  however the lack of computer vision use in law enforcement is 
exacerbated by a lack of computer vision software development designed with law enforcement 
needs in mind and the absence of field trials to justify agency costs for upgrading to computer 
vision enhanced camera networks 
computer vision as solution  an emerging approach to the shortfalls of human 
monitored camera systems is computer vision also known as machine vision  a literature 
search reveals little current use of computer vision capabilities by law enforcement agencies 
although calls for its incorporation and discussions of potential applications have been forwarded 
see baldwin  baird 2001 barett todd miller  blythe 2005 piza caplan  kennedy 
2014b shah javed  shafique 2007 thomas  cook 2006  the bulk of law enforcement 
computer applications have concentrated not on computer vision but on datamining coupled 
with crime mapping to identify crime hot spots lohr 2012 wang ding lo stepinski 
salazar  morabito 2013 yu ward morabito  ding 2011  the common current computer 
vision uses are facial recognition applications and license plate readers while computer vision 
as a public safety tool remains underexplored the recent coupling of surveillance cameras to 
5 fast inexpensive computers have made computer vision solutions feasible  the primary benefit 
th
at computer vision offers law enforcement agencies is the substitution of automated analysis of 
camera video streams for human monitors  with computer vision the human in a computer 
vision enhanced security camera network assumes a supervisory assessment and response 
decision role 
the first step in understanding computer vision involves comprehending digitization of a 
visual image into a grid of pixels where each pixel is assigned a numerical value representing its 
color  this initial process generates for an image or in the case of a video each frame a two
dimensional grid of numbers which mathematically renders the original image as digits hence 
the term digital photo a simplified example is provided in figure 1 these assigned numbers 
are the foundation for all subsequent manipulation analysis interpretation labeling and other 
higherlevel computer vision capabilities  when a digital photo is opened for viewing the 
process is reversed by a photo processing program which uses each pixels value to instruct the 
viewing device a computer smart phone or digital camera on how to color a corresponding 
screen pixel  converting numbers back to colored pixels and reconstructing the picture in a form 
that humans can see  this picture to number to picture process makes computer vision 
possible 
figure 1 about here 
the key to computer vision is the analysis made possible by the pixel values when the 
state of the image is not visual but numerical   the art of computer vision moves quickly from 
input that looks loosely analogous to an image for example the numbers assigned to each 
pixel in figure 1 to working with data and outputs that do not appear to correspond to the 
6 original image in any straightforward fashion as the photo and histogram in figure 2 
dem
onstrate 
figure 2 about here 
in essence computer vision involves determining what a quantitative analysis of pixel 
values can tell about an image  from its numerical foundation the core tasks of computer vision 
proceed and in turn allow the development of common computer vision applications such as 
locating people and places in images face recognition and image stabilization 
an example of a core computer vision task with direct criminal justice applications is the 
tracking of objects across a set of video frames yilmaz javed  shah 2006  the 
mathematical representations of the tracked objects are derived from determining key points 
unique sets of pixels in an image so that a probability distribution function pdf can be 
generated a pdf is analogous to a visual fingerprint without being as individually unique 
thus an object will usually have a similar pdf that can be tracked across video frames  the 
objects tracked can be automatically set by a computer vision object detector or can be assigned 
manually by a human placing an outline around a region of interest within a video frame  recent 
tracking methods can lose and reacquire objects as they move into and out of camera fields of 
view comaniciu ramesh  meer 2000 and work well when the tracked object is 
substantially different from its background  multiple objects that are similar in appearance or 
that cross in front or behind one another are more difficult to track4
anot
her useful computer vision task is the assignment of a name to objects and actions 
not only is it useful to be able to name objects in a picture but it is an important goal to 
determine whether a human is present and to determine who they are and what they are doing a 
crucial public safety surveillance task for a computer vision program to be able to recognize and 
7 name objects a set of images are initially used to train classifiers computer vision subroutines 
tha
t assign labels to images  once developed classifiers can be used to answer queries about 
unlabeled images such as is there a handgun in this video  the classifier training process for a 
previously unknown object proceeds after manual annotation during which training data is 
created by humans who assign labels to a set of representative images of the object positive 
visual examples as shown in figure 3 as well as those which do not contain the object negative 
examples the training images allow the computer vision program to mathematically 
differentiate among objects for example providing examples of weapons and not weapons 
trains a classifier that can then better calculate the probability of a weapon being in an image 
such an approach is termed supervised as it assumes availability of annotated training data in 
contrast to semisupervised and unsupervised approaches that require partiallyannotated or 
no training data respectively in general the performance of a particular computer vision task is 
proportional to amount of humanlabeled data available 
figure 3 about here 
the now common task of matching a particular individuals face with a face in an image 
database is also useful turk  kentland 1991  to accurately match a face the computer 
vision program must consider between class variation different people who share features 
such as blue eyes and within class variation the same person who looks differently due to 
differences in image aspects frontal face compared with profile for example  as with object 
and action labelling face recognition is set as a probabilistic outcome with some threshold level 
of image similarity needed to be reached before a match is declared  a recent improvement 
works to maximize the distinction between the faces of different persons and takes into account 
8 differences ie normalizes within class variation observed across multiple images of the same 
per
son5   with these and other capabilities under development potential computer vision
sol
utions for police surveillance camera tasks are now within reach 
computer vision applications in policing   general law enforcement surveillance needs 
that computer vision can address fall under two umbrellas  the first set revolves around the need 
for automated realtime live video stream analysis  the second involves the need for posthoc 
searches of archived video files computer vision based automated identification of public safety 
events of interest addresses the first task and querybased searches of video files addresses the 
second 
related to the first task live realtime video analysis involves the need to rapidly identify 
and correctly respond to ongoing incidents  this capability is important because effectiveness of 
a surveillance system in reducing crime has been linked to realtime intervention unless 
surveillance results in someone showing up to address an observed problem camera deterrent 
effects wane ariel 2016 gill 2003 goold 2004 piza et al 2014a 2014b welsh  farrington 
2002 the development of live event analysis is conducted along two computer vision paths 
action and event detection of preidentified events of interest and detection of anomalous new 
unanticipated but potentially noteworthy events  for realtime detection of activities it is 
imperative that the system analyzes the surveillance video as it is captured and classifies actions 
and events as they appear  of particular interest to public safety monitors are many activities 
which occur infrequently and are precursors to criminal activity for example car hopping 
where a person pulls on car door handles as they walk along a street would be a precursor to theft 
from vehicles  these activities are more difficult to program because first they are rare and 
therefore have a limited number of examples available for analysis and second they can be 
9 ambiguous and difficult to define mathematically  hence a murderous assault will likely occur 
onl
y once in the lifetime of a cameras viewshed but it is crucial that it be noted by a computer 
vision program and that it be distinguished from one person giving another a vigorous friendly 
hug  humans quickly distinguish the two activities however computer vision programs must be 
quantitatively trained to do so to be useful anomaly models also must continuously update and 
incorporate environmental changes for instance changes in weather crowd density or lightning 
conditions at different times of the day 
computer vision can also reduce the immense amount of time currently spent reviewing 
and searching videos  even when it is known that a video contains specific images such as 
weapons the minutes or seconds of interest are often buried within hours of output  a computer 
vision solution to this issue is querybased searches  to be useful querybased searches require 
search options that permits retrieval of objects with particular properties such as a person with 
specific height weight race gender or appearance or objects such as an item a person was 
carrying like an umbrella or backpack  the ability to submit an object and attributebased 
search would significantly reduce the number of irrelevant video clips that an investigator must 
review  independent of specific querylinked searches it is also useful to have computer vision 
based video summarization programs for the distillation of videos into shortened but accurate 
summaries  an eighthour video can typically be reduced to an edited change only video 
lasting minutes chen wang  wang 2009 evangelopoulos et al 2009 gao wang yong  
gu 2009 
in another potential use of computer vision recent criminal justice research has used 
camera footage to study precrime visual cues  for example piza caplan  kennedy 2014b 
and levine taylor and best 2011 used video footage to examine violence precursors and 
10 moeller 2016 and piza and sytsma 2016 searched for correlates of illegal drug sales 
com
puter vision has the potential to significantly aid these research efforts and increase the use 
of surveillance videos as a data source a number of prior research efforts have employed 
surveillance video as data see piza  sytsma 2016 piza caplan  kennedy 2014a sampson 
 raudenbush 1999 st jean 2007 but their usefulness has been limited by heavy processing 
and time demands  despite having a number of years of video piza and sytsma 2016 had to 
limit analysis to a single year and 62 incidents due to processing workload in their study each 
minute of video equaled 20 minutes of transcription time  lastly as implied by piza  sytsma 
2016 and moeller 2016 a set of criminological theories and concepts such as routine 
activities environmental crime crime displacement and hotspot analysis could benefit from the 
exploration of computer vision generated data 
ongoing research  a national institute of justice funded study is underway to address 
three research questions associated with police use of computer vision shah idees  surette 
2015  in this study computer vision analytics for a large surveillance camera network is being 
developed and their integration into a public safety visual analytics workstation psvaw 
within a municipal police department will be field tested  the law enforcement targeted 
computer vision analytics under development include the retrieval of objects concepts and 
events mazaheri kalayeh idrees  shah 2015 the localization of actions in long untrimmed 
videos soomro idrees  shah 2016 the interactive detection of anomalies without annotated 
training examples zavesky  chang 2008 and multiple methods for video summarization 
rodriguez 2010  the research questions addressed are what is the accuracy and speed of the 
analytics what is the organizational fit of computer vision in a police department and what is 
the impact of a computer vision capability on a municipal criminal justice system 
11 research question 1 how well do the computer vision algorithms work in the lab 
com
puter vision algorithms are being evaluated on standard precurated annotated datasets 
which are partitioned for training and testing for many computer vision tasks prior algorithm 
accuracy has been high above 90 percent for easy action recognition datasets  however for 
challenging datasets accuracy drops to around 60 percent kuehne jhuang garrote poggio  
serre 2011 a level that would generate numerous false hits in police applications  the goal is 
to produce computer vision algorithms that are sensitive enough to not miss significant events 
but also do not swamp human reviewers with large number of erroneously flagged video clips a 
second programming goal is to achieve significant reduction in storage and computational cost 
over largescale surveillance video archives  the practical impact for a law enforcement agency 
would be significant gains in search speed and the ability to search thousands of hours of video 
data ye liu wang  chang 2013 
a computer vision based method for detection of static concepts and dynamic events is 
also being developed for object detection such as weapons police officers police vehicles and 
complex event detection like assaults thefts and car crashes  both use features from deep neural 
networks for processing images and video frames6  in the static concept search a human can
que
ry a single concept such as police officer and the system will return a sorted list of video 
clips in which the concept police officer appears the complex event detection categorizes 
video into broad categorical classes of behaviors beyond a brief appearance of single objects 
thus more challenging activities can be dealt with and video clips can be robustly classified into 
events such as robberies and assaults 
regarding the need for realtime video analysis computer vision software for live online 
abnormality detection is additionally being created javan roshtkhari  levine 2013 the 
12 quantitative problem amounts to finding patterns in the digital data that significantly deviate 
fro
m behaviors previously identified empirically the detection of abnormal behaviors is a 
difficult task  first the quantitative definition of a normal versus abnormal visual pattern is not 
well defined  second normal behavior evolves over time and may change significantly as time 
passes for instance many people walking during daylight versus few people walking during 
nighttime differ visually but both may be normal activity when it comes to crime detection lim 
and wilcox 2016 moeller 2016   third because abnormal events are rare it is difficult to 
obtain enough examples to train classifiers 
to cope with these challenges an online dictionary learning approach to detect 
abnormalities is being pursued which divides long videos into small nonoverlapping meaningful 
clips since these segments are computed based on appearance and motion information many 
will contain tracked vehicles and people which are then compared with existing elements in a 
dictionary thereby permitting the detection of abnormalities tran bourdev fergus torresani 
 paluri 2015 if the flagged anomaly is deemed a normal event it is added to the dictionary 
this allows the computer program to interactively update and recognize a new normal such as 
when a crowded day time street becomes a sparsely populated nighttime scene  the anomaly 
detection process flags anomalous events from an unsupervised approach so that labeled training 
data is not required the normalcy models will also be unique for each camera since abnormal 
behavior may vary by camera across a network 
in addition to detection a computer vision benefit is the ability to automatically 
summarize video files and screen out irrelevant information mccarthy  omahony 2015 
one approach is to use computer vision to identify a small set of suspicious video clips in real 
time from multiple camera feeds or from a large video archive a promising approach being 
13 pursued is built on semantic indexing sin which uses ideas from deep learning and foreground 
obje
ct detection shah idees  surette 2015  a temporal action localization finding an action 
in long videos approach automatically decomposes an action into several subactions models 
each subaction on appearance and duration into distinct steps and detects subactions in an 
original untrimmed video  an action event usually consists of a sequence of subactionssub
events in a specific order for example a robbery action can be decomposed into person a 
approaching person b person a producing a weapon and gesturing at person b person b 
holding hands aloft handing over wallet or phone and the two separating  the approach for 
localization automatically discovers the number of subactions for each actionevent from a set 
of training videos registering the point in time the action begins and ends in a video once 
identified these segments can be flagged for human monitor review and deployment decisions 
a second video summarization approach renders a new video that highlights interesting 
activities in the original video and skims through redundant information to save viewing time 
along these lines a hierarchical video summarization method is being created which will first 
identify small video regions termed supervoxels regions with similar appearance and coherent 
motion based on information such as color and motion  next highlevel objects of interest such 
as moving humans or vehicles will be incorporated  these information sources will be combined 
and the defined video segments matched with previously detected and labeled objects  by 
detecting interesting regions as well as objects analysis of human and object interactions are 
possible eg a theft involving a person in a car or a fight involving multiple people 
research question 2 does computer vision work in the field   if a large automated 
camera system results in event swamping from the flagging of numerous events for review or has 
no significant impact on daily agency operations computer visions promise will be unmet to 
14 address this issue a set of events of interest to law enforcement and the design and installation of 
a co
mputer vision workstation in a municipal police department will be evaluated  table 1 lists 
18 objects events and interactions of interest to law enforcement that computer vision 
algorithms are being developed to detect 
table 1 about here 
some of the events of interest are rare and have proven difficult to locate sufficient 
numbers of training examples from police surveillance cameras  in addition some events occur 
in conjunction with other crimes or are ambiguous these events seldom happen without other 
confounding criminal activity or they are hard to identify by annotators eg injured officer and 
custody events  thus it is difficult to train detectors for such less straightforward events to flag 
them reliably  correspondingly anomaly detection in the realworld assumes greater 
importance 
in terms of agency impact the key computer vision field component will be a public 
safety visual analytics workstation psvaw  see figure 4  the psvaw will have multiple 
capabilities ranging from detection and localizing objects in camera feeds labeling actions and 
events associated with training data and allowing query based searches for specific events in 
videos  it will also be programmed to flag pretrained criminal and new nontrained abnormal 
events   using human monitor feedback the psvaw will refine the retrieval parameters and 
improve its search results over time after repeating a number of iterations the psvaw will 
create an inductive model to detect new activity of interest in realtime so that an initial anomaly 
will over time become a computer vision trained recognized and labeled event 
figure 4 about here 
15 research question 3  what are computer vision impacts on a criminal justice system 
the   p
resence of surveillance cameras has been forwarded as both possibly increasing the 
reporting of events to the police or suppressing citizen guardianship levels surette 2006 
besides issues of loss of privacy costs and effectiveness because computer vision surveillance 
cameras are expected to catch events that humans would miss more people may be arrested as 
the criminal justice net becomes wider and finer surette 2005 
the systemwide impact of a computer vision enhanced camera network will be assessed 
along two dimensions its impact on the policing of a community through time series analysis of 
all reported crime and calls for service data and the local criminal justice systems use of 
computer vision generated video for investigations and evidence utilizing measures of time 
spent by human monitors on video requests and processing flagged events and response times 
and use of camera video for investigations and case evidence the systemwide impact of 
computer vision capabilities will be evaluated 
 conclusion  computer vision has the potential to address a wide set of problems 
associated with current public space camera surveillance systems from inappropriate use such as 
profiling and voyeurism to inherent unintended errors by human monitors  an automated 
camera monitor system will not view what it has not been programmed to view and when 
appropriately programmed will reduce the surveillance gaze from falling on unsuitable subjects 
computer vision systems can also greatly reduce the two sources of error and ineffectiveness in 
the use of public space surveillance cameras  a computer algorithm will not become bored or 
distracted during realtime monitoring so events of interest are less likely to go unseen 
simultaneously an algorithm will not be so focused on a search for a specific event that other 
16 important events go unnoticed  these benefits are currently being developed and tested in 
com
puter vision lab settings
computer vision should not be expected to be a panacea  f or law enforcements 
surveillance needs and software gaps remain such as algorithms confidently misidentifying 
images nguyen yosinski  clune 2014  additional shortfalls are due to object size number 
of pixels presenting detection errors in labeling small objects such as guns and tracking can be 
hampered by the occlusion of people and objects hence following computer vision 
developments from the lab to the field will be an important step  the promise of computer 
vision is that the automation of monitoring can upgrade the current reality of a poorly utilized 
technology expenditure to a reliable public safety tool  to already budget conscious lowon
manpower agencies a field evaluated computer vision capability stands as potentially invaluable 
references 
alla
rd t wortley r  stewart a 2008 the effect of cctv on prisoner misbehavior the 
prison journal 883 404422 
ariel b 2016  do police body cameras really work ieee spectrum   posted may 4 2016 
downloaded may 24 2016 from httpspectrumieeeorgconsumerelectronicsportable
devicesdopolicebodycamerasreallywork 
ariel b farrar w  sutherland a 2015 the effect of police bodyworn cameras on use of 
force and citizens complaints against the police a randomized controlled trial journal of 
quantitative criminology  313 509535 
assari s m idrees h  shah m 2016 human reidentification in crowd videos using 
personal social and environmental constraints  european conference on computer vision 
eccv 
17 baldwin d  baird j 2001  discerning intentions in dynamic human action tre nds in 
cognitive sciences 5 171178 
barrett h todd p miller g  blythe p 2005  accurate judgments of intention from 
motion cues alone a crosscultural study evolution and human behavior 26 313331 
becklen r  cervone d 1983  selective looking and the noticing of unexpected events 
memory  cognition 11 601608 
boksem m meijman t  lorist f 2005 effects of mental fatigue on attention an erp 
study cognitive brain research 251 107116 
bredemeier k  simons d 2012 working memory and inattentional blindness 
psychological bulletin review  19 239244 
buckland g  2001 shots in the dark true crime pictures  ny little brown  co 
chen b wang j  wang j 2009 a novel video summarization based on mining the story
structure  semantic relations among concept entities multimedia ieee transactions on 
112 295312 
comaniciu d ramesh v  meer p 2000 realtime tracking of nonrigid objects using 
mean shift computer vision and pattern recognition 2 142149 
driver j 1998 the neuropsychology of spatial attention in h pashler ed attention pp 
297340 london taylor francis 
evangelopoulos g zlatintsi a skoumas g rapantzikos k potamianos a maragos p  
avrithis y 2009 video event detection and summarization using audio visual and text 
saliency acoustics speech and signal processing icassp ieee international conference 
faber l maurits n  lorist m 2012 mental fatigue affects visual selective attention 
plos one  710 e48073 
fougnie d  marois r 2007 executive working memory load induces inattentional 
blindness  psychonomic bulletin and review 141 142147 
gao y wang d yong j  gu h 2009 dynamic video summarization using two level 
redundancy detection multimedia tools and applications  422 233250 
18 gill m 2003 cctv leicester uk perpetuity press 
gil
l m  loveday k 2003 what do offenders think about cctv crime prevention and 
community safety 53 1725 
gill m spriggs a allen j hemming m jessiman p and kara d 2005 control room 
operation findings from control room observations  london home office 
goold b 2004 cctv  policing oxford uk oxford university press 
haggerty k  gozso a 2005 seeing beyond the ruins surveillance as a response to 
terrorist threats canadian journal of sociology 302 169187 
hier s greenberg j walby k  lett d 2007  media communication  the 
establishment of public camera surveillance programmes in canada media culture and 
society  295 727751 
hyman i boss e matthew s wise b mckenzie m kira e  caggiano j 2009 did 
you see the unicycling clown inattentional blindness while walking and talking on a cell 
phone  applied cognitive psychology  245 597607 
 idrees h warner n  shah m 2014 tracking in dense crowds using prominence and 
neighborhood motion concurrence image and vision computing  321 1426 
javan roshtkhari m  levine m 2013 online dominant and anomalous behavior detection 
in videos proceedings of the ieee conference on computer vision and pattern recognition  
pp 26112618 
javed o rasheed z alatas o  shah m 2003 knight a real time surveillance 
system for multiple and nonoverlapping cameras ieee international conference on 
multimedia and expo  
javed o rasheed z shafique k  shah m 2003 tracking across multiple cameras with 
disjoint views international conference on computer vision 
johnston w hawley k plewe s elliott j  dewitt m 1990 attention capture by novel 
stimuli journal of experimental psychology general  1194 397  keval h and sasse m 
19 2010 not the usual suspects a study of factors reducing the effectiveness of cctv 
sec
urity journal 232 134154  
keval h  sasse m 2010 not the usual suspects a study of factors reducing the 
effectiveness of cctv security journal  232 134154 
kuehne h jhuang h garrote e poggio t  serre t 2011 hmdb a large video 
database for human motion recognition in international conference on computer vision  
pp 25562563  
kroener i 2014  cctv a technology under the radar  burlington vt ashgate 
la vigne n lowry s markman j and dwyer a  2011 evaluating the use of public 
surveillance cameras for crime control and prevention  washington dc us department of 
justice office of community oriented policing services urban institute justice policy 
center   
levine m taylor p j  best r 2011 third parties violence and conflict resolution the 
role of group size and collective action in the microregulation of violence psychological 
science  
lim h  wilcox p 2016 crimereduction effects of openstreet cctv conditionality 
considerations justice quarterly  130 
lohr s 2012 the age of big data new york times  11 
mack a 2003  inattentional blindness looking without seeing current directions in 
psychological science  125 180184 
mack a  rock i 1998 inattentional blindness   cambridge ma mit press 
marx g 1988 undercover police surveillance in america berkley university of california 
press 
mazaheri a kalayeh m idrees h  shah m 2015 ucfcrcv at trecvid 2015 
semantic indexing nist trecvid 
mccarthy o  omahony m 2016 end user response to an event detection  route 
reconstruction security system prototype for use in airports and public transport hubs 
20 transportation research board of the national academies  downloaded from 
ht
tpamonlinetrborgtrb60693201612807374t0011282343625412823593165450
1298069316545012993283qr1 
memmert d 2006  the effects of eye movement age and expertise on inattentional 
blindness consciousness and cognition  153 620627 doi 101016jconcog200601001 
pmid 16487725  
moeller k 2016  temporal transaction patterns in an openair cannabis market  police 
practice and research 171 3750 
most s scholl b clifford e  simons d 2005 what you see is what you set sustained 
inattentional blindness and the capture of awareness  psychological review  1121 217
242 
nsholm e rohlfing s  sauer j d 2014 pirate stealth or inattentional blindness the 
effects of target relevance and sustained attention on security monitoring for experienced and 
nave operators plos one 91   downloaded from  
httpjournalsplosorgplosonearticleid101371journalpone0086157 
neisser u  becklen r 1975  selective looking attending to visually specified events 
cognitive psychology  7 480494 
neisser u 1979  the control of information pickup in selective looking  in ad pick eds 
perception  its development a tribute to eleanor j gibson pp 201219 hillsdale nj 
erlbaum   
norris c  armstrong g  1999a the maximum surveillance society  the rise of cctv 
oxford uk berg 
norris c  armstrong g  1999b cctv and the social structuring of surveillance  in n 
tilley and k painter eds surveillance of public space cctv street lighting and crime 
prevention  crime prevention studies vol 10 pp 157178  monsey ny criminal justice 
press   
21  nguyen a yosinski j  clune j 2014  deep neural networks are easily fooled high 
con
fidence predictions for unrecognizable images in 2015 ieee conference on computer 
vision and pattern recognition cvpr  pp 427436 ieee 
piza e caplan j  kennedy l 2014a cctv as a tool for early police intervention 
preliminary lessons from nine case studies security journal  doi101057sj201417 
downloaded from httplinkspringercomarticle1010572fsj201417 
 piza e caplan j  kennedy l 2014b is the punishment more certain an analysis of 
cctv detections and enforcement justice quarterly  316 10151043 
piza e caplan j kennedy l  gilchrist a  2015 the effects of merging proactive cctv 
monitoring with directed police patrol a randomized controlled trial  journal of 
experimental criminology  11 4369 
piza e  sytsma v 2016 exploring the defensive actions of drug sellers in openair markets 
a systematic social observation  journal of research in crime and delinquency 531 36
65 
rodriguez m 2010 cram compact representation of actions in movies computer vision and 
pattern recognition cvpr  
sampson r  raudenbush s 1999  systematic social observation of public spaces a new 
look at disorder in urban neighborhoods  american journal of sociology 105603651 
sasse a 2010  not seeing the crime for the cameras communications of the acm  53 2225 
shah m idees h  surette r 2015  studying the impact of video analytics for pre live 
and post event analysis on outcomes of criminal justice  orlando fl university of central 
florida center for research on computer vision  funded by us department of justice 
nij2015r2cxk025 
shah m javed o  shafique k 2007 automated visual surveillance in realistic 
scenarios ieee multimedia 1 3039 
short e  ditton j 1996  does closed circuit television prevent crime  monograph of the 
scottish office central records unit edinburgh scotland 
22 soomro k idrees h  shah m 2016 predicting the where and what of actors and actions 
thr
ough online action localization ieee conference on computer vision and pattern 
recognition 
st jean lp 2007  pockets of crime broken windows collective efficacy and the criminal 
point of view  chicago university of chicago press 
staples w 2014  everyday surveillance   new york rowman  littlefield 
surette r 2005  the thinking eye pros and cons of second generation cctv surveillance 
systems  policing an international journal of police strategies  management  281 152
173 
surette r 2006  cctv and citizen guardianship suppression a questionable proposition 
police quarterly  91 100125 
sutton a  wilson d  2004 openstreet cctv in australia politics and expansion 
surveillance and society  223 310322 
thomas j  cook k 2006 a visual analytics agenda ieee computer graphics and 
applications  261 1013 
tran d bourdev l fergus r torresani l  paluri m 2015 learning spatiotemporal 
features with 3d convolutional networks in ieee international conference on computer 
vision iccv  
turk m  pentland a 1991 face recognition using eigenfaces computer vision and 
pattern recognition 1991 
vlahos j 2009  surveillance society new hightech cameras are watching you  popular 
mechanics  oct 1 
wang d ding w lo h stepinski t salazar j  morabito m 2013 crime hotspot 
mapping using the crime related factorsa spatial data mining approach applied 
intelligence  394 772781 
welsh b  farrington d 2002  crime prevention effects of closed circuit television a 
systematic review   home office research study 252 london home office 
23 welsh b  farrington d  2004 evidencebased crime prevention the effectiveness of 
cctv
 crime prevention and community safety 6 2133 
welsh b  farrington d 2009 public area cctv and crime prevention an updated 
systematic review and meta analysis justice quarterly  264 716745 
williams d 2007 effective cctv and the challenge of constructing legitimate suspicion 
using remote visual images  journal of investigative psychology and offender profiling  4 
97107 
wolfe j 1994 guided search 20 a revised model of visual search psychonomic bulletin  
review  1 202238 
ye g liu d wang j  chang s 2013 largescale video hashing via structure learning 
proceedings of the ieee international conference on computer vision  pp 22722279 
yilmaz a javed o  shah m 2006 object tracking a survey acm computing surveys 
csur  384 13 
yu c ward m morabito m  ding w 2011 crime forecasting using data mining 
techniques 2011 ieee 11th international conference on data mining workshops  pp 779
786 ieee 
zavesky e  chang s 2008 cuzero embracing the frontier of interactive visual search for 
informed users proceedings of the 1st acm international conference on multimedia 
information retrieval  pp 237244 
1 when and how these systems work in specific applications remains under debate ariel 2016 
ari
el farrar  sutherland 2015 williams 2007 indirect evidence suggests that offenders take 
into account the perceived level of surveillance and the likelihood of intervention when deciding 
24 whether to commit certain types of crimes especially instrumental street crimes such as car 
breakins  this suggests that easily visible cameras with signage can deter certain offenders 
gill  loveday 2003 allard wortley  stewart 2008 short  ditton 1996 welsh  
farrington 2009  spontaneous crimes such as assaults appear to be less affected  and overall 
came
ras appear most effective in reducing crime when combined with other interventions lim 
 wilcox 2016 piza caplan  kennedy 2014a 2014b welsh  farrington 2004 
2 inattentional blindness is defined as the failure to  s ee highly visible objects directly looked at 
when cognitive attention is elsewhere mack 2003 p 180 mack  rock 1998 see also 
becklen  cervone 1983 neisser 1979 and neisser  becklen 1975   attention capture 
refers to the ability of novel stimuli to gain the focus of someone otherwise cognitively engaged 
johnston hawley plewe elliott  dewitt 1990 most scholl clifford  simons 2005 
wolfe 1994  the use of technology has been reported to effect both processes hyman boss 
matthew wise mckenzie kira  caggiano 2009 p 605 and inattentional blindness has been 
found to be a common phenomenon associated with watching video streams most scholl 
clifford  simons 2005 
3a classic example is demonstrated by viewers tasked with counting passes failing to see a 
gor
illa walk through a group of people tossing a ball around 
httpswwwyoutubecomwatchvvjg698u2mvo
4over the past decade more sophisticated alternate approaches to tracking have been presented 
in t
he computer vision literature including those that track single and multiple objects or persons 
across nonoverlapping multiple camera field of views called the handoff problem and 
people in dense crowds assari idrees  shah 2016 idrees warner  shah 2014 javed 
rasheed alatas  shah 2003 javed rasheed shafique  shah 2003 
25 5the goal in this method termed linear discriminative analysis or lda is to maximize the 
sep
aration of the set of images of one person from the set of images of different but possibly 
similar looking on some characteristics persons by using sets of image portraits
6 recent research has shown that training deep network s  containing large number of hidden 
layers significantly improves performance on computer vision tasks such as object detection 
face identification and action recognition    however deeper networks require larger quantities 
of training data compared to traditional machine learning algorithms and thus may be limited for 
public safety applications 
figure 1 a digitized letter 
the ori
ginal letter a is rendered into a 14 by 12 array of 168 pixels with each pixel assigned 
a value representing a tone from white scored 10 to black squares scored 00  gray squares 
are scored from light to dark gray squares scored 05 light gray ones scored from 01 to 04 
dark gray squares scored from 06 to 09  the digitized photo of the original a image 
results where the a can be vaguely discerned in the pattern of 00 scored pixels  for 
capturing color multiple values typically corresponding to red green and blue are stored for 
each pixel location  source  
httppippingimporgimageprocessingimagessamplegridasquarepng 
figure 2 an image histogram 
the hi
stogram on the right is constructed from 
grayscale pixels from the image by quantizing them 
into 256 bins  the significance of image histograms 
for computer vision is that histograms are often 
unique for different objects and correlate with their 
shape texture and color and can be used for assigning 
labels to images  
figure 3 positive training examples for assault theft vandalism and robbery 

figure 4  public safety visual analytics workstation psvaw 

table 1  objects crimes and events of interest to law enforcement for computer vision 
algorithm development and field testing 
interest ope rational definition
 o
bjects  
weapons handguns rifles 
police cars marked law enforcement vehicles 
police officers uniformed law enforcement officers 
emergency vehicles fire trucks ambulances flashing emergency lights 
left property abandoned bags backpacks etc flagged after a set time limit 
 crimes  
criminal mischief graffiti vandalism destroying property tipping over objects spray 
painting or other property damage 
theft removal of property need to differentiate legal from illegal removal of 
property such as associated with property damage like breaking a car 
window 
robbery need to differentiate legal from illegal exchanges associated with force or 
weapon  
burglary burglary from auto with window or other property damage business 
burglaries constrained to persons entering a business after closing 
possibly flagging constrained from midnight to 6 am time frame  
drug transactions need to differentiate legal from illegal exchanges associated with time of 
day or established illicit drug markets 
batteries  assaults shootings stabbings and fistfights  
 public safety events  
car crashes damage to moving vehicles 
crowd activity rioting crowd density threshold reached running fighting falling 
property destruction 
individual activity running falling immobile exceeds set time limit blocking traffic 
standing in roadway exceeds set time limit 
injured officer officer down exceeds set time limit 
custody events  arrests mental health retentions 
citizen  police  citizen requests for assistance officer informationidentification requests 
fire  explosions set to minimum size limit","['httpjournalsplosorgplosonearticleid101371journalpone0086157', '101016jconcog200601001', 'nij2015r2cxk025', '1298069316545012993283qr1', 'tpamonlinetrborgtrb60693201612807374t0011282343625412823593165450', 'httplinkspringercomarticle1010572fsj201417', 'unanticipated', 'unrecognizable', 'httpswwwyoutubecomwatchvvjg698u2mvo', '427436']"
"Are object detection assessment criteria ready for maritime computer
  vision?","['Dilip K. Prasad', 'Huixu Dong', 'Deepu Rajan', 'Chai Quek']",2018,http://arxiv.org/abs/1809.04659v2,"1
Are object detection assessment criteria ready
for maritime computer vision?
Dilip K. Prasad1;2;, Huixu Dong3, Deepu Rajan2, and Chai Quek2
Abstract â€”Maritime vessels equipped with visible and
infrared cameras can complement other conventional sen-
sors for object detection. However, application of computer
vision techniques in maritime domain received attention
only recently. The maritime environment offers its own
unique requirements and challenges. Assessment of the
quality of detections is a fundamental need in computer
vision. However, the conventional assessment metrics suitable
for usual object detection are deï¬cient in the maritime
setting. Thus, a large body of related work in computer
vision appears inapplicable to the maritime setting at the
ï¬rst sight. We discuss the problem of deï¬ning assessment
metrics suitable for maritime computer vision. We consider
new bottom edge proximity metrics as assessment metrics
for maritime computer vision. These metrics indicate that
existing computer vision approaches are indeed promising
for maritime computer vision and can play a foundational
role in the emerging ï¬eld of maritime computer vision.
I. I NTRODUCTION
Maritime vessels (MV) are equipped with sensors such
as radar, sonar and LIDAR for situational awareness. The
automatic identiï¬cation system (AIS) supports trafï¬c data
exchange over maritime communication channels, through
which each MV with on-board AIS declares its position,
speed, and intended path. The International Regulations
for Preventing Collisions at Sea 1972 (COLREGs) impose
that all cargo ships weighing more than 300 tonnes and
all passenger ships are equipped with AIS. There is
no such imposition on smaller MVs, including ï¬shing
boats and small-medium sized cargo MVs. Such MVs are
invisible in trafï¬c data. Moreover, the AIS channel may be
inaccessible for several minutes to few hours at a time [1].
Cameras in the visible and infrared (IR) range now play
a complementary role by overcoming disadvantages of
traditional sensors like the minimum range associated with
radar and sonar [2]. Thus, computer vision (CV) techniques
should play an important role in detecting objects in
the maritime environment , especially in detecting small
and medium sized MVs that have weak radar or sonar
signatures and lack on-board AIS.
Maritime CV for object detection faces several chal-
lenges. Maritime video streams are characterized by scene
ï¬‚atness , i.e. lack of landmarks and marked lanes as in
1Department of Computer Science, UiT The Arctic University of
Norway, TromsÃ¸ 9037, Norway, Email-dilipprasad@gmail.com
2School of Computer Science and Engineering, Nanyang Technolog-
ical University, Singapore 639798
3Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213(a) Physical distances vary non-linearly in image [4], [8]
(b) 10 Examples of maritime objectsâ€™ appearance
Fig. 1: What is an acceptable detection of a maritime ves-
sel? (a) Collision avoidance requires accurate estimate of the
distance, which is related to the bottom edge of the vessel,
and the minimum span of a maritime object. (b) Green, blue,
and red boxes denote ground truth, acceptable detection, and
unacceptable detections, respectively.
roads. The maritime scene offers difï¬cult to model dy-
namic background features because of challenges such as
a semi-stochastic wave background, the sharp contrasts
of wakes, possibilities of occlusion of MVs, and weather
and illumination conditions such as rain, haze and glint
[3]. Further, planning the manoeuver and deceleration for
collision avoidance (CA) is challenging since the distance
and span of the MVs in the scene is related non-linearly to
the pixels along the y","1
are object detection assessment criteria ready
for maritime computer vision
dilip k prasad12 huixu dong3 deepu rajan2 and chai quek2
abstract maritime vessels equipped with visible and
infrared cameras can complement other conventional sen
sors for object detection however application of computer
vision techniques in maritime domain received attention
only recently the maritime environment offers its own
unique requirements and challenges assessment of the
quality of detections is a fundamental need in computer
vision however the conventional assessment metrics suitable
for usual object detection are decient in the maritime
setting thus a large body of related work in computer
vision appears inapplicable to the maritime setting at the
rst sight we discuss the problem of dening assessment
metrics suitable for maritime computer vision we consider
new bottom edge proximity metrics as assessment metrics
for maritime computer vision these metrics indicate that
existing computer vision approaches are indeed promising
for maritime computer vision and can play a foundational
role in the emerging eld of maritime computer vision
i i ntroduction
maritime vessels mv are equipped with sensors such
as radar sonar and lidar for situational awareness the
automatic identication system ais supports trafc data
exchange over maritime communication channels through
which each mv with onboard ais declares its position
speed and intended path the international regulations
for preventing collisions at sea 1972 colregs impose
that all cargo ships weighing more than 300 tonnes and
all passenger ships are equipped with ais there is
no such imposition on smaller mvs including shing
boats and smallmedium sized cargo mvs such mvs are
invisible in trafc data moreover the ais channel may be
inaccessible for several minutes to few hours at a time 1
cameras in the visible and infrared ir range now play
a complementary role by overcoming disadvantages of
traditional sensors like the minimum range associated with
radar and sonar 2 thus computer vision cv techniques
should play an important role in detecting objects in
the maritime environment  especially in detecting small
and medium sized mvs that have weak radar or sonar
signatures and lack onboard ais
maritime cv for object detection faces several chal
lenges maritime video streams are characterized by scene
atness  ie lack of landmarks and marked lanes as in
1department of computer science uit the arctic university of
norway troms 9037 norway emaildilipprasadgmailcom
2school of computer science and engineering nanyang technolog
ical university singapore 639798
3robotics institute carnegie mellon university pittsburgh pa 15213a physical distances vary nonlinearly in image 4 8
b 10 examples of maritime objects appearance
fig 1 what is an acceptable detection of a maritime ves
sel a collision avoidance requires accurate estimate of the
distance which is related to the bottom edge of the vessel
and the minimum span of a maritime object b green blue
and red boxes denote ground truth acceptable detection and
unacceptable detections respectively
roads the maritime scene offers difcult to model dy
namic background features because of challenges such as
a semistochastic wave background the sharp contrasts
of wakes possibilities of occlusion of mvs and weather
and illumination conditions such as rain haze and glint
3 further planning the manoeuver and deceleration for
collision avoidance ca is challenging since the distance
and span of the mvs in the scene is related nonlinearly to
the pixels along the y","['emaildilipprasadgmailcom', '639798', 'semistochastic', 'manoeuver', '3robotics', 'inapplicable', 'identication', '15213a', 'ntroduction', 'detections']"
BMVC 2019: Workshop on Interpretable and Explainable Machine Vision,['Alun Preece'],2019,http://arxiv.org/abs/1909.07245v1,,,[]
"Vision Transformers in Medical Computer Vision -- A Contemplative
  Retrospection","['Arshi Parvaiz', 'Muhammad Anwaar Khalid', 'Rukhsana Zafar', 'Huma Ameer', 'Muhammad Ali', 'Muhammad Moazam Fraz']",2022,http://arxiv.org/abs/2203.15269v1,"Vision Transformers in Medical Computer Vision - A
Contemplative Retrospection
Arshi Parvaiza, Muhammad Anwaar Khalida, Rukhsana Zafara, Huma Ameera,
Muhammad Aliaand Muhammad Moazam Fraza,âˆ—
aSchool of Electrical Engineering and Computer Science,
National University of Sciences and Technology (NUST), Islamabad, 44000, Pakistan
ARTICLE INFO
Keywords :
Vision Transformers
Medical Image Analytics
Self Attention
Medical Computer Vision
Diagnostic Image Analysis
Literature SurveyABSTRACT
Recent escalation in the field of computer vision underpins a huddle of algorithms with the
magnificentpotentialtounraveltheinformationcontainedwithinimages.Thesecomputervision
algorithmsarebeingpracticedinmedicalimageanalysisandaretransfiguringtheperceptionand
interpretationofImagingdata.Amongthesealgorithms,VisionTransformers(ViTs)areevolved
as one of the most contemporary and dominant architectures that are being used in the field of
computervision.Theseareimmenselyutilizedbyaplentyofresearcherstoperformnewaswell
asformerexperiments.Here,inthisarticleweinvestigatetheintersectionofVisionTransformers
andMedicalimagesandprofferedanoverviewofvariousViTsbasedframeworksthatarebeing
usedbydifferentresearchersinordertodeciphertheobstaclesinMedicalComputerVision.We
surveyed the application of Vision transformers in different areas of medical computer vision
such as image-based disease classification, anatomical structure segmentation, registration,
region-based lesion Detection, captioning, report generation, reconstruction using multiple
medicalimagingmodalitiesthatgreatlyassistinmedicaldiagnosisandhencetreatmentprocess.
Along with this, we also demystify several imaging modalities used in Medical Computer
Vision. Moreover, to get more insight and deeper understanding, self-attention mechanism of
transformers is also explained briefly. Conclusively, we also put some light on available data
sets, adopted methodology, their performance measures, challenges and their solutions in form
of discussion. We hope that this review article will open future directions for researchers in
medical computer vision.
1. Introduction
Advances in medical imaging modalities have made them indispensable in clinical practice. The analysis of these
images by analysts is limited to human subjectivity, time constraints, and variation of interpretation, which leads to
delusion[1,2].Medicalimagescontainanampleinformationthatisthekeyformedicaldiagnosisandhencetreatment.
Thehealthcaredatacomprises90%ofimagingdata,soconsideredastheprimarysourceformedicalinterventionand
analysis. Multiple medical imaging modalities such as Computed Tomography (CT), ultrasound, X-ray radiography,
MR Imaging (MRI), and pathology are commonly used for medical imaging diagnostics. Several challenging factors
associated with medical imaging modalities such as expensive data acquisition [3], dense pixel resolution [4], lack of
standardimageacquisitiontechniquesintermsoftoolandscanningsettings[5],modality-specificartefacts[6],hugely
imbalanced data in negative and positive classes [7], sparse and noisy annotated datasets [8] are major hindrance in
translating AI based diagnosis into clinical practice.
Since its surge, deep learning has shown remarkable success in automatic image analyses of medical imaging
modalities. The advancements in deep learning have been flourished and perfected with time, revolving primarily
around one algorithm called Convolutional Neural Networks (CNN). CNNs are potentially the most popular deep
learning architecture for its distinguished capabilities to exploit the spatial and temporal relationship between the
features of images, which need to be deciphered for extracting meaningful information hidden in images [9, 10, 11].
It has achieved notable accomplishment in medical imaging applications [12, 13] such as, determining the presence
and then identifying the type of malignancy (Classification), locating the patientâ€™s lesion (Detection), extracting the
desiredobject(organ)fromamedicalimage(Segmentation),placingseparateimagesinacommonframeofreference
moazam.fraz@seecs.edu.pk (M.M. Fraz)
ORCID(s):0000-0003-0495-463X (M.M. Fraz)
. : Page 1 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
for comparing or integrating the information they contain (Registration), synthesizing images for balancing dataset
(Generative Modeling) [14].
Despite that CNNs are very good at feature extraction tasks, they fail to encode the relative position of different
features. In a CNN, the deeper layers are limited to view at whatever the initial layers have passed to them. This way
theylosetheglobalcontextofthefeatures.Increasingthenumberoffiltersimprovestherepresentationcapacitybutat
the cost of computation [15]. Various architectural changes are suggested by researchers for an efficient solution over
thecourseoftimeandleadingtoattentionmechanisms[16].Usingattentionmechanism,theregionsoftheimageare
captured,towhichthe CNNshouldpayattention,andforwardedto thedeeperlayers.Researchershavedemonstrated
that replacing the convolutional layer with attention has improved performance [17, 18]. The breakthrough from
Transformer network [16] in Natural Language Processing (NLP) tasks has inspired researchers to leverage this
architecture for various computer vision tasks. Dosovitskiy et al. [19] proposed an adaptation to the transformer,
known as Vision Transformer (ViT) that can be applied directly to sequences of image patches for extracting fine-
grained features. In ViT, global attention is applied on 16x16 patches of the entire image, focusing on the global
salient features of the image, which resolve the long-range dependency among image content. It gets the best out
of the attention mechanism to incorporate global context in the image features without compromising computational
efficiency.
The potential of the vision transformers is further explored by many researchers for solving various problems.
However, in this survey we aim to highlight the contribution of vision transformers to circumvent the challenges in
automatic diagnostic of diseases using medical imaging modalities and their applications in medical computer vision
tasks. Our intended audience for this review are research practitioners from medical and interdisciplinary fields of
computer vision. For their assistance, we have described commonly used terminologies and their description in table
1.
This review is organized into seven sections. Section 1 briefly discusses the role of deep learning, the emergence
of the transformers, and the replacement of CNNs by transformers in medical computer vision. Section 2 discuss the
organization and papers selection methodology and distribution of the Review. Section 3 discusses different medical
imaging modalities and their application in the diagnostic and treatment of various diseases. Section 4 discusses the
emergence of vision transformers and lists all the publicly available datasets used by the reviewed paper in every
modality and deep learning task. Section 5 discusses visual recognition tasks to established the domain knowledge
for the audience of the interdisciplinary field. Section 6 gives the details about the reviewed techniques categorized
according to each deep learning task such as classification, segmentation, detection clinical report generation, and
Miscellaneous which also include image registration. Section 7 identifies research gaps in the review papers and
discusses the future directions for using transformers in medical computer vision.
1.1. Scope/Objective of the Review
The aim of writing this review paper is to highlight and discuss the contribution of Vision Transformers in
medical computer vision across different medical imaging modalities. For this purpose, we have searched out papers
from different top Conferences and Journals, excluding pre-prints, within the time span of three years from 2019-
2022. The results achieved and the adopted methodology of each paper is reviewed comprehensively. The distinct
categories that we reviewed belonging to the medical image analysis includes classification, detection, segmentation,
registration,clinicalreportgeneration,imageenhancement,imagereconstructionsandimagesynthesis.Theliterature
of these categories is further divide into different medical imaging modalities. Ultimately, in addition to accentuating
interesting techniques in the literature, we also put some light on research gaps and future directions. We hope this
review will bridge the gap between computer vision community and medical specialists to foster the future research
and development in medical computer vision This article is written keeping in mind the intended audience from the
interdisciplinary fields, medical and AI. Publicly available datasets and downloadable links are listed in the table 3.
1.2. Comparison with other Reviews
Although there exists some reviews on transformers already which enfolds a significant amount of work, yet we
feelthatthereisalotofroomforimprovement.Forexample,noreviewisprimarilyfocusedonapplicationsofvision
transformersinmedicaldomain.Tobridgethisgap,wecomeupwiththissurveyinwhichourpointofconvergenceis
to scrutinize the exertion of ViTs in medical computer vision. To initiate this process we collected a bunch of articles
addressingdifferenttransformersarchitectureandtheirutilizationonmultimodalmedicalimages.Weincludedalmost
80peerreviewedarticlesinoursurveyfromprestigiousplatformslikePubMed,Springer,IEEE,ScienceDirectwhich
. : Page 2 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Table 1
List of accronyms and abbreviations used in paper
Acronyms Words Acronyms Words
KID Kernel & Inception Distance MFTEMulti-Branch Features Transformation
and Extraction
AHA Align Hierarchical Attention MA Microaneurysms
MSAM Multi modal Spatial Attention Module CLAMClustering-Constrained-Attention
Multiple-Instance Learning
CAC Coronary Artery Calcium KFS Key Factor Sampling
EMVTEfficient Multi Scale Fusion Trans-
formerMSTGANetMulti Scale Transformer Global Atten-
tion Network
NSCF NonLocal Sparse Net Fusion MCAT Multimodal Co-Attention Transformer
TETRISTemplate Transformer Image Segmen-
tationFMNet Feature Mapping Sub-Network
RDLs Regionalized Dynamic Learners CRC Colorectal cancer
IDH Isocitrate Dehydrogenase MTTU Multi Task Transformer Unet
VITBISVision Transformer for Biomedical Im-
age SegmentationCIDErConsensus-based Image Description
Evaluation
DAFNet Disentangled Alligned and Fuse Net ASFT Adjacent Slices Feature Transformer
HYBRIDCTRM Hybrid Convolutional Transformer MTI Multi Text Indexer
VIF Visual Information Fidelity PCR Rpolymerase Chain Reaction
ABVS Automated Breast Volume Scanner PRCC Papillary Renal Cell Carcinoma
FID Frechet Inception Distance GSM Genitourinary syndrome of menopause
CEDT Cross Encoder-Decoder Transformer GLVE Global-Local Visual Extractor
makes our paper unique from other review articles. In addition to that we also explained different imaging modalities
used in medical computer comprehensively. Moreover, we have given a brief note on available medical data sets in
tabularform.Thedownloadablelinkstothesedatasetsarealsomentioninthesetables.Wealsodiscussedtheresultsof
state-of-the-artapproachesinawellstructuredtabularforminwhichwedescribedtheperformancemetricesalongwith
theirresultsontheavailabledatasets.Intheend,wehavealsopointedoutsomechallengesalongwiththeirinsightful
future directions. For comparative analysis of our review with Khan et al. [20] and Kai et al. [21] we visualized the
main points in Figure 1.
. : Page 3 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 1: Comparison with recent published reviews on Vision Transformers
2. Survey Methodology
In this section we will discuss the study selection criteria based on which articles are chosen for the review, and
distribution of the included articles according to venues (journals, conferences), medical imaging modalities, deep
learning tasks (classification, segmentation, detection etc.) and impact factors.
2.1. Papers Selection
We have demonstrated the details of the searched and included research papers in this review article through
PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses). Figure 2 shows the summary of
papers selection. We searched our papers on PubMed, Springer, IEEE Xplore, Science direct, and finally on google
scholar. In the result of search queries, we have found 11060 papers. Among which 3060 were duplicate and were
excluded from the study. We screened remaining 8000 and found 7,600 were not fulfilling the criteria of legitimacy
forthissurveyassomeofthemwasonlyaboutmedicalapplicationwithouttransformersandsomeofthemwasusing
transformer word in the different context than vision transformer. We further screened the remaining 400 articles and
excluded the preprints from our study. In the PRISMA we have shown the categorization of our included 80 papers
according to their application in medical domain. We have also demonstrated the distribution of the papers according
to the medical imaging modalities.
2.2. Data Extraction Methods
We searched different platforms such as PubMed, Springer, Science Direct, IEEE Xplore and google scholar for
extracting the research articles. We targeted top journals and conferences, in duration of last four years from 2019 to
2022. For extracting relevant papers for our study, we used different key words and combine them with the logical
operators â€˜ANDâ€™, â€˜ORâ€™ to get the better search results. The key words we used are:
. : Page 4 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 2: PRISMA - flow diagram of selected research articles for the review
â€¢Computed tomography (CT) scans, Magnetic resonance images (MRI), Ultrasound, X-rays, Optical coher-
ence tomography (OCT), Fundus images, Positron emission tomography-Computed tomography (PET-CT),
Histopathology, Histology, WSI, Whole Slide Images
â€¢Classification, Reconstruction, Segmentation, Registration, Detection, Report Generation, Enhancement
â€¢Transformer, Vision Transformer
Weextractedthekeywordsfromallthearticlesonvisiontransformersincludedinourreviewandgeneratethetag
cloud,whichisshowninFigure3.ThetagcloudillustratesthetrendingtermsinViTapplicationsinmedicalcomputer
vision.Asourcentralfocusinthisreviewistorecapitulatetheapplicationofvisiontransformersonmedicalimaging
modalities, this word cloud mostly highlighting the applications (classification, segmentation, detection, denoising,
captioning), medical imaging modalities (X-rays, PET, OCT, Whole-slide, CT, Histopathological, Fundus), disease
(cancer,glaucoma,covid,diabetic,tumor,carcinoma),organs(retinal,chest,breast,pulmonary,brain)andotherdeep
learning related terms like transformer, vision, attention, encoder, decoder, multi-model.
. : Page 5 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 3: A visual depiction of most frequently used keywords in the reviewed articles
Table 2
Inclusion and exclusion criteria for papers selection
Inclusion criteria Exclusion criteria
Articles that address the medical computer
vision tasks such as registration, segmentation,
detection,classification, enactment, reconstruction
and report generation using medical imaging modalities
and vision transformer.Articles which are not using medical imaging modalities
and vision transformer.
Papers with proper evaluation metrics and detailed sum-
mary of proposed architecture including training parame-
ters.Articles that are not peer-reviewed.
Articles that are based on vision transformers. Articles that are survey papers.
The papers inclusion and exclusion criteria is given in table 2. Firstly, papers were selected on the basis of titles,
if it does not match the inclusion and exclusion criteria then we read the abstract, conclusion, and model diagram for
the final selection.
2.3. Papers Distribution
In this section we have shown the distribution of the published papers across journals, conferences, imaging
modalities, impact factors, and medical computer vision tasks. The purpose of this section is to give the birdâ€™s eye
view of the published work, that how much literature is available in top journals and conferences, what is the impact
of the work, what imaging modalities are used and what is the progress of works across the years.
TheFigure4showsthedistributionofreviewedarticlesacrosstheyears.Itcanbeseeninthegraphthattheliterature
forvisiontransformershasgrownthroughouttheyearsfrom2019to2022astheapplicationsofvisiontransformersin
medicalimagingmodalitiesstartedgrowingfrom2019onward,withalargenumberofpublicationsintheyear2021,
in which more than 50 articles were published.
. : Page 6 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 4: A chronological distribution of vision transformers research publications in medical image analytic.
Figure5showsthecategorizationofresearcharticlesbasedonvisualrecognitiontasksusingvisiontransformers.
The recapitulation of the reviewed article is categorized based on the tasks such as classification, segmentation,
detection, report generation, registration, and miscellaneous. Miscellaneous further contains different tasks such as
reconstruction,enhancementandvisualNeuralVisualContentgeneration.Thegraphshowsthatmostofthereviewed
articles applied vision transformers on classification task which is 31%, segmentation 25%, Miscellaneous 19%,
Detection 16%, Report generation 7%, and Registration 2%.
Figure 5: Distribution of reviewed articles based on visual recognition tasks.
As the review paper is focusing on the application of vision transformers in medical imaging modalities. Figure 6
depictsthestatisticsofimagingmodalitiesusedinourreviewedarticles.Eighttypesofimagingmodalitiesareusedin
this survey paperexploiting vision transformers include X-rays imagingmodality which is 35%, CT Scans26%, MRI
Scans 13%, Histopathology Images 11%, OCT/Fundus Images 8%, PET 3%, Endoscopy 2%, and Microscopy 2%.
. : Page 7 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 6: Dispensation graph of medical imaging modalities among the articles that are reviewed.
Figure7showsthecountofvisiontransformerbasedmedicalimagingarticlestakenfromvarioustopjournalsand
conferences. Each bubble represents the number of a specific journal or a specific conference and number of articles
taken from these journals and conferences.
Figure 7: Bubble graph representing number of articles chosen from top ranked journals and conferences.
Figure 8 shows the distribution of vision transformer based reviewed papers across various journals of various
impact factors according to JCR year 2020. The bubble size shows the number of reviewed articles retrieved from
each journal. According to this figure the five papers are reviewed from IEEE Transaction on medical imaging with
. : Page 8 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
the impact factor of 10.048. The highest impact factor journal included in this survey paper is Nature Biomedical
engineering which is 25.671.
Figure 8: Visual representation of selected research publications from top tier journals along their impact factor.
3. A Delineation of Medical Imaging Modalities
Thissectionisaddedforassistingcomputervisionpractitionerstoestablishthebasicdomainknowledgeofmedical
imaging modalities and their application in the diagnostic and treatment of various diseases. Medical images differ
from natural images as they have specialized acquisition techniques. Physical phenomena such as electromagnetic
radiation, sound, light, nuclear magnetic resonance, and radioactivity are used for generating medical images of
externalorinternalorgansofhumanbody.Theseimagingtechniquescanbeappliedasnon-invasivemethodstoview
inside the human body, without any surgical intervention. Because of their importance in medical diagnostic a lot
of advancement has taken place in image acquisition devices called image modalities. These image modalities play
an important role in patients follow-up, regarding the growth of the already diagnosed disease state or undergoing
a treatment procedure as 90% of the health data comprises of images. These imaging modalities are very crucial in
public health and preventive measures as they help in establishing the accurate diagnosis. These medical images can
capturedifferentbodyregionssuchaseyes,chest,brain,heart,arms,andlegs.Therearedifferentmodalitiesofmedical
images such as computed tomography (CT), ultrasound, X-ray radiography, MR imaging (MRI), Positron emission
tomographyâ€“computed tomography (PET-CT), pathology fundus images and Optical coherence tomography (OCT).
The images acquired from these modalities are shown in Figure 9. Details about these image modalities are given in
the subsequent section.
. : Page 9 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
(a)
 (b)
 (c)
 (d)
(e)
 (f)
 (g)
 (h)
Figure 9: A catalogue of medical imaging modalities that vision transformers employed for assisted diagnosis. (a)Chest
X-rays that are widely used for COVID-19 or pneumonia detection. (b)Brain MRI scans that are being used for diagnosis of
aneurysms and tumors. (c)Brain CT scans that are employed to locate injuries, tumors, or clots leading to stroke. (d)OCT
images that are playing an important role in diagnosis of retinal diseases such as age-related macular degeneration (AMD)
anddiabeticeyediseases. (e)FundusimagescapturedtherearofeyeandareusedfordetectionandgradingofHypertensive
Retinopathy (f)Liver Ultrasound (g)Whole Slide Images (WSIs) that are being widely used in computational pathology
(h)PET-CT scans that are responsible for detection and diagnosis of cancer, determining the spread or recurrence of
cancer or metastasis
3.1. X-ray Imaging
AccordingtoNationalInstituteofHealth(NIH),US[22],X-raysimagesarecapturednon-invasivelyusingradiation
that is part of the electromagnetic spectrum. X-rays are mostly captured for diagnosing bone fracture [23], but chest
x-rays are also used for detecting pneumonia [24]. X-rays are also used by mammograms for breast cancer detection
[25]. Other most familiar uses of X-rays are for breast tumors [26], enlarged heart [27], blocked blood vessels [28],
conditions affecting your lungs [29] , infections [30], osteoporosis [31], arthritis [32], tooth decay [33].
3.2. Computed Tomography (CT) Scans
National Institute of Health (NIH), US [22] described computed tomography (CT) scan is a computerized x-ray
imagingtechniqueinwhichanarrowbeamofradiationisfocusedandthenquicklyrotatedaroundthebodytocapture
the detailed internal images, called tomographic images, of the bodyâ€™s slice non-invasively. CT Scan produces 2-
dimesionalaswell3-dimensionalimagesofsliceofthebody.Onceseveralimagesaretakentheseimagesaredigitally
stacked together to form three-dimensional images. CT Scans are used for identifying the various organs/slices of
the body for example CT scan of the heart is used for detecting various types of heart disease or abnormalities [34].
CT Scans of the head, to locate injuries [35], tumors [36], clots leading to stroke, hemorrhage, and other conditions
[37].CTScansofthelungsisusedfordetectingcancer[38],tumorsexcessfluid,pulmonaryembolisms(bloodclots)
[39],lung infections [40] and emphysema or pneumonia [41].
3.3. Optical Coherence Tomography (OCT) & Fundus Images
AccordingtoAmericanAcademyofOphthalmology(AOA)[42],Opticalcoherencetomography(OCT)captures
invasivecross-sectionimagesoftheretinausinglightwaves.OCTcanbeusedtoexaminetheretinaâ€™sdistinctivelayers
whichhelpinmappingandmeasuringtheirthicknessandplayanimportantroleindiagnosisofretinaldiseasessuchas
. : Page 10 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
age-relatedmaculardegeneration(AMD)[43]anddiabeticeyedisease[44].OCTcanbe,further,helpfulindiagnosis
ofglaucoma[45],macularpucker[46],macularedema[47],centralserousretinopathy[48],diabeticretinopathy[49],
macular hole [50].
Another type of images, discussed by [51], that can be helpful in the diagnosis of age-related macular degeneration
(AMD) [52] are fundus images which capture the rear of the eye. It is 2D imaging modality and since glaucoma is a
â€œ3D diseaseâ€, 3D image modality such as OCT is considered more efficient for diagnosis. Fundus images can also be
used for detection and grading of hypertensive retinopathy [53]
3.4. Magnetic Resonance Imaging (MRI)
MagneticResonanceImaging(MRI)modality,describedbytheNationalInstituteofHealth(NIH),US[22],capture
3d anatomical images noninvasively. MRI scanning does not use any radiation which make it an ultimate choice of
capturingwhenfrequentimagingisrequiredinthetreatmentprocessespeciallyinthebrain.MRIisparticularlysuitable
forcapturingthesofttissuesofthebody,butitismorecostlyascomparedtox-raysandCTscanning.MRIcanbeused
tocapturedifferentpartsofthebodyforexampleMRIareusedfordiagnosisofaneurysmsandtumors[54]aswellfor
differentiating between white matter and grey, in brain. MRI can further be used for spinal cord [55] and nerves[56],
muscles [57], and ligaments [58].
ThereisaspecializedMRIcalledfunctionalMagneticResonanceImaging(fMRI),whichisusedforobservingbrain
structure and locating the areas of the brain which are activated during cognitive tasks.[59]
3.5. Ultrasound
Radiologyinfo.org for patients [60] described ultrasound as an imaging modality that invasively create image of
organs, tissues, and other structures inside the body invasively by using sound waves without using any radiation.
Ultrasound can be used to internal organs within the body, noninvasively. For example, capturing the heart, eyes,
brain,thyroid,bloodvessels,breast,abdominalorgans,skin,andmuscles.Ultrasoundimagesarecapturedin2D,3D,
but it can also capture 4D images which is 3D in motion such as a heart beating [61] or blood flowing through blood
vessels [62].
3.6. Histopathology or Whole-Slide Imaging (WSI)
TheWhole-SlideImaging(WSI)referstocapturingthemicroscopictissuespecimensfromaglassslideofbiopsy
orsurgicalspecimenwhichresultsinhigh-resolutiondigitizedimages.Theseimagesarecapturedthrough,firsttaking
smallhigh-resolutionimagetilesorstripsandthenmontagingthemtocreateafullimageofahistologicalsection[63].
Specimens on glass slides transformed into high-resolution digital files can be efficiently stored, accessed, analyzed,
and shared with scientists from across the web using slide management technologies. Moreover, WSI is changing the
workflows of many laboratories. It is used in various disease diagnostics, prognostic and treatments such as survival
prediction [64],detection of tissue phenotypes [65], Automated grade classification [66, 67, 68], segmentation of
microvessels and nerves [69, 70], Multi-Organ Nuclei Segmentation [71].
3.7. Positron Emission Tomography â€“ Computed Tomography (PET-CT) Scans
According to Radiologyinfo.org [60], Positron Emission Tomography, also called PET imaging or a PET scan,
small amounts of radioactive material called radiotracers for capturing images. PET-CT scans can be used for cancer
detectionanddiagnosis[72],determiningspreadofthecancer,determiningtherecurrenceofcancer,metastasis[73],
evaluating brain abnormalities like tumor [36] and memory disorder [74], mapping normal human brain and heart
function.
4. Deep Neural Networks - Enhancing Representation Learning from CNNs to Vision
Transformers
The goal of this section is to bridge the gap between AI and healthcare analysts. It introduces the deep learning
concepts, techniques, and architectures that is found in the papers surveyed for this review article.
The progression of deep neural networks in computer vision has contributed to various fields of study, and
it primarily revolves around convolutional neural networks (CNN). For instance, while assessing medical images,
practitioners can recognize if there is an anomaly. Similarly, this mechanism can be taught to a computer via CNNs
to diagnose a disease or an anomaly while taking images as input, hence, giving vision to a computer.The model of a
. : Page 11 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
basic CNNis illustrated inFigure 10. Ittakes theimage input asa matrix ofpixel values, assignsweights to learnthe
variousdifferentiablefeatures[15].Itthenpassestheimagethroughmultiplelayersandusesmultiplefilterstocapture
thediscriminatedfeaturesfromtheimage.CNNsgenerallyconsistofthreekindsoflayers:convolutionlayers,pooling
layers,andfull-connectedlayers[75].ConvolutionlayersareresponsibleforlearningfeaturesandcapturingtheSpatial
andTemporaldependenciesbetweenthefeaturesbyapplicationofrelevantfilters.Thepoolinglayerisresponsiblefor
reducingthesizeoffeaturemapstocapturemoresemanticinformationthanspatialinformation.Inconvolutionallayer
filtersofsizeNxNwhereNisequalto1,3,5,7,oranyotheroddnumber.Thepoolinglayerusesawindowofsize2x2,
3x3,oranyotherdesiredsizetotakeaverageormaximumvalueinthatwindow.Beforethefullyconnectedlayer,the
output of the convolutional and pooling layer which is called feature map is flattened to make a fully connected layer
at a function such as softmax is applied to make a prediction and a loss function is used to calculate the error and the
is back propagated to update the values of learnable parameters.
Figure 10: A general framework of Convolutional Neural Networks (CNNs)
Depending on the application for example image classification, fully connected layers are added at the end of
the network. Stacking these layers on top of each other with a specific arrangement with the help of a differentiable
function is known as CNN architecture. In recent years several CNN architectures are developed with various such
arrangements:AlexNet[76],VGGNet[77],GoogleNet[78],ResNet[79],ResNeXt[80],SqueezeandExcitationNet
[81], DenseNet [82], and EfficientNet [83].
Convolutionalneuralnetworksareusedinvariousapplicationsinthecategoriesofimageclassification,detection,
and segmentation, etc. For example face detection [84], identification of emotions [85], Speech recognition, and
MachinetranslationusingCNNs[86],etc.ConsideringtheapplicationsofCNN,itcanbeinferredthattheycanenable
commendableresults[87].Theyareknowntobeablackbox,asthetrainingisaccordingtothetaskanddomain.One
majorlimitationistheunclarityofresultsi.e.thereasonforaparticularoutcome.Especiallyinthemedicaldomain,it
is imperative to know the cause of a specific outcome, otherwise, a wrong diagnosis can be a threat to human lives.
Onewaytotacklethisproblemhead-onistohavesuchamodelthatfocusesonrelevantpartsoftheimageandcan
be visualized by the doctors. To elucidate this issue, Attention models were proposed [16, 69]. The attention model
focusesonthepartsthatarerelevanttotheinputsequence.Moreover,amodelwasproposedknownasTransformers,
itusedtheconceptofAttentiontoenhancethetrainingspeed[19].Transformersconsistofmultipleblocksofidentical
encoders and decoders, which were composed of self-attention block and feed-forward networks. In addition, the
decoderconsistsofanextraattentionblock,whichfocusesontherelevantpartofthesequence.Theembeddedwords
oftheinputwerepassedtotheencodersequentiallyandwerepropagatedtoalltheencoders.Theoutcomesofthelast
encoderwerethenfedtothedecoders.Theperformanceofthetransfermodelswasstate-of-the-artinthetasksrelated
to natural language processing.
Inspiredbythetransformermodel,Dosovitskiyetal[19],appliedittotheimagesanditcanbeusedtoreplaceCNNs
. This model was called Vision Transformers (ViT) and its structure is illustrated in Figure 11. ViT model introduced
global attention, but not on the entire image, rather they divided the entire image into small image patches of 16x16.
They introduced simple numbers 1, 2, up to n as positional embeddings for specifying the positions of the patches.
. : Page 12 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
A lookup table was introduced which contained a learnable vector against each number representing the position of
the patch. These embeddings were passed to the network along with the patch. Each patch was unrolled to a linear
vector and projected linearly on embedding matrix. The final embedding along with positional embedding was then
fed into the transformer encoder. Along with embedding for patches, an extra embedding with number 0 is also fed
intothenetworkanditsoutputwasobtained.Thus,Visiontransformershavethecapabilityofmodelingglobalcontext
which assists in more accurate results. Lastly, in this review, medical images are considered as the input for vision
transformers.
Figure 11: Structural representation of Vision Transformers
4.1. Open Access Medical Imaging Datasets employed in ViT Applications
In table 3, we have summarized and structured open-accessed datasets in tabular form. The table includes
information regarding the tasks i.e. classification, segmentation, detection, report generations, and miscellaneous.
Furthermore, the respective image modalities and their applications are also included. Next, we have also compiled
thedescriptionandlinkstothecorrespondingdatasets.Hence,itwillassistresearcherstoidentifytheseresourcesthat
can be utilized against different applications.
. : Page 13 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Table 3: Publicly available datasets of medical imaging modalities used by researchers for assisted diagnosis
Tasks Modality Applications Datasets DescriptionDownload
Links
Classification CT ScansPulmonary Nod-
ule Characteriza-
tionLUNA16 [88]A commonly used dataset
for lung nodule identifica-
tion and false positive re-
duction.Download
LIDC-IDRI [89]One of the largest publicly
available lung cancer
screening datasets,
made up of diagnostic
and screening thoracic
computed tomography
(CT) images.Download
Emphysema
ClassificationComputed
Tomography
Emphysema
Database [90]A freely available dataset
that includes 115 high-
resolution CT (HRCT)
scans and 168 square
patchesthatweremanually
annotatedinasubsetofthe
slices.Download
COVID-CT [91]A freely accessible collec-
tionof CT-scanimagesex-
tracted from a number of
scholarly articles.Download
COVID-19
DetectionSars-CoV-2 [92]A multi-class CT scan
dataset for identification of
SARS-CoV-2 infection.Download
COVD19-CT-DB
[93]COVID19-CT-Database
consists of chest CT scans
that are annotated for the
existence of COVID-19.Download
COVID-CTset
[94]One of the largest open ac-
cess COVID-19 lung CT
datasetthatisavailableon-
line.Download
X-raysPneumonia Clas-
sificationChest X-ray Im-
ages [95]A Dataset of validated
OCT and Chest X-Ray
images.Download
. : Page 14 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Tuberculosis
Prognosis and
DetectionMontgomery
County (MC)
CXRSmall tuberculosis dataset
from USA.Download
Tuberculosis
Prognosis and
DetectionShenzhen datasetSmall tuberculosis dataset
from Shenzhen (China).Download
COVID Chest X-
ray datasetAn open access dataset of
chest X-ray and CT im-
ages of patients which are
positive or suspected of
COVID-19 or other viral
and bacterial Pneumonias.Download
ClassificationBIMCV COVID-
19+ [96]BIMCV-COVID19+
dataset is a large dataset
with chest X-ray images
CXR (CR, DX) and
computed tomography
(CT) imaging of COVID-
19patientsalongwiththeir
radiographic findings.Download
X-raysInterpretable
COVID-19
Detection
& Severity
QuantificationCOVID-19
Posterior-
Anterior Chest
Radiography
Images [97]This is a curated COVID-
19 Chest X-ray image
dataset that was created
by combining 15 publicly
accessible datasets.Download
Extensive
COVID-19
X-Ray and CT
Chest Images
[98]In this COVID-19 dataset,
both Non-COVID and
COVID cases are included
of both X-ray and CT
images.Download
COVIDx Dataset
[99]A database of chest X-ray
imagesforCOVID-19pos-
itive cases along with Nor-
mal and Viral Pneumonia
images.Download
MRI ScansMulti-Modal
Medical Image
ClassificationMRNet Dataset
[92]The MRNet dataset con-
sistsof1,370kneeMRIex-
ams performed at Stanford
UniversityMedicalCenter.Download
. : Page 15 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Fundus Im-
agesRetinal Image
Synthesis
and Disease
PredictionColorFundusIm-
ages [100]A database that has both
FFA image and color fun-
dus image in this DR grad-
ing system.Download
Histopatho-
logy ImagesColorectal
Histopathology
Image
ClassificationColorectal
Cancer Histology
Dataset [101]This data set represents a
collection of textures in
histological images of hu-
man colorectal cancer.Download
DetectionX-raysCOVID-19 Diag-
nosisCOVIDx [99]A database of chest X-ray
imagesforCOVID-19pos-
itive cases along with Nor-
mal and Viral Pneumonia
images.Download
COVIDGR-E
[102]It is built by adding 426
pneumonia images from
the ChestX-ray8 database
to the COVIDGR-1.0
datasetDownload
Fundus Im-
agesMicroaneurysms
DetectionIDRiD [103]This dataset consists of 81
images.Download
CT ScansCOV19-CT-DB
[93]COVID19-CT-Database
consists of chest CT scans
that are annotated for the
existence of COVID-19.Download
COVID-19 Diag-
nosisCOVIDx-CT-2A
[104]A benchmark dataset:
the largest comprising a
multinational cohort of
4,501patientsfromatleast
15 countries and contains
three classes â€“ COVID-19
Pneumonia, non-COVID-
19 Pneumonia, and
normal.Download
Histopathol-
ogy ImagesCancer DetectionThe Cancer
Genome Atlas
[105]The compendium includes
heat maps for 33 differ-
ent tumor types and three
platforms:geneexpression,
reverse-phase protein ar-
rays (RPPA), and miRNA
expression.Download
Segmentation iSeg-2017 [106]this datasets contains brain
MRIâ€™s of 39 subjectsDownload
. : Page 16 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
MRI ScansBrainTissueSeg-
mentationBraTS-2020
[107]It contains 1756 MRI of
439 subjectsDownload
MRBrainS [108]It contains 20 Fully an-
notated multi sequence 3T
MRI BrainsDownload
UKBB [109] Download
Cardiac Segmen-
tationM&MS-2It contains 360 subjects
having 200 training and
160 testing imagesDownload
MRI Scans ERI [110]It contains LGE Data of
28 patients. the number of
segmented images are 358Download
Abdominal
SegmentationCHAOS [111]This Data set contains im-
ages for T2 Segmentation
of Liver and KidneysDownload
SegmentationX-raysTooth Root Seg-
mentationDRIVE [112]It include 40 color fundus
retinal images that are ran-
domly selectedDownload
Knee Segmenta-
tionOAI212Kneeimageswereseg-
mented randomly and the
training and testing split
was 100 and 112Download
Lung Segmenta-
tionJSRT [113]The database includes 154
conventional chest radio-
graphs with a lung nodule
(100 malignant and 54 be-
nign nodules)Download
CT ScansPediatric
SegmentationKiTS19 [114]ThisDatasetcontainsrenal
Tumor Segmentation Im-
ages of 210 AdultsDownload
Drusen Segmen-
tation from Reti-
nal OCT ImagesUSCD [115]This dataset contains 8616
retinal OCT B-scansDownload
. : Page 17 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
IU Chest X-ray
Collection [116]A public and open
source OpenI or Indiana
University Chest X-rays
database which contains
3955 medical reports with
7470 frontal and lateral
chest x-ray images.Download
Clinical
Report
GenerationX-raysMIMIC-CXR
[117]It is the recently released
largest dataset, consists of
377,110 chest X-rays im-
ages and 227,835 reports
from 64,588 patients.Download
PEIR GROSS
[118]It consists of publicly ac-
cessible 7442 teaching im-
ages, spread across 21 pre-
defined subcategories. The
vocabulary size of the to-
talimagecaptionsis4,452.
Each image on average
containsa12wordcaption.Download
MiscellaneousNIH-AAPM-
Mayo Clinic
LDCT Grand
Challenge [119]A public and open source
30 contrast-enhanced ab-
dominal CT patient scans.Download
PET-CT
ScansMedical Image
Enhancement
Kirby21 Dataset
(KKI01 to
KKI05) [120]A public and open source
dataset containing correla-
tion data for 20 subjects
from Kennedy KriegerDownload
MRI ImagesMedical Image
ReconstructionDIVerse 2K reso-
lution high qual-
ity (DIV2K) im-
agesdataset[121]It contains a total of 1000
2K resolution RGB im-
ages.Download
fastMRI Scans
[122]T1- and T2-weighted
images from 150 subjects
were analyzed (100 for
training, 10 for validation,
40 for testing).Download
. : Page 18 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Fluorescence
MicroscopyDenoising
of Celullar
Microscopy
Images for
Assisted
Augmented
MicroscopyFlywing, Planaria
and Tribolium
datasets [123]The training data 17,005
and 14,725 small cropped
patches of size 64 Ã—64Ã—16
for Planaria and Tribolium
datasets, while the testing
data are 20 testing images
ofsize1024 Ã—1024Ã—95and
6testingimagesofaverage
size around 700 Ã—700Ã—45
forthetwodatasets,respec-
tivelyDownload
5. Visual Recognition Tasks in Medical Images
Artificial intelligence and deep learning have played a vital role in assisting clinicians for better diagnosis. In
this context, the application of CNNs and ViTs on medical images assists the healthcare professional in disease
classification, lesion detection, segmenting the anatomical structures, automated report generation, denoising the
images,medicalimageregistration,andvariousothertasks.Thissectiongivesabriefoverviewoftheabove-mentioned
visual recognition tasks performed by the application of CNNs and ViTs on medical images.
5.1. Medical Image Classification
The practitioners give their diagnosis by analyzing the medical images, hence, determining the presence and type
of the disease. This conventional diagnosing way can be assisted with deep learning techniques. Figure 12 shows a
generalized classification network. Through these techniques, the ambiguity in diagnosis among different doctors can
bemitigatedandtheoutcomeswillbemoreaccurate.Thus,resultsachievedthroughCNNsarenotonlytimeefficient
butcanalsoassisthealthcareprofessionals.UsingCNNmodels,aninputimageisfedintothenetwork,whichisthen
assessed,analyzed,andinterpretedtodeterminethetargetandobjectofdifferentmodes[124].Thisprocessisknown
asimageclassification.Forexample,consideringthefigure5,thechestX-rayimagesaregivenasinput,andthemodel
classifies them into normal and pneumonia images. At present, image classification has various applications in the
medical domain such as; skin cancer [125], diabetic retinopathy [126], tuberculosis [127], etc.
ThesignificanceofimageclassificationusingCNNscanbedeterminedbytheaforementionedapplications.These
applications were achieved through various CNN architectures such as AlexNet [76], VGGNet [77], GoogleNet [78],
ResNet[79].Later,moreresource-efficientarchitectureswereproposedi.e.MobileNet[128],SqueezeandExcitation
Net[81],andEfficientNet[83],etc.ThroughtheseConvolutionalneuralnetworks,commendableresultswereachieved
in medical applications, however, in terms of resources, improvements are still required.
. : Page 19 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 12: Medical image classification pipeline in which Chest X-rays (CXRs) are fed into a Deep CNN architecture, which
assigns each image a class, such as pneumonia patient or healthy patient.
5.2. Lesions Detection
Classifyingimageswasindeedastepforwardtowardsautomateddiagnosis.Nevertheless,locatingtheobjectplays
animportantrolewhiledevelopingamorefunctioningapplication.Inthefieldofcomputervision,oneoftheunderlying
goals is to classify the object into a category and determine the location of the object in a given image, this technique
isknownasobjectdetection.Figure13figurativelyexplainstheageneraldetectionnetwork.Forinstance,iftheinput
image isan X-rayof ahand, the objectdetection modelwill not onlyclassify thecategory i.e. fracturedbone butalso
localize it by using bounding boxes. Considering a situation, where bones were fractured on multiple locations, here,
object detection will be a better technique to opt for as it will be more helpful to the diagnostician. The applications
of object detection involve; face detection [84], plant disease identification [129],weapon detection [130] , emotion
detection [85], etc.
Figure 13: A CNN architecture detecting a colony of tumorous cells given a histopathology image.
The task of object detection is composed of two types; two-stage networks and one-stage networks [124]. The
two-stage networks are based on region proposal algorithms such as R-CNN [131], Fast R-CNN [132] , and Faster-
RCNN [133]. The other technique is designed in a way that it works directly on images, examples include; YOLO
[134], SSD [135], etc. The two design types have a trade-off between accuracy and time efficiency. The two-stage
networks are capable of more accuracy, whilst, one-stage networks have more speed. Thus, it depends on the task at
hand and dataset, while choosing these networks for object detection. The limited datasets in the medical domain are
. : Page 20 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
the biggest constraint while training the models, therefore, researchers are working on models which can work with
limited datasets.
5.3. Anatomical Structure Segmentation
In the medical domain, there are numerous cases where it is difficult to distinguish between two different lesions
as there are minor differences. Since lesions can have different treatment strategies, determining them as separates is
vital. It is indeed a challenge to recognize such subtle differences, but it is not impossible. If the images are classified
pixel-wise, they can be localized as well, and result in a fine outline of the object. Such a technique is known as
Imagesegmentation.Forexample,ifanMRIimageisfedintothemodel,theoutcomewillnotjustclassifythetumor
type, its anatomical structure will also be highlighted as seen in Figure 14. There are various applications of image
segmentation, which includes ; Cardiovascular structure [136] , prostate cancer [137], blood vessel [138] etc.
Figure 14: A segmentation framework in which brain MRI scans are fed into a deep CNN architecture which is not just
classifying and locating the tumorous region but also highlighting the anatomical structure.
Fine-grainedsegmentationisadecisivestepinimage-guidedtreatmentandcomputer-aideddiagnosis.Thewidely
used architectures for image segmentation are; U-Net [139], DeepLab [140], Masked R-CNN [141] etc. The usage of
thesesegmentationmodelsdependsontheproblemthatisbeingsolved.Forinstance,multi-scaleobjectsintheimage,
deeplab,anditsvariousstructureswillbeawisechoice.Lastly,theconcernregardingimagesegmentationisthelack
of labeled data due to which researchers are considering more unsupervised approaches, however, it is still a work in
progress [142].
5.4. Clinical Report Generation
In the healthcare domain, while examining radiology images i.e. Chest X-rays, CT Scans, MRI, etc, the doctors
have to write detailed reports of the assessment. This conventional method of report writing is tedious, monotonous,
time-consuming,anderror-proneforradiologists[143].Immenseprogresshasbeenmadeindeeplearningtogenerate
medical reports automatically. Automatic report generation can assist clinical practitioners in quick and accurate
decision-making [144]. For example; if the CT scan of a brain is given as input, the output would be a complete
report such as, if the tumour exists, the location of the tumour, its size and other details. Figure 15 shows a general
workflow of clinical report generation. Medical report generation is an application of image caption in which these
models are applied to medical data. Image captioning refers to computers generating captions by giving images as
input.Itconsistsofmainlyanencoder-decoderwhereCNNisusedtoextractfeatures,andLSTMorRNNareusedto
generate the captions [145]. The initial work on its architectures include Show and Tell [146], Show Attend and Tell
[147],NeuralTalk[148],etc.SomeoftheStateoftheArtincludeDenseCap[149],SemStyle[150],Dense-CaptionNet
[151], etc. Lastly, as in supervised learning, a large amount of annotated data is required for these models to perform
well, in the future unsupervised learning could be a more powerful way to proceed with them [152].
. : Page 21 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 15: A visual representation of clinical report generation mechanism in which different types of image modalities
are independently diagnosed by Radiologist and AI model, and then compared to each other to provide a precise medical
report.
6. A Recapitulation on Vision Transformer Application to Medical Image Analytics for
Assisted Diagnosis
6.1. ViT in Image-based Disease Classification
Deep learning has recently come to the power in a variety of research domains. Convolutional Neural Networks
(CNNs)havebeenthemostdominantdeepneuralnetworksforautonomousmedicalimageanalysisapplicationssuch
as image classification during the last decade. These models, however, have shown poor performance in learning the
long-range information, due to their localized receptive field, which limits their capabilities for vision related tasks.
Transformer architecture, proposed by Vaswani et al. [16], is currently the most popular model in the field of natural
language processing (NLP). Getting inspiration from the success of self-attention based deep neural architectures,
Dosovitskiy et al. [19] introduced Vision Transformer (ViT) model for image classification based applications. In
thesemodels,theoveralltrainingprocessispredicatedondividingtheinputimageintopatchesandconsideringeach
embedded patch as a word in NLP. Self-attention modules are used in these models to learn the relationship between
the embedded patches.
In the following section, we will take a step forward in exploiting the potential applications of self-attention based
architectureslikeVisionTransformers(ViT)forthetaskofmedicalimageclassificationsuchasCOVID-19detection
and severity quantification, Emphysema classification, tuberculosis prognosis etc. The section is further divided into
medical imaging modalities, with a focus on contributions made by vision transformers to the respective medical
applications. The section ends with a tabular summary of the proposed approaches and their performance matrices.
6.1.1. Computed Tomography (CT) Scans:
PulmonaryNoduleCharacterization: Lungcancerisoneofthemostfrequentlyreportedcausesofcancer-related
morbiditiesandmortalities[153].Earlydetectionandtreatmentofmultiplepulmonarynoduleshasbecomeachallenge
in clinical practice as it is one of the most efficacious ways to reduce the number of fatalities associated with the
condition. Prior research [154, 155, 156] on detection and characterization of lung nodules focused on learning the
. : Page 22 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
associations between various nodules. In other words, they particularly use solitary nodule approaches on several
nodular patients, ignoring the relational/contextual information. To overcome this issue, Yang et al. [157] proposed a
MultipleInstanceLearning(MIL)strategy,whichempiricallyprovedtheutilityoflearningtherelationshipsbetween
nodules. Itâ€™s the first time researchers have looked into the relationships between several lung nodules and extracted
critical relational information between solitary-nodule voxels. Instead of using typical pooling-based aggregation in
multiple instance learning, they created Set Attention Transformers (SATs) based on self-attention to understand the
relationships between nodules. A 3D DenseNet is employed as the backbone to learn representations of voxels at the
solitary-nodule level. The SATs are then used to determine how several nodules from the same patient are related.
This data-driven methodology might aid in understanding of etiologic and biologic processes as well as metastasis
diagnosis of multiple pulmonary nodules and motivate clinical and biological research on this important topic.
Emphysema Classification: Chronic obstructive pulmonary disease (COPD) is a heterogeneous disorder with a
varietyofcharacteristics,includingsmallandlargerespiratoryinflammation,aswellasEmphysema,whichisthemost
common causeof progressive lungtissue loss. Emphysema,as characterized by thedestruction and persistentgrowth
ofthe alveoli,canbe classifiedautomaticallywhich canaidin determiningandquantifying lungdestructionpatterns.
In this regard, Convolutional Neural Networks (CNNs) serve an essential role, particularly in pulmonary CT image
classification, but transformers have yet to be explored. As a result, Wu et al. [158] conducted a thorough assessment
and extensive evaluation of the ViT model for Emphysema classification. First, large image patches (16 x 16) are
cropped from CT scans. After resizing, the patches are flattened and linearly embedded to create a sequence of patch
embeddings. The positional information is kept by concatenating the class embeddings with the patch embeddings.
To acquire the representation, the final embedding sequence is passed into the transformer encoder module. Finally,
the learnable class embedding is fed to a softmax layer for Emphysema classification. Despite the fact that this study
employed a vision transformer model to classify emphysema, unlike other techniques that use CNN models, it still
hasseverallimitations.Forexample,patch-basedclassificationmaynotbeasconvenientaspixel-basedsegmentation.
Furthermore, the architecture just uses the transformer encoder block, and the CNNâ€™s benefit is not utilized. In short,
Emphysema quantification is difficult, and classification is merely the first step. Proposing more efficient networks
capableoflearningsemanticinformationofEmphysemabypartialoraccurateannotationsmaybeapressingneed.In
the near future, more research on the segmentation and quantification of Emphysema will be conducted.
COVID-19 Detection: The infectious Coronavirus (COVID-19) and lung disorders have been at the vanguard of
the research community as the pandemic has caused significant public health concerns throughout the world. Using
computer vision methods, several attempts [159, 160, 161] are being undertaken to create automated systems for
faster and more effective diagnosis of COVID-19. As per several research studies [162, 163], some radiographic
manifestations, such as broncho vascular thickening, Ground Glass Opacities (GGO), crazy-paving pattern, and
consolidation, have been found in chest CT images. However, with the rapidly growing number of patients in the
current situation, radiologists have a significant challenge in manually interpreting CT scans.
Ambita et al. [164] were the first to use a vision transformer to the task of COVID-19 detection from computed
tomography (CT) scans. They implement a variety of vision transformers (e.g., ViT-B 16, ViT-B 32, ViT-L 16, ViT-
L 32, and ViT-H 14) for image classification. They employed ResNet-based Self-Attention Generative Adversarial
Network (SAGAN-ResNet) as a data augmentation approach for synthetic image generation to alleviate the problem
ofinsufficientdata.Furthermore,theydemonstratedhowViToffersvisualizationsfortheimagesbyexhibitingwhich
sectionsoftheinputimagethemodelfocusesitsattentiononinthevariouslayers.Thismightbeusefulforradiologists
whenanalyzingCTscans.Improvementsmayalsobemadebyevaluatingtheproposedapproachondifferentdatasets
and customizing the architecture of transformers or GAN.
Zhang et al. [165] attempted to broaden the scope of vision transformers such that they may be used as a robust
feature learner for COVID-19 diagnosis in 3D CTs. Inspired by the success of Swin vision transformer and CT
classification work in [166, 167], their framework is made of two key stages: lung segmentation followed by image
classificationusingSwintransformerasabackbone.Inthefirststage,apre-trainedUnetisusedforlungsegmentation
in CT scans and produced a lung mask that restricted learning to certain lung regions. The features from each 2D
CT slice are then extracted using a Swin vision transformer, which are subsequently aggregated into 3D volume level
features using a max-pooling layer. However, itâ€™s worth noting that the framework equipped with the backbone of
EfficientNetV2-M achieves a good speed-accuracy tradeoff according to the results on the validation dataset. This
implies that in future study, merely increasing the model size might result in an improvement in classification.
. : Page 23 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Abovereviewedstudiesareevidentthatvisiontransformers(ViT)isanovelandquicklyevolvingapproachthathas
demonstratedexcellentresultsusingCOVID-19datasets.Thanetal.[168]conductedapreliminarystudytoinvestigate
the efficacy of using ViT with different sized patches on CT scans of diseased lungs, COVID-19 infected lungs, and
normal lungs. A default positional embedding is used and the combination is then passed to a transformer encoder
which is composed of alternating layers of multi-headed self-attention and multi-layer perceptron (MLP) units. The
transformerencoderâ€™soutputissentintoanMLPhead,whichoutputsthepredictedclass.Theproposedmethodology
issimplerandlessdemandingoncomputationalresourcesascomparedtoCNNs,however,pretrainingthetransformer
andaddingaconvolutionallayerinfrontofit(i.e.Convolutionalvisiontransformers)mayincreaseperformance.Aside
fromthat,otherhyper-parameterscanbemodifiedtoincreaseperformance,andexplainableartificialintelligence(XAI)
will be used in the future to explain how deep learning networks like ViT make decisions.
COVID-19 CT scans contain not only the local features, such as local crazy-paving and local hemangiectasis,
but also have global characteristics. Since it is characterized by the combination of both local and global features,
extracting image features with relatively complex features is a challenging classification problem of such medical
imagingmodalities.Fanetal.[169]proposedaparallelbi-branchnetwork(TransCNNNet)thatisessentiallybasedon
theTransformerandConvolutionalNeuralNetwork.Unliketheconventionalapproaches,thesizeoftheconvolutional
filter kernel is not changed to extract features at different scales; instead, they use the global receptive field of the
transformer network. A bi-directional feature fusion structure is then designed, which fuses the extracted features
fromtwobranchesbi-directionally,forminganetworkthathasthepotentialtoextractmorecomprehensiveandample
features.
6.1.2. X-ray or Radiographic Images:
Pneumonia Classification: Pneumonia is an infectious disease in which the alveoli in the lungs is to be filled with
fluid or pus, making it painful to breath as well as decreasing oxygen intake. A detailed inspection of chest X-ray
imagesbytheradiographerorradiotherapistisrequiredtodiagnosePneumonia.Asaresult,pneumoniadetectionisa
time-consuming process, and even a slight error can result in an excruciatingly painful outcome. Several researchers
haveexploredvariouscomputervisionapproachestodiagnosePneumonia,usingX-Rayimagesofhumanchests.Tuagi
etal[170],haveproposedavisiontransformer(ViT)modelfortheclassificationofpneumonia.Further,tovalidatethe
performanceoftheproposedmodel,itwasalsocomparedwiththeCNNmodelandVGG16.ItwasobservedthatViT
outperformedothertechniquesreachingthehighestaccuracyof0.96.Inaddition,itoutperformedothermodelsinterms
of computational cost as well. It is observed that the model still requires further experimentations to investigate the
robustnessofthemodelwithmoreheterogeneousdata.Therehasnâ€™tbeenalotofworkdoneusingvisiontransformers
in the field of chest x-ray diagnosis. It may be useful in the diagnosis of other disorders, such as Covid-19 detection,
Cystic Fibrosis or Emphysema, Edema, Pleural thickening, Effusion, and even Cancer.
Tuberculosis Prognosis and Detection: Tuberculosis refers to an infection that affects the lungs and can travel
to other parts of the body. It can be diagnosed and assessed by referring to chest x-rays. If the infection is cured at
an earlier stage, it can save a person from further misery of painful treatment. For the classification of infected and
non-infected chest x-rays, a methodology was introduced by Duong et al. [171]. The authors have used EfficientNet
withvisiontransformersforthedetectionoftuberculosisusingchestX-raysimages.Theperformancemetricsinclude;
accuracy,precisionandrecall,f1-score,andareaunderthecurve.Thehighestaccuracyachievedwas97percentwith
the backbone of efficient net B1. Hence, the paper validates the robustness of vision transformer models used on a
heterogeneous dataset. However, it should be further evaluated on different baselines models.
Interpretable COVID-19 Detection & Severity Quantification: The novel coronavirus disease 2019 (COVID-
19), caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has become one of the deadliest
virusesofthecenturyasofApril2021,infectingover137millionpeoplewithover2.9milliondeathworldwide.Inthe
contextofunprecedentedCOVID-19pandemic,publichealthsystemshavebeenhitbyaslewofchallenges,including
scarcity of medical resources, that are pushing healthcare workers to face the threat of infection. Deep learning and
Computer Vision are commonly employed in numerous fields of medical imaging for the diagnosis of COVID-19
from radio-graphic images, X-rays or CT scans. Even though these techniques have yielded commendable outcomes,
thecostisalwaysconsideredasanimportantfactorforthesystemtobeapplicable.Theuseofacomputedtomography
(CT) scan for COVID-19 diagnosis offers great sensitivity and specificity [172], but it is a severe burden due to its
high cost and risk for cross-contamination in the radiology suite. In comparison of CT Imaging, X-rays have been
. : Page 24 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
widelyutilizedforCOVID-19screening,astheyrequirelessimagingtime,arelessexpensive,andX-rayscannersare
generally available even in remote regions [173].
Hence, many researchers have worked on diagnosing COVID-19 using Chest X-ray (CXR) images and through
theseautomatedmethods,counter-checkthePCRtestresults.Forinstance,inthestudypresentedbyParketal.[174],
vision transformers were utilized for both classification and quantification of the severity of COVID-19. Firstly, the
low-level features were extracted using state-of-the-art CNN architectures. Secondly, these extracted features were
given to the transformer model for classification and severity measurement. Thirdly, the severity was demonstrated
through heat maps which gives interpretability to the generated results. The robustness of the model was illustrated
throughvariousexperimentationsondifferentbaselinemodelsandtestedonexternaldatasets.Themodelwasevaluated
onmetricsincluding;AUC,sensitivity,specificity,andaccuracy.Thehighestaccuracyachievedontheexternaldataset
was above 84.8 percent.
Similarly, the study presented by Shome et al. [175] have proposed a pipeline based on Vision Transformers for
the classification of COVID-19. The model was also compared with other baseline models to validate the robustness
ofvisiontransformers.Ascomparedto[174],itwastrainedonalargerandmoreheterogeneousdatasetachievingthe
accuracy of 98 percent and AUC of 99 percent in the binary classification. Furthermore, for the multi-classification
which includes pneumonia x-rays as well, the model achieves the accuracy and AUC of 92 percent and 98 percent,
respectively. In addition, the model uses Grad-CAM for the visualization through heatmaps, for the explainability of
theoutcome.Althoughthemodelperformedbetterthan[174],themodeldoesnotquantifytheseverityoftheinfection.
Further, this research could be expanded to predict the rate at which the infection can spread.
Since the medical data in most domains is inadequate, it can be difficult to build robust models. Considering this
issue, Rahhal et al. [176] have put forth a methodology that performs well with small training data. To diagnose
covid-19 in both CT scans and X-ray images, the proposed method uses vision transformers as a backbone with the
employment of a Siamese encoder. The input image with a corresponding augmented image was fed into the siamese
encoder,itwasthenconnectedtotwoclassifiers.Further,theoutputoftheseclassifierswasfedintoafusionlayerfor
theoutcome,followedbyheatmapsondifferentlayerstointerprettheresults.Moreover,thesiameseencodercatersto
theissueregardingdeficientdata.Theproposedmodelhasachievedanaccuracyof94.2percentwhichiscommendable
in limited training data.
Untilnow,onlyafewCAD-basedapproachesfordetectingCOVID-19havebeendeveloped,buttheireffectiveness
has been hampered due to a number of factors. Taking an inspiration from Convolutional Block Attention Module
(CBAM), Nawshad et al. [177] proposed an attention-based Convolutional Module using ResNet32 as the backbone
architecture with a 97.69% accuracy. They also conducted a comparative assessment of a variety of deep learning
models, including VGG, ResNet, and Xception, for the successful detection of COVID-19 and viral pneumonia.
Utilizing the attention module with various CNN-based architectures produced much better results than using the
base CNN architectures.
Since it is believed that the newly developing pathogen would have similar low-level CXR features with existing
diseases,theuniqueconceptofproducinghigher-leveldiagnosesbyaggregatinglow-levelfeaturecorpusmaybeused
to swiftly construct a robust algorithm against it.
6.1.3. Magnetic Resonance Imaging (MRI):
Multi-Modal Medical Image Classification: The inadequacy of medical data is discussed in the aforementioned
papers, and it can be inferred that researchers are trying to overcome the issue by modeling various architectures.
For instance, Dai et al. [178], put forth a hybrid transformer model for the classification of multi-modal images. The
pipeline consists of a CNN to extract low-level features, and then transformers are utilized for the global context. The
model was applied on two different datasets: for classification of parotid gland tumors and classification of a knee
injury.ThehighestaccuracyachievedbytheParotidglandtumordatasetandkneeinjurydatasetwas88.9percentand
94.9 percent, respectively. Nevertheless, the study has not presented any means for interpretability of the model, as it
is essential, especially in the medical field.
6.1.4. OCT or Fundus Images:
RetinalImageSynthesisandDiseasePrediction: Forthediagnosisofretinalabnormality,FluoresceinAngiogra-
phy(FA)canbeused.Tocapturethevascularstructureoftheretina,afluidisinjectedintothebloodstream.However,
several reactions have been reported with the usage of FA. Another approach to diagnosing abnormality is by using
Fundus images. The vascular structure of the eye is not captured in these images. In the paper [179], the authors
. : Page 25 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
used fundus images and synthesize them to produce FAâ€™s using a generative adversarial network(GAN). Next, these
imagesarethengiventoatransformermodeltoclassifyanormalandabnormalretina.Themodelachievedthehighest
accuracy, sensitivity, and specificity of 85.7, 83.3, and, 90.0, respectively. Moreover, this model needs to be validated
with a more heterogeneous dataset, along with applying this model for predicting other retinal diseases as well.
6.1.5. Histopathology Images:
Colorectal Histopathology Image Classification: Colorectal Cancer (CRC) is a type of cancer that begins in the
rectumorcolonandisdefinedbytheuncontrolledgrowthofaberrantcellswiththelatentabilitytoinvadeothertissues.
Despitethefactthatmanualinspectionofhistopathologyslidesisstillcrucialinexperimentalpractise,automaticimage
processing enables the quantitative and rapid analysis of malignant tissues. Early detection is crucial for identifying
theappropriatetreatmentapproachandincreasingthepatientâ€™schancesofsurvival.Asaresult,automatedtechniques
are needed to save time and eliminate the risk of human error. Artificial intelligence has recently been put to use
in the diagnosis and prediction of several forms of cancer. Zeid et al. [180] used Vision Transformers to perform
a multiclass tissue classification of colorectal cancer, highlighting the potential of employing Transformers in the
histopathological image domain. First of all, a standard vision transformer model was proposed and it achieved an
accuracy of 93.3 percent. Since vision transformers demand more data, a hybrid approach combining CNN and
transformer was developed. Low-level features are extracted using the CNN model, and the embeddings are sent to
the transformer. This model is known as a Compact Convolutional transformer, and it achieved an accuracy of 94
percent.However,experimentationwithvariousdatasetsanddifferentformsofcancermayalsobedonetoimprovethe
modelâ€™soverallperformance.Deeplearningalgorithmsarenowbecomingincreasinglyessentialfortheidentification
andclassificationofcolorectalhistopathologyimages.Existingtechniques,ontheotherhand,aremoreconcernedwith
end-to-end automatic classification using computers than with human-computer interaction. Hence, Chen et al. [181]
presented an IL-MCAM framework. It is based on interactive learning and attention techniques. Automatic learning
(AL) and interactive learning (IL) are two steps in the proposed IL-MCAM system (IL). To extract multichannel
features for classification in the AL stage, a multi-channel attention mechanism model with three separate attention
mechanism channels and convolutional neural networks is implemented. The proposed IL-MCAM system constantly
adds misclassified images to the training set in an interactive method during the IL stage, improving the MCAM
modelâ€™s classification ability. To handle different colorectal histopathological image classification tasks in the future,
permutation and combination can be used to identify the best model for the current task from attention mechanisms
anddeeplearningmodels.Moreover,attentionmechanismscanalsobeincludedinvariouslocationsofadeeplearning
model, in order to investigate the influence of convolutional layers on classification performance.
The table 4 given below summarized the performance gain by the reviewed articles of the classification category.
Table 4: List of datasets and performance measures employed by researchers for medical image classification.
Modality Publication Dataset Performance Measures
CT ScansYang et al. [157]LUNA16 [88] CPM Score = 0.916
LIDC-IDRI [89] Accuracy (%) = 93.17
Wu et al. [158]Accuracy (%) = 95.95
ComputedTomographyEm-
physema Database [90]Precision (%) = 98.0
Recall (%) = 97.1
Specificity (%) = 98.6
CT ScansAmbita et al. [164]COVID-CT [91]Accuracy (%) = 87.19
. : Page 26 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Positive Precision (%) =
89.11
Sensitivity (%) = 85.71
F1 Score = 0.8738
CT Scans
Sars-CoV-2 [92]Accuracy (%) = 95.41
Positive Precision (%) =
94.30
Sensitivity (%) = 98.03
F1 Score = 0.9613
Zhang et al. [165] COV19-CT-DB [93]Accuracy (%) = 94.3
Precision (%) = 93.7
Recall (%) = 93.8
F1 Score = 0.935
Macro F1 Score = 0.94
Than et al. [165] COVID-CTset [94]Accuracy (%) = 95.36
Senstivity (%) = 83.00
Li et al.[182]Private dataset from eight
different Hospital [182]Macro F1 (Score) = 0.97
Micro F1 (Score) = 0.98
X-ray ImagesTuagi et al. [170] Chest X-ray Images [95] Accuracy (%) = 96.45
Duong et al. [171]Montgomery County (MC)
CXR Images [183]
Accuracy (%) = 97.92
Shenzhen CXR Dataset
[183]
COVID-19 Dataset [184]
Park et al. [174] BIMCV COVID-19+ [96]AUC = 0.949
Accuracy (%) = 86.8
Sensitivity (%) = 90.2
. : Page 27 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Specificity (%) = 86.2
Shome et al. [175]Accuracy (%) = 92
COVID-19 Posterior-
Anterior Chest Radiography
Images [97]Precision (%) = 93
ExtensiveCOVID-19X-Ray
and CT Chest Images [98]Recall (%) = 89
F1 Score = 0.91
AUC = 0.98
Rahha et al. [176]Accuracy (%) = 94.62
Precision (%) = 96.77
Sars-CoV-2 [92] Recall (%) = 96.77
Specificity (%) = 99.65
F1 Score = 0.967
MRI Scans Dai et al. [178] MRNet Dataset [92]AUC-ROC = 0.976
Accuracy (%) = 91.8
Sensitivity (%) = 96.8
Specificity (%) = 72.8
OCT/Fundus Images Kamran et al. [179] Color Fundus Images [100] Accuracy (%) = 85.7
Sensitivity (%) = 83.3
Specificity (%) = 90.0
Histopathology Images Zeid et al. [180]Accuracy (%) = 93.3
ColorectalCancerHistology
Dataset [101]Precision (%) = 93.33
Recall (%) = 93.44
F1 Score = 0.933
. : Page 28 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
6.2. ViT in Region-based Lesion Detection
The proceeding section discusses the detection of anomalies in medical images using Vision Transformers, in
relation to the aforementioned modalities.
6.2.1. Computed Tomography (CT) Scans:
COVID-19Detection AframeworkforthedetectionofCovid-19wasproposedbyLiangetal.,withchestCTimages
asinput[185].TheframeworkwascomposedofaCNNmodelforfeatureextraction,andthentheSEattentionmodule
wasintegratedforthegenerationofattentionvectors.Next,thetransformermodelwasusedtodistinguishthefeatures
in the input. The study also proposed a method to resample the inputs, which also contributed to the efficiency of the
model. The highest f1-score was 88.21 percent which was a 10 percent improvement from the baseline model. The
dataset used, however, was small and imbalanced which doesnot validate the generalizability of the proposed model.
AnomalyDetection Inthemedicaldomain,variousmethodologiesareproposedforthedetectionofanomalies.The
authorsofthepaper[186],haveproposedatransformer-basedmodel,whichwasappliedonvariousimagesi.e.retinal
OCT, Head- CT Scans, and Brain- MRIs. The representation of the features was learned using autoencoders which
were based on transformers. In addition, to detect the anomalies in multiscale, a transformer model was proposed
with skip-connections, thus it reduced the usage of memory and cost of computation. The models were evaluated on
AUROC, achieving 93 %, 95.81 %, and 98.38% for the datasets related to Head-CT, Brain- MRI, and retinal OCT,
respectively.However,theproposedmodelstillrequiresafurtherreductionincomputationalcostsothatitcanbeused
in real-time.
6.2.2. X-ray or Radiographic Images:
COVID-19Diagnosis SinceX-raysarecomparativelycost-effectiveandafasterwayofdiagnosingthevirus,several
researchershaveproposedmethodsfordetectionusingchestx-rays.Inthepaper[187],anotherapproachisproposedto
detect covid-19 using chest X-rays. An adaptive attention network is used which consists of ResNet and an attention-
basedencoder.ResNetisusedtolearnthefeaturerepresentationsandtheAttentionmoduleisthenutilisedfordetection
of the infectious areas. The proposed model was compared with different CNN models on three different datasets.
The evaluation indicated that the proposed model performed significantly better. Moreover, the performance metrics
included Accuracy, sensitivity, precision, and F1 score. The highest accuracy achieved by the model was 98.5
Similarly, To capture the global context, the authors Kumar et al. have used vision transformers on both X-ray
images and CT images of the chest for the diagnosis of Covid-19 [188]. The data used was labelled as normal,
pneumonia and covid-19. Furthermore, to address the issue of scarce data, transfer learning is used followed by
explainability through visualisation of the infected areas. The proposed method was compared with other models
i.e InceptionV3, CoroNet, CovidNet, etc. The results were evaluated using the metrics; precision, recall, f1-score,
accuracy,andspecificity.TheproposedmodeloutperformedtheCNNmodelsreachingtheaccuracyof0.96and0.98
for CT scans and X-ray images, respectively. However, work on severity information requires attention in both [187]
[188].
Pulmonary Lesions Detection In the initial assessment of lung cancer, one of the most used techniques is chest
radiography. Since it is essential to diagnose cancer at an earlier stage, many methodologies have been proposed to
detect pulmonary lesions. The study is presented by [189], it has proposed two architectures; convolution networks
with attention feedback, and recurrent attention model with annotation feedback. The first method uses CNN to
learn the features, and generate saliency maps as the soft attention mechanism was incorporated. Next, a recurrent
attention model with attention feedback was proposed. The proposed architecture uses reinforcement learning for
better performance of the model. The architectures were evaluated through precision, recall, f1-score, and accuracy.
Thehighestaccuracyachievedwas85percentforclassification,and74percentforlocalization.Thus,thearchitectures
require improvement regarding the reduction of computation time and accuracy.
6.2.3. OCT or Fundus Images:
Microaneurysms Detection The early diagnosis of lesions in diabetic retinopathy can be done by the detection
of microaneurysms (MA). Since it is difficult to locate them because of their size, several methodologies have been
proposed. In this study [190], the proposed methodology for the detection of MAs comprises three stages. First, the
images are preprocessed to improve the quality. Second, a deep network is used with an attention mechanism for
detection. Third, the correspondence between Microaneaurysms and blood vessels is exploited for the final results.
. : Page 29 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
The performance metrics used for evaluation were precision, recall, sensitivity, and f1-score. The proposed method
outperformed prior proposed models with a sensitivity of 0.86. Nevertheless, the model was trained on the images
from one type of camera, which does not validate the generalizability of the proposed methodology.
Glaucoma Detection The authors of the paper [191], have proposed a methodology for the detection of a disease
knownasGlaucoma.Itcausesthelossofvisionandisirreversible.ThepaperhaspresentedaCNNmodelwhichwas
attention-based for the detection of the disease. Furthermore, due to the attention module, the localized features were
also visualized, giving results more explainability. The proposed architecture first locate the area and then classify
the disease. The detection was evaluated using the performance metrics; accuracy, sensitivity, specificity, AUC, and,
F2-score,withthehighestaccuracyachievedof96.2percent.However,inthenetwork,themodelsmayidentifyregions
with useless information which may hinder the performance of the model.
Further,Xuetal.havepresentedamodelwhichconsistsofanattentionmodulealongwithtransferlearningforthe
detection of glaucoma [192]. This work has contributed towards the discrimination of general and specific features.
Since the models are not able to identify the regions that may give no information, the proposed methodology can
extract the regions with more information. In addition, with the attention module, the regions can also be visualized.
The model was then evaluated on two different datasets, achieving the highest accuracy, sensitivity, specificity, and
AUCof85.77percent,84.9percent,86.9percent,and0.929,respectively.Lastly,thismethodcanbefurthervalidated,
by applying it to various other eye diseases.
6.2.4. Histopathology Images:
Cancer Detection Barretâ€™s esophagus (BE) refers to the damaging of the swallowing tube that connects the mouth
to the stomach because of acid reflux [193]. Ultimately, it increases the risk of esophagus cancer i.e. adenocarcinoma
[194]. Moreover, patients that suffer from BE are at a higher risk of cancer. The detection of lesions at an early stage
can prevent the suffering of patients from cancer, with a better survival rate.In the paper[195], attention-based deep
neural networks were proposed for the detection of cancerous and precancerous esophagus tissues. The model uses
attention-based mechanisms to detect the cancerous tissues belonging to the classes; normal, BE-no-dysplasia, BE-
with-dysplasia, and adenocarcinoma. The mechanism does not require annotations for regions of interest, thus, it
dynamically identifies the ROIs. Hence, it is independent of the annotated bounding box and does not require a fixed
sizeofinputimages.Theproposedmethodwascomparedwiththeslidingwindowapproachbasedontheperformance
metrics; accuracy, recall, precision, and accuracy. The model outperformed the sliding window method in all classes
withanaverageaccuracyof0.83.However,themodelwastrainedonasmalldataset,hencetherobustnessofthemodel
still needs to be verified using more data.
The issues regarding whole-slide images in terms of detection, include poor adaptability of the model, explain-
ability, and resource-efficient model. The authors of the paper[196], have proposed a model known as clustering-
constrained-attention multiple-instance learning(CLAM). It was applied to detect three types of cancers; renal cell
carcinoma, non-small cell lung cancer, and breast cancer lymph node metastasis. The proposed method CLAM is a
weakly supervised algorithm, it uses an attention module to determine the regions, and classify the cancer type. In
addition, it also localized the affected regions with interpretability. The models were evaluated using AUC, hence, it
was greater than 0.95. On contrary to this data-efficient model, it considers various locations as independent, thus,
leading to a less context-aware model.
Next, another model was used for the detection of cancer leading to the prediction of survival prediction [197].
Theframeworkisamultimodalco-attentiontransformer(MCAT),thatlearnsthecorrespondencebetweenWSIâ€™sand
genomic features. The attention module ensures interpretability along with the reduction of memory usage of image
bags. The model was applied to five different cancer datasets, and the results were compared with the state-of-the-art
models.
The table 5 given below summarized the performance gain by the reviewed articles of the detection category.
. : Page 30 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Table 5: List of datasets and performance measures employed by researchers for Region-based Lesion Detection.
Modality Publication Dataset Performance Measures
CT Scans Mondal et al. [188] COVIDx-CT-2A [104]Accuracy (%) = 98.1
Recall(%)=96
Precision(%)=96
Specificity(%)=98.8
F1(Score)=0.96
Liang et al. [185] COV19-CT-DB [93] Macro F1 (Score) = 88.21
Micro F1 (Score) = 0.98
X-raysLin et al. [187]COVIDx [99]Accuracy (%) = 95
sensitivity(%)=97
Precision(%)=98.98
Specificity(%)=99.47
F1(Score)=0.97
COVIDGR-E [102]Accuracy (%) = 89.53
sensitivity(%)=86.05
Precision(%)=83.15
Specificity(%)=91.28
F1(Score)=0.84
DLAI3 Accuracy (%) = 98.55
sensitivity(%)=98.63
Lin et al. [187] Precision(%)=98.63
Specificity(%)=99.90
F1(Score)=0.98
Precision (%) = 15
. : Page 31 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Pesce et al. [189]A dataset consisting of
745,479 chest x-ray exams
collected from the historical
archives of Guyâ€™s and St.
Thomasâ€™ NHS Foundation
Trust in London from
January2005toMarch2016Sensitivity (%) = 65
Average Overlap (%) = 43
Fundus ImagesZhang et al. [190] IDRiD [103]Accuracy (%) = 94.3
Precision (%) = 87.2
Recall (%) = 81.0
F1 Score = 0.840
Sensitivity (%) = 86.8
Accuracy (%) = 96.2
Sensitivity (%) = 95.4
Li et al. [191]LAG Database (obtained
from Chinese Glaucoma
Study Alliance (CGSA) and
Beijing Tongren Hospital.)
[191]Specificity (%) = 96.7
AUC = 0.983
F2 Score = 0.954
Xu et al. [192]LAG Database (obtained
from Chinese Glaucoma
Study Alliance (CGSA) and
Beijing Tongren Hospital.)
[191]Accuracy (%) = 85.7
Sensitivity (%) = 84.9
Specificity (%) = 86.9
AUC = 0.929
Tomita et al. [195] Accuracy (%) = 83.0
Recall (%) = 60.0
. : Page 32 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Histopathology ImagesHistologicalimagesbetween
January 1, 2016, and
December 31, 2018, at
Dartmouth-Hitchcock
Medical Center (Lebanon,
New Hampshire) were
collected.Precision (%) = 62.0
F1 Score = 0.59
Chen [197]The Cancer Genome Atlas
[105]Concordance Index (c-
Index) = 0.653
6.3. ViT in Anatomical Structure Segmentation
Clear cut and detailed segmentation is a decisive step in image guided treatment and computer-aided diagnosis.
A great deal of image segmentation models have been proposed In the last 40 years from traditional models to deep
neural networks. But since the emergence of transformers, they have outperformed all the state of art segmentation
models. Transformers functions prominently in error free segmentation of medical images because of their capability
to model the global context. As the organs lay out over a wide receptive field, hence, transformers can easily encode
these organs by modeling the association of pixels that are distant spatially. Moreover, the background is dispersed in
medical scans, for that reason gaining the understanding of the global context between those pixels that relate to the
backgroundwillbebeneficialforthemodeltodotheunerringclassification.Belowwereviewedexperimentsthattried
to exploit ViT based models for a faultless segmentation. We divided these experiments in accordance with different
modalities used for medical imaging. In the end we give all the results obtained during these experiments on specific
datasets in tabular form.
6.3.1. Computed Tomography (CT) Scans:
Coronary Artery Segmentation A precise and correct segmentation of CAC is advantageous for early CVD
diagnosis. But as CAC has blurry and distorted boundaries, the task of segmentation is not very much satisfactory.
To address this issue Ning et al. [198] introduced an efficient multiscale vision Transformer for the segmentation of
coronaryarterycalciumandnameditasCAC-EMVT.Thisarchitectureutilizedthelocalaswellasglobalfeaturesand
thenusedthemcollectivelytomodelthelongandshort-termdependencies.Theirmodelwascomprisedofthreemain
modules,(a)KFS,KeyFactorSamplingmodulethattheyutilizedforextractingthekeyfactorsfromtheimage.These
key factors were made use for low-level reconstruction of highly structured features, (b) NSCF, a non local sparse
net fusion module, that was used to model the information of high level features of texture, (c) NMCA, a non local
multiscale context aggregation, that was used to get the dependencies of long range at different scales. Experimental
results showed that their model outperformed the state of the art methods at that that by giving a Dice similarity
co-efficient of 75.39% Â±3.17.
Leeetal.[199]introducedanewconceptoftemplatetransformernetworksforsegmentationthroughshapepriors
(TETRIS), and performed coronary artery segmentation through their model. In this concept they used an end to
end trainable Spatial Transformer (STN)[200] to deform a shape template to complement the under laying region of
interest.TheyalsousedthisconceptofincorporatingthepriorstothestateoftheartCNNandU-Netusedforbinary
classification.TheexperimentalresultsofTETRISandU-Net[139]incorporatingtheprior,wereabletoproducesingly
connected components because they were given the prior information and gave to dice scores of 0.787 and 0.854
respectively. They also compared the U-Net[139] with FCN by giving the prior shape but FCN[201] didnâ€™t perform
that well giving the Dice Score of 0.79 only.
Lung Tumor Segmentation PET-CT segmentation requires information from both PET and CT modality. Most of
the models get the segmentation information of these modalities separately. In a study, Fu et al. [202] established a
moduleMSAM,MultimodelSpatialAttentionModule,adeeplearningbasedframeworkforlungtumorsegmentation
. : Page 33 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
inPET-CT.MSAMwasanimpulsivemoduleanditwasabletohighlightthespatialareasorregionsthatarelinkedto
thetumorandcensoredthenormalregionsofinputspontaneously.ThismodulewasfollowedbyaCNNthatwasacting
asabackboneandthisCNNwasperformingthesegmentationtaskonthemapthatwasprovidedtoitasaninputfrom
the multi model spatial attention module. U-Net[139] was used as backbone for that purpose. They performed their
experimentsontwoclinicalPET-CTdatasetsofNSCLCandSTS.Theresultsoftheexperimentsgaveadicesimilarity
coefficient of 71.44% and 62.26% for NSCLC and STS datasets respectively. in order to refine this architecture, more
better procedures can be used to further improve the segmentation.
RenalTumorSegmentation Duetothediversificationthatispresentinsizeandpose,thetaskofsegmentationhas
become a strenuous task. Hence La et al. [203] proposed a network that was both size and pose invariant and they
tested their network for renal tumor segmentation on 2D CT Scan images. Their architecture was comprised of three
sequentialmodulesthatworkedtogetherinthetrainingprocess.Firstwastheregressionmodulethattheyusedtofind
the similarity matrix of input image to the ground truth. Second module was used to find the region of interest and
theynameditasdifferentiablemodule.Thethirdandlastmodulewasusedtoperformthesegmentationtaskandthey
usedU-Net[139]forthispurpose.TheyusedtheSpatialTransformer(STN)[200]intheirarchitecturetoautomatically
detect the bounding box which saved time. Results indicated that the training time was reduced by 8 hours and the
Dice Score for kidney almost remained same which was 88.01%, but in case of renal tumor, the score got better from
85.52% to 87.12%. one of the shortcomings of their model was that it was valid for only small set of data.
Aortic Valve Segmentation Basic CNN models for segmentation were performing good on 2D images and they
were struggling against 3D medical imaging. Hence Pak et al. [204] proposed a deep learning based architecture for
thesegmentationof3DCTScanimages.ThisnetworkwascomprisedofabaselineU-NetArchitecturethatperformed
thebasicsegmentationtaskandaSpatialTransformer[200]thatwasusedtoperformsomeaffinetransformation.The
use of only U-Net[139] was not sufficient for the segmentation tasks as it requires a lot of memory and also result in
decrease of accuracy. Hence they used a spatial Transformer (STN)[200] which reduced the size of input image by
performingsometransformationandhenceitresultedinbettercomputation.theyutilizetheirmodeltoperformaortic
valvesegmentation.Upontestingtheirmodelondifferentpatientsdata,theDiceScorecoefficienttheygetwas0.717.
Bone Segmentation In order to perform the segmentation of bone as well as the localization of the anatomical
landmarks of cone beamed computed tomography data simultaneously Lian et al. [205] proposed a network called
dynamictransformernetwork(DTNet).Theirmodelcontributedinthreeparts.Inthefirstpart,asynergicarchitecture
was made to accurately catch the global context and fine grained details of image for volume to volume prediction.
Secondly, by using the anatomic reliance between landmarks RDLs are made to collectively degenerate the large 3D
heatmapsofeverylandmarks.Thirdly,ATMsaremadeforthecompliantlearningofcontextspecificfeatureembedding
frommutualfeaturebases.WhiledoingtheexperimentsonCTscansofmandible,thesegmentationDSCcameoutto
be 93.95% with a std dev of 1.30 whereas for localization the RMSE was 1.95 Â±0.43. these results were better than
U-Net and other models that were used for comparison.
Lesion Segmentation The early diagnosis of AIS provides valuable knowledge about the disease. But for a human
eyeitisburdensometodiscriminatedelicatechangesinpathology.Hence,Luoetal.Luoetal.[206]proposedanetwork
forthesegmentationofAcuteIschemicStroke(AIS)thatwasbasedonself-attention.Thismechanismhadanencoder
and a decoder. The encoder was comprised of a CNN as a backbone and a transformer. This encoder part picked the
globalcontextfeatures.ThedecoderpartconsistedofMultiHeadCrossAttention(MHCA)modulewhichupsampled
the feature maps that were coming from the encoder. These feature maps were connected via skip connections. The
backboneCNNusedwasRESNET-50.TheirexperimentalresultswerecomparedtoattentionU-Net[207],U-Net[139]
andTransUNet[208]buttheirmodeloutperformedthembygivingtheDiceSimilarityCoefficientofsegmentationof
lesion up to 73.58% which was better than all other compared models.
Segmentation of Organs Although transformers help in capturing the long term dependencies but when it comes
to the segmentation of 3D images, the dependencies face extreme computation. Hence to reduce some computations,
Xie et al. [209] presented CoTr, which is a combination of convolutional network and transformer. Instead of using
simpletransformer,theyintroduceddeformabletransfersforcatchingthelongrangedependencies(DeTrans).DeTrans
focusses on only a few key points, which greatly reduces the computational complexity, which also allows to process
multiscaleimages,whicharequiteimportanttoattainanaccuratesegmentation.theytestedtheirmodelonBCVdataset
. : Page 34 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
thatincludestheimagesof11differenthumanorgans.CoTrachievedaDiceScoreof85andHausdorffDistanceof4.01
onavgandthesemeasureswerebetterthanothermethodsthattheyusedtocomparetheirmodel.Furtherenhancement
in their model could be that it can be enhanced by extended it to operate on different modalities.
6.3.2. Magnetic Resonance Imaging (MRI) Scans:
Brain Tumor Segmentation Glioma segmentation and prediction of IDH genotyping is an important and difficult
taskduetosimilaritiespresentinintraandinter-tumor.ToaddressthisproblemChengetal.Chengetal.[210]proposed
an MRI based fully automated multi model that could predict IDH genotyping and Glioma segmentation at the same
time.Thepreexistingmethodswerenotabletoperformthebothtasksatthesametime,alsothesemethodsfacedthe
problemsofinterandintra-tumourheterogeneity.SotheyaddresstheseissuesbyusingajointCNNandTransformer
encoder. The transformer was used to extract the global features that were used for the glioma segmentation. It also
contained a multi-scale classifier, which was used for IDH genotyping. A multi-task loss was then used to balance
the segmentation and IDH genotyping and this loss collectively joined the classification loss and segmentation loss.
In the end they proposed an unpredictability aware pseudo-label-selection to make pseudo-labels for IDH on a large
unlabeledDataset.TheynamedtheirmodelasMTTU-Net.OnexperimentstheirmodelimprovedtheHD95andDice
score 1.69mm and 1.23% for glioma segmentation and 2.13% and 4.28% in case of AUC and accuracy respectively.
Sagar et al. [211] proposed ViTBIS, vision transformer for bio medical image segmentation, that was based on
encoder and decoder architecture. Both encoder and decoder had transformer inside them. The feauture map of input
image was split into three different convolutions before it was fed to the transformer. These convolutions were, 1X1,
3X3and5X5.Thesethreedifferentfeaturemapswereconcatenatedwiththehelpofconcatoperator,thenitwasfedto
thetransformerintheencoder.Thesetransformerhadtheattentionmechanisminsidethemthetransformersofencoder
and decoder were joined together via skip connections. The same architecture of multiscale was used in the decoder
as well. Before producing a segmentation mask after linear projection, different sizes were concatenated via concat
operator. Upon testing their architecture on a public dataset for brain tumor segmentation the DSC achieved was 0.86
which was better than other state of the art CNN and transformer networks.
Brain Tissue Segmentation In order to solve the problem of multi model medical image segmentation, Sun et al.
[212] presented a novel multi model architecture based on transformer and Convolutional Neural Network for multi
modelimagesegmentationandnameditasHybridCTrm,andusedthismodeltosegmentdifferentbraintissues.This
networkusedtwopathsfortheimageencoding,onepathwasfromtheCNNandtheotherpathwasfromTransformer.
Thentherepresentationofimagefrombothpathswerejoinedtogetherfordecodingandthesegmentationpurpose.The
CNN controlled the rapid convergence of gradient descent while extracting the local features, whereas the non local
features were extracted by the transformer. They used two strategies for the fusion, one was the single path strategy
and the other was the multiple path strategy and used both of these strategies in their experiments. Experiments were
carried out on two different datasets and by following both strategies. On MRBrainS dataset the DSC came out to be
82.98and83.47forsigleandmultiplepathstrategiesrespectivelywhereasontheiSeg-2017datasetthesescoreswere
86.75 and 87.16 which were better than the models they used to do the comparison like HyperDenseNet[213],
BrainStructureSegmentation Agooddealofdeeplearningarchitecturesusedtoperformthetaskofsegmentation
on medical images confront the problem of noise at the inference time and result in inaccurate result. To address this
problem Sinclair et al. [214] proposed a network, Atlas-ISTN, atlas image and spatial transformer network, that was
able to perform both registration as well as segmentation on 2D and 3D data of Brain structure. This network could
perform segmentation on numerous interest regions/ structures and to register the atlas label map to an in-between
segmentation(pixel-wise). This model was also able to do the fine tuning of the parameters at the inference time in
ordertoachievebetterpixelwisesegmentation,duetowhichtheeffectofnoiseintheimagealsoreduced.Thismodel
was then tested on three different datasets, two 3D and one 2D. the results were compared with U-Net and this model
was performing better than U-Net[139] giving a DSC of 0.888.
CardiacSegmentation Combiningthesharedinformationofanyorganfromdifferentmodalitiesisveryhelpfulfor
learningandmultimodalityprocessing.Inordertoschievethis,Chartsiasetal.[215]proposeddisentangled,alignand
fuse network, DAFNet, that was able to learn the information present in different modalities input, hence producing
a more precise segmentation mask. Anatomical factors from different inputs are combined and processed at the same
time.DAFNetcollectedtheinformationpresentindifferentmodalitiesdespiteofthefactthatfewlabels(supervised)are
. : Page 35 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
there or even no labels (unsupervised). Spatial Transformer was used to align the anatomical factors in case of image
misregistration. They evaluated their model by performing L2, T1 and Cardiac segmentation on different datasets.
Theirmodelwasabletoperformonbothsinglemodelandmultimodelinputsanditoutperformedothermodelswhen
it was trained on single modality input whether with few labels(semi supervised) or no labels (unsupervised).
Definingtherightventricle(RV)structureincardiacsegmentationisastretchingworktodobecauseofitscomplex
andmultiplexstructure.Henceitrequiresshortaxisaswellaslongaxisimages.InordertoaddressthisissueGaoetal.
[216] established a consistency based co training mechanism that used the geometric relationships between different
view CMR images for the segmentation. Along with this mechanism, they also used the U-Net[139] architecture in
ordertocapturesomelongrangedependencies.EvaluationofthemodelwasdoneontheM&MS-2challengedataset
and the Dice score came out to be 0.83 and 0.86 for short axis and long axis respectively.
ColerectalCancerSegmentation InastudydonetosegmentthecolorectalcancerregionSuietal.[217]established
a novel approach, based on transformer, that performed the segmentation as well as detection of colorectal cancer
region collectively. Their model was based on two pipelines, one for the detection and the other for the segmentation.
In the detection part, region proposals were generated. They utilize image level decision approach that was based
on auto encoders. Whereas in the segmentation part they used patches of the image as input and to make the final
mask prediction, class embeddings were used. They compared their model with the Faster CNN and Yolo-v3 for the
detection task and their model performed exceptionally well on the used dataset, giving an accuracy of 88.6% where
as the segmentation score came out to be 91.1% which was way better than U-Net[139] and FCN[201].
6.3.3. X-ray or Radiographic Images:
Breast Tumor Segmentation Correct and accurate segmentation of tumor in ABVS is a difficult task because the
sizeofimageishugeanditsqualityislow.InordertosegmenttumorfromtheABVSimages,Liuetal.[218]adopted
the use of both transformers and CNNs and named their model as 3D-UNet. They joined the attention module and
the U-Net[139] model. For further improvements in the performance they also made use of Atrous Spatial Pyramid
Pooling(ASPP)intheirmodel.ASPPcanhelpcatchtheinformationatmultiscales.Theycomparedtheirmodelwith
different3Dsegmentationmodelslike3DFCN[219],3DPSPNet[220]andrecordedtheDiceScoreCoefficient.Their
score recorded as 76.36 Â±6.11, which was better than the networks that were used for the comparison.
AnatomicalStructureSegmentation Mostofthesegmentationnetworksworkonsupervisedlearningwhereexpert
labeled image is required as a label and this is an obstacle if there arenâ€™t much experts available. Hence in order to
make an annotation efficient Lu et al. [221] introduced Contour Transformer Network, CTN, which is an annotation
efficient segmentation method for anatomical structures. They copied the human ability doing the segmentation of
anatomical structures with very less exemplars available. To achieve this, they proposed a semi supervised learning
mechanismthatutilizetheresemblanceofstructureandappearanceofthedesiredobjectbetweenunlabeledandlabeled
images. They made the segmentations of anatomy in the form of contour evolution process and model the behavior
by GCNs. They named their model as one shot anatomy segmentation model. On performing the segmentation on
four different anatomies, their model comprehensively performed better than u supervised learning mechanisms and
performed competitively against the supervised state of the art methods. Upon experiments the accuracy of one shot
model came out to be 96.58% which was almost 15% better than Braintorm, which is another one shot based model.
Whereas in comparison with non-learning based model, the accuracy was 16% improved. one shortcoming of their
networkisthattheirnetworkonlyperformedon2Ddata.henceextendingthisarchitecturetoworkon3Ddatawould
be an important step in the field of 3D segmentation.
Guide Wire Segmentation A study done by researchers tried to resolve the task of segmentation of guide wire
in X-ray fluoroscopy sequence. Zhang et al. [222] proposed a network that takes in account current frame as well
as the previous frame while taking input for the guide-wire segmentation. By considering both frames helped them
in obtaining the temporary information. Their network contained two parts, one was a CNN and the other was a
transformer. The CNN wasnâ€™t able to capture the global features hence transformer came into play that can learn the
global features by using its attention mechanism. CNN and transformer lied in the encoder part whereas decoder
contained up sampling, concatenating operations and convolutions. They evaluated their model on datasets from
three different hospitals and measured the F1-score and compared their score with other state of the art models like
Frrnet[223], Parnet[224] and U-Net[139] and their model was outperforming all other models.
. : Page 36 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Tooth Root Segmentation An accurate and precise segmentation of the boundaries present in the roots of the tooth
isnecessarytoattainaperfectrootcanaltherapyassessment.Lietal.[225]introducedaGroupTransformerNetwork
(GTU-Net)inordertoachievesegmentationofrootboundaries.Theirmodelâ€™sstructurewassimilartotheU-Netbut
they used group of transformers in place of encoders and decoders. Also in order to incorporate the prior knowledge
theyusedFourierDescriptorLoss.Theirmodelachievedanaccuracyof96.31%andf1scoreof84.58%outperforming
other state of the art models.
6.3.4. OCT or Fundus Images:
Drusen Segmentation It is very crucial to diagnose the AMD at an early stage in retinal OCT images via Drusen
Segmentation.InordertoachieveanaccuratesegmentationWangetal.[226]proposedamultiscaletransformerglobal
attention network MsTGANet for the segmentation of drusen in retinal OCT images. Their model was composed of
a U-shaped architecture containing an encoder and decoder. To collect the non-local features at different scales with
ling term dependencies from multiple encoder layers, a novel multi scale transformer non local module is proposed
and used at encoderâ€™s top. Another module, MsGCS was introduced to assist the model to join different semantic
knowledgebetweenencoderanddecoder.TheyalsointroducedasemisupervisedversionofMsTGANet.Thisversion
wascomprisedofpseudo-labeleddataaugmentationstrategy.Thismodelcanusedhugeamountofunlabeleddatain
order to increase the performance on segmentation, upon experiments the DSC came out to be 0.8692 with a std of
0.0052outperformingtheotherstateoftheartmodels.thismodelwastrainedonasmallerdataset,howeveritwillbe
better to collect a larger set of data in order to see its efficiency. Also, different semi-supervised learning approaches
can also be used to further improve its performance.
The table 6 given below summarized the performance gain by the reviewed articles of the segmentation category.
Table 6: A list of datasets and performance measures adopted by researchers for Segmentation.
Modality Publication Dataset Performance Measures
MRICheng et al.[210] BRaTS2020[107]Dice Score = 0.90
Hausdorff Distance = 4.4
AUC(%) = 91.04
Accuracy(%) = 90
Sensitivity(%) = 87.50
Specificity(%) = 92.11
Chartasis et al.[215]ERI[110] Dice Score = 0.82
CHAOS[111] Dice Score = 0.85
Sun et al.[212]MRBrainS[108] Dice Score = 0.83
iSeg2017[106] Dice Score = 0.87
Sinclair et al.[214] UKBB[109] Dice Score = 0.86
Hausdorff Distance = 7.2
Sagar et al.[211] BRaTS2019 Dice Score = 0.86
. : Page 37 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Hausdorff Distance = 7.1
Gao et al.[216] M&MS2Dice Score = 0.86
Hausdorff Distance = 9.6
X-ray ImagesLu et al.[221]OAIIOU(%) = 97.32
Hausdorff Distance = 6.0
JSRT[113]IOU(%) = 94.75
Hausdorff Distance = 12.1
Li et al. [225] DRIVE[112] Dice Score = 0.92
CT Scans La et al.[203] KiTS2019[114] Dice Score = 0.88
OCT/Fundus Images Wang et al.[226] USCD[115] Dice Score = 0.86
6.4. Clinical Report Generation
In this section, we briefly describe various transformer models to generate the medical reports and address the
preceding challenges associated with automatic clinical report generation.
6.4.1. Supervised Learning Based Approaches
Supervisedlearningreferstoatypeoflearningalgorithmsthatlearnunderthepresenceofasupervisor.Aninput
fromthetrainingsetispassedthroughthenetworkthentheoutputofthenetworkiscomparedtothedesiredoutputand
learningweightsareupdatedaccordingly.Followingstudieshaveemployedsupervisedlearningintheirmethodologies.
IncorporatingGlobalLevelFeatures Globallevelfeaturesareextractedfromtheentiremedicalimagei.e.encoded
features of both normal and disease regions in the image. Following studies have incorporated this notion into their
methodologies.Youetal.[144]proposedatransformer-basedarchitecture,AlignTransformer.Theyresolvedthedata
bias and long sequence modeling problems to generate a coherent medical report by delineating the normal and
abnormalregions.TheyusedResNet-50pre-trainedonImageNetandfine-tunedonCheXpertdatasettoextractvisual
features. Furthermore, they fed the extracted visual features into the pre-trained multi-label classification netwrok to
predict the disease tags. Align hierarchical attention as an encoder aligned the disease tags and visual regions by
learning the correlation and relationship between them. Moreover, they acquired the multi-grained disease-grounded
visual features from the aligned disease tags and visual regions to alleviate the data bias problem. Multi-grained
transformer as a decoder exploited the multi-grained disease grounded visual features to generate a proper medical
report. In automatic evaluation, they compared their experimental results with the previous state-of-the-art models
i.e.R2Gen,PPKED,andSentSAT+KGetc.andachievedcompetitiveresultsonIUX-rayandMIMIC-CXRdatasets.
In human evaluation, the results of their model were far better than that of R2Gen model. Similarly, Amjoud et al.
[227] also proposed a transformer-based deep learning model for generating long and detailed reports of chest x-ray
images. They used a pre-trained DenseNet-121 [82] instead of ResNet-50 [144] to avoid gradient vanishing problem
and redundant feature maps. They suppressed the last classification layer of the pre-trained model to extract global
andregionalfeaturesfrommedicalimages.Afterthat,theextractedfeatureswerefedasinputintotheencodertomap
themintoasequenceofcontinuousrepresentations.Theymodifiedthedecoderofthevanillatransformerbyaddinga
relational memory module to the normalization layer. Experiments demonstrated that their model generated detailed
findings reports for IU chest x-rays test images and outperformed the state-of-the-art models for BLUE-1, BLUE-2,
and ROUGE metrics with 0.479, 0.359, and 0.380 scores respectively. However, the model could not perform well
. : Page 38 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
forBLUE-3,BLUE-4,andMETEORmetrics.Also,theyusedasmallcorpusfortraining,asaresult,somesentences
were unseen during inference which lead to the scattering problem.
In another work, Pahwa et al. [143] leveraged the skip connections by proposing a transformer-based architecture
namedMEDSKIPbymodifyingahigh-resolutionnetwork(HRNet).Theymodified(HRNet)[228]forvisualfeatures
extraction by incorporating skip connections along with convolutional block attention modules (CBAM). First, they
extracted features representation from each down-sampled layer and after extracting crucial features using attention,
CBAMconcatenatedthem.CBAMconstitutedspatiallyandchannelattentionsub-modulesforinferringa1Dchannel
attention map and a 2D spatial attention map respectively. The proposed architecture also contained a memory-
driven transformer which constituted a standard encoder but the decoder contained a memory-driven conditional
normalization layer to incorporate relational memory. The decoder facilitated the learning from patterns in reports
and recorded key information of the generated process. Extensive experiments on two publicly available datasets
PEIR GROSS and IU chest x-rays showed that their proposed model had given the state-of-the-art results for BLEU,
METEOR, and ROGUE metrics.
Incorporating Global and Local Level Features A medical image contains both normal and disease regions. To
encode the disease regions of the image, previous studies encoded the complete image which lead to the encoding of
irrelevant visual content which is adverse for radiology report generation. Some diseases have strong correlation and
findingthosecorrelationisbeneficialforgeneratingreportforrarediseases.Variousstudiestriedtotakeadvantageof
thisfeaturebyconstructingcorrelationmatrixintheencodingstagebydata-drivenmethodologiesorexpertknowledge
but these studies failed to decode these correlations effectively while decoding.
To address these problems, Jia et al. [229] leveraged the transformer-based architecture and proposed a few-shot
radiology report generation model, namely TransGen. In the encoding stage, they introduced a semantic-aware visual
Learning(SVL)moduleinwhichtheyusedResNet101toidentifyandcapturethediseaseregionsofrarediseases.They
capturedthediseaseregionsfromtheimageitselfandthefeaturemapgeneratedattimestep(t-1)bylearningthetwo
masksrespectivelytorefinethevisualrepresentationofrarediseases.Theyadoptedaweightedsumofthesetwomasks
attimestepttolearnthevisualrepresentationsefficientlybyincorporatingbothglobalandlocallevelinformation.For
efficientdecodingofencodedcorrelationamongthediseases,thememoryaugmentedsemanticenhancementmodule
wasintroducedatthedecodingstage.Experimentsdemonstratedthattheirmodeloutperformedthestate-of-artmodels
on the MIMIC-CXR dataset but could not perform well for the IU X-ray dataset.
Similarly, Lee et al.[230] also incorporated both local and global level features by proposing Cross Encoder-
Decoder Transformer (CEDT) contained a Global-Local Visual Extractor. They used a convolution neural network
(e.g.ResNet101)asaglobalvisualextractortoencodethecompleteradiologyimageintoasequenceofpatchfeatures
toaccuratelycapturethefeaturesatthegloballeveli.e.bonestructureorsizeoftheorgan.However,whileincorporating
global-levelfeatures,itwasdifficulttoencodetheexactlocationandthesizeofthelesionarea.Toaddressthisproblem,
theycroppedthediseaseregionsoftheimagewiththelastlayeroftheCNNusingtheattention-guidedmaskinference
process and after resizing to the same size as the image, used them as input to the local feature extractor to extract
the local level features. Then, they concatenated the local visual features and global visual features and used them as
input to the CEDT. The standard transformer uses only the last layer information but they [230] also used low-level
features in addition to the high-level features by using the concept of [231]. They used multiple encoders to get the
all-level information from them and utilize the outputs of all encoders on each decoder using parallel multi-headed
attention.Theyaddedtheextractedfeaturesofeachencoderlayerwhichresultedinbettercaptioningthanthebaseline
modelR2Gen.Furthermore,theyalsoemployedMCLNandRMforrecordingandutilizingtheimportantinformation.
ExtensiveexperimentsdemonstratedthattheirmodeloutperformedthebaselinemodelforBLEU-1,BLEU-2,BLEU-
3,METEOR,andROUGE-LonIUX-raydataset.Theyalsoperformedexperimentwithpre-trainedGLVEbutitcould
not perform well for the BLEU-4 metric.
6.4.2. Reinforcement Learning Based Approach
Previous studies [143, 144, 227, 229, 230] have used supervised learning approaches to generate medical reports.
Supervised learning approaches are prone to exposure bias problems in language modeling methods. To address this
problem,Xiongetal.[232]proposedanovelhierarchicalneuralnetworkarchitectureusingreinforcementlearningto
generate a long coherent medical report. They incorporated the self-critical reinforcement learning method into the
detector, encoder, and captioning decoder. Previous studies used only top-down visual encoders, however, this was
the first study that incorporated a bottom-up visual detector as well to extract semantic rich features from the medical
. : Page 39 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
images.Forthispurpose,firstlytheyusedDenseNet-121,pre-trainedonchestX-ray14dataset,todetecttheregionof
interest(ROI)proposalsusingabottom-upattentionmechanism.TheregiondetectoroutputtedasetofROIproposals
alongwithclassifiedclassesandsomeassociatedattributes.Secondly,theyusedtop-downtransformervisualencoder
to extract further pixel-wise visual information from proposed ROI using pooling operations. Lastly, the transformer
captioningmoduleusedimprovedROIproposalsasinputfromthetransformervisualencoderandgenerateddescriptive
sentences for each proposed ROI by calculating reward directly using the CIDEr metric. Their proposed architecture
outperformedthestate-of-the-artmethodsfortheCIDErevaluationmetricontheIUX-raydatasetbutfortheBELU-1
metric, their model could not perform state-of-the-art. Their model over-fitted as they used only the findings portion
of the generated medical report. This problem can be resolved using a larger labelled dataset.
Thetable7givenbelowsummarizedtheperformancegainbythereviewedarticlesoftheclinicalreportgeneration.
Table 7: A list of datasets and performance measures adopted by researchers for Clinical Report Generation.
Modality Publication Dataset Performance Measures
X-ray ImagesYou et al. [144]IU X-ray [116]BLEU-1 = 0.484
BLEU-2 = 0.313
BLEU-3 = 0.225
BLEU-4=0.173
METERO=0.204
ROUGE-L=0.379
MIMIC-CXR [117]BLEU-1=0.378
BLEU-2=0.235
BLEU-3=0.156
BLEU-4=0.112
METERO=0.158
ROUGE-L = 0.283
BLEU-1=0.479
X-ray ImagesAmjoud et al.[227] IU X-ray [116]BLEU-2=0.359
ROUGE-L = 0.380
. : Page 40 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
X-raysPahwa et al. [143]IU X-ray [116]BLEU-1= 0.467
BLEU-2=0.297
BLEU-3=0.214
BLEU-4=0.162
METERO=0.187
ROUGE-L=0.355
PEIR GROSS [118]BLEU-1= 0.399
BLEU-2=0.278
BLEU-3=0.209
BLEU-4=0.148
METERO=0.176
ROUGE-L=0.414
Jia et al.[229]IU X-ray [116]BLEU-1=0.461
BLEU-2=0.285
BLEU-3=0.196
BLEU-4=0.145
ROUGE-L= 0.367
KA (%) = 0.367
MIMIC-CXR [117]BLEU-1=0.368
BLEU-2=0.243
BLEU-3=0.178
. : Page 41 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
BLEU-4=0.138
ROUGE-L= 0.338
BLEU-1=0.5064
IU X-ray [116]BLEU-2=0.3195
BLEU-3=0.2201
Lee et al.[230]BLEU-4=0.1924
ROUGE-L= 0.3802
X-ray Images
Xiong et al. [232] X-ray [116]BLEU-1=0.350
BLEU-2=0.234
BLEU-3=0.143
BLEU-4=0.096
CIDEr = 0.323
6.5. Miscellaneous ViT Applications in Medical Imaging
Tranformer-based architecture has also played a vital role in other applications of medical field i.e. in image
synthesis, denoising the low dose computed tomography, and positron emission tomography images, enhancing the
resolution of medical images etc.
6.5.1. functional Magnetic Resonance Imaging (fMRI) Scans:
Visualizing Regenerated Neural Visual Content In the past decades, few studies have been conducted to decode
thehumanbrainneuralactivitiesintonaturallanguagesentences.Themainpurposeofdecodingbrainneuralactivity
is basically to know the human brainâ€™s perception of textual or visual content. In the past, most deep learning studies
focused on different task specific decodings, i.e. detection, classification, recognition etc., using functional magnetic
resonance imaging (fMRI) data. With the advancement in technology, several research has been done in language
decoding to decode the human brain semantics evoked by linguistic stimuli into natural language words or sentences.
Inspiringfromthesetasks,Zhangetal.[59]proposedahybridlanguagedecodingmodel:CNN-Transformertodecode
the visual stimuli evoked at multi-times by natural images into descriptive sentences. They exploited the concept of
neuralmachinetranslation(NMT)[17]butthedifferencewasinsourcesequencei.e.naturalimagesinNMTbutvisual
neuralactivitiesin[59].Toachievethistask,firstlytheyextractedmeaningfulsemanticlow-dimensionalfeaturesfrom
high-dimensionalvisualneuralactivities(low-levelrawfMRIdata)usingtwo-layeronedimensionalCNN.Secondly,
the encoder part of the transformer encoded the semantic features into multi-level abstract representation. Lastly, the
. : Page 42 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
decoder of the transformer decoded the multi-level representation into descriptive natural language sentences. They
comparedtheirmodelwithotherdecodingmodelsandachievedstate-of-the-artresultsforBLEU,CIDEr,andROUGE
metricswith0.17,0.66,and0.18scoresrespectively.Infuture,thistransformer-basedbraindecodingtechnologywill
beusefulforthepeoplewhoareunabletotransmittheirvisualperceptionintospeechandwillalsobeabreakthrough
for neuro-scientists in understanding and decoding the neural activities of human brain.
6.5.2. PET-CT Scans:
Medical Image Enhancement Computed tomography is a non-invasive imaging technique for medical diagnosis.
Since high exposure to X-rays radiation is deadly for humans and has become the main concern for medical
practitioners. To lessen this effect of X-rays radiation, it is used in less quantity in CT scans but it poses some serious
problems, i.e. less contrast, sharp features, corners, edges, and stronger noise, which affects the quality of CT scans.
Although low-dose computed tomography (LDCT) is mainstream in clinical applications, but the posed problems
causehindranceinaneffectiveclinicaldiagnosis.Manytraditionalmethods(iterativemethods)andconvolution-based
deep learning approaches were employed to acquire high quality LDCT images by deblurring and suppressing the
artifacts.Thehigh-frequencysub-bandofimagesarenoisyareaswhilethelow-frequencysub-bandarenoise-freeareas
containingmainimagecontent.Sinceconvolution-basedmethodsarelimitedtoextractingfeaturesfromthelocalareas
of images due to limited receptive fields. Therefore, transformers came into the scientific field and revolutionized the
world with their facts of capturing long-range dependencies between image regions.
Keepinginaccountalltheseobservations,Zhangetal.[233]proposedatransformer-basedarchitecturetodenoise
the LDCT images by decomposing them into high frequency (HF) and low-frequency parts. Hence, the noise was
only retained in the HF part and it also contained a plethora of image textures. To ensure the relationship between
HF and LF parts of the image, they denoised the noisy HF part with the assistance of the latent texture of the LF
part. For this purpose, they employed CNNs to extract corresponding texture and content features from the LF image
part.Furthermore,theyacquiredthehigh-levelfeaturesfromthetransformerusingthetexturefeaturesfromthenoisy
LF and embeddings from the HF part. They used a modified transformer with three encoders and three decoders.
Finally, they reconstructed the high-quality LDCT image piecewise by combining these high-level features with the
content features from the LF part of the image. ent features from the LF part of the image. Extensive experiments
demonstrated that their model outperformed all the baseline methods achieving 93.7% for VIT metrics, improved
structuresimilarityby12.3%,androotmeansquareerrorloweredby40.5%onMayolow-dosecomputedtomography
images dataset. Since convolution-based methods cannot capture global contextual information, Wang et al. [6] first
time proposed a convolution-free token-to-token vision transformer-based dilation network to denoise the LDCT
images.Theycapturedthenoisefrominputmedicalimagesbylearningdeepfeatures,afterthat,theyremovethenoisy
estimated residual images in order to clean them. Firstly, they used tokenization block to tokenize the feature map
patches into tokens. Secondly, they fed those tokens into transformer block, further, for enhancing the tokenization,
they applied tokenization in cascaded form in token-to-token block. They further enlarged the receptive field and
refinedthecontextualinformationusingdilationintokenizationprocedure.Theyperformeddilationusingreshaping,
soft split and cyclic shift to enhance the context. They compared their model with other state-of-the-art models and
their model outperformed for SSIM and RMSE metrics with 0.9144 and 8.7681 scores respectively, in denoising the
images. Without down-scaling tokenization of the image can be enhanced.
PET/MRIcanconcurrentlyprovideanatomicalandmorphologicalimaginginformationthataidsinclinicaldisease
diagnosis. PET acquires metabolic imaging information with the help of radio-tracers while MRI uses magnetic field
gradientsandradiowavestoacquireimagesofsoftbodytissues.Although,theseimagingmodalitieshaveapplications
indiseasediagnosis,i.e.cancer,tumor,andbraindiseases,butalsoposesomeseriousconcerns.Sincetimerequirement
for PET imaging acquisition is high and as a result, patient discomfort can affect the image quality i.e. low contrast-
to-noise ratio. The Information from MRI can assist in denoising the PET images using registration approach. Many
traditional deep learning and computer vision methods proposed to enhance the PET image quality using MRI but
duetodiscrepancyinmodalitiestheycouldnotextractcontextualandspatialinformationefficiently.Toaddressthese
problems. Zhang et al. [234] proposed a spatial adaptive and transformer fusion network (STFNet) for denoising low
count PET with MRI. They adapted dual path using the spatial-adaptive block to extract features. For the fusion of
high-levelfeatures,theymodifiedthetraditionaltransformerencoderandincorporatedglobalattentiontoformapixel-
to-pixelrelationshipbetweenMRIandPET.ThefusedfeaturemapwasusedasinputtothedecoderforPETdenoising.
Their model obtained promising results for on RMSE,PSNR,SSIM, and PCC metrics.
. : Page 43 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Similarly, Luo et al. [235] proposed 3D Transformer-GAN to build a standard dose PET image from the low-
dosePETimage.TheyleveragedtheCNN-Transformerarchitecturetoincorporatebothglobalandlocalinformation.
CNN-based-Encoderextractedenrichedspatialinformationfromtheinputmedicalimage.Moreover,thetransformer
captured the long-range dependency from the extracted features from the CNN-based-Encoder. The learned repre-
sentation from the transformer are incorporated into the CNN-based-Decoder to restore them and reconstructed the
standard PET image. Extensive experiments demonstrated that their model outperformed the state-of-the-art on real
human brain dataset.
6.5.3. Magnetic Resonance Imaging (MRI) Scans:
Medical Image Reconstruction MRI is a prevalent non-invasive imaging technique but its acquisition process is
slow.Consequently,thereisaneedtodevelopacceleratedMRImethods.Simultaneously,severalstudiesondeepneural
networkshavebeenconductedtodevelopstate-of-the-artmethodsforacceleratedMRI.Therefore,Korkmazetal.[3]
accelerated MRI by reconstructing full-sampled MRI images using unsupervised learning incorporating deep image
prior framework to alleviate the problem of under-sampled acquisition. They proposed generative vision transformer
basedunsupervisedMRIreconstructionarchitecturetoincreasethereceptivefield.Firstly,theyperformedgenerative
non-linear mapping over latent and noisy space to improve the invertibility of the model. Secondly, they used cross-
attention to improve the context, i.e. both global and local context, of image and latent features. They did not use
anypre-trainedmodel.Lastly,theyperformedinferenceoneachindividualsubjecttoincreasethegeneralization.They
performedextensiveexperimentsonacceleratedmulti-contrastbrainMRIdatasetandoutperformedtheconvolutional-
based generative models for PSNR and SSIM metrics.
Wang et al. [236] proposed a super-resolution approach to reconstruct the high resolution MRI scans from low
resolution scans. They proposed adjacent slices feature transformer (ASFT) network. Firstly, they incorporated extra
slices in the consecutive in-plane slices of an-isotropic 3D MRI images. Secondly, to harness the similarity between
theconsecutiveslicestheyintroducedamulti-branchfeaturestransformationandextraction(MFTE)block.Thirdly,to
enrichthetargethighresolutionsliceswiththeinformationfromthelowresolutionreferenceslicestheyfilteredoutthe
uselessinformationusingMFTEblock.Moreover,theyusedspatialadaptiveblocktorefinethefeaturesspatially.They
used channel attention to incorporate the multi-scale features and consequently, they enhanced the super resolution.
Their model achieved the state-of-the-art performance for super-resolution task.
Medical Image Synthesis Tissue morphology information acquired from multimodal medical images play an
important role in the clinical practice. However, it is not commonly used because of the expensive capturing of these
information. Generative models such as generative adversarial network (GAN) are in practice to artificially synthesis
theseimages.GANisaCNNbasedarchitecturethatshowslocalitybiasandspatialinvarianceacrossallthepositions.
Huetal.[237]introducedatransformerbaseddouble-scaledeeplearningarchitectureforcross-modalmedicalimage
synthesis to incorporate log-range dependencies Double-scale GAN showed efficient performance on benchmark IXI
MRI dataset.
MedicalImageRegistration Tissuedeformationinahighlydeformableanatomyisestimatedusingimageregistra-
tion.Diffeomorphicregistrationisoneoftheimageregistrationtechniquesthatpreservestheinvertibleandonetoone
mappingbetweenimages.Currentdeeplearningtechniqueslacktheabilitytohandlelongrangerelevancethuslimiting
the meaningful contextual correspondence between images. In the paper [238] a dual transformer network (DTN)
modelisproposedforthediffeomorphicregistrationofMRimages.DTN,usingself-attentionmechanisms,modelthe
relevance from both the separate and concatenated images embeddings, which facilitate contextual correspondence
between anatomies. DTN consists of learnable embedding module, relevance learning module and registration field
inferencemodule.Diffeomorphicregistrationfieldisestimatedusingmovingfixedandmovingimagesforone-to-one
mapping. DTN has two branches to learn the relevance based on the embeddings of separate one-channel images
andconcatenatedtwo-channelimages.FirsttheLow-levelimagefeaturefortheconcatenatedandseparateimagesare
extracted using CNN. Second, the image features, converted into sequences, are fed to DTN for feature enhancement,
basedonglobalcorrespondence.Concatenatedfeaturesformbothbranchesarethenusedtoinferthevelocityfieldand
registration filed. The deformation field is represented as the exponential of the velocity which ensure the invertible
mapping. Metric space is used to optimize the proposed DTN in an unsupervised manner.
. : Page 44 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
6.5.4. Endoscopy Images
MedicalImageReconstruction Endoscopeisaminimallyinvasivemedicalimagingmodality.Itassistsinmedical
diagnoses by acquiring accurate images of the internal organs of a patientâ€™s body. However, the small imaging sensor
in the endoscope causes problems in acquiring the high magnified blood vessels images. Many traditional methods
incorporatinginterpolatedmethodsanddeeplearninghaveapplicationsinreconstructionofsuper-resolutionofsingle
images. Mainly convolution-based deep learning methods are there in reconstructing high-resolution images which
have problems in capturing the global context. Consequently, Gu et al. [239] leveraged transformers and proposed
hybrid architecture with the convolutional neural network to enhance the texture of blood vessels. Firstly, CNN
extracted the low-level features from low-resolution (LR) images and the transformer sampled the LR image into
threedifferentsamplestoextracttexturefromtheimage.Secondly,similaritywasexaminedbetweendifferentfeatures
extracted from transformer-based extractor and CNN-based extractor. Thirdly, they employed a texture migration
method to interchange the information between multi-scale features extracted from the transformer and CNN to
synthesizetheimage.Lastly,sub-pixelconvolutionoperationwasperformedonmigratedbasicimagestosynthesizea
high-resolution.TheirmodelacquiredpromisingqualitativeandquantitativeresultsthantheCNNbasedsingleimage
super-resolution methods.
6.5.5. Fluorescence Microscopy
Quantitative Characterization of Anatomical Structure using Combination of Markers in Bone Marrow
VasculatureandFetalLiverTissues Fluorescencemicroscopyisavariantoftraditionalmicroscopy,whichusesa
higherintensitylightsourcethatexcitesafluorescentspeciesinasampleofinterest.Itisusedfordifferentpurposessuch
asdetailedexaminationofanatomicallandmarks,cells,andcellularnetworks.AlvaroGomarizetal.[240]proposeda
MarkerSamplingandExcitenetworktoexploitthepotentialofattention-basednetworkonthefluorescencemicroscopy
datasets which are underexplored by the deep learning. The capability of the network is tested by the quantitative
characterization in various datasets of microvessels in fetal liver tissues and bone marrow vasculature in 3D confocal
microscopydatasets.ProposedmodelgivesaconvincingperformancewithF1Scoreof91.2%forsinusoidsand71.2%
for arteries in the liver vasculature dataset.
Denoising of Celullar Microscopy Images for Assisted Augmented Microscopy Deep learning has greatly
assisted augmented microscopy that enable high-quality microscopy images with using costly microscopy apparatus.
Zhengyang Wang et al. [241] proposed global voxel transformer networks (GVTNets) that uses attention operators
to address the limitations in already existing U-Net based neural. Instead of local operator that lack dynamic non-
local information aggregation they used attention operators that allow global receptive field during prediction. They
measured the performance of the model on three existing datasets for three different augmented microscopy tasks.
6.5.6. Histopathology Images
WholeSlideImagescontainrichcontentabouttheanatomicalandmorphologicalcharacteristics.Tobetterdepict
theimagecontent,pathologistsfrequentlyexaminethetagsassociatedwiththeseimages.Sincethistaggingprocessis
labour-intensive, consequently Li et al. [242] first time proposed a Patch Transformer based architecture to automate
the multi-tagging whole slide images process. They incorporated attention mechanism to extract global level features
from the patches of images. They employed multi-tag attention module to build tag on the basis of weight. Extensive
experiments demonstrated that their model outperformed on 4920 WSI for macro and micro F1 metric.
6.5.7. OCT or Fundus Images
Diabetes damages the retina and causes diabetic retinopathy. This disease can lead to vision loss, therefore, early
detectioniscrucial.Variousdeeplearningapproacheshaveautomatedtherecognitionofdiabeticretinopathygrading.
However, Wu et al. [158] proposed vision transformer based architecture to assist the ophthalmologist in recognising
the diabetic retinopathy grade. They divided the fundus images into patches, flatten them to generate sequence, and
thenconvertedthemintolinearandpositionalembeddings.Togeneratethefinalrepresentations,theyfedthepositional
embeddingsintomulti-headattentionlayers.TheirmodeloutperformedtheCNN-basedarchitecturewithanaccuracy
of 91.4%.
Thetable8givenbelowsummarizedtheperformancegainbythereviewedarticlesofthemiscellaneouscategory.
. : Page 45 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Table 8: A list of datasets and performance measures adopted by researchers for Miscellaneous Application in ViT
Modality Publication Dataset Performance Measures
CT ScansWang et al. [6]NIH-AAPM-Mayo Clinic
LDCT Grand Challenge
[119]RMSE = 8.7681
SSIM = 0.9144
RMSE = 21.199+
âˆ’2.054
Zhang et al. [233]NIH-AAPM-Mayo
Clinic LDCT Grand
Challenge[119]SSIM = 0.933+
âˆ’0.012
VIF = 0.144+
âˆ’0.025
PET ScansZhang et al.[234] uPMR790RMSE = 0.0447
PSNR (db) = 27.5321
SSIM = 0.9291
PCC = 0.9899
Luo et al. [235]PSNR = 24.818
Clinical dataset which in-
cludes eight normal control
(NC) subjects.SSIM = 0.986
NMSE = 0.0212
PSNR = 25.249
Clinical dataset which in-
cludes eight mild cognitive
impairment (MCI) subjects.SSIM = 0.987
NMSE = 0.0231
Endoscopy Images
Gu et al. [239]DIVerse 2K resolution high
quality (DIV2K) images
dataset [121]
Set5 PSNR (db) = 31.94
. : Page 46 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
SSIM = 0.8935
Set14 PSNR (db) = 28.11
SSIM = 0.7842
B100 PSNR (db) = 27.54
SSIM = 0.7464
Urban100 PSNR (db) = 25.87
SSIM = 0.7844
Manga109 PSNR (db) = 30.09
SSIM = 0.9077
MRI ScansWang et al. [236]Kirby21 dataset (KKI01 to
KKI05) [120] PSNR = 40.19+
âˆ’2.04
SSIM= 0.9882+
âˆ’0.0034
Dice = 0.91
Tang et al. [238]T1-weighted images (of size
182Ã—218Ã—182)of102drug-
addicts and 10 healthy vol-
unteers.HD = 2.68
ASD = 0.59
IXI dataset
T1, R=4 PSNR = 32.55+
âˆ’1.77
SSIM (%) = 94.58+
âˆ’0.82
Kokmaz et al. [3] T1, R=8 PSNR = 30.28+
âˆ’1.68
SSIM (%) = 91.64+
âˆ’1.42
T2, R=4 PSNR = 32.71+
âˆ’0.73
SSIM (%) = 87.66+
âˆ’1.67
T2, R=8 PSNR = 29.90+
âˆ’0.70
SSIM (%) = 84.03+
âˆ’1.89
. : Page 47 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Hu et al. [237] IXI dataset
T1 PSNR = 34.91+
âˆ’1.00
SSIM = 0.895+
âˆ’0.07
T2 PSNR = 35.34+
âˆ’0.95
SSIM (%) = 0.895+
âˆ’0.07
fMRI ScansCIDEr = 0.741
Zhang et al. [59]fMRI experiments(the vi-
sual stimulus consisted of
2750 natural images from
ImageNet data)ROUGE-L= 0.2009
BLEU= 0.186
Fluorescence
MicroscopyGomariz et al. [240]3D confocal microscopy
datasetsF1 Score = 0.912+
âˆ’3.9
Fundus Images Wu et al. [158]DiabeticRetinopathyDetec-
tion datasetAccuracy (%) = 91.4
Specificity = 0.977
Precision = 0.928
Sensitivity = 0.926
Quadratic weighted kappa
score = 0.935
AUC = 0.986
WSI Li et al. [242]4,920 WSIs provided by a
histopathology service com-
panyMcro F1 = 0.910
Micro F1 = 0.944
7. Discussion and Conclusion
Vision Transformers (ViT) are now one of the hottest topics in the discipline of computer vision because of its
exemplaryperformanceandtremendouspotentialcomparedwithCNNs.AlthoughCNNsarematuredenoughforthe
development of applications that can ensure an efficient and accurate diagnosis. However, in the medical field, where
an inaccurate output might endanger lives, the concept of attention in vision transformers has paved its way for more
precise outcomes. Since ViT models assess the global context of the image along with the interpretability through
. : Page 48 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
the attention module, their performance is more precise than CNNs. A variety of approaches have been proposed in
recentyears,asoutlinedandsummarizedinthisreview,toexploreandutilizethecompetencyofvisiontransformers.
Theseapproachesshowedexcellentperformanceonawiderangeofvisualrecognitiontasks,includingclassification,
lesionsdetection,anatomicalstructuresegmentation,andclinicalreportgeneration.Nevertheless,therealpotentialof
transformersforcomputervisionhasyettobefullyexplored,whichmeansthatsignificantchallengesarestillthereto
be resolved. The following section envisages these challenges as well as provides insights on future prospects.
7.1. Current Trends and Open Challenges
Althoughseveraltransformer-basedmodelshavebeenproposedbyresearcherstoaddressvisualrecognitiontasks,
thesemethodsaresimplytheinitialstepsinthisfieldandthereisstillconsiderableroomforimprovement.Forexample,
the transformer architecture in ViT [19], is based on the standard transformer for NLP [16], although an enhanced
versionparticularlytailoredforCVneedstobeexplored.Furthermore,itisnecessarytoemploytransformerstoother
medical domains such as orthodotics, medical report generation, and phenotype extraction etc., to discover the power
of vision transformers.
In this multidisciplinary study, we provide a comprehensive view of how vision transformers employed medical
imaging modalities in visual recognition tasks for assisted diagnosis. We have reviewed the research articles selected
from top tier journals and conferences in which researchers have proposed excellent frameworks that employed
vision transformers to accelerate the efficiency and performance of already proposed CNN based models. The
Figure 5 is a clear evidence that vision transformers are widely employed in classification and segmentation related
visual recognition tasks whereas their significance in registration related tasks have not been greatly explored. The
miscellaneous studies in this review include the work on enhancing the resolutions for better outcomes, denoising of
CT scans with a low dosage of X-ray radiation, image registration etc. Nevertheless, the undertaken studies are based
ondiscreteapplications,therefore,theyarenotcomparable.Thefieldofresearchisquiteopenatthisstageasbaselines
areavailable.Then,reportgenerationcoveredonly7%whichindicatesanareatoresearchuponasitsapplicationscan
assist both doctors and patients.
Similarly, in the imaging modalities, it can be observed in Figure 6 that 61% of the papers found, are related to
X-rays,andCTscans.Thevisiontransformermodelwasfirstproposedintheyear2020,andmostofthecorresponding
literature available was related to the diagnosis of coronavirus as it was prevailing in the same year. The transformer
models achieved admirable results in regards to COVID-19, however, other domains especially digital pathology,
retinal, breast, etc., require attention as not much literature is available. Also, the literature related to COVID-19 is
eitherclassifyingordetectingthevirus,thus,nostudieswerefoundthatwereperformingsegmentation.Eventhough
MRI produces images with better resolution than CT scans, it covered only 13% of the total modalities. The papers
aremostlyusedforimagesegmentationandhaveproducedrespectableoutcomes.Theseoutcomesarebasedonafew
studies targeting distinct diseases, hence, there is much more to explore.
Inaddition,itwasobservedinclinicalreportgenerationâ€™sreviewthatamongvariousmodalities,thestudiesrelated
to x-ray images were only available. Thus, it means that the data required for report generation is only present in
correspondence to x-ray images. As X-rays are a cost-efficient method for examining various parts of the body, it
is comprehensible that due to more reports and X-ray images available, it was possible to collect enough data for
automaticreportgeneration.Sinceitassistsdoctorsinwritingmedicalreports,thecollectionofcorrespondingimages
and text should be collected to implement automated report generations in other domains.
Next, one of the major limitations regarding medical datasets is that not enough data is available to train a
transformermodel.Thesemodelsrequiredatainhugeamountstoperformwellandadapttogeneralizability.Another
observationisthenon-availabilityofabenchmarkasobservedintheFigure16.Sincealmostallthestudiesarebased
on different datasets, the performances of the models are not comparable. Even if the datasets are the same in some
studies, they are either used for different purposes or are combined with other datasets to form one large repository.
Now, as long as there is no standard dataset for experimenting with different proposed models, research can not take
its step forwards towards development using vision transformers.
. : Page 49 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
Figure 16: Long-tail graphical representation of datasets employed in literature.
7.2. Future Prospects:
In order to propel the development of vision transformers in medical domain, we propose various significant
future directions. One direction is that, several papers in this study have done a comparative analysis of CNNs and
ViT, thus, it can be inferred that transformer models have given better results in terms of efficiency, computational
cost, and accuracy. Since these models have achieve such outcomes where researchers are talking about CNNâ€™s being
replaceable,visiontransformersrequiremoreresearchandimplementationastheyareunravelingapathtowardsmore
resource-efficient models.
The paper demonstrates the significance of using vision transformers on medical images. The future directions of
research involve working with more heterogeneous data sets, and more extensive comparisons with other models to
give validity to the proposed transformer models. Next, the studies, may they be related to any category or modality,
are using different datasets, hence there cannot be a comparative analysis in terms of the proposed vision transformer
models. Therefore, we should work on creating a benchmark dataset.
Similarly, new knowledge emerging in cancer biology and deep learning enabled us to step into this rapidly evolving
domain. Genomic profiling is prevalent; however, it is crucial to correlate cancer genomic and phenotypic data to
fully understand cancer behavior. Cancer phenotype information includes tumor morphology (e.g., histopathologic
diagnosis),laboratoryresults(e.g.,geneamplificationstatus),specifictumorbehaviors(e.g.,metastasis),andresponse
totreatment(e.g.,theeffectofachemotherapeuticagentontumorvolume).Visiontransformerscanalsobeappliedto
medical images in order extract phenotypic information in order to better diagnose cancer related disorders. Another
directionisrelatedtothelimitationregardingcoronaviruscasesbeingdetectedandclassified,butnotbeingsegmented.
The usage of segmenting the virus can help determine the rate of spread of the virus, that is why it should be worked
uponinthefuture.Furthermore,theliteratureforclinicalreportgenerationisrelatedtoX-raysimages,andasdiscussed
abovemostofthevisiontransformersusedforX-rayimagesarebeingusedtodetectcoronavirus.Asperourknowledge,
none of the literature is related to medical report generation for coronavirus, and considering the work that has been
done in this regard, research in medical image captioning can propel towards the application side and assist the
practitioners. Overall, since it covers only 7 % of the total tasks in this study, there is more to explore in terms of
X-raysandotherimagingmodalities.Althoughvisiontransformershaveachievedanothermilestoneforimprovedand
accuratediagnosisinthemedicaldomain,thereisstillroomforimprovementintermsofresourceefficiency.Inother
words, we still have to discover the undiscovered.
. : Page 50 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
7.3. Conclusion
Thepaperenvisagesthesummaryanalysisofvisiontransformermodelsusedinthemedicaldomain.Thisstudywill
serve researchers from multidisciplinary backgrounds. The visual recognition tasks considered in this paper include;
classification,detection,segmentation,andclinicalreportgenerations.Otherthanthat,somemiscellaneoustaskswere
also included such as image registration, image reconstruction, etc. Furthermore, medical imaging modalities such as
X-rays,CTScans,MRIs,etc.wereusedasinputtorecognizemedicalimagingthroughvariousmodels.Ourgoalisto
presentthestudyinawaythatapprehendsthestatusandfuturedirectionsofvisiontransformers.Hence,westructured
the information of datasets, categories, modalities, and their results in a tabular form which will assist researchers to
move forward in the medical field conveniently.
References
[1] Isabella Castiglioni, Leonardo Rundo, Marina Codari, Giovanni Di Leo, Christian Salvatore, Matteo Interlenghi, Francesca Gallivanone,
Andrea Cozzi, Natascha Claudia Dâ€™Amico, and Francesco Sardanelli. Ai applications to medical images: From machine learning to deep
learning. Physica Medica , 83:9â€“24, 2021.
[2] S Kevin Zhou, Hayit Greenspan, Christos Davatzikos, James S Duncan, Bram Van Ginneken, Anant Madabhushi, Jerry L Prince, Daniel
Rueckert, and Ronald M Summers. A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with
progress highlights, and future promises. Proceedings of the IEEE , 2021.
[3] YilmazKorkmaz,MahmutYurt,SalmanUlHassanDar,MuzafferÃ–zbey,andTolgaCukur. Deepmrireconstructionwithgenerativevision
transformers. In International Workshop on Machine Learning for Medical Image Reconstruction , pages 54â€“64. Springer, 2021.
[4] James A Diao, Jason K Wang, Wan Fung Chui, Victoria Mountain, Sai Chowdary Gullapally, Ramprakash Srinivasan, Richard N Mitchell,
Benjamin Glass, Sara Hoffman, Sudha K Rao, et al. Human-interpretable image features derived from densely mapped cancer pathology
slides predict diverse molecular phenotypes. Nature communications , 12(1):1â€“15, 2021.
[5] GeorgiosAKaissis,MarcusRMakowski,DanielRÃ¼ckert,andRickmerFBraren. Secure,privacy-preservingandfederatedmachinelearning
in medical imaging. Nature Machine Intelligence , 2(6):305â€“311, 2020.
[6] Dayang Wang, Zhan Wu, and Hengyong Yu. Ted-net: Convolution-free t2t vision transformer-based encoder-decoder dilation network for
low-dose ct denoising. In International Workshop on Machine Learning in Medical Imaging , pages 416â€“425. Springer, 2021.
[7] Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance. Journal of Big Data , 6(1):1â€“54, 2019.
[8] Nima Tajbakhsh, Laura Jeyaseelan, Qian Li, Jeffrey N Chiang, Zhihao Wu, and Xiaowei Ding. Embracing imperfect datasets: A review of
deep learning solutions for medical image segmentation. Medical Image Analysis , 63:101693, 2020.
[9] DandiYang, CristhianMartinez,LaraVisuÃ±a, HardevKhandhar,Chintan Bhatt,andJesusCarretero. Detectionand analysisofcovid-19 in
medical images using deep learning techniques. Scientific Reports , 11(1):1â€“13, 2021.
[10] Chen Li, Hao Chen, Xiaoyan Li, Ning Xu, Zhijie Hu, Dan Xue, Shouliang Qi, He Ma, Le Zhang, and Hongzan Sun. A review for cervical
histopathology image analysis using machine vision approaches. Artificial Intelligence Review , 53(7):4821â€“4862, 2020.
[11] Chaoran Yu and Ernest Johann Helwig. The role of ai technology in prediction, diagnosis and treatment of colorectal cancer. Artificial
Intelligence Review , 55(1):323â€“343, 2022.
[12] GhulamMurtaza,LiyanaShuib,AinuddinWahidAbdulWahab,GhulamMujtaba,HenryFridayNweke,MohammedAliAl-garadi,Fariha
Zulfiqar,GhulamRaza,andNorAnizaAzmi. Deeplearning-basedbreastcancerclassificationthroughmedicalimagingmodalities:stateof
the art and research challenges. Artificial Intelligence Review , 53(3):1655â€“1720, 2020.
[13] InÃªs Domingues, GisÃ¨le Pereira, Pedro Martins, Hugo Duarte, JoÃ£o Santos, and Pedro Henriques Abreu. Using deep learning techniques in
medical imaging: a systematic review of applications on ct and pet. Artificial Intelligence Review , 53(6):4093â€“4160, 2020.
[14] GeertLitjens,ThijsKooi,BabakEhteshamiBejnordi,ArnaudArindraAdiyosoSetio,FrancescoCiompi,MohsenGhafoorian,JeroenA.W.M.
van der Laak, Bram van Ginneken, and Clara I. SÃ¡nchez. A survey on deep learning in medical image analysis. Medical Image Analysis ,
42:60â€“88, 2017.
[15] AsifullahKhan,AnabiaSohail,UmmeZahoora,andAqsaSaeedQureshi. Asurveyoftherecentarchitecturesofdeepconvolutionalneural
networks. Artificial intelligence review , 53(8):5455â€“5516, 2020.
[16] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ÅukaszKaiser,andIlliaPolosukhin. Attention
is all you need. Advances in neural information processing systems , 30, 2017.
[17] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio.Neuralmachinetranslationbyjointlylearningtoalignandtranslate. arXivpreprint
arXiv:1409.0473 , 2014.
[18] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural machine translation. arXiv
preprint arXiv:1508.04025 , 2015.
[19] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,Matthias
Minderer,GeorgHeigold,SylvainGelly,etal. Animageisworth16x16words:Transformersforimagerecognitionatscale. arXivpreprint
arXiv:2010.11929 , 2020.
[20] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision:
A survey. ACM Computing Surveys (CSUR) , 2021.
[21] KaiHan,YunheWang,HantingChen,XinghaoChen,JianyuanGuo,ZhenhuaLiu,YehuiTang,AnXiao,ChunjingXu,YixingXu,etal. A
survey on vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2022.
[22] NIH. National institutes of health, us. https://www.nibib.nih.gov/science-education/science-topics .
. : Page 51 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[23] Bin Guan, Guoshan Zhang, Jinkun Yao, Xinbo Wang, and Mengxuan Wang. Arm fracture detection in x-rays based on improved deep
convolutional neural network. Computers & Electrical Engineering , 81:106530, 2020.
[24] Amit Kumar Jaiswal, Prayag Tiwari, Sachin Kumar, Deepak Gupta, Ashish Khanna, and Joel J.P.C. Rodrigues. Identifying pneumonia in
chest x-rays: A deep learning approach. Measurement , 145:511â€“518, 2019.
[25] Naveed Chouhan, Asifullah Khan, Jehan Zeb Shah, Mazhar Hussnain, and Muhammad Waleed Khan. Deep convolutional neural network
and emotional learning based breast cancer detection using digital mammography. Computers in Biology and Medicine , 132:104318, 2021.
[26] Ademola Enitan Ilesanmi, Utairat Chaumrattanakul, and Stanislav S Makhanov. A method for segmentation of tumors in breast ultrasound
images using the variant enhanced deep learning. Biocybernetics and Biomedical Engineering , 41(2):802â€“818, 2021.
[27] Lifang Chen, Tengfei Mao, and Qian Zhang. Identifying cardiomegaly in chest x-rays using dual attention network. Applied Intelligence ,
pages 1â€“10, 2022.
[28] PearlMarySamuelandThanikaiselvanVeeramalai. Vsscnet:vesselspecificskipchainconvolutionalnetworkforbloodvesselsegmentation.
Computer methods and programs in biomedicine , 198:105769, 2021.
[29] AbdelbakiSouid,NizarSakli,andHediSakli. Classificationandpredictionsoflungdiseasesfromchestx-raysusingmobilenetv2. Applied
Sciences, 11(6):2751, 2021.
[30] Abdul Qayyum, Imran Razzak, M Tanveer, and Ajay Kumar. Depth-wise dense neural network for automatic covid19 infection detection
and diagnosis. Annals of operations research , pages 1â€“21, 2021.
[31] UM Prakash, Kottilingam Kottursamy, Korhan Cengiz, Utku Kose, and Bui Thanh Hung. 4x-expert systems for early prediction of
osteoporosis using multi-model algorithms. Measurement , 180:109543, 2021.
[32] NeslihanBayramoglu,MiikaTNieminen,andSimoSaarakkala. Machinelearningbasedtextureanalysisofpatellafromx-raysfordetecting
patellofemoral osteoarthritis. International journal of medical informatics , 157:104627, 2022.
[33] AtÄ±f Emre YÃ¼ksel, Sadullah GÃ¼ltekin, Enis Simsar, Åžerife Damla Ã–zdemir, Mustafa GÃ¼ndoÄŸar, Salih BarkÄ±n TokgÃ¶z, and Ä°brahim Ethem
HamamcÄ±. Dental enumeration and multiple treatment detection on panoramic x-rays using deep learning. Scientific reports , 11(1):1â€“10,
2021.
[34] Rima Arnaout, Lara Curran, Yili Zhao, Jami C Levine, Erin Chinn, and Anita J Moon-Grady. An ensemble of neural networks provides
expert-level prenatal detection of complex congenital heart disease. Nature medicine , 27(5):882â€“891, 2021.
[35] HanemEllethy,ShekharSChandra,andFatimaANasrallah. Thedetectionofmildtraumaticbraininjuryinpaediatricsusingartificialneural
networks. Computers in Biology and Medicine , 135:104614, 2021.
[36] Marcin WoÅºniak, Jakub SiÅ‚ka, and MichaÅ‚ Wieczorek. Deep neural network correlation learning mechanism for ct brain tumor detection.
Neural Computing and Applications , pages 1â€“16, 2021.
[37] DhimanDas,KathyayiniSivasubramanian,PraveenbalajiRajendran,andManojitPramanik.Label-freehighframerateimagingofcirculating
blood clots using a dual modal ultrasound and photoacoustic system. Journal of Biophotonics , 14(3):e202000371, 2021.
[38] Jordan Chamberlin, Madison R Kocher, Jeffrey Waltz, Madalyn Snoddy, Natalie FC Stringer, Joseph Stephenson, Pooyan Sahbaee, Puneet
Sharma, Saikiran Rapaka, U Joseph Schoepf, et al. Automated detection of lung nodules and coronary artery calcium using artificial
intelligence on low-dose ct scans for lung cancer screening: accuracy and prognostic value. BMC medicine , 19(1):1â€“14, 2021.
[39] J Akilandeswari, G Jothi, A Naveenkumar, RS Sabeenian, P Iyyanar, and ME Paramasivam. Detecting pulmonary embolism using deep
neural networks. International Journal of Performability Engineering , 17(3), 2021.
[40] Adel Oulefki, Sos Agaian, Thaweesak Trongtirakul, and Azzeddine Kassah Laouar. Automatic covid-19 lung infected region segmentation
and measurement using ct-scans images. Pattern recognition , 114:107747, 2021.
[41] SumitaMondal,AnupKSadhu,andPranabKumarDutta. Adaptivelocalternarypatternonparameteroptimized-fasterregionconvolutional
neural network for pulmonary emphysema diagnosis. IEEE Access , 9:114135â€“114152, 2021.
[42] AAO. American academy of ophthalmology. https://www.aao.org/ .
[43] Gabriella Moraes, Dun Jack Fu, Marc Wilson, Hagar Khalid, Siegfried K Wagner, Edward Korot, Daniel Ferraz, Livia Faes, Christopher J
Kelly,TerrySpitz,etal. Quantitativeanalysisofoctforneovascularage-relatedmaculardegenerationusingdeeplearning. Ophthalmology ,
128(5):693â€“705, 2021.
[44] GahyungRyu,KyungminLee,DonggeunPark,SangHyunPark,andMinSagong. Adeeplearningmodelforidentifyingdiabeticretinopathy
using optical coherence tomography angiography. Scientific reports , 11(1):1â€“9, 2021.
[45] Shotaro Asano, Ryo Asaoka, Hiroshi Murata, Yohei Hashimoto, Atsuya Miki, Kazuhiko Mori, Yoko Ikeda, Takashi Kanamoto, Junkichi
Yamagami, and Kenji Inoue. Predicting the central 10 degrees visual field in glaucoma by applying a deep learning algorithm to optical
coherence tomography images. Scientific Reports , 11(1):1â€“10, 2021.
[46] Esther Parra-Mora, Alex CazaÃ±as-Gordon, Rui ProenÃ§a, and LuÃ­s A da Silva Cruz. Epiretinal membrane detection in optical coherence
tomography retinal images using deep learning. IEEE Access , 9:99201â€“99219, 2021.
[47] Zhenhua Wang, Yuanfu Zhong, Mudi Yao, Yan Ma, Wenping Zhang, Chaopeng Li, Zhifu Tao, Qin Jiang, and Biao Yan. Automated
segmentation of macular edema for the diagnosis of ocular disease using deep learning method. Scientific Reports , 11(1):1â€“12, 2021.
[48] SyedAlEHassan,ShahzadAkbar,SaharGull,AmjadRehman,andHindAlaska. Deeplearning-basedautomaticdetectionofcentralserous
retinopathyusingopticalcoherencetomographicimages. In 20211stInternationalConferenceonArtificialIntelligenceandDataAnalytics
(CAIDA), pages 206â€“211. IEEE, 2021.
[49] M Kashif Yaqoob, Syed Farooq Ali, Irfan Kareem, and Muhammad Moazam Fraz. Feature-based optimized deep residual network
architecture for diabetic retinopathy detection. In 2020 IEEE 23rd International Multitopic Conference (INMIC) , pages 1â€“6. IEEE, 2020.
[50] Shumpei Obata, Yusuke Ichiyama, Masashi Kakinoki, Osamu Sawada, Yoshitsugu Saishin, Taku Ito, Mari Tomioka, and Masahito Ohji.
Predictionofpostoperativevisualacuityaftervitrectomyformacularholeusingdeeplearningâ€“basedartificialintelligence. Graefeâ€™sArchive
for Clinical and Experimental Ophthalmology , pages 1â€“11, 2021.
[51] MichaelAbrÃ moffandChristineN.Kay. Chapter6-imageprocessing. InStephenJ.Ryan,SriniVasR.Sadda,DavidR.Hinton,AndrewP.
Schachat, SriniVas R. Sadda, C.P. Wilkinson, Peter Wiedemann, and Andrew P. Schachat, editors, Retina (Fifth Edition) , pages 151â€“176.
. : Page 52 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
W.B. Saunders, London, fifth edition edition, 2013.
[52] Quang TM Pham, Sangil Ahn, Jitae Shin, and Su Jeong Song. Generating future fundus images for early age-related macular degeneration
based on generative adversarial networks. Computer Methods and Programs in Biomedicine , page 106648, 2022.
[53] Sufian A Badawi, Muhammad Moazam Fraz, Muhammad Shehzad, Imran Mahmood, Sajid Javed, Emad Mosalam, and Ajay Kamath
Nileshwar. Detection and grading of hypertensive retinopathy using vessels tortuosity and arteriovenous ratio. Journal of Digital Imaging ,
pages 1â€“21, 2022.
[54] HariMohanRaiandKalyanChatterjee. 2dmriimageanalysisandbraintumordetectionusingdeeplearningcnnmodelleu-net. Multimedia
Tools and Applications , 80(28):36111â€“36141, 2021.
[55] ZamirMerali,JustinZWang,JetanHBadhiwala,ChristopherDWitiw,JeffersonRWilson,andMichaelGFehlings. Adeeplearningmodel
for detection of cervical spinal cord compression in mri scans. Scientific reports , 11(1):1â€“11, 2021.
[56] SaeedaNaz,AbidaAshraf,andAhmadZaib. Transferlearningusingfreezefeaturesforalzheimerneurologicaldisorderdetectionusingadni
dataset.Multimedia Systems , 28(1):85â€“94, 2022.
[57] Mei Yang, Yiming Zheng, Zhiying Xie, Zhaoxia Wang, Jiangxi Xiao, Jue Zhang, and Yun Yuan. A deep learning model for diagnosing
dystrophinopathies on thigh muscle mri images. BMC neurology , 21(1):1â€“9, 2021.
[58] Mazhar Javed Awan, Mohd Shafry Mohd Rahim, Naomie Salim, Mazin Abed Mohammed, Begonya Garcia-Zapirain, and Karrar Hameed
Abdulkareem. Efficient detection of knee anterior cruciate ligament from magnetic resonance imaging using deep learning approach.
Diagnostics , 11(1):105, 2021.
[59] Jiang Zhang, Chen Li, Ganwanming Liu, Min Min, Chong Wang, Jiyi Li, Yuting Wang, Hongmei Yan, Zhentao Zuo, Wei Huang, et al. A
cnn-transformerhybridapproachfordecodingvisualneuralactivityintotext. ComputerMethodsandProgramsinBiomedicine ,214:106586,
2022.
[60] Radiologyinfo.org. Radiologyinfo.org for patients. https://www.radiologyinfo.org/ .
[61] ZhenWang,GuangxuLi,JingjieZhou,andPhilipO.Ogunbona. Opticalflownetworksforheartbeatestimationin4dultrasoundimages. In
2021 7th International Conference on Computing and Artificial Intelligence , pages 127â€“131, 2021.
[62] Yuyu Guo, Lei Bi, Euijoon Ahn, Dagan Feng, Qian Wang, and Jinman Kim. A spatiotemporal volumetric interpolation network for 4d
dynamic medical image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 4726â€“4735,
2020.
[63] Anil V Parwani. Whole Slide Imaging: Current Applications and Future Directions . Springer Nature, 2021.
[64] Muhammad Shaban, Syed Ali Khurram, Muhammad Moazam Fraz, Najah Alsubaie, Iqra Masood, Sajid Mushtaq, Mariam Hassan, Asif
Loya, and Nasir M Rajpoot. A novel digital score for abundance of tumour infiltrating lymphocytes predicts disease free survival in oral
squamous cell carcinoma. Scientific reports , 9(1):1â€“13, 2019.
[65] SajidJaved,ArifMahmood,MuhammadMoazamFraz,NavidAlemiKoohbanani,KsenijaBenes,Yee-WahTsang,KatherineHewitt,David
Epstein,DavidSnead,andNasirRajpoot.Cellularcommunitydetectionfortissuephenotypingincolorectalcancerhistologyimages. Medical
image analysis , 63:101696, 2020.
[66] RM Saad Bashir, Hanya Mahmood, Muhammad Shaban, Shan E Ahmed Raza, M Moazam Fraz, Syed Ali Khurram, and Nasir M Rajpoot.
Automated grade classification of oral epithelial dysplasia using morphometric analysis of histology images. In Medical Imaging 2020:
Digital Pathology , volume 11320, page 1132011. International Society for Optics and Photonics, 2020.
[67] Yanning Zhou, Simon Graham, Navid Alemi Koohbanani, Muhammad Shaban, Pheng-Ann Heng, and Nasir Rajpoot. Cgc-net: Cell graph
convolutional network for grading of colorectal cancer histology images. In Proceedings of the IEEE/CVF International Conference on
Computer Vision Workshops , pages 0â€“0, 2019.
[68] MuhammadShaban,RuqayyaAwan,MuhammadMoazamFraz,AyeshaAzam,Yee-WahTsang,DavidSnead,andNasirMRajpoot.Context-
aware convolutional neural network for grading of colorectal cancer histology images. IEEE Transactions on Medical Imaging , 2020.
[69] MMFraz,SAKhurram,SGraham,MShaban,MHassan,ALoya,andNMRajpoot.Fabnet:featureattention-basednetworkforsimultaneous
segmentationofmicrovesselsandnervesinroutinehistologyimagesoforalcancer. NeuralComputingandApplications ,pages1â€“14,2019.
[70] MMFraz,MuhammadShaban,SimonGraham,SyedAliKhurram,andNasirMRajpoot.Uncertaintydrivenpoolingnetworkformicrovessel
segmentation in routine histology images. In Computational pathology and ophthalmic medical image analysis , pages 156â€“164. Springer,
2018.
[71] SNRashid,MMFraz,andSJaved. Multiscaledilatedunetforsegmentationofmulti-organnucleiindigitalhistologyimages. In 2020IEEE
17thInternationalConferenceonSmartCommunities:ImprovingQualityofLifeUsingICT,IoTandAI(HONET) ,pages68â€“72.IEEE,2020.
[72] Moritz Schwyzer, Daniela A Ferraro, Urs J Muehlematter, Alessandra Curioni-Fontecedro, Martin W Huellner, Gustav K Von Schulthess,
Philipp A Kaufmann, Irene A Burger, and Michael Messerli. Automated detection of lung cancer at ultralow dose pet/ct by deep neural
networksâ€“initial results. Lung Cancer , 126:170â€“173, 2018.
[73] Benjamin H Kann, Sanjay Aneja, Gokoulakrichenane V Loganadane, Jacqueline R Kelly, Stephen M Smith, Roy H Decker, James B Yu,
HenrySPark,WendellGYarbrough,AjayMalhotra,etal.Pretreatmentidentificationofheadandneckcancernodalmetastasisandextranodal
extension using deep learning neural networks. Scientific reports , 8(1):1â€“11, 2018.
[74] Kobra Etminani, Amira Soliman, Anette Davidsson, Jose R Chang, BegoÃ±a MartÃ­nez-Sanchis, Stefan Byttner, Valle Camacho, Matteo
Bauckneht, Roxana Stegeran, Marcus Ressner, et al. A 3d deep learning model to predict the diagnosis of dementia with lewy bodies,
alzheimerâ€™sdisease,andmildcognitiveimpairmentusingbrain18f-fdgpet. Europeanjournalofnuclearmedicineandmolecularimaging ,
49(2):563â€“584, 2022.
[75] RikiyaYamashita,MizuhoNishio,RichardKinhGianDo,andKaoriTogashi. Convolutionalneuralnetworks:anoverviewandapplication
in radiology. Insights into imaging , 9(4):611â€“629, 2018.
[76] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in
neural information processing systems , 25, 2012.
. : Page 53 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[77] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556 , 2014.
[78] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and
Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 1â€“9, 2015.
[79] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pages 770â€“778, 2016.
[80] Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1492â€“1500, 2017.
[81] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pages 7132â€“7141, 2018.
[82] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings
of the IEEE conference on computer vision and pattern recognition , pages 4700â€“4708, 2017.
[83] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on
machine learning , pages 6105â€“6114. PMLR, 2019.
[84] Hafiz Syed Ahmed Qasim, Muhammad Shahzad, and Muhammad Moazam Fraz. Deep learning for face detection: Recent advancements.
In2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2) , pages 1â€“6. IEEE, 2021.
[85] Muhammad Arslan Hashmi, Qaiser Riaz, Muhammad Zeeshan, Muhammad Shahzad, and Muhammad Moazam Fraz. Motion reveal
emotions: identifying emotions from human walk using chest mounted smartphone. IEEE Sensors Journal , 20(22):13511â€“13522, 2020.
[86] WeiWangandJianxunGang. Applicationofconvolutionalneuralnetworkinnaturallanguageprocessing. In 2018InternationalConference
on Information Systems and Computer Aided Education (ICISCAE) , pages 64â€“70. IEEE, 2018.
[87] Qi Zhan, Wenjin Wang, and Gerard de Haan. Analysis of cnn-based remote-ppg to understand limitations and sensitivities. Biomedical
optics express , 11(3):1268â€“1283, 2020.
[88] ArnaudArindraAdiyosoSetio,AlbertoTraverso,ThomasDeBel,MoiraSNBerens,CasVanDenBogaard,PiergiorgioCerello,HaoChen,
Qi Dou, Maria Evelina Fantacci, Bram Geurts, et al. Validation, comparison, and combination of algorithms for automatic detection of
pulmonary nodules in computed tomography images: the luna16 challenge. Medical image analysis , 42:1â€“13, 2017.
[89] Samuel G Armato III, Geoffrey McLennan, Luc Bidaut, Michael F McNitt-Gray, Charles R Meyer, Anthony P Reeves, Binsheng Zhao,
Denise R Aberle, Claudia I Henschke, Eric A Hoffman, et al. The lung image database consortium (lidc) and image database resource
initiative (idri): a completed reference database of lung nodules on ct scans. Medical physics , 38(2):915â€“931, 2011.
[90] LaugeSorensen,SaherBShaker,andMarleenDeBruijne. Quantitativeanalysisofpulmonaryemphysemausinglocalbinarypatterns. IEEE
transactions on medical imaging , 29(2):559â€“569, 2010.
[91] XingyiYang,XuehaiHe,JinyuZhao,YichenZhang,ShanghangZhang,andPengtaoXie. Covid-ct-dataset:actscandatasetaboutcovid-19.
arXiv preprint arXiv:2003.13865 , 2020.
[92] PlamenAngelovandEduardoAlmeidaSoares.Sars-cov-2ct-scandataset:Alargedatasetofrealpatientsctscansforsars-cov-2identification.
MedRxiv, 2020.
[93] DimitriosKollias,AnastasiosArsenos,LevonSoukissian,andStefanosKollias. Mia-cov19d:Covid-19detectionthrough3-dchestctimage
analysis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 537â€“544, 2021.
[94] Mohammad Rahimzadeh, Abolfazl Attar, and Seyed Mohammad Sakhaei. A fully automated deep learning-based network for detecting
covid-19 from a new and large lung ct scan dataset. Biomedical Signal Processing and Control , 68:102588, 2021.
[95] DanielKermany,KangZhang,andMichaelGoldbaum. Labeledopticalcoherencetomography(oct)andchestx-rayimagesforclassification.
Mendeley Data , 2, 2018.
[96] Maria de la Iglesia VayÃ¡, Jose Manuel Saborit, Joaquim Angel Montell, Antonio Pertusa, Aurelia Bustos, Miguel Cazorla, Joaquin Galant,
Xavier Barber, Domingo Orozco-BeltrÃ¡n, Francisco GarcÃ­a-GarcÃ­a, et al. Bimcv covid-19+: a large annotated dataset of rx and ct images
from covid-19 patients. arXiv preprint arXiv:2006.01174 , 2020.
[97] Unais Sait, Gokul Lal KV, Sunny Prakash Prajapati, Rahul Bhaumik, Tarun Kumar, Sanjana Shivakumar, and Kriti Bhalla. Curated dataset
for covid-19 posterior-anterior chest radiography images (x-rays). Mendeley Data , 3, 2021.
[98] Fathi El-Shafai, Walid; Abd El-Samie. Extensive covid-19 x-ray and ct chest images dataset. Mendeley Data , 3, 2020.
[99] LindaWang,ZhongQiuLin,andAlexanderWong. Covid-net:Atailoreddeepconvolutionalneuralnetworkdesignfordetectionofcovid-19
cases from chest x-ray images. Scientific Reports , 10(1):1â€“12, 2020.
[100] Shirin Hajeb Mohammad Alipour, Hossein Rabbani, and Mohammad Reza Akhlaghi. Diabetic retinopathy grading by digital curvelet
transform. Computational and mathematical methods in medicine , 2012, 2012.
[101] JakobNikolasKather,FGZÃ¶llner,FBianconi,SMMelchers,LRSchad,TGaiser,AMarx,andCAWeis. Collectionoftexturesincolorectal
cancer histology. Zenodo https://doi. org/10 , 5281, 2016.
[102] Siham Tabik, Anabel GÃ³mez-RÃ­os, JosÃ© Luis MartÃ­n-RodrÃ­guez, IvÃ¡n Sevillano-GarcÃ­a, Manuel Rey-Area, David Charte, Emilio Guirado,
Juan-Luis SuÃ¡rez, JuliÃ¡n Luengo, MA Valero-GonzÃ¡lez, et al. Covidgr dataset and covid-sdnet methodology for predicting covid-19 based
on chest x-ray images. IEEE journal of biomedical and health informatics , 24(12):3595â€“3605, 2020.
[103] Prasanna Porwal, Samiksha Pachade, Ravi Kamble, Manesh Kokare, Girish Deshmukh, Vivek Sahasrabuddhe, and Fabrice Meriaudeau.
Indian diabetic retinopathy image dataset (idrid): a database for diabetic retinopathy screening research. Data, 3(3):25, 2018.
[104] HaydenGunraj,AliSabri,DavidKoff,andAlexanderWong. Covid-netct-2:Enhanceddeepneuralnetworksfordetectionofcovid-19from
chest ct images through bigger, more diverse learning. arXiv preprint arXiv:2101.07433 , 2021.
[105] John N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Shaw, Brad A Ozenberger, Kyle Ellrott, Ilya Shmulevich, Chris Sander, and
Joshua M Stuart. The cancer genome atlas pan-cancer analysis project. Nature genetics , 45(10):1113â€“1120, 2013.
. : Page 54 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[106] Li Wang, Dong Nie, Guannan Li, Ã‰lodie Puybareau, Jose Dolz, Qian Zhang, Fan Wang, Jing Xia, Zhengwang Wu, Jia-Wei Chen, et al.
Benchmark on automatic six-month-old infant brain segmentation algorithms: the iseg-2017 challenge. IEEE transactions on medical
imaging, 38(9):2219â€“2230, 2019.
[107] Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S Kirby, John B Freymann, Keyvan Farahani,
andChristosDavatzikos. Advancingthecancergenomeatlasgliomamricollectionswithexpertsegmentationlabelsandradiomicfeatures.
Scientific data , 4(1):1â€“13, 2017.
[108] AdriÃ«nne M Mendrik, Koen L Vincken, Hugo J Kuijf, Marcel Breeuwer, Willem H Bouvy, Jeroen De Bresser, Amir Alansary, Marleen
De Bruijne, Aaron Carass, Ayman El-Baz, et al. Mrbrains challenge: online evaluation framework for brain image segmentation in 3t mri
scans.Computational intelligence and neuroscience , 2015, 2015.
[109] Cathie Sudlow, John Gallacher, Naomi Allen, Valerie Beral, Paul Burton, John Danesh, Paul Downey, Paul Elliott, Jane Green, Martin
Landray, et al. Uk biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age.
PLoS medicine , 12(3):e1001779, 2015.
[110] Colin G Stirrat, Shirjel R Alam, Thomas J MacGillivray, Calum D Gray, Marc R Dweck, Jennifer Raftis, William SA Jenkins, William A
Wallace,RenzoPessotto,KelvinHHLim,etal.Ferumoxytol-enhancedmagneticresonanceimagingassessinginflammationaftermyocardial
infarction. Heart, 103(19):1528â€“1535, 2017.
[111] A Emre Kavur, N Sinem Gezer, Mustafa BarÄ±ÅŸ, Sinem Aslan, Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee,
Philipp Ernst, SavaÅŸ Ã–zkan, et al. Chaos challenge-combined (ct-mr) healthy abdominal organ segmentation. Medical Image Analysis ,
69:101950, 2021.
[112] JoesStaal,MichaelDAbrÃ moff,MeindertNiemeijer,MaxAViergever,andBramVanGinneken. Ridge-basedvesselsegmentationincolor
images of the retina. IEEE transactions on medical imaging , 23(4):501â€“509, 2004.
[113] Junji Shiraishi, Shigehiko Katsuragawa, Junpei Ikezoe, Tsuneo Matsumoto, Takeshi Kobayashi, Ken-ichi Komatsu, Mitate Matsui, Hiroshi
Fujita,YoshieKodera,andKunioDoi.Developmentofadigitalimagedatabaseforchestradiographswithandwithoutalungnodule:receiver
operatingcharacteristicanalysisofradiologistsâ€™detectionofpulmonarynodules. AmericanJournalofRoentgenology ,174(1):71â€“74,2000.
[114] Nicholas Heller, Fabian Isensee, Klaus H Maier-Hein, Xiaoshuai Hou, Chunmei Xie, Fengyi Li, Yang Nan, Guangrui Mu, Zhiyong Lin,
Miofei Han, et al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct imaging: Results of the kits19
challenge. Medical image analysis , 67:101821, 2021.
[115] Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang,
XiaokangWu,FangbingYan,etal. Identifyingmedicaldiagnosesandtreatablediseasesbyimage-baseddeeplearning. Cell,172(5):1122â€“
1131, 2018.
[116] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and
Clement J McDonald. Preparing a collection of radiology examinations for distribution and retrieval. Journal of the American Medical
Informatics Association , 23(2):304â€“310, 2016.
[117] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and
Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. Scientific data , 6(1):1â€“8,
2019.
[118] Baoyu Jing, Pengtao Xie, and Eric Xing. On the automatic generation of medical imaging reports. arXiv preprint arXiv:1711.08195 , 2017.
[119] Baiyu Chen, Xinhui Duan, Zhicong Yu, Shuai Leng, Lifeng Yu, and Cynthia McCollough. Development and validation of an open data
format for ct projection data. Medical physics , 42(12):6964â€“6972, 2015.
[120] BennettALandman,AlanJHuang,AliyaGifford,DeeptiSVikram,IsselAnneLLim,JonathanADFarrell,JohnABogovic,JunHua,Min
Chen, Samson Jarso, et al. Multi-parametric neuroimaging reproducibility: a 3-t resource study. Neuroimage , 54(4):2854â€“2866, 2011.
[121] EirikurAgustssonandRaduTimofte. Ntire2017challengeonsingleimagesuper-resolution:Datasetandstudy. In ProceedingsoftheIEEE
conference on computer vision and pattern recognition workshops , pages 126â€“135, 2017.
[122] Jure Zbontar, Florian Knoll, Anuroop Sriram, Tullie Murrell, Zhengnan Huang, Matthew J Muckley, Aaron Defazio, Ruben Stern, Patricia
Johnson, Mary Bruno, et al. fastmri: An open dataset and benchmarks for accelerated mri. arXiv preprint arXiv:1811.08839 , 2018.
[123] Martin Weigert, Uwe Schmidt, Tobias Boothe, Andreas MÃ¼ller, Alexandr Dibrov, Akanksha Jain, Benjamin Wilhelm, Deborah Schmidt,
Coleman Broaddus, SiÃ¢n Culley, et al. Content-aware image restoration: pushing the limits of fluorescence microscopy. Nature methods ,
15(12):1090â€“1097, 2018.
[124] Mingyu Kim, Jihye Yun, Yongwon Cho, Keewon Shin, Ryoungwoo Jang, Hyun-jin Bae, and Namkug Kim. Deep learning in medical
imaging. Neurospine , 16(4):657, 2019.
[125] Md Shahin Ali, Md Sipon Miah, Jahurul Haque, Md Mahbubur Rahman, and Md Khairul Islam. An enhanced technique of skin cancer
classificationusingdeepconvolutionalneuralnetworkwithtransferlearningmodels. MachineLearningwithApplications ,5:100036,2021.
[126] Jyostna Devi Bodapati, Nagur Shareef Shaik, and Veeranjaneyulu Naralasetti. Composite deep neural network with gated-attention
mechanism for diabetic retinopathy severity classification. Journal of Ambient Intelligence and Humanized Computing , 12(10):9825â€“9839,
2021.
[127] Vinayakumar Ravi, Harini Narasimhan, and Tuan D Pham. Efficientnet-based convolutional neural networks for tuberculosis classification.
InAdvances in Artificial Intelligence, Computation, and Data Science , pages 227â€“244. Springer, 2021.
[128] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.
Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 , 2017.
[129] Muhammad Sufyan Arshad, Usman Abdur Rehman, and Muhammad Moazam Fraz. Plant disease identification using transfer learning. In
2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2) , pages 1â€“5. IEEE, 2021.
[130] TufailSajjadShahHashmi,NazeefUlHaq,MuhammadMoazamFraz,andMuhammadShahzad. Applicationofdeeplearningforweapons
detection in surveillance videos. In 2021 International Conference on Digital Futures and Transformative Technologies (ICoDT2) , pages
1â€“6. IEEE, 2021.
. : Page 55 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[131] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 580â€“587, 2014.
[132] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision , pages 1440â€“1448, 2015.
[133] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks.
Advances in neural information processing systems , 28, 2015.
[134] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings
of the IEEE conference on computer vision and pattern recognition , pages 779â€“788, 2016.
[135] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot
multibox detector. In European conference on computer vision , pages 21â€“37. Springer, 2016.
[136] Lohendran Baskaran, Subhi J Alâ€™Aref, Gabriel Maliakal, Benjamin C Lee, Zhuoran Xu, Jeong W Choi, Sang-Eun Lee, Ji Min Sung, Fay Y
Lin, Simon Dunham, et al. Automatic segmentation of multiple cardiovascular structures from cardiac computed tomography angiography
images using deep learning. PloS one, 15(5):e0232573, 2020.
[137] Kunal Nagpal, Davis Foote, Yun Liu, Po-Hsuan Cameron Chen, Ellery Wulczyn, Fraser Tan, Niels Olson, Jenny L Smith, Arash
Mohtashamian, James H Wren, et al. Development and validation of a deep learning algorithm for improving gleason scoring of prostate
cancer.NPJ digital medicine , 2(1):1â€“10, 2019.
[138] Sufian A Badawi and Muhammad Moazam Fraz. Optimizing the trainable b-cosfire filter for retinal blood vessel segmentation. PeerJ,
6:e5855, 2018.
[139] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International
Conference on Medical image computing and computer-assisted intervention , pages 234â€“241. Springer, 2015.
[140] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation
withdeepconvolutionalnets,atrousconvolution,andfullyconnectedcrfs. IEEEtransactionsonpatternanalysisandmachineintelligence ,
40(4):834â€“848, 2017.
[141] Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on
computer vision , pages 2961â€“2969, 2017.
[142] Ruixin Yang and Yingyan Yu. Artificial convolutional neural network in object detection and semantic segmentation for medical imaging
analysis. Frontiers in Oncology , 11:573, 2021.
[143] EshaPahwa,DwijMehta,SanjeetKapadia,DevanshJain,andAchleshwarLuthra.Medskip:Medicalreportgenerationusingskipconnections
and integrated attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 3409â€“3415, 2021.
[144] Di You, Fenglin Liu, Shen Ge, Xiaoxia Xie, Jing Zhang, and Xian Wu. Aligntransformer: Hierarchical alignment of visual regions and
disease tags for medical report generation. In International Conference on Medical Image Computing and Computer-Assisted Intervention ,
pages 72â€“82. Springer, 2021.
[145] RunyiLi,ZizhouWang,andLeiZhang.Imagecaptionandmedicalreportgenerationbasedondeeplearning:areviewandalgorithmanalysis.
In2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI) , pages 373â€“379. IEEE, 2021.
[146] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of
the IEEE conference on computer vision and pattern recognition , pages 3156â€“3164, 2015.
[147] KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,AaronCourville,RuslanSalakhudinov,RichZemel,andYoshuaBengio. Show,attend
andtell:Neuralimagecaptiongenerationwithvisualattention. In Internationalconferenceonmachinelearning ,pages2048â€“2057.PMLR,
2015.
[148] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 7219â€“7228, 2018.
[149] JustinJohnson,AndrejKarpathy,andLiFei-Fei. Densecap:Fullyconvolutionallocalizationnetworksfordensecaptioning. In Proceedings
of the IEEE conference on computer vision and pattern recognition , pages 4565â€“4574, 2016.
[150] Alexander Mathews, Lexing Xie, and Xuming He. Semstyle: Learning to generate stylised image captions using unaligned text. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8591â€“8600, 2018.
[151] ImranKhurram,MMFraz,MuhammadShahzad,andNasirMRajpoot. Dense-captionnet:asentencegenerationarchitectureforfine-grained
description of image semantics. Cognitive Computation , 13(3):595â€“611, 2021.
[152] MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga. A comprehensive survey of deep learning for image
captioning. ACM Computing Surveys (CsUR) , 51(6):1â€“36, 2019.
[153] TomotakaSobue,NoriyukiMoriyama,MasahiroKaneko,MasahikoKusumoto,ToshiakiKobayashi,RyosukeTsuchiya,RyutaroKakinuma,
HironobuOhmatsu,KanjiNagai,HiroyukiNishiyama,etal.Screeningforlungcancerwithlow-dosehelicalcomputedtomography:anti-lung
cancer association project. Journal of clinical oncology , 20(4):911â€“920, 2002.
[154] WeiZhao,JianchengYang,YingliSun,ChengLi,WeilanWu,LiangJin,ZhimingYang,BingbingNi,PanGao,PeijunWang,etal. 3ddeep
learning from ct scans predicts tumor invasiveness of subcentimeter pulmonary adenocarcinomas. Cancer research , 78(24):6881â€“6889,
2018.
[155] Wei Zhao, Jiancheng Yang, Bingbing Ni, Dexi Bi, Yingli Sun, Mengdi Xu, Xiaoxia Zhu, Cheng Li, Liang Jin, Pan Gao, et al. Toward
automaticpredictionofegfrmutationstatusinpulmonaryadenocarcinomawith3ddeeplearning. Cancermedicine ,8(7):3532â€“3543,2019.
[156] Fangzhou Liao, Ming Liang, Zhe Li, Xiaolin Hu, and Sen Song. Evaluate the malignancy of pulmonary nodules using the 3-d deep leaky
noisy-or network. IEEE transactions on neural networks and learning systems , 30(11):3484â€“3495, 2019.
[157] JianchengYang,HaoranDeng,XiaoyangHuang,BingbingNi,andYiXu. Relationallearningbetweenmultiplepulmonarynodulesviadeep
set attention transformers. In 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI) , pages 1875â€“1878. IEEE, 2020.
[158] Yanan Wu, Shouliang Qi, Yu Sun, Shuyue Xia, Yudong Yao, and Wei Qian. A vision transformer for emphysema classification using ct
images.Physics in Medicine & Biology , 66(24):245016, 2021.
. : Page 56 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[159] Cuong Do and Lan Vu. An approach for recognizing COVID-19 cases using convolutional neural networks applied to CT scan images.
In Andrew G. Tescher and Touradj Ebrahimi, editors, Applications of Digital Image Processing XLIII , volume 11510, pages 719 â€“ 727.
International Society for Optics and Photonics, SPIE, 2020.
[160] HalgurdSMaghdid,ArasTAsaad,KayhanZrarGhafoor,AliSafaaSadiq,SeyedaliMirjalili,andMuhammadKhurramKhan. Diagnosing
covid-19 pneumonia from x-ray and ct images using deep learning and transfer learning algorithms. In Multimodal image exploitation and
learning 2021 , volume 11734, page 117340E. International Society for Optics and Photonics, 2021.
[161] Matteo Polsinelli, Luigi Cinque, and Giuseppe Placidi. A light cnn for detecting covid-19 from ct scans of the chest. Pattern recognition
letters, 140:95â€“100, 2020.
[162] LeiwenFu,BingyiWang,TanweiYuan,XiaotingChen,YunlongAo,ThomasFitzpatrick,PeiyangLi,YiguoZhou,Yi-fanLin,QibinDuan,
et al. Clinical characteristics of coronavirus disease 2019 (covid-19) in china: a systematic review and meta-analysis. Journal of Infection ,
80(6):656â€“665, 2020.
[163] FengPan,TianheYe,PengSun,ShanGui,BoLiang,LingliLi,DandanZheng,JiazhengWang,RichardLHesketh,LianYang,etal. Time
course of lung changes on chest ct during recovery from 2019 novel coronavirus (covid-19) pneumonia. Radiology , 2020.
[164] Ara Abigail E. Ambita, Eujene Nikka V. Boquio, and Prospero C. Naval. Covit-gan: Vision transformer forcovid-19 detection in ct scan
imageswith self-attention gan fordataaugmentation. In Igor FarkaÅ¡, Paolo Masulli, Sebastian Otte, and Stefan Wermter, editors, Artificial
Neural Networks and Machine Learning â€“ ICANN 2021 , pages 587â€“598, Cham, 2021. Springer International Publishing.
[165] Lei Zhang and Yan Wen. A transformer-based framework for automatic covid19 diagnosis in chest cts. In 2021 IEEE/CVF International
Conference on Computer Vision Workshops (ICCVW) , pages 513â€“518, 2021.
[166] Xinggang Wang, Xianbo Deng, Qing Fu, Qiang Zhou, Jiapei Feng, Hui Ma, Wenyu Liu, and Chuansheng Zheng. A weakly-supervised
frameworkforcovid-19classificationandlesionlocalizationfromchestct. IEEEtransactionsonmedicalimaging ,39(8):2615â€“2625,2020.
[167] Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Qi Song, et al. Using artificial
intelligencetodetectcovid-19andcommunity-acquiredpneumoniabasedonpulmonaryct:evaluationofthediagnosticaccuracy. Radiology ,
296(2):E65â€“E71, 2020.
[168] JoelCMThan,PunLiangThon,OmarMohdRijal,RosminahMKassim,AshariYunus,NorlizaMNoor,andPatrickThen.Preliminarystudy
on patch sizes in vision transformers (vit) for covid-19 and diseased lungs classification. In 2021 IEEE National Biomedical Engineering
Conference (NBEC) , pages 146â€“150. IEEE, 2021.
[169] Xiaole Fan, Xiufang Feng, Yunyun Dong, and Huichao Hou. Covid-19 ct image recognition algorithm based on transformer and cnn.
Displays, page 102150, 2022.
[170] Khushal Tyagi, Gaurav Pathak, Rahul Nijhawan, and Ankush Mittal. Detecting pneumonia using vision transformer and comparing with
other techniques. In 2021 5th International Conference on Electronics, Communication and Aerospace Technology (ICECA) , pages 12â€“16.
IEEE, 2021.
[171] LinhTDuong,NhiHLe,ToanBTran,VuongMNgo,andPhuongTNguyen. Detectionoftuberculosisfromchestx-rayimages:boosting
the performance with vision transformer and transfer learning. Expert Systems with Applications , 184:115519, 2021.
[172] AdamBernheim,XueyanMei,MingqianHuang,YangYang,ZahiAFayad,NingZhang,KaiyueDiao,BinLin,XiqiZhu,KunweiLi,etal.
Chest ct findings in coronavirus disease-19 (covid-19): relationship to duration of infection. Radiology , page 200463, 2020.
[173] Soumya Ranjan Nayak, Deepak Ranjan Nayak, Utkarsh Sinha, Vaibhav Arora, and Ram Bilas Pachori. Application of deep learning
techniques for detection of covid-19 cases using chest x-ray images: A comprehensive study. Biomedical Signal Processing and Control ,
64:102365, 2021.
[174] SangjoonPark,GwanghyunKim,YujinOh,JoonBeomSeo,SangMinLee,JinHwanKim,SungjunMoon,Jae-KwangLim,andJongChul
Ye. Multi-task vision transformer using low-level chest x-ray feature corpus for covid-19 diagnosis and severity quantification. Medical
Image Analysis , 75:102299, 2022.
[175] Debaditya Shome, T Kar, Sachi Nandan Mohanty, Prayag Tiwari, Khan Muhammad, Abdullah AlTameem, Yazhou Zhang, and Abdul
Khader Jilani Saudagar. Covid-transformer: Interpretable covid-19 detection using vision transformer for healthcare. International Journal
of Environmental Research and Public Health , 18(21):11086, 2021.
[176] Mohamad Mahmoud Al Rahhal, Yakoub Bazi, Rami M Jomaa, Ahmad AlShibli, Naif Alajlan, Mohamed Lamine Mekhalfi, and Farid
Melgani. Covid-19 detection in ct/x-ray imagery using vision transformers. Journal of Personalized Medicine , 12(2):310, 2022.
[177] Muhammad Aasharib Nawshad, Usama Aleem Shami, Sana Sajid, and Muhammad Moazam Fraz. Attention based residual network for
effectivedetectionofcovid-19andviralpneumonia. In 2021InternationalConferenceonDigitalFuturesandTransformativeTechnologies
(ICoDT2) , pages 1â€“7. IEEE, 2021.
[178] Yin Dai, Yifan Gao, and Fayu Liu. Transmed: Transformers advance multi-modal medical image classification. Diagnostics , 11(8):1384,
2021.
[179] Sharif Amit Kamran, Khondker Fariha Hossain, Alireza Tavakkoli, Stewart Lee Zuckerbrod, and Salah A Baker. Vtgan: Semi-supervised
retinal image synthesis and disease prediction using vision transformers. In Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3235â€“3245, 2021.
[180] Magdy Abd-Elghany Zeid, Khaled El-Bahnasy, and SE Abo-Youssef. Multiclass colorectal cancer histology images classification using
vision transformers. In 2021 Tenth International Conference on Intelligent Computing and Information Systems (ICICIS) , pages 224â€“230.
IEEE, 2021.
[181] Haoyuan Chen, Chen Li, Xiaoyan Li, Md Mamunur Rahaman, Weiming Hu, Yixin Li, Wanli Liu, Changhao Sun, Hongzan Sun, Xinyu
Huang, et al. Il-mcam: An interactive learning and multi-channel attention mechanism-based weakly supervised colorectal histopathology
image classification approach. Computers in Biology and Medicine , page 105265, 2022.
[182] Jingxing Li, Zhanglei Yang, and Yifan Yu. A medical ai diagnosis platform based on vision transformer for coronavirus. In 2021 IEEE
International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI) , pages 246â€“
252. IEEE, 2021.
. : Page 57 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[183] Stefan Jaeger, Sema Candemir, Sameer Antani, YÃ¬-XiÃ¡ng J WÃ¡ng, Pu-Xuan Lu, and George Thoma. Two public chest x-ray datasets for
computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery , 4(6):475, 2014.
[184] Joseph Paul Cohen, Paul Morrison, Lan Dao, Karsten Roth, Tim Q Duong, and Marzyeh Ghassemi. Covid-19 image data collection:
Prospective predictions are the future. arXiv preprint arXiv:2006.11988 , 2020.
[185] Shuang Liang, Weicun Zhang, and Yu Gu. A hybrid and fast deep learning framework for covid-19 detection via 3d chest ct images. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 508â€“512, 2021.
[186] Liyang Chen, Zhiyuan You, Nian Zhang, Juntong Xi, and Xinyi Le. Utrad: Anomaly detection and localization with u-transformer. Neural
Networks, 147:53â€“62, 2022.
[187] Zhijie Lin, Zhaoshui He, Shengli Xie, Xu Wang, Ji Tan, Jun Lu, and Beihai Tan. Aanet: Adaptive attention network for covid-19 detection
from chest x-ray images. IEEE Transactions on Neural Networks and Learning Systems , 32(11):4781â€“4792, 2021.
[188] Arnab Kumar Mondal, Arnab Bhattacharjee, Parag Singla, and AP Prathosh. xvitcos: Explainable vision transformer based covid-19
screening using radiography. IEEE Journal of Translational Engineering in Health and Medicine , 10:1â€“10, 2021.
[189] EmanuelePesce,SamuelJosephWithey,Petros-PavlosYpsilantis,RobertBakewell,VickyGoh,andGiovanniMontana. Learningtodetect
chest radiographs containing pulmonary lesions using visual attention networks. Medical image analysis , 53:26â€“38, 2019.
[190] LizongZhang,ShuxinFeng,GuiduoDuan,YingLi,andGuisongLiu. Detectionofmicroaneurysmsinfundusimagesbasedonanattention
mechanism. Genes, 10(10):817, 2019.
[191] LiuLi,MaiXu,HanruoLiu,YangLi,XiaofeiWang,LaiJiang,ZulinWang,XiangFan,andNingliWang. Alarge-scaledatabaseandacnn
model for attention-based glaucoma detection. IEEE transactions on medical imaging , 39(2):413â€“424, 2019.
[192] Xi Xu, Yu Guan, Jianqiang Li, Zerui Ma, Li Zhang, and Li Li. Automatic glaucoma detection based on transfer induced attention network.
BioMedical Engineering OnLine , 20(1):1â€“19, 2021.
[193] Rodger C Haggitt. Barrettâ€™s esophagus, dysplasia, and adenocarcinoma. Human pathology , 25(10):982â€“993, 1994.
[194] Christopher P Wild and Laura J Hardie. Reflux, barrettâ€™s oesophagus and adenocarcinoma: burning questions. Nature Reviews Cancer ,
3(9):676â€“684, 2003.
[195] NaofumiTomita,BehnazAbdollahi,JasonWei,BingRen,AriefSuriawinata,andSaeedHassanpour. Attention-baseddeepneuralnetworks
for detection of cancerous and precancerous esophagus tissue on histopathological slides. JAMA network open , 2(11):e1914645â€“e1914645,
2019.
[196] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and Faisal Mahmood. Data-efficient and weakly
supervised computational pathology on whole-slide images. Nature biomedical engineering , 5(6):555â€“570, 2021.
[197] Richard J Chen, Ming Y Lu, Wei-Hung Weng, Tiffany Y Chen, Drew FK Williamson, Trevor Manz, Maha Shady, and Faisal Mahmood.
Multimodalco-attentiontransformerforsurvivalpredictioningigapixelwholeslideimages. In ProceedingsoftheIEEE/CVFInternational
Conference on Computer Vision , pages 4015â€“4025, 2021.
[198] YangNing,ShouyiZhang,XiaomingXi,JieGuo,PeideLiu,andCaimingZhang. Cac-emvt:Efficientcoronaryarterycalciumsegmentation
withmulti-scalevisiontransformers. In 2021IEEEInternationalConferenceonBioinformaticsandBiomedicine(BIBM) ,pages1462â€“1467.
IEEE, 2021.
[199] Matthew Chung Hai Lee, Kersten Petersen, Nick Pawlowski, Ben Glocker, and Michiel Schaap. Tetris: Template transformer networks for
image segmentation with shape priors. IEEE transactions on medical imaging , 38(11):2596â€“2606, 2019.
[200] MaxJaderberg,KarenSimonyan,AndrewZisserman,etal.Spatialtransformernetworks. Advancesinneuralinformationprocessingsystems ,
28, 2015.
[201] XiaomengLi,QiDou,HaoChen,Chi-WingFu,XiaojuanQi,DanielLBelav `y,GabrieleArmbrecht,DieterFelsenberg,GuoyanZheng,and
Pheng-AnnHeng. 3dmulti-scalefcnwithrandommodalityvoxeldropoutlearningforintervertebraldisclocalizationandsegmentationfrom
multi-modality mr images. Medical image analysis , 45:41â€“54, 2018.
[202] XiaohangFu,LeiBi,AshnilKumar,MichaelFulham,andJinmanKim. Multimodalspatialattentionmodulefortargetingmultimodalpet-ct
lung tumor segmentation. IEEE Journal of Biomedical and Health Informatics , 25(9):3507â€“3516, 2021.
[203] Giammarco La Barbera, Pietro Gori, Haithem Boussaid, Bruno Belucci, Alessandro Delmonte, Jeanne Goulin, Sabine Sarnacki, Laurence
Rouet, and Isabelle Bloch. Automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric
segmentation. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI) , pages 1773â€“1776. IEEE, 2021.
[204] Daniel H Pak, AndrÃ©s Caballero, Wei Sun, and James S Duncan. Efficient aortic valve multilabel segmentation using a spatial transformer
network. In 2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI) , pages 1738â€“1742. IEEE, 2020.
[205] Chunfeng Lian, Fan Wang, Hannah H Deng, Li Wang, Deqiang Xiao, Tianshu Kuang, Hung-Ying Lin, Jaime Gateno, Steve GF Shen,
Pew-ThianYap,etal. Multi-taskdynamictransformernetworkforconcurrentbonesegmentationandlarge-scalelandmarklocalizationwith
dentalcbct. In InternationalConferenceonMedicalImageComputingandComputer-AssistedIntervention ,pages807â€“816.Springer,2020.
[206] Chun Luo, Jing Zhang, Xinglin Chen, Yinhao Tang, Xiechuan Weng, and Fan Xu. Ucatr: Based on cnn and transformer encoding and
cross-attention decoding for lesion segmentation of acute ischemic stroke in non-contrast computed tomography images. In 2021 43rd
Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC) , pages 3565â€“3568. IEEE, 2021.
[207] OlivierPetit,NicolasThome,ClementRambour,LoicThemyr,TobyCollins,andLucSoler. U-nettransformer:Selfandcrossattentionfor
medical image segmentation. In International Workshop on Machine Learning in Medical Imaging , pages 267â€“276. Springer, 2021.
[208] JienengChen,YongyiLu,QihangYu,XiangdeLuo,EhsanAdeli,YanWang,LeLu,AlanLYuille,andYuyinZhou.Transunet:Transformers
make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306 , 2021.
[209] Yutong Xie, Jianpeng Zhang, Chunhua Shen, and Yong Xia. Cotr: Efficiently bridging cnn and transformer for 3d medical image
segmentation. In Internationalconferenceonmedicalimagecomputingandcomputer-assistedintervention ,pages171â€“180.Springer,2021.
[210] Jianhong Cheng, Jin Liu, Hulin Kuang, and Jianxin Wang. A fully automated multimodal mri-based multi-task learning for glioma
segmentation and idh genotyping. IEEE Transactions on Medical Imaging , 2022.
. : Page 58 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[211] Abhinav Sagar. Vitbis: Vision transformer for biomedical image segmentation. In Clinical Image-Based Procedures, Distributed and
Collaborative Learning, Artificial Intelligence for Combating COVID-19 and Secure and Privacy-Preserving Machine Learning , pages
34â€“45. Springer, 2021.
[212] Qixuan Sun, Nianhua Fang, Zhuo Liu, Liang Zhao, Youpeng Wen, and Hongxiang Lin. Hybridctrm: Bridging cnn and transformer for
multimodal brain image segmentation. Journal of Healthcare Engineering , 2021, 2021.
[213] Jose Dolz, Karthik Gopinath, Jing Yuan, Herve Lombaert, Christian Desrosiers, and Ismail Ben Ayed. Hyperdense-net: a hyper-densely
connected cnn for multi-modal image segmentation. IEEE transactions on medical imaging , 38(5):1116â€“1126, 2018.
[214] MatthewSinclair,AndreasSchuh,KarlHahn,KerstenPetersen,YingBai,JamesBatten,MichielSchaap,andBenGlocker. Atlas-istn:joint
segmentation,registrationandatlasconstructionwithimage-and-spatialtransformernetworks. MedicalImageAnalysis ,page102383,2022.
[215] AgisilaosChartsias,GiorgosPapanastasiou,ChengjiaWang,ScottSemple,DavidENewby,RohanDharmakumar,andSotiriosATsaftaris.
Disentangle,alignandfuseformultimodalandsemi-supervisedimagesegmentation. IEEEtransactionsonmedicalimaging ,40(3):781â€“792,
2020.
[216] Zheyao Gao and Xiahai Zhuang. Consistency based co-segmentation for multi-view cardiac mri using vision transformer. In International
Workshop on Statistical Atlases and Computational Models of the Heart , pages 306â€“314. Springer, 2021.
[217] Dong Sui, Kang Zhang, Weifeng Liu, Jing Chen, Xiaoxuan Ma, and Zhaofeng Tian. Cst: A multitask learning framework for colorectal
cancer region mining based on transformer. BioMed Research International , 2021, 2021.
[218] YiyaoLiu,YiYang,WeiJiang,TianfuWang,andBaiyingLei. 3ddeepattentiveu-netwithtransformerforbreasttumorsegmentationfrom
automated breast volume scanner. In 2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society
(EMBC), pages 4011â€“4014. IEEE, 2021.
[219] HolgerRRoth,HirohisaOda,XiangrongZhou,NatsukiShimizu,YingYang,YuichiroHayashi,MasahiroOda,MichitakaFujiwara,Kazunari
Misawa, and Kensaku Mori. An application of cascaded 3d fully convolutional networks for medical image segmentation. Computerized
Medical Imaging and Graphics , 66:90â€“99, 2018.
[220] Xiliang Zhu, Zhaoyun Cheng, Sheng Wang, Xianjie Chen, and Guoqing Lu. Coronary angiography image segmentation based on pspnet.
Computer Methods and Programs in Biomedicine , 200:105897, 2021.
[221] Yuhang Lu, Kang Zheng, Weijian Li, Yirui Wang, Adam P Harrison, Chihung Lin, Song Wang, Jing Xiao, Le Lu, Chang-Fu Kuo, et al.
Contourtransformernetworkforone-shotsegmentationofanatomicalstructures. IEEEtransactionsonmedicalimaging ,40(10):2672â€“2684,
2020.
[222] GuifangZhang,Hon-ChengWong,ChengWang,JianjunZhu,LigongLu,andGaojunTeng.Atemporarytransformernetworkforguide-wire
segmentation. In 202114thInternationalCongressonImageandSignalProcessing,BioMedicalEngineeringandInformatics(CISP-BMEI) ,
pages 1â€“5. IEEE, 2021.
[223] Yan-JieZhou,Xiao-LiangXie,Zeng-GuangHou,Gui-BinBian,Shi-QiLiu,andXiao-HuZhou. Frr-net:Fastrecurrentresidualnetworksfor
real-time catheter segmentation and tracking in endovascular aneurysm repair. In 2020 IEEE 17th International Symposium on Biomedical
Imaging (ISBI) , pages 961â€“964. IEEE, 2020.
[224] Yan-Jie Zhou, Xiao-Liang Xie, Xiao-Hu Zhou, Shi-Qi Liu, Gui-Bin Bian, and Zeng-Guang Hou. Pyramid attention recurrent networks for
real-timeguidewiresegmentationandtrackinginintraoperativex-rayfluoroscopy. ComputerizedMedicalImagingandGraphics ,83:101734,
2020.
[225] Yunxiang Li, Shuai Wang, Jun Wang, Guodong Zeng, Wenjun Liu, Qianni Zhang, Qun Jin, and Yaqi Wang. Gt u-net: A u-net like group
transformer network for tooth root segmentation. In International Workshop on Machine Learning in Medical Imaging , pages 386â€“395.
Springer, 2021.
[226] MengWang,WeifangZhu,FeiShi,JinzhuSu,HaoyuChen,KaiYu,YiZhou,YuanyuanPeng,ZhongyueChen,andXinjianChen. Mstganet:
Automatic drusen segmentation from retinal oct images. IEEE Transactions on Medical Imaging , 2021.
[227] AyoubBenaliAmjoudandMustaphaAmrouch. Automaticgenerationofchestx-rayreportsusingatransformer-baseddeeplearningmodel.
In2021 Fifth International Conference On Intelligent Computing in Data Sciences (ICDS) , pages 1â€“5. IEEE, 2021.
[228] KeSun,BinXiao,DongLiu,andJingdongWang. Deephigh-resolutionrepresentationlearningforhumanposeestimation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 5693â€“5703, 2019.
[229] Xing Jia, Yun Xiong, Jiawei Zhang, Yao Zhang, Blackley Suzanne, Yangyong Zhu, and Chunlei Tang. Radiology report generation for rare
diseases via few-shot transformer. In 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) , pages 1347â€“1352.
IEEE, 2021.
[230] Hojun Lee, Hyunjun Cho, Jieun Park, Jinyeong Chae, and Jihie Kim. Cross encoder-decoder transformer with global-local visual extractor
for medical image captioning. Sensors, 22(4):1429, 2022.
[231] MarcellaCornia,MatteoStefanini,LorenzoBaraldi,andRitaCucchiara. Meshed-memorytransformerforimagecaptioning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 10578â€“10587, 2020.
[232] Yuxuan Xiong, Bo Du, and Pingkun Yan. Reinforced transformer for medical image captioning. In International Workshop on Machine
Learning in Medical Imaging , pages 673â€“680. Springer, 2019.
[233] Zhicheng Zhang, Lequan Yu, Xiaokun Liang, Wei Zhao, and Lei Xing. Transct: dual-path transformer for low dose computed tomography.
InInternational Conference on Medical Image Computing and Computer-Assisted Intervention , pages 55â€“64. Springer, 2021.
[234] Lipei Zhang, Zizheng Xiao, Chao Zhou, Jianmin Yuan, Qiang He, Yongfeng Yang, Xin Liu, Dong Liang, Hairong Zheng, Wei Fan, et al.
Spatialadaptiveandtransformerfusionnetwork(stfnet)forlow-countpetblinddenoisingwithmri. MedicalPhysics ,49(1):343â€“356,2022.
[235] Yanmei Luo, Yan Wang, Chen Zu, Bo Zhan, Xi Wu, Jiliu Zhou, Dinggang Shen, and Luping Zhou. 3d transformer-gan for high-quality pet
reconstruction. In International Conference on Medical Image Computing and Computer-Assisted Intervention , pages 276â€“285. Springer,
2021.
[236] LuluWang,HuazhengZhu,ZhongshiHe,YuanyuanJia,andJinglongDu. Adjacentslicesfeaturetransformernetworkforsingleanisotropic
3d brain mri image super-resolution. Biomedical Signal Processing and Control , 72:103339, 2022.
. : Page 59 of 60
Vision Transformers in Medical Computer Vision - A Contemplative Retrospection
[237] ZebinHu,HaoLiu,ZhendongLi,andZekuanYu. Data-enabledintelligenceincomplexindustrialsystemscross-modeltransformermethod
for medical image synthesis. Complexity , 2021, 2021.
[238] KunTang,ZhiLi,LiliTian,LihuiWang,andYueminZhu. Admirâ€“affineanddeformablemedicalimageregistrationfordrug-addictedbrain
images.IEEE Access , 8:70960â€“70968, 2020.
[239] XiaogangGu,FeixiangZhou,RongfeiChen,XinzhenRen,andWenjuZhou. Endoscopicsingleimagesuper-resolutionbasedontransformer
and convolutional neural network. In Intelligent Life System Modelling, Image Processing and Analysis , pages 24â€“32. Springer, 2021.
[240] Alvaro Gomariz, Tiziano Portenier, Patrick M Helbling, Stephan Isringhausen, Ute Suessbier, CÃ©sar Nombela-Arrieta, and Orcun Goksel.
Modalityattentionandsamplingenablesdeeplearningwithheterogeneousmarkercombinationsinfluorescencemicroscopy. Naturemachine
intelligence , 3(9):799â€“811, 2021.
[241] Zhengyang Wang, Yaochen Xie, and Shuiwang Ji. Global voxel transformer networks for augmented microscopy. Nature Machine
Intelligence , 3(2):161â€“171, 2021.
[242] Weijian Li, Viet-Duy Nguyen, Haofu Liao, Matt Wilder, Ke Cheng, and Jiebo Luo. Patch transformer for multi-tagging whole slide
histopathology images. In International Conference on Medical Image Computing and Computer-Assisted Intervention , pages 532â€“540.
Springer, 2019.
. : Page 60 of 60","vision transformers in medical computer vision  a
contemplative retrospection
arshi parvaiza muhammad anwaar khalida rukhsana zafara huma ameera
muhammad aliaand muhammad moazam fraza
aschool of electrical engineering and computer science
national university of sciences and technology nust islamabad 44000 pakistan
article info
keywords 
vision transformers
medical image analytics
self attention
medical computer vision
diagnostic image analysis
literature surveyabstract
recent escalation in the field of computer vision underpins a huddle of algorithms with the
magnificentpotentialtounraveltheinformationcontainedwithinimagesthesecomputervision
algorithmsarebeingpracticedinmedicalimageanalysisandaretransfiguringtheperceptionand
interpretationofimagingdataamongthesealgorithmsvisiontransformersvitsareevolved
as one of the most contemporary and dominant architectures that are being used in the field of
computervisiontheseareimmenselyutilizedbyaplentyofresearcherstoperformnewaswell
asformerexperimentshereinthisarticleweinvestigatetheintersectionofvisiontransformers
andmedicalimagesandprofferedanoverviewofvariousvitsbasedframeworksthatarebeing
usedbydifferentresearchersinordertodeciphertheobstaclesinmedicalcomputervisionwe
surveyed the application of vision transformers in different areas of medical computer vision
such as imagebased disease classification anatomical structure segmentation registration
regionbased lesion detection captioning report generation reconstruction using multiple
medicalimagingmodalitiesthatgreatlyassistinmedicaldiagnosisandhencetreatmentprocess
along with this we also demystify several imaging modalities used in medical computer
vision moreover to get more insight and deeper understanding selfattention mechanism of
transformers is also explained briefly conclusively we also put some light on available data
sets adopted methodology their performance measures challenges and their solutions in form
of discussion we hope that this review article will open future directions for researchers in
medical computer vision
1 introduction
advances in medical imaging modalities have made them indispensable in clinical practice the analysis of these
images by analysts is limited to human subjectivity time constraints and variation of interpretation which leads to
delusion12medicalimagescontainanampleinformationthatisthekeyformedicaldiagnosisandhencetreatment
thehealthcaredatacomprises90ofimagingdatasoconsideredastheprimarysourceformedicalinterventionand
analysis multiple medical imaging modalities such as computed tomography ct ultrasound xray radiography
mr imaging mri and pathology are commonly used for medical imaging diagnostics several challenging factors
associated with medical imaging modalities such as expensive data acquisition 3 dense pixel resolution 4 lack of
standardimageacquisitiontechniquesintermsoftoolandscanningsettings5modalityspecificartefacts6hugely
imbalanced data in negative and positive classes 7 sparse and noisy annotated datasets 8 are major hindrance in
translating ai based diagnosis into clinical practice
since its surge deep learning has shown remarkable success in automatic image analyses of medical imaging
modalities the advancements in deep learning have been flourished and perfected with time revolving primarily
around one algorithm called convolutional neural networks cnn cnns are potentially the most popular deep
learning architecture for its distinguished capabilities to exploit the spatial and temporal relationship between the
features of images which need to be deciphered for extracting meaningful information hidden in images 9 10 11
it has achieved notable accomplishment in medical imaging applications 12 13 such as determining the presence
and then identifying the type of malignancy classification locating the patients lesion detection extracting the
desiredobjectorganfromamedicalimagesegmentationplacingseparateimagesinacommonframeofreference
moazamfrazseecsedupk mm fraz
orcids000000030495463x mm fraz
  page 1 of 60
vision transformers in medical computer vision  a contemplative retrospection
for comparing or integrating the information they contain registration synthesizing images for balancing dataset
generative modeling 14
despite that cnns are very good at feature extraction tasks they fail to encode the relative position of different
features in a cnn the deeper layers are limited to view at whatever the initial layers have passed to them this way
theylosetheglobalcontextofthefeaturesincreasingthenumberoffiltersimprovestherepresentationcapacitybutat
the cost of computation 15 various architectural changes are suggested by researchers for an efficient solution over
thecourseoftimeandleadingtoattentionmechanisms16usingattentionmechanismtheregionsoftheimageare
capturedtowhichthe cnnshouldpayattentionandforwardedto thedeeperlayersresearchershavedemonstrated
that replacing the convolutional layer with attention has improved performance 17 18 the breakthrough from
transformer network 16 in natural language processing nlp tasks has inspired researchers to leverage this
architecture for various computer vision tasks dosovitskiy et al 19 proposed an adaptation to the transformer
known as vision transformer vit that can be applied directly to sequences of image patches for extracting fine
grained features in vit global attention is applied on 16x16 patches of the entire image focusing on the global
salient features of the image which resolve the longrange dependency among image content it gets the best out
of the attention mechanism to incorporate global context in the image features without compromising computational
efficiency
the potential of the vision transformers is further explored by many researchers for solving various problems
however in this survey we aim to highlight the contribution of vision transformers to circumvent the challenges in
automatic diagnostic of diseases using medical imaging modalities and their applications in medical computer vision
tasks our intended audience for this review are research practitioners from medical and interdisciplinary fields of
computer vision for their assistance we have described commonly used terminologies and their description in table
1
this review is organized into seven sections section 1 briefly discusses the role of deep learning the emergence
of the transformers and the replacement of cnns by transformers in medical computer vision section 2 discuss the
organization and papers selection methodology and distribution of the review section 3 discusses different medical
imaging modalities and their application in the diagnostic and treatment of various diseases section 4 discusses the
emergence of vision transformers and lists all the publicly available datasets used by the reviewed paper in every
modality and deep learning task section 5 discusses visual recognition tasks to established the domain knowledge
for the audience of the interdisciplinary field section 6 gives the details about the reviewed techniques categorized
according to each deep learning task such as classification segmentation detection clinical report generation and
miscellaneous which also include image registration section 7 identifies research gaps in the review papers and
discusses the future directions for using transformers in medical computer vision
11 scopeobjective of the review
the aim of writing this review paper is to highlight and discuss the contribution of vision transformers in
medical computer vision across different medical imaging modalities for this purpose we have searched out papers
from different top conferences and journals excluding preprints within the time span of three years from 2019
2022 the results achieved and the adopted methodology of each paper is reviewed comprehensively the distinct
categories that we reviewed belonging to the medical image analysis includes classification detection segmentation
registrationclinicalreportgenerationimageenhancementimagereconstructionsandimagesynthesistheliterature
of these categories is further divide into different medical imaging modalities ultimately in addition to accentuating
interesting techniques in the literature we also put some light on research gaps and future directions we hope this
review will bridge the gap between computer vision community and medical specialists to foster the future research
and development in medical computer vision this article is written keeping in mind the intended audience from the
interdisciplinary fields medical and ai publicly available datasets and downloadable links are listed in the table 3
12 comparison with other reviews
although there exists some reviews on transformers already which enfolds a significant amount of work yet we
feelthatthereisalotofroomforimprovementforexamplenoreviewisprimarilyfocusedonapplicationsofvision
transformersinmedicaldomaintobridgethisgapwecomeupwiththissurveyinwhichourpointofconvergenceis
to scrutinize the exertion of vits in medical computer vision to initiate this process we collected a bunch of articles
addressingdifferenttransformersarchitectureandtheirutilizationonmultimodalmedicalimagesweincludedalmost
80peerreviewedarticlesinoursurveyfromprestigiousplatformslikepubmedspringerieeesciencedirectwhich
  page 2 of 60
vision transformers in medical computer vision  a contemplative retrospection
table 1
list of accronyms and abbreviations used in paper
acronyms words acronyms words
kid kernel  inception distance mftemultibranch features transformation
and extraction
aha align hierarchical attention ma microaneurysms
msam multi modal spatial attention module clamclusteringconstrainedattention
multipleinstance learning
cac coronary artery calcium kfs key factor sampling
emvtefficient multi scale fusion trans
formermstganetmulti scale transformer global atten
tion network
nscf nonlocal sparse net fusion mcat multimodal coattention transformer
tetristemplate transformer image segmen
tationfmnet feature mapping subnetwork
rdls regionalized dynamic learners crc colorectal cancer
idh isocitrate dehydrogenase mttu multi task transformer unet
vitbisvision transformer for biomedical im
age segmentationciderconsensusbased image description
evaluation
dafnet disentangled alligned and fuse net asft adjacent slices feature transformer
hybridctrm hybrid convolutional transformer mti multi text indexer
vif visual information fidelity pcr rpolymerase chain reaction
abvs automated breast volume scanner prcc papillary renal cell carcinoma
fid frechet inception distance gsm genitourinary syndrome of menopause
cedt cross encoderdecoder transformer glve globallocal visual extractor
makes our paper unique from other review articles in addition to that we also explained different imaging modalities
used in medical computer comprehensively moreover we have given a brief note on available medical data sets in
tabularformthedownloadablelinkstothesedatasetsarealsomentioninthesetableswealsodiscussedtheresultsof
stateoftheartapproachesinawellstructuredtabularforminwhichwedescribedtheperformancemetricesalongwith
theirresultsontheavailabledatasetsintheendwehavealsopointedoutsomechallengesalongwiththeirinsightful
future directions for comparative analysis of our review with khan et al 20 and kai et al 21 we visualized the
main points in figure 1
  page 3 of 60
vision transformers in medical computer vision  a contemplative retrospection
figure 1 comparison with recent published reviews on vision transformers
2 survey methodology
in this section we will discuss the study selection criteria based on which articles are chosen for the review and
distribution of the included articles according to venues journals conferences medical imaging modalities deep
learning tasks classification segmentation detection etc and impact factors
21 papers selection
we have demonstrated the details of the searched and included research papers in this review article through
prisma preferred reporting items for systematic reviews and metaanalyses figure 2 shows the summary of
papers selection we searched our papers on pubmed springer ieee xplore science direct and finally on google
scholar in the result of search queries we have found 11060 papers among which 3060 were duplicate and were
excluded from the study we screened remaining 8000 and found 7600 were not fulfilling the criteria of legitimacy
forthissurveyassomeofthemwasonlyaboutmedicalapplicationwithouttransformersandsomeofthemwasusing
transformer word in the different context than vision transformer we further screened the remaining 400 articles and
excluded the preprints from our study in the prisma we have shown the categorization of our included 80 papers
according to their application in medical domain we have also demonstrated the distribution of the papers according
to the medical imaging modalities
22 data extraction methods
we searched different platforms such as pubmed springer science direct ieee xplore and google scholar for
extracting the research articles we targeted top journals and conferences in duration of last four years from 2019 to
2022 for extracting relevant papers for our study we used different key words and combine them with the logical
operators and or to get the better search results the key words we used are
  page 4 of 60
vision transformers in medical computer vision  a contemplative retrospection
figure 2 prisma  flow diagram of selected research articles for the review
computed tomography ct scans magnetic resonance images mri ultrasound xrays optical coher
ence tomography oct fundus images positron emission tomographycomputed tomography petct
histopathology histology wsi whole slide images
classification reconstruction segmentation registration detection report generation enhancement
transformer vision transformer
weextractedthekeywordsfromallthearticlesonvisiontransformersincludedinourreviewandgeneratethetag
cloudwhichisshowninfigure3thetagcloudillustratesthetrendingtermsinvitapplicationsinmedicalcomputer
visionasourcentralfocusinthisreviewistorecapitulatetheapplicationofvisiontransformersonmedicalimaging
modalities this word cloud mostly highlighting the applications classification segmentation detection denoising
captioning medical imaging modalities xrays pet oct wholeslide ct histopathological fundus disease
cancerglaucomacoviddiabetictumorcarcinomaorgansretinalchestbreastpulmonarybrainandotherdeep
learning related terms like transformer vision attention encoder decoder multimodel
  page 5 of 60
vision transformers in medical computer vision  a contemplative retrospection
figure 3 a visual depiction of most frequently used keywords in the reviewed articles
table 2
inclusion and exclusion criteria for papers selection
inclusion criteria exclusion criteria
articles that address the medical computer
vision tasks such as registration segmentation
detectionclassification enactment reconstruction
and report generation using medical imaging modalities
and vision transformerarticles which are not using medical imaging modalities
and vision transformer
papers with proper evaluation metrics and detailed sum
mary of proposed architecture including training parame
tersarticles that are not peerreviewed
articles that are based on vision transformers articles that are survey papers
the papers inclusion and exclusion criteria is given in table 2 firstly papers were selected on the basis of titles
if it does not match the inclusion and exclusion criteria then we read the abstract conclusion and model diagram for
the final selection
23 papers distribution
in this section we have shown the distribution of the published papers across journals conferences imaging
modalities impact factors and medical computer vision tasks the purpose of this section is to give the birds eye
view of the published work that how much literature is available in top journals and conferences what is the impact
of the work what imaging modalities are used and what is the progress of works across the years
thefigure4showsthedistributionofreviewedarticlesacrosstheyearsitcanbeseeninthegraphthattheliterature
forvisiontransformershasgrownthroughouttheyearsfrom2019to2022astheapplicationsofvisiontransformersin
medicalimagingmodalitiesstartedgrowingfrom2019onwardwithalargenumberofpublicationsintheyear2021
in which more than 50 articles were published
  page 6 of 60
vision transformers in medical computer vision  a contemplative retrospection
figure 4 a chronological distribution of vision transformers research publications in medical image analytic
figure5showsthecategorizationofresearcharticlesbasedonvisualrecognitiontasksusingvisiontransformers
the recapitulation of the reviewed article is categorized based on the tasks such as classification segmentation
detection report generation registration and miscellaneous miscellaneous further contains different tasks such as
reconstructionenhancementandvisualneuralvisualcontentgenerationthegraphshowsthatmostofthereviewed
articles applied vision transformers on classification task which is 31 segmentation 25 miscellaneous 19
detection 16 report generation 7 and registration 2
figure 5 distribution of reviewed articles based on visual recognition tasks
as the review paper is focusing on the application of vision transformers in medical imaging modalities figure 6
depictsthestatisticsofimagingmodalitiesusedinourreviewedarticleseighttypesofimagingmodalitiesareusedin
this survey paperexploiting vision transformers include xrays imagingmodality which is 35 ct scans26 mri
scans 13 histopathology images 11 octfundus images 8 pet 3 endoscopy 2 and microscopy 2
  page 7 of 60
vision transformers in medical computer vision  a contemplative retrospection
figure 6 dispensation graph of medical imaging modalities among the articles that are reviewed
figure7showsthecountofvisiontransformerbasedmedicalimagingarticlestakenfromvarioustopjournalsand
conferences each bubble represents the number of a specific journal or a specific conference and number of articles
taken from these journals and conferences
figure 7 bubble graph representing number of articles chosen from top ranked journals and conferences
figure 8 shows the distribution of vision transformer based reviewed papers across various journals of various
impact factors according to jcr year 2020 the bubble size shows the number of reviewed articles retrieved from
each journal according to this figure the five papers are reviewed from ieee transaction on medical imaging with
  page 8 of 60
vision transformers in medical computer vision  a contemplative retrospection
the impact factor of 10048 the highest impact factor journal included in this survey paper is nature biomedical
engineering which is 25671
figure 8 visual representation of selected research publications from top tier journals along their impact factor
3 a delineation of medical imaging modalities
thissectionisaddedforassistingcomputervisionpractitionerstoestablishthebasicdomainknowledgeofmedical
imaging modalities and their application in the diagnostic and treatment of various diseases medical images differ
from natural images as they have specialized acquisition techniques physical phenomena such as electromagnetic
radiation sound light nuclear magnetic resonance and radioactivity are used for generating medical images of
externalorinternalorgansofhumanbodytheseimagingtechniquescanbeappliedasnoninvasivemethodstoview
inside the human body without any surgical intervention because of their importance in medical diagnostic a lot
of advancement has taken place in image acquisition devices called image modalities these image modalities play
an important role in patients followup regarding the growth of the already diagnosed disease state or undergoing
a treatment procedure as 90 of the health data comprises of images these imaging modalities are very crucial in
public health and preventive measures as they help in establishing the accurate diagnosis these medical images can
capturedifferentbodyregionssuchaseyeschestbrainheartarmsandlegstherearedifferentmodalitiesofmedical
images such as computed tomography ct ultrasound xray radiography mr imaging mri positron emission
tomographycomputed tomography petct pathology fundus images and optical coherence tomography oct
the images acquired from these modalities are shown in figure 9 details about these image modalities are given in
the subsequent section
  page 9 of 60
vision transformers in medical computer vision  a contemplative retrospection
a
 b
 c
 d
e
 f
 g
 h
figure 9 a catalogue of medical imaging modalities that vision transformers employed for assisted diagnosis achest
xrays that are widely used for covid19 or pneumonia detection bbrain mri scans that are being used for diagnosis of
aneurysms and tumors cbrain ct scans that are employed to locate injuries tumors or clots leading to stroke doct
images that are playing an important role in diagnosis of retinal diseases such as agerelated macular degeneration amd
anddiabeticeyediseases efundusimagescapturedtherearofeyeandareusedfordetectionandgradingofhypertensive
retinopathy fliver ultrasound gwhole slide images wsis that are being widely used in computational pathology
hpetct scans that are responsible for detection and diagnosis of cancer determining the spread or recurrence of
cancer or metastasis
31 xray imaging
accordingtonationalinstituteofhealthnihus22xraysimagesarecapturednoninvasivelyusingradiation
that is part of the electromagnetic spectrum xrays are mostly captured for diagnosing bone fracture 23 but chest
xrays are also used for detecting pneumonia 24 xrays are also used by mammograms for breast cancer detection
25 other most familiar uses of xrays are for breast tumors 26 enlarged heart 27 blocked blood vessels 28
conditions affecting your lungs 29  infections 30 osteoporosis 31 arthritis 32 tooth decay 33
32 computed tomography ct scans
national institute of health nih us 22 described computed tomography ct scan is a computerized xray
imagingtechniqueinwhichanarrowbeamofradiationisfocusedandthenquicklyrotatedaroundthebodytocapture
the detailed internal images called tomographic images of the bodys slice noninvasively ct scan produces 2
dimesionalaswell3dimensionalimagesofsliceofthebodyonceseveralimagesaretakentheseimagesaredigitally
stacked together to form threedimensional images ct scans are used for identifying the various organsslices of
the body for example ct scan of the heart is used for detecting various types of heart disease or abnormalities 34
ct scans of the head to locate injuries 35 tumors 36 clots leading to stroke hemorrhage and other conditions
37ctscansofthelungsisusedfordetectingcancer38tumorsexcessfluidpulmonaryembolismsbloodclots
39lung infections 40 and emphysema or pneumonia 41
33 optical coherence tomography oct  fundus images
accordingtoamericanacademyofophthalmologyaoa42opticalcoherencetomographyoctcaptures
invasivecrosssectionimagesoftheretinausinglightwavesoctcanbeusedtoexaminetheretinasdistinctivelayers
whichhelpinmappingandmeasuringtheirthicknessandplayanimportantroleindiagnosisofretinaldiseasessuchas
  page 10 of 60
vision transformers in medical computer vision  a contemplative retrospection
agerelatedmaculardegenerationamd43anddiabeticeyedisease44octcanbefurtherhelpfulindiagnosis
ofglaucoma45macularpucker46macularedema47centralserousretinopathy48diabeticretinopathy49
macular hole 50
another type of images discussed by 51 that can be helpful in the diagnosis of agerelated macular degeneration
amd 52 are fundus images which capture the rear of the eye it is 2d imaging modality and since glaucoma is a
3d disease 3d image modality such as oct is considered more efficient for diagnosis fundus images can also be
used for detection and grading of hypertensive retinopathy 53
34 magnetic resonance imaging mri
magneticresonanceimagingmrimodalitydescribedbythenationalinstituteofhealthnihus22capture
3d anatomical images noninvasively mri scanning does not use any radiation which make it an ultimate choice of
capturingwhenfrequentimagingisrequiredinthetreatmentprocessespeciallyinthebrainmriisparticularlysuitable
forcapturingthesofttissuesofthebodybutitismorecostlyascomparedtoxraysandctscanningmricanbeused
tocapturedifferentpartsofthebodyforexamplemriareusedfordiagnosisofaneurysmsandtumors54aswellfor
differentiating between white matter and grey in brain mri can further be used for spinal cord 55 and nerves56
muscles 57 and ligaments 58
thereisaspecializedmricalledfunctionalmagneticresonanceimagingfmriwhichisusedforobservingbrain
structure and locating the areas of the brain which are activated during cognitive tasks59
35 ultrasound
radiologyinfoorg for patients 60 described ultrasound as an imaging modality that invasively create image of
organs tissues and other structures inside the body invasively by using sound waves without using any radiation
ultrasound can be used to internal organs within the body noninvasively for example capturing the heart eyes
brainthyroidbloodvesselsbreastabdominalorgansskinandmusclesultrasoundimagesarecapturedin2d3d
but it can also capture 4d images which is 3d in motion such as a heart beating 61 or blood flowing through blood
vessels 62
36 histopathology or wholeslide imaging wsi
thewholeslideimagingwsireferstocapturingthemicroscopictissuespecimensfromaglassslideofbiopsy
orsurgicalspecimenwhichresultsinhighresolutiondigitizedimagestheseimagesarecapturedthroughfirsttaking
smallhighresolutionimagetilesorstripsandthenmontagingthemtocreateafullimageofahistologicalsection63
specimens on glass slides transformed into highresolution digital files can be efficiently stored accessed analyzed
and shared with scientists from across the web using slide management technologies moreover wsi is changing the
workflows of many laboratories it is used in various disease diagnostics prognostic and treatments such as survival
prediction 64detection of tissue phenotypes 65 automated grade classification 66 67 68 segmentation of
microvessels and nerves 69 70 multiorgan nuclei segmentation 71
37 positron emission tomography  computed tomography petct scans
according to radiologyinfoorg 60 positron emission tomography also called pet imaging or a pet scan
small amounts of radioactive material called radiotracers for capturing images petct scans can be used for cancer
detectionanddiagnosis72determiningspreadofthecancerdeterminingtherecurrenceofcancermetastasis73
evaluating brain abnormalities like tumor 36 and memory disorder 74 mapping normal human brain and heart
function
4 deep neural networks  enhancing representation learning from cnns to vision
transformers
the goal of this section is to bridge the gap between ai and healthcare analysts it introduces the deep learning
concepts techniques and architectures that is found in the papers surveyed for this review article
the progression of deep neural networks in computer vision has contributed to various fields of study and
it primarily revolves around convolutional neural networks cnn for instance while assessing medical images
practitioners can recognize if there is an anomaly similarly this mechanism can be taught to a computer via cnns
to diagnose a disease or an anomaly while taking images as input hence giving vision to a computerthe model of a
  page 11 of 60
vision transformers in medical computer vision  a contemplative retrospection
basic cnnis illustrated infigure 10 ittakes theimage input asa matrix ofpixel values assignsweights to learnthe
variousdifferentiablefeatures15itthenpassestheimagethroughmultiplelayersandusesmultiplefilterstocapture
thediscriminatedfeaturesfromtheimagecnnsgenerallyconsistofthreekindsoflayersconvolutionlayerspooling
layersandfullconnectedlayers75convolutionlayersareresponsibleforlearningfeaturesandcapturingthespatial
andtemporaldependenciesbetweenthefeaturesbyapplicationofrelevantfiltersthepoolinglayerisresponsiblefor
reducingthesizeoffeaturemapstocapturemoresemanticinformationthanspatialinformationinconvolutionallayer
filtersofsizenxnwherenisequalto1357oranyotheroddnumberthepoolinglayerusesawindowofsize2x2
3x3oranyotherdesiredsizetotakeaverageormaximumvalueinthatwindowbeforethefullyconnectedlayerthe
output of the convolutional and pooling layer which is called feature map is flattened to make a fully connected layer
at a function such as softmax is applied to make a prediction and a loss function is used to calculate the error and the
is back propagated to update the values of learnable parameters
figure 10 a general framework of convolutional neural networks cnns
depending on the application for example image classification fully connected layers are added at the end of
the network stacking these layers on top of each other with a specific arrangement with the help of a differentiable
function is known as cnn architecture in recent years several cnn architectures are developed with various such
arrangementsalexnet76vggnet77googlenet78resnet79resnext80squeezeandexcitationnet
81 densenet 82 and efficientnet 83
convolutionalneuralnetworksareusedinvariousapplicationsinthecategoriesofimageclassificationdetection
and segmentation etc for example face detection 84 identification of emotions 85 speech recognition and
machinetranslationusingcnns86etcconsideringtheapplicationsofcnnitcanbeinferredthattheycanenable
commendableresults87theyareknowntobeablackboxasthetrainingisaccordingtothetaskanddomainone
majorlimitationistheunclarityofresultsiethereasonforaparticularoutcomeespeciallyinthemedicaldomainit
is imperative to know the cause of a specific outcome otherwise a wrong diagnosis can be a threat to human lives
onewaytotacklethisproblemheadonistohavesuchamodelthatfocusesonrelevantpartsoftheimageandcan
be visualized by the doctors to elucidate this issue attention models were proposed 16 69 the attention model
focusesonthepartsthatarerelevanttotheinputsequencemoreoveramodelwasproposedknownastransformers
itusedtheconceptofattentiontoenhancethetrainingspeed19transformersconsistofmultipleblocksofidentical
encoders and decoders which were composed of selfattention block and feedforward networks in addition the
decoderconsistsofanextraattentionblockwhichfocusesontherelevantpartofthesequencetheembeddedwords
oftheinputwerepassedtotheencodersequentiallyandwerepropagatedtoalltheencoderstheoutcomesofthelast
encoderwerethenfedtothedecoderstheperformanceofthetransfermodelswasstateoftheartinthetasksrelated
to natural language processing
inspiredbythetransformermodeldosovitskiyetal19appliedittotheimagesanditcanbeusedtoreplacecnns
 this model was called vision transformers vit and its structure is illustrated in figure 11 vit model introduced
global attention but not on the entire image rather they divided the entire image into small image patches of 16x16
they introduced simple numbers 1 2 up to n as positional embeddings for specifying the positions of the patches
  page 12 of 60
vision transformers in medical computer vision  a contemplative retrospection
a lookup table was introduced which contained a learnable vector against each number representing the position of
the patch these embeddings were passed to the network along with the patch each patch was unrolled to a linear
vector and projected linearly on embedding matrix the final embedding along with positional embedding was then
fed into the transformer encoder along with embedding for patches an extra embedding with number 0 is also fed
intothenetworkanditsoutputwasobtainedthusvisiontransformershavethecapabilityofmodelingglobalcontext
which assists in more accurate results lastly in this review medical images are considered as the input for vision
transformers
figure 11 structural representation of vision transformers
41 open access medical imaging datasets employed in vit applications
in table 3 we have summarized and structured openaccessed datasets in tabular form the table includes
information regarding the tasks ie classification segmentation detection report generations and miscellaneous
furthermore the respective image modalities and their applications are also included next we have also compiled
thedescriptionandlinkstothecorrespondingdatasetshenceitwillassistresearcherstoidentifytheseresourcesthat
can be utilized against different applications
  page 13 of 60
vision transformers in medical computer vision  a contemplative retrospection
table 3 publicly available datasets of medical imaging modalities used by researchers for assisted diagnosis
tasks modality applications datasets descriptiondownload
links
classification ct scanspulmonary nod
ule characteriza
tionluna16 88a commonly used dataset
for lung nodule identifica
tion and false positive re
ductiondownload
lidcidri 89one of the largest publicly
available lung cancer
screening datasets
made up of diagnostic
and screening thoracic
computed tomography
ct imagesdownload
emphysema
classificationcomputed
tomography
emphysema
database 90a freely available dataset
that includes 115 high
resolution ct hrct
scans and 168 square
patchesthatweremanually
annotatedinasubsetofthe
slicesdownload
covidct 91a freely accessible collec
tionof ctscanimagesex
tracted from a number of
scholarly articlesdownload
covid19
detectionsarscov2 92a multiclass ct scan
dataset for identification of
sarscov2 infectiondownload
covd19ctdb
93covid19ctdatabase
consists of chest ct scans
that are annotated for the
existence of covid19download
covidctset
94one of the largest open ac
cess covid19 lung ct
datasetthatisavailableon
linedownload
xrayspneumonia clas
sificationchest xray im
ages 95a dataset of validated
oct and chest xray
imagesdownload
  page 14 of 60
vision transformers in medical computer vision  a contemplative retrospection
tuberculosis
prognosis and
detectionmontgomery
county mc
cxrsmall tuberculosis dataset
from usadownload
tuberculosis
prognosis and
detectionshenzhen datasetsmall tuberculosis dataset
from shenzhen chinadownload
covid chest x
ray datasetan open access dataset of
chest xray and ct im
ages of patients which are
positive or suspected of
covid19 or other viral
and bacterial pneumoniasdownload
classificationbimcv covid
19 96bimcvcovid19
dataset is a large dataset
with chest xray images
cxr cr dx and
computed tomography
ct imaging of covid
19patientsalongwiththeir
radiographic findingsdownload
xraysinterpretable
covid19
detection
 severity
quantificationcovid19
posterior
anterior chest
radiography
images 97this is a curated covid
19 chest xray image
dataset that was created
by combining 15 publicly
accessible datasetsdownload
extensive
covid19
xray and ct
chest images
98in this covid19 dataset
both noncovid and
covid cases are included
of both xray and ct
imagesdownload
covidx dataset
99a database of chest xray
imagesforcovid19pos
itive cases along with nor
mal and viral pneumonia
imagesdownload
mri scansmultimodal
medical image
classificationmrnet dataset
92the mrnet dataset con
sistsof1370kneemriex
ams performed at stanford
universitymedicalcenterdownload
  page 15 of 60
vision transformers in medical computer vision  a contemplative retrospection
fundus im
agesretinal image
synthesis
and disease
predictioncolorfundusim
ages 100a database that has both
ffa image and color fun
dus image in this dr grad
ing systemdownload
histopatho
logy imagescolorectal
histopathology
image
classificationcolorectal
cancer histology
dataset 101this data set represents a
collection of textures in
histological images of hu
man colorectal cancerdownload
detectionxrayscovid19 diag
nosiscovidx 99a database of chest xray
imagesforcovid19pos
itive cases along with nor
mal and viral pneumonia
imagesdownload
covidgre
102it is built by adding 426
pneumonia images from
the chestxray8 database
to the covidgr10
datasetdownload
fundus im
agesmicroaneurysms
detectionidrid 103this dataset consists of 81
imagesdownload
ct scanscov19ctdb
93covid19ctdatabase
consists of chest ct scans
that are annotated for the
existence of covid19download
covid19 diag
nosiscovidxct2a
104a benchmark dataset
the largest comprising a
multinational cohort of
4501patientsfromatleast
15 countries and contains
three classes  covid19
pneumonia noncovid
19 pneumonia and
normaldownload
histopathol
ogy imagescancer detectionthe cancer
genome atlas
105the compendium includes
heat maps for 33 differ
ent tumor types and three
platformsgeneexpression
reversephase protein ar
rays rppa and mirna
expressiondownload
segmentation iseg2017 106this datasets contains brain
mris of 39 subjectsdownload
  page 16 of 60
vision transformers in medical computer vision  a contemplative retrospection
mri scansbraintissueseg
mentationbrats2020
107it contains 1756 mri of
439 subjectsdownload
mrbrains 108it contains 20 fully an
notated multi sequence 3t
mri brainsdownload
ukbb 109 download
cardiac segmen
tationmms2it contains 360 subjects
having 200 training and
160 testing imagesdownload
mri scans eri 110it contains lge data of
28 patients the number of
segmented images are 358download
abdominal
segmentationchaos 111this data set contains im
ages for t2 segmentation
of liver and kidneysdownload
segmentationxraystooth root seg
mentationdrive 112it include 40 color fundus
retinal images that are ran
domly selecteddownload
knee segmenta
tionoai212kneeimageswereseg
mented randomly and the
training and testing split
was 100 and 112download
lung segmenta
tionjsrt 113the database includes 154
conventional chest radio
graphs with a lung nodule
100 malignant and 54 be
nign nodulesdownload
ct scanspediatric
segmentationkits19 114thisdatasetcontainsrenal
tumor segmentation im
ages of 210 adultsdownload
drusen segmen
tation from reti
nal oct imagesuscd 115this dataset contains 8616
retinal oct bscansdownload
  page 17 of 60
vision transformers in medical computer vision  a contemplative retrospection
iu chest xray
collection 116a public and open
source openi or indiana
university chest xrays
database which contains
3955 medical reports with
7470 frontal and lateral
chest xray imagesdownload
clinical
report
generationxraysmimiccxr
117it is the recently released
largest dataset consists of
377110 chest xrays im
ages and 227835 reports
from 64588 patientsdownload
peir gross
118it consists of publicly ac
cessible 7442 teaching im
ages spread across 21 pre
defined subcategories the
vocabulary size of the to
talimagecaptionsis4452
each image on average
containsa12wordcaptiondownload
miscellaneousnihaapm
mayo clinic
ldct grand
challenge 119a public and open source
30 contrastenhanced ab
dominal ct patient scansdownload
petct
scansmedical image
enhancement
kirby21 dataset
kki01 to
kki05 120a public and open source
dataset containing correla
tion data for 20 subjects
from kennedy kriegerdownload
mri imagesmedical image
reconstructiondiverse 2k reso
lution high qual
ity div2k im
agesdataset121it contains a total of 1000
2k resolution rgb im
agesdownload
fastmri scans
122t1 and t2weighted
images from 150 subjects
were analyzed 100 for
training 10 for validation
40 for testingdownload
  page 18 of 60
vision transformers in medical computer vision  a contemplative retrospection
fluorescence
microscopydenoising
of celullar
microscopy
images for
assisted
augmented
microscopyflywing planaria
and tribolium
datasets 123the training data 17005
and 14725 small cropped
patches of size 64 6416
for planaria and tribolium
datasets while the testing
data are 20 testing images
ofsize1024 102495and
6testingimagesofaverage
size around 700 70045
forthetwodatasetsrespec
tivelydownload
5 visual recognition tasks in medical images
artificial intelligence and deep learning have played a vital role in assisting clinicians for better diagnosis in
this context the application of cnns and vits on medical images assists the healthcare professional in disease
classification lesion detection segmenting the anatomical structures automated report generation denoising the
imagesmedicalimageregistrationandvariousothertasksthissectiongivesabriefoverviewoftheabovementioned
visual recognition tasks performed by the application of cnns and vits on medical images
51 medical image classification
the practitioners give their diagnosis by analyzing the medical images hence determining the presence and type
of the disease this conventional diagnosing way can be assisted with deep learning techniques figure 12 shows a
generalized classification network through these techniques the ambiguity in diagnosis among different doctors can
bemitigatedandtheoutcomeswillbemoreaccuratethusresultsachievedthroughcnnsarenotonlytimeefficient
butcanalsoassisthealthcareprofessionalsusingcnnmodelsaninputimageisfedintothenetworkwhichisthen
assessedanalyzedandinterpretedtodeterminethetargetandobjectofdifferentmodes124thisprocessisknown
asimageclassificationforexampleconsideringthefigure5thechestxrayimagesaregivenasinputandthemodel
classifies them into normal and pneumonia images at present image classification has various applications in the
medical domain such as skin cancer 125 diabetic retinopathy 126 tuberculosis 127 etc
thesignificanceofimageclassificationusingcnnscanbedeterminedbytheaforementionedapplicationsthese
applications were achieved through various cnn architectures such as alexnet 76 vggnet 77 googlenet 78
resnet79latermoreresourceefficientarchitectureswereproposediemobilenet128squeezeandexcitation
net81andefficientnet83etcthroughtheseconvolutionalneuralnetworkscommendableresultswereachieved
in medical applications however in terms of resources improvements are still required
  page 19 of 60
vision transformers in medical computer vision  a contemplative retrospection
figure 12 medical image classification pipeline in which chest xrays cxrs are fed into a deep cnn architecture which
assigns each image a class such as pneumonia patient or healthy patient
52 lesions detection
classifyingimageswasindeedastepforwardtowardsautomateddiagnosisneverthelesslocatingtheobjectplays
animportantrolewhiledevelopingamorefunctioningapplicationinthefieldofcomputervisiononeoftheunderlying
goals is to classify the object into a category and determine the location of the object in a given image this technique
isknownasobjectdetectionfigure13figurativelyexplainstheageneraldetectionnetworkforinstanceiftheinput
image isan xrayof ahand the objectdetection modelwill not onlyclassify thecategory ie fracturedbone butalso
localize it by using bounding boxes considering a situation where bones were fractured on multiple locations here
object detection will be a better technique to opt for as it will be more helpful to the diagnostician the applications
of object detection involve face detection 84 plant disease identification 129weapon detection 130  emotion
detection 85 etc
figure 13 a cnn architecture detecting a colony of tumorous cells given a histopathology image
the task of object detection is composed of two types twostage networks and onestage networks 124 the
twostage networks are based on region proposal algorithms such as rcnn 131 fast rcnn 132  and faster
rcnn 133 the other technique is designed in a way that it works directly on images examples include yolo
134 ssd 135 etc the two design types have a tradeoff between accuracy and time efficiency the twostage
networks are capable of more accuracy whilst onestage networks have more speed thus it depends on the task at
hand and dataset while choosing these networks for object detection the limited datasets in the medical domain are
  page 20 of 60
vision transformers in medical computer vision  a contemplative retrospection
the biggest constraint while training the models therefore researchers are working on models which can work with
limited datasets
53 anatomical structure segmentation
in the medical domain there are numerous cases where it is difficult to distinguish between two different lesions
as there are minor differences since lesions can have different treatment strategies determining them as separates is
vital it is indeed a challenge to recognize such subtle differences but it is not impossible if the images are classified
pixelwise they can be localized as well and result in a fine outline of the object such a technique is known as
imagesegmentationforexampleifanmriimageisfedintothemodeltheoutcomewillnotjustclassifythetumor
type its anatomical structure will also be highlighted as seen in figure 14 there are various applications of image
segmentation which includes  cardiovascular structure 136  prostate cancer 137 blood vessel 138 etc
figure 14 a segmentation framework in which brain mri scans are fed into a deep cnn architecture which is not just
classifying and locating the tumorous region but also highlighting the anatomical structure
finegrainedsegmentationisadecisivestepinimageguidedtreatmentandcomputeraideddiagnosisthewidely
used architectures for image segmentation are unet 139 deeplab 140 masked rcnn 141 etc the usage of
thesesegmentationmodelsdependsontheproblemthatisbeingsolvedforinstancemultiscaleobjectsintheimage
deeplabanditsvariousstructureswillbeawisechoicelastlytheconcernregardingimagesegmentationisthelack
of labeled data due to which researchers are considering more unsupervised approaches however it is still a work in
progress 142
54 clinical report generation
in the healthcare domain while examining radiology images ie chest xrays ct scans mri etc the doctors
have to write detailed reports of the assessment this conventional method of report writing is tedious monotonous
timeconsuminganderrorproneforradiologists143immenseprogresshasbeenmadeindeeplearningtogenerate
medical reports automatically automatic report generation can assist clinical practitioners in quick and accurate
decisionmaking 144 for example if the ct scan of a brain is given as input the output would be a complete
report such as if the tumour exists the location of the tumour its size and other details figure 15 shows a general
workflow of clinical report generation medical report generation is an application of image caption in which these
models are applied to medical data image captioning refers to computers generating captions by giving images as
inputitconsistsofmainlyanencoderdecoderwherecnnisusedtoextractfeaturesandlstmorrnnareusedto
generate the captions 145 the initial work on its architectures include show and tell 146 show attend and tell
147neuraltalk148etcsomeofthestateoftheartincludedensecap149semstyle150densecaptionnet
151 etc lastly as in supervised learning a large amount of annotated data is required for these models to perform
well in the future unsupervised learning could be a more powerful way to proceed with them 152
  page 21 of 60
vision transformers in medical computer vision  a contemplative retrospection
figure 15 a visual representation of clinical report generation mechanism in which different types of image modalities
are independently diagnosed by radiologist and ai model and then compared to each other to provide a precise medical
report
6 a recapitulation on vision transformer application to medical image analytics for
assisted diagnosis
61 vit in imagebased disease classification
deep learning has recently come to the power in a variety of research domains convolutional neural networks
cnnshavebeenthemostdominantdeepneuralnetworksforautonomousmedicalimageanalysisapplicationssuch
as image classification during the last decade these models however have shown poor performance in learning the
longrange information due to their localized receptive field which limits their capabilities for vision related tasks
transformer architecture proposed by vaswani et al 16 is currently the most popular model in the field of natural
language processing nlp getting inspiration from the success of selfattention based deep neural architectures
dosovitskiy et al 19 introduced vision transformer vit model for image classification based applications in
thesemodelstheoveralltrainingprocessispredicatedondividingtheinputimageintopatchesandconsideringeach
embedded patch as a word in nlp selfattention modules are used in these models to learn the relationship between
the embedded patches
in the following section we will take a step forward in exploiting the potential applications of selfattention based
architectureslikevisiontransformersvitforthetaskofmedicalimageclassificationsuchascovid19detection
and severity quantification emphysema classification tuberculosis prognosis etc the section is further divided into
medical imaging modalities with a focus on contributions made by vision transformers to the respective medical
applications the section ends with a tabular summary of the proposed approaches and their performance matrices
611 computed tomography ct scans
pulmonarynodulecharacterization lungcancerisoneofthemostfrequentlyreportedcausesofcancerrelated
morbiditiesandmortalities153earlydetectionandtreatmentofmultiplepulmonarynoduleshasbecomeachallenge
in clinical practice as it is one of the most efficacious ways to reduce the number of fatalities associated with the
condition prior research 154 155 156 on detection and characterization of lung nodules focused on learning the
  page 22 of 60
vision transformers in medical computer vision  a contemplative retrospection
associations between various nodules in other words they particularly use solitary nodule approaches on several
nodular patients ignoring the relationalcontextual information to overcome this issue yang et al 157 proposed a
multipleinstancelearningmilstrategywhichempiricallyprovedtheutilityoflearningtherelationshipsbetween
nodules its the first time researchers have looked into the relationships between several lung nodules and extracted
critical relational information between solitarynodule voxels instead of using typical poolingbased aggregation in
multiple instance learning they created set attention transformers sats based on selfattention to understand the
relationships between nodules a 3d densenet is employed as the backbone to learn representations of voxels at the
solitarynodule level the sats are then used to determine how several nodules from the same patient are related
this datadriven methodology might aid in understanding of etiologic and biologic processes as well as metastasis
diagnosis of multiple pulmonary nodules and motivate clinical and biological research on this important topic
emphysema classification chronic obstructive pulmonary disease copd is a heterogeneous disorder with a
varietyofcharacteristicsincludingsmallandlargerespiratoryinflammationaswellasemphysemawhichisthemost
common causeof progressive lungtissue loss emphysemaas characterized by thedestruction and persistentgrowth
ofthe alveolicanbe classifiedautomaticallywhich canaidin determiningandquantifying lungdestructionpatterns
in this regard convolutional neural networks cnns serve an essential role particularly in pulmonary ct image
classification but transformers have yet to be explored as a result wu et al 158 conducted a thorough assessment
and extensive evaluation of the vit model for emphysema classification first large image patches 16 x 16 are
cropped from ct scans after resizing the patches are flattened and linearly embedded to create a sequence of patch
embeddings the positional information is kept by concatenating the class embeddings with the patch embeddings
to acquire the representation the final embedding sequence is passed into the transformer encoder module finally
the learnable class embedding is fed to a softmax layer for emphysema classification despite the fact that this study
employed a vision transformer model to classify emphysema unlike other techniques that use cnn models it still
hasseverallimitationsforexamplepatchbasedclassificationmaynotbeasconvenientaspixelbasedsegmentation
furthermore the architecture just uses the transformer encoder block and the cnns benefit is not utilized in short
emphysema quantification is difficult and classification is merely the first step proposing more efficient networks
capableoflearningsemanticinformationofemphysemabypartialoraccurateannotationsmaybeapressingneedin
the near future more research on the segmentation and quantification of emphysema will be conducted
covid19 detection the infectious coronavirus covid19 and lung disorders have been at the vanguard of
the research community as the pandemic has caused significant public health concerns throughout the world using
computer vision methods several attempts 159 160 161 are being undertaken to create automated systems for
faster and more effective diagnosis of covid19 as per several research studies 162 163 some radiographic
manifestations such as broncho vascular thickening ground glass opacities ggo crazypaving pattern and
consolidation have been found in chest ct images however with the rapidly growing number of patients in the
current situation radiologists have a significant challenge in manually interpreting ct scans
ambita et al 164 were the first to use a vision transformer to the task of covid19 detection from computed
tomography ct scans they implement a variety of vision transformers eg vitb 16 vitb 32 vitl 16 vit
l 32 and vith 14 for image classification they employed resnetbased selfattention generative adversarial
network saganresnet as a data augmentation approach for synthetic image generation to alleviate the problem
ofinsufficientdatafurthermoretheydemonstratedhowvitoffersvisualizationsfortheimagesbyexhibitingwhich
sectionsoftheinputimagethemodelfocusesitsattentiononinthevariouslayersthismightbeusefulforradiologists
whenanalyzingctscansimprovementsmayalsobemadebyevaluatingtheproposedapproachondifferentdatasets
and customizing the architecture of transformers or gan
zhang et al 165 attempted to broaden the scope of vision transformers such that they may be used as a robust
feature learner for covid19 diagnosis in 3d cts inspired by the success of swin vision transformer and ct
classification work in 166 167 their framework is made of two key stages lung segmentation followed by image
classificationusingswintransformerasabackboneinthefirststageapretrainedunetisusedforlungsegmentation
in ct scans and produced a lung mask that restricted learning to certain lung regions the features from each 2d
ct slice are then extracted using a swin vision transformer which are subsequently aggregated into 3d volume level
features using a maxpooling layer however its worth noting that the framework equipped with the backbone of
efficientnetv2m achieves a good speedaccuracy tradeoff according to the results on the validation dataset this
implies that in future study merely increasing the model size might result in an improvement in classification
  page 23 of 60
vision transformers in medical computer vision  a contemplative retrospection
abovereviewedstudiesareevidentthatvisiontransformersvitisanovelandquicklyevolvingapproachthathas
demonstratedexcellentresultsusingcovid19datasetsthanetal168conductedapreliminarystudytoinvestigate
the efficacy of using vit with different sized patches on ct scans of diseased lungs covid19 infected lungs and
normal lungs a default positional embedding is used and the combination is then passed to a transformer encoder
which is composed of alternating layers of multiheaded selfattention and multilayer perceptron mlp units the
transformerencodersoutputissentintoanmlpheadwhichoutputsthepredictedclasstheproposedmethodology
issimplerandlessdemandingoncomputationalresourcesascomparedtocnnshoweverpretrainingthetransformer
andaddingaconvolutionallayerinfrontofitieconvolutionalvisiontransformersmayincreaseperformanceaside
fromthatotherhyperparameterscanbemodifiedtoincreaseperformanceandexplainableartificialintelligencexai
will be used in the future to explain how deep learning networks like vit make decisions
covid19 ct scans contain not only the local features such as local crazypaving and local hemangiectasis
but also have global characteristics since it is characterized by the combination of both local and global features
extracting image features with relatively complex features is a challenging classification problem of such medical
imagingmodalitiesfanetal169proposedaparallelbibranchnetworktranscnnnetthatisessentiallybasedon
thetransformerandconvolutionalneuralnetworkunliketheconventionalapproachesthesizeoftheconvolutional
filter kernel is not changed to extract features at different scales instead they use the global receptive field of the
transformer network a bidirectional feature fusion structure is then designed which fuses the extracted features
fromtwobranchesbidirectionallyforminganetworkthathasthepotentialtoextractmorecomprehensiveandample
features
612 xray or radiographic images
pneumonia classification pneumonia is an infectious disease in which the alveoli in the lungs is to be filled with
fluid or pus making it painful to breath as well as decreasing oxygen intake a detailed inspection of chest xray
imagesbytheradiographerorradiotherapistisrequiredtodiagnosepneumoniaasaresultpneumoniadetectionisa
timeconsuming process and even a slight error can result in an excruciatingly painful outcome several researchers
haveexploredvariouscomputervisionapproachestodiagnosepneumoniausingxrayimagesofhumancheststuagi
etal170haveproposedavisiontransformervitmodelfortheclassificationofpneumoniafurthertovalidatethe
performanceoftheproposedmodelitwasalsocomparedwiththecnnmodelandvgg16itwasobservedthatvit
outperformedothertechniquesreachingthehighestaccuracyof096inadditionitoutperformedothermodelsinterms
of computational cost as well it is observed that the model still requires further experimentations to investigate the
robustnessofthemodelwithmoreheterogeneousdatatherehasntbeenalotofworkdoneusingvisiontransformers
in the field of chest xray diagnosis it may be useful in the diagnosis of other disorders such as covid19 detection
cystic fibrosis or emphysema edema pleural thickening effusion and even cancer
tuberculosis prognosis and detection tuberculosis refers to an infection that affects the lungs and can travel
to other parts of the body it can be diagnosed and assessed by referring to chest xrays if the infection is cured at
an earlier stage it can save a person from further misery of painful treatment for the classification of infected and
noninfected chest xrays a methodology was introduced by duong et al 171 the authors have used efficientnet
withvisiontransformersforthedetectionoftuberculosisusingchestxraysimagestheperformancemetricsinclude
accuracyprecisionandrecallf1scoreandareaunderthecurvethehighestaccuracyachievedwas97percentwith
the backbone of efficient net b1 hence the paper validates the robustness of vision transformer models used on a
heterogeneous dataset however it should be further evaluated on different baselines models
interpretable covid19 detection  severity quantification the novel coronavirus disease 2019 covid
19 caused by the severe acute respiratory syndrome coronavirus 2 sarscov2 has become one of the deadliest
virusesofthecenturyasofapril2021infectingover137millionpeoplewithover29milliondeathworldwideinthe
contextofunprecedentedcovid19pandemicpublichealthsystemshavebeenhitbyaslewofchallengesincluding
scarcity of medical resources that are pushing healthcare workers to face the threat of infection deep learning and
computer vision are commonly employed in numerous fields of medical imaging for the diagnosis of covid19
from radiographic images xrays or ct scans even though these techniques have yielded commendable outcomes
thecostisalwaysconsideredasanimportantfactorforthesystemtobeapplicabletheuseofacomputedtomography
ct scan for covid19 diagnosis offers great sensitivity and specificity 172 but it is a severe burden due to its
high cost and risk for crosscontamination in the radiology suite in comparison of ct imaging xrays have been
  page 24 of 60
vision transformers in medical computer vision  a contemplative retrospection
widelyutilizedforcovid19screeningastheyrequirelessimagingtimearelessexpensiveandxrayscannersare
generally available even in remote regions 173
hence many researchers have worked on diagnosing covid19 using chest xray cxr images and through
theseautomatedmethodscountercheckthepcrtestresultsforinstanceinthestudypresentedbyparketal174
vision transformers were utilized for both classification and quantification of the severity of covid19 firstly the
lowlevel features were extracted using stateoftheart cnn architectures secondly these extracted features were
given to the transformer model for classification and severity measurement thirdly the severity was demonstrated
through heat maps which gives interpretability to the generated results the robustness of the model was illustrated
throughvariousexperimentationsondifferentbaselinemodelsandtestedonexternaldatasetsthemodelwasevaluated
onmetricsincludingaucsensitivityspecificityandaccuracythehighestaccuracyachievedontheexternaldataset
was above 848 percent
similarly the study presented by shome et al 175 have proposed a pipeline based on vision transformers for
the classification of covid19 the model was also compared with other baseline models to validate the robustness
ofvisiontransformersascomparedto174itwastrainedonalargerandmoreheterogeneousdatasetachievingthe
accuracy of 98 percent and auc of 99 percent in the binary classification furthermore for the multiclassification
which includes pneumonia xrays as well the model achieves the accuracy and auc of 92 percent and 98 percent
respectively in addition the model uses gradcam for the visualization through heatmaps for the explainability of
theoutcomealthoughthemodelperformedbetterthan174themodeldoesnotquantifytheseverityoftheinfection
further this research could be expanded to predict the rate at which the infection can spread
since the medical data in most domains is inadequate it can be difficult to build robust models considering this
issue rahhal et al 176 have put forth a methodology that performs well with small training data to diagnose
covid19 in both ct scans and xray images the proposed method uses vision transformers as a backbone with the
employment of a siamese encoder the input image with a corresponding augmented image was fed into the siamese
encoderitwasthenconnectedtotwoclassifiersfurthertheoutputoftheseclassifierswasfedintoafusionlayerfor
theoutcomefollowedbyheatmapsondifferentlayerstointerprettheresultsmoreoverthesiameseencodercatersto
theissueregardingdeficientdatatheproposedmodelhasachievedanaccuracyof942percentwhichiscommendable
in limited training data
untilnowonlyafewcadbasedapproachesfordetectingcovid19havebeendevelopedbuttheireffectiveness
has been hampered due to a number of factors taking an inspiration from convolutional block attention module
cbam nawshad et al 177 proposed an attentionbased convolutional module using resnet32 as the backbone
architecture with a 9769 accuracy they also conducted a comparative assessment of a variety of deep learning
models including vgg resnet and xception for the successful detection of covid19 and viral pneumonia
utilizing the attention module with various cnnbased architectures produced much better results than using the
base cnn architectures
since it is believed that the newly developing pathogen would have similar lowlevel cxr features with existing
diseasestheuniqueconceptofproducinghigherleveldiagnosesbyaggregatinglowlevelfeaturecorpusmaybeused
to swiftly construct a robust algorithm against it
613 magnetic resonance imaging mri
multimodal medical image classification the inadequacy of medical data is discussed in the aforementioned
papers and it can be inferred that researchers are trying to overcome the issue by modeling various architectures
for instance dai et al 178 put forth a hybrid transformer model for the classification of multimodal images the
pipeline consists of a cnn to extract lowlevel features and then transformers are utilized for the global context the
model was applied on two different datasets for classification of parotid gland tumors and classification of a knee
injurythehighestaccuracyachievedbytheparotidglandtumordatasetandkneeinjurydatasetwas889percentand
949 percent respectively nevertheless the study has not presented any means for interpretability of the model as it
is essential especially in the medical field
614 oct or fundus images
retinalimagesynthesisanddiseaseprediction forthediagnosisofretinalabnormalityfluoresceinangiogra
phyfacanbeusedtocapturethevascularstructureoftheretinaafluidisinjectedintothebloodstreamhowever
several reactions have been reported with the usage of fa another approach to diagnosing abnormality is by using
fundus images the vascular structure of the eye is not captured in these images in the paper 179 the authors
  page 25 of 60
vision transformers in medical computer vision  a contemplative retrospection
used fundus images and synthesize them to produce fas using a generative adversarial networkgan next these
imagesarethengiventoatransformermodeltoclassifyanormalandabnormalretinathemodelachievedthehighest
accuracy sensitivity and specificity of 857 833 and 900 respectively moreover this model needs to be validated
with a more heterogeneous dataset along with applying this model for predicting other retinal diseases as well
615 histopathology images
colorectal histopathology image classification colorectal cancer crc is a type of cancer that begins in the
rectumorcolonandisdefinedbytheuncontrolledgrowthofaberrantcellswiththelatentabilitytoinvadeothertissues
despitethefactthatmanualinspectionofhistopathologyslidesisstillcrucialinexperimentalpractiseautomaticimage
processing enables the quantitative and rapid analysis of malignant tissues early detection is crucial for identifying
theappropriatetreatmentapproachandincreasingthepatientschancesofsurvivalasaresultautomatedtechniques
are needed to save time and eliminate the risk of human error artificial intelligence has recently been put to use
in the diagnosis and prediction of several forms of cancer zeid et al 180 used vision transformers to perform
a multiclass tissue classification of colorectal cancer highlighting the potential of employing transformers in the
histopathological image domain first of all a standard vision transformer model was proposed and it achieved an
accuracy of 933 percent since vision transformers demand more data a hybrid approach combining cnn and
transformer was developed lowlevel features are extracted using the cnn model and the embeddings are sent to
the transformer this model is known as a compact convolutional transformer and it achieved an accuracy of 94
percenthoweverexperimentationwithvariousdatasetsanddifferentformsofcancermayalsobedonetoimprovethe
modelsoverallperformancedeeplearningalgorithmsarenowbecomingincreasinglyessentialfortheidentification
andclassificationofcolorectalhistopathologyimagesexistingtechniquesontheotherhandaremoreconcernedwith
endtoend automatic classification using computers than with humancomputer interaction hence chen et al 181
presented an ilmcam framework it is based on interactive learning and attention techniques automatic learning
al and interactive learning il are two steps in the proposed ilmcam system il to extract multichannel
features for classification in the al stage a multichannel attention mechanism model with three separate attention
mechanism channels and convolutional neural networks is implemented the proposed ilmcam system constantly
adds misclassified images to the training set in an interactive method during the il stage improving the mcam
models classification ability to handle different colorectal histopathological image classification tasks in the future
permutation and combination can be used to identify the best model for the current task from attention mechanisms
anddeeplearningmodelsmoreoverattentionmechanismscanalsobeincludedinvariouslocationsofadeeplearning
model in order to investigate the influence of convolutional layers on classification performance
the table 4 given below summarized the performance gain by the reviewed articles of the classification category
table 4 list of datasets and performance measures employed by researchers for medical image classification
modality publication dataset performance measures
ct scansyang et al 157luna16 88 cpm score  0916
lidcidri 89 accuracy   9317
wu et al 158accuracy   9595
computedtomographyem
physema database 90precision   980
recall   971
specificity   986
ct scansambita et al 164covidct 91accuracy   8719
  page 26 of 60
vision transformers in medical computer vision  a contemplative retrospection
positive precision  
8911
sensitivity   8571
f1 score  08738
ct scans
sarscov2 92accuracy   9541
positive precision  
9430
sensitivity   9803
f1 score  09613
zhang et al 165 cov19ctdb 93accuracy   943
precision   937
recall   938
f1 score  0935
macro f1 score  094
than et al 165 covidctset 94accuracy   9536
senstivity   8300
li et al182private dataset from eight
different hospital 182macro f1 score  097
micro f1 score  098
xray imagestuagi et al 170 chest xray images 95 accuracy   9645
duong et al 171montgomery county mc
cxr images 183
accuracy   9792
shenzhen cxr dataset
183
covid19 dataset 184
park et al 174 bimcv covid19 96auc  0949
accuracy   868
sensitivity   902
  page 27 of 60
vision transformers in medical computer vision  a contemplative retrospection
specificity   862
shome et al 175accuracy   92
covid19 posterior
anterior chest radiography
images 97precision   93
extensivecovid19xray
and ct chest images 98recall   89
f1 score  091
auc  098
rahha et al 176accuracy   9462
precision   9677
sarscov2 92 recall   9677
specificity   9965
f1 score  0967
mri scans dai et al 178 mrnet dataset 92aucroc  0976
accuracy   918
sensitivity   968
specificity   728
octfundus images kamran et al 179 color fundus images 100 accuracy   857
sensitivity   833
specificity   900
histopathology images zeid et al 180accuracy   933
colorectalcancerhistology
dataset 101precision   9333
recall   9344
f1 score  0933
  page 28 of 60
vision transformers in medical computer vision  a contemplative retrospection
62 vit in regionbased lesion detection
the proceeding section discusses the detection of anomalies in medical images using vision transformers in
relation to the aforementioned modalities
621 computed tomography ct scans
covid19detection aframeworkforthedetectionofcovid19wasproposedbyliangetalwithchestctimages
asinput185theframeworkwascomposedofacnnmodelforfeatureextractionandthentheseattentionmodule
wasintegratedforthegenerationofattentionvectorsnextthetransformermodelwasusedtodistinguishthefeatures
in the input the study also proposed a method to resample the inputs which also contributed to the efficiency of the
model the highest f1score was 8821 percent which was a 10 percent improvement from the baseline model the
dataset used however was small and imbalanced which doesnot validate the generalizability of the proposed model
anomalydetection inthemedicaldomainvariousmethodologiesareproposedforthedetectionofanomaliesthe
authorsofthepaper186haveproposedatransformerbasedmodelwhichwasappliedonvariousimagesieretinal
oct head ct scans and brain mris the representation of the features was learned using autoencoders which
were based on transformers in addition to detect the anomalies in multiscale a transformer model was proposed
with skipconnections thus it reduced the usage of memory and cost of computation the models were evaluated on
auroc achieving 93  9581  and 9838 for the datasets related to headct brain mri and retinal oct
respectivelyhowevertheproposedmodelstillrequiresafurtherreductionincomputationalcostsothatitcanbeused
in realtime
622 xray or radiographic images
covid19diagnosis sincexraysarecomparativelycosteffectiveandafasterwayofdiagnosingthevirusseveral
researchershaveproposedmethodsfordetectionusingchestxraysinthepaper187anotherapproachisproposedto
detect covid19 using chest xrays an adaptive attention network is used which consists of resnet and an attention
basedencoderresnetisusedtolearnthefeaturerepresentationsandtheattentionmoduleisthenutilisedfordetection
of the infectious areas the proposed model was compared with different cnn models on three different datasets
the evaluation indicated that the proposed model performed significantly better moreover the performance metrics
included accuracy sensitivity precision and f1 score the highest accuracy achieved by the model was 985
similarly to capture the global context the authors kumar et al have used vision transformers on both xray
images and ct images of the chest for the diagnosis of covid19 188 the data used was labelled as normal
pneumonia and covid19 furthermore to address the issue of scarce data transfer learning is used followed by
explainability through visualisation of the infected areas the proposed method was compared with other models
ie inceptionv3 coronet covidnet etc the results were evaluated using the metrics precision recall f1score
accuracyandspecificitytheproposedmodeloutperformedthecnnmodelsreachingtheaccuracyof096and098
for ct scans and xray images respectively however work on severity information requires attention in both 187
188
pulmonary lesions detection in the initial assessment of lung cancer one of the most used techniques is chest
radiography since it is essential to diagnose cancer at an earlier stage many methodologies have been proposed to
detect pulmonary lesions the study is presented by 189 it has proposed two architectures convolution networks
with attention feedback and recurrent attention model with annotation feedback the first method uses cnn to
learn the features and generate saliency maps as the soft attention mechanism was incorporated next a recurrent
attention model with attention feedback was proposed the proposed architecture uses reinforcement learning for
better performance of the model the architectures were evaluated through precision recall f1score and accuracy
thehighestaccuracyachievedwas85percentforclassificationand74percentforlocalizationthusthearchitectures
require improvement regarding the reduction of computation time and accuracy
623 oct or fundus images
microaneurysms detection the early diagnosis of lesions in diabetic retinopathy can be done by the detection
of microaneurysms ma since it is difficult to locate them because of their size several methodologies have been
proposed in this study 190 the proposed methodology for the detection of mas comprises three stages first the
images are preprocessed to improve the quality second a deep network is used with an attention mechanism for
detection third the correspondence between microaneaurysms and blood vessels is exploited for the final results
  page 29 of 60
vision transformers in medical computer vision  a contemplative retrospection
the performance metrics used for evaluation were precision recall sensitivity and f1score the proposed method
outperformed prior proposed models with a sensitivity of 086 nevertheless the model was trained on the images
from one type of camera which does not validate the generalizability of the proposed methodology
glaucoma detection the authors of the paper 191 have proposed a methodology for the detection of a disease
knownasglaucomaitcausesthelossofvisionandisirreversiblethepaperhaspresentedacnnmodelwhichwas
attentionbased for the detection of the disease furthermore due to the attention module the localized features were
also visualized giving results more explainability the proposed architecture first locate the area and then classify
the disease the detection was evaluated using the performance metrics accuracy sensitivity specificity auc and
f2scorewiththehighestaccuracyachievedof962percenthoweverinthenetworkthemodelsmayidentifyregions
with useless information which may hinder the performance of the model
furtherxuetalhavepresentedamodelwhichconsistsofanattentionmodulealongwithtransferlearningforthe
detection of glaucoma 192 this work has contributed towards the discrimination of general and specific features
since the models are not able to identify the regions that may give no information the proposed methodology can
extract the regions with more information in addition with the attention module the regions can also be visualized
the model was then evaluated on two different datasets achieving the highest accuracy sensitivity specificity and
aucof8577percent849percent869percentand0929respectivelylastlythismethodcanbefurthervalidated
by applying it to various other eye diseases
624 histopathology images
cancer detection barrets esophagus be refers to the damaging of the swallowing tube that connects the mouth
to the stomach because of acid reflux 193 ultimately it increases the risk of esophagus cancer ie adenocarcinoma
194 moreover patients that suffer from be are at a higher risk of cancer the detection of lesions at an early stage
can prevent the suffering of patients from cancer with a better survival ratein the paper195 attentionbased deep
neural networks were proposed for the detection of cancerous and precancerous esophagus tissues the model uses
attentionbased mechanisms to detect the cancerous tissues belonging to the classes normal benodysplasia be
withdysplasia and adenocarcinoma the mechanism does not require annotations for regions of interest thus it
dynamically identifies the rois hence it is independent of the annotated bounding box and does not require a fixed
sizeofinputimagestheproposedmethodwascomparedwiththeslidingwindowapproachbasedontheperformance
metrics accuracy recall precision and accuracy the model outperformed the sliding window method in all classes
withanaverageaccuracyof083howeverthemodelwastrainedonasmalldatasethencetherobustnessofthemodel
still needs to be verified using more data
the issues regarding wholeslide images in terms of detection include poor adaptability of the model explain
ability and resourceefficient model the authors of the paper196 have proposed a model known as clustering
constrainedattention multipleinstance learningclam it was applied to detect three types of cancers renal cell
carcinoma nonsmall cell lung cancer and breast cancer lymph node metastasis the proposed method clam is a
weakly supervised algorithm it uses an attention module to determine the regions and classify the cancer type in
addition it also localized the affected regions with interpretability the models were evaluated using auc hence it
was greater than 095 on contrary to this dataefficient model it considers various locations as independent thus
leading to a less contextaware model
next another model was used for the detection of cancer leading to the prediction of survival prediction 197
theframeworkisamultimodalcoattentiontransformermcatthatlearnsthecorrespondencebetweenwsisand
genomic features the attention module ensures interpretability along with the reduction of memory usage of image
bags the model was applied to five different cancer datasets and the results were compared with the stateoftheart
models
the table 5 given below summarized the performance gain by the reviewed articles of the detection category
  page 30 of 60
vision transformers in medical computer vision  a contemplative retrospection
table 5 list of datasets and performance measures employed by researchers for regionbased lesion detection
modality publication dataset performance measures
ct scans mondal et al 188 covidxct2a 104accuracy   981
recall96
precision96
specificity988
f1score096
liang et al 185 cov19ctdb 93 macro f1 score  8821
micro f1 score  098
xrayslin et al 187covidx 99accuracy   95
sensitivity97
precision9898
specificity9947
f1score097
covidgre 102accuracy   8953
sensitivity8605
precision8315
specificity9128
f1score084
dlai3 accuracy   9855
sensitivity9863
lin et al 187 precision9863
specificity9990
f1score098
precision   15
  page 31 of 60
vision transformers in medical computer vision  a contemplative retrospection
pesce et al 189a dataset consisting of
745479 chest xray exams
collected from the historical
archives of guys and st
thomas nhs foundation
trust in london from
january2005tomarch2016sensitivity   65
average overlap   43
fundus imageszhang et al 190 idrid 103accuracy   943
precision   872
recall   810
f1 score  0840
sensitivity   868
accuracy   962
sensitivity   954
li et al 191lag database obtained
from chinese glaucoma
study alliance cgsa and
beijing tongren hospital
191specificity   967
auc  0983
f2 score  0954
xu et al 192lag database obtained
from chinese glaucoma
study alliance cgsa and
beijing tongren hospital
191accuracy   857
sensitivity   849
specificity   869
auc  0929
tomita et al 195 accuracy   830
recall   600
  page 32 of 60
vision transformers in medical computer vision  a contemplative retrospection
histopathology imageshistologicalimagesbetween
january 1 2016 and
december 31 2018 at
dartmouthhitchcock
medical center lebanon
new hampshire were
collectedprecision   620
f1 score  059
chen 197the cancer genome atlas
105concordance index c
index  0653
63 vit in anatomical structure segmentation
clear cut and detailed segmentation is a decisive step in image guided treatment and computeraided diagnosis
a great deal of image segmentation models have been proposed in the last 40 years from traditional models to deep
neural networks but since the emergence of transformers they have outperformed all the state of art segmentation
models transformers functions prominently in error free segmentation of medical images because of their capability
to model the global context as the organs lay out over a wide receptive field hence transformers can easily encode
these organs by modeling the association of pixels that are distant spatially moreover the background is dispersed in
medical scans for that reason gaining the understanding of the global context between those pixels that relate to the
backgroundwillbebeneficialforthemodeltodotheunerringclassificationbelowwereviewedexperimentsthattried
to exploit vit based models for a faultless segmentation we divided these experiments in accordance with different
modalities used for medical imaging in the end we give all the results obtained during these experiments on specific
datasets in tabular form
631 computed tomography ct scans
coronary artery segmentation a precise and correct segmentation of cac is advantageous for early cvd
diagnosis but as cac has blurry and distorted boundaries the task of segmentation is not very much satisfactory
to address this issue ning et al 198 introduced an efficient multiscale vision transformer for the segmentation of
coronaryarterycalciumandnameditascacemvtthisarchitectureutilizedthelocalaswellasglobalfeaturesand
thenusedthemcollectivelytomodelthelongandshorttermdependenciestheirmodelwascomprisedofthreemain
modulesakfskeyfactorsamplingmodulethattheyutilizedforextractingthekeyfactorsfromtheimagethese
key factors were made use for lowlevel reconstruction of highly structured features b nscf a non local sparse
net fusion module that was used to model the information of high level features of texture c nmca a non local
multiscale context aggregation that was used to get the dependencies of long range at different scales experimental
results showed that their model outperformed the state of the art methods at that that by giving a dice similarity
coefficient of 7539 317
leeetal199introducedanewconceptoftemplatetransformernetworksforsegmentationthroughshapepriors
tetris and performed coronary artery segmentation through their model in this concept they used an end to
end trainable spatial transformer stn200 to deform a shape template to complement the under laying region of
interesttheyalsousedthisconceptofincorporatingthepriorstothestateoftheartcnnandunetusedforbinary
classificationtheexperimentalresultsoftetrisandunet139incorporatingthepriorwereabletoproducesingly
connected components because they were given the prior information and gave to dice scores of 0787 and 0854
respectively they also compared the unet139 with fcn by giving the prior shape but fcn201 didnt perform
that well giving the dice score of 079 only
lung tumor segmentation petct segmentation requires information from both pet and ct modality most of
the models get the segmentation information of these modalities separately in a study fu et al 202 established a
modulemsammultimodelspatialattentionmoduleadeeplearningbasedframeworkforlungtumorsegmentation
  page 33 of 60
vision transformers in medical computer vision  a contemplative retrospection
inpetctmsamwasanimpulsivemoduleanditwasabletohighlightthespatialareasorregionsthatarelinkedto
thetumorandcensoredthenormalregionsofinputspontaneouslythismodulewasfollowedbyacnnthatwasacting
asabackboneandthiscnnwasperformingthesegmentationtaskonthemapthatwasprovidedtoitasaninputfrom
the multi model spatial attention module unet139 was used as backbone for that purpose they performed their
experimentsontwoclinicalpetctdatasetsofnsclcandststheresultsoftheexperimentsgaveadicesimilarity
coefficient of 7144 and 6226 for nsclc and sts datasets respectively in order to refine this architecture more
better procedures can be used to further improve the segmentation
renaltumorsegmentation duetothediversificationthatispresentinsizeandposethetaskofsegmentationhas
become a strenuous task hence la et al 203 proposed a network that was both size and pose invariant and they
tested their network for renal tumor segmentation on 2d ct scan images their architecture was comprised of three
sequentialmodulesthatworkedtogetherinthetrainingprocessfirstwastheregressionmodulethattheyusedtofind
the similarity matrix of input image to the ground truth second module was used to find the region of interest and
theynameditasdifferentiablemodulethethirdandlastmodulewasusedtoperformthesegmentationtaskandthey
usedunet139forthispurposetheyusedthespatialtransformerstn200intheirarchitecturetoautomatically
detect the bounding box which saved time results indicated that the training time was reduced by 8 hours and the
dice score for kidney almost remained same which was 8801 but in case of renal tumor the score got better from
8552 to 8712 one of the shortcomings of their model was that it was valid for only small set of data
aortic valve segmentation basic cnn models for segmentation were performing good on 2d images and they
were struggling against 3d medical imaging hence pak et al 204 proposed a deep learning based architecture for
thesegmentationof3dctscanimagesthisnetworkwascomprisedofabaselineunetarchitecturethatperformed
thebasicsegmentationtaskandaspatialtransformer200thatwasusedtoperformsomeaffinetransformationthe
use of only unet139 was not sufficient for the segmentation tasks as it requires a lot of memory and also result in
decrease of accuracy hence they used a spatial transformer stn200 which reduced the size of input image by
performingsometransformationandhenceitresultedinbettercomputationtheyutilizetheirmodeltoperformaortic
valvesegmentationupontestingtheirmodelondifferentpatientsdatathedicescorecoefficienttheygetwas0717
bone segmentation in order to perform the segmentation of bone as well as the localization of the anatomical
landmarks of cone beamed computed tomography data simultaneously lian et al 205 proposed a network called
dynamictransformernetworkdtnettheirmodelcontributedinthreepartsinthefirstpartasynergicarchitecture
was made to accurately catch the global context and fine grained details of image for volume to volume prediction
secondly by using the anatomic reliance between landmarks rdls are made to collectively degenerate the large 3d
heatmapsofeverylandmarksthirdlyatmsaremadeforthecompliantlearningofcontextspecificfeatureembedding
frommutualfeaturebaseswhiledoingtheexperimentsonctscansofmandiblethesegmentationdsccameoutto
be 9395 with a std dev of 130 whereas for localization the rmse was 195 043 these results were better than
unet and other models that were used for comparison
lesion segmentation the early diagnosis of ais provides valuable knowledge about the disease but for a human
eyeitisburdensometodiscriminatedelicatechangesinpathologyhenceluoetalluoetal206proposedanetwork
forthesegmentationofacuteischemicstrokeaisthatwasbasedonselfattentionthismechanismhadanencoder
and a decoder the encoder was comprised of a cnn as a backbone and a transformer this encoder part picked the
globalcontextfeaturesthedecoderpartconsistedofmultiheadcrossattentionmhcamodulewhichupsampled
the feature maps that were coming from the encoder these feature maps were connected via skip connections the
backbonecnnusedwasresnet50theirexperimentalresultswerecomparedtoattentionunet207unet139
andtransunet208buttheirmodeloutperformedthembygivingthedicesimilaritycoefficientofsegmentationof
lesion up to 7358 which was better than all other compared models
segmentation of organs although transformers help in capturing the long term dependencies but when it comes
to the segmentation of 3d images the dependencies face extreme computation hence to reduce some computations
xie et al 209 presented cotr which is a combination of convolutional network and transformer instead of using
simpletransformertheyintroduceddeformabletransfersforcatchingthelongrangedependenciesdetransdetrans
focusses on only a few key points which greatly reduces the computational complexity which also allows to process
multiscaleimageswhicharequiteimportanttoattainanaccuratesegmentationtheytestedtheirmodelonbcvdataset
  page 34 of 60
vision transformers in medical computer vision  a contemplative retrospection
thatincludestheimagesof11differenthumanorganscotrachievedadicescoreof85andhausdorffdistanceof401
onavgandthesemeasureswerebetterthanothermethodsthattheyusedtocomparetheirmodelfurtherenhancement
in their model could be that it can be enhanced by extended it to operate on different modalities
632 magnetic resonance imaging mri scans
brain tumor segmentation glioma segmentation and prediction of idh genotyping is an important and difficult
taskduetosimilaritiespresentinintraandintertumortoaddressthisproblemchengetalchengetal210proposed
an mri based fully automated multi model that could predict idh genotyping and glioma segmentation at the same
timethepreexistingmethodswerenotabletoperformthebothtasksatthesametimealsothesemethodsfacedthe
problemsofinterandintratumourheterogeneitysotheyaddresstheseissuesbyusingajointcnnandtransformer
encoder the transformer was used to extract the global features that were used for the glioma segmentation it also
contained a multiscale classifier which was used for idh genotyping a multitask loss was then used to balance
the segmentation and idh genotyping and this loss collectively joined the classification loss and segmentation loss
in the end they proposed an unpredictability aware pseudolabelselection to make pseudolabels for idh on a large
unlabeleddatasettheynamedtheirmodelasmttunetonexperimentstheirmodelimprovedthehd95anddice
score 169mm and 123 for glioma segmentation and 213 and 428 in case of auc and accuracy respectively
sagar et al 211 proposed vitbis vision transformer for bio medical image segmentation that was based on
encoder and decoder architecture both encoder and decoder had transformer inside them the feauture map of input
image was split into three different convolutions before it was fed to the transformer these convolutions were 1x1
3x3and5x5thesethreedifferentfeaturemapswereconcatenatedwiththehelpofconcatoperatorthenitwasfedto
thetransformerintheencoderthesetransformerhadtheattentionmechanisminsidethemthetransformersofencoder
and decoder were joined together via skip connections the same architecture of multiscale was used in the decoder
as well before producing a segmentation mask after linear projection different sizes were concatenated via concat
operator upon testing their architecture on a public dataset for brain tumor segmentation the dsc achieved was 086
which was better than other state of the art cnn and transformer networks
brain tissue segmentation in order to solve the problem of multi model medical image segmentation sun et al
212 presented a novel multi model architecture based on transformer and convolutional neural network for multi
modelimagesegmentationandnameditashybridctrmandusedthismodeltosegmentdifferentbraintissuesthis
networkusedtwopathsfortheimageencodingonepathwasfromthecnnandtheotherpathwasfromtransformer
thentherepresentationofimagefrombothpathswerejoinedtogetherfordecodingandthesegmentationpurposethe
cnn controlled the rapid convergence of gradient descent while extracting the local features whereas the non local
features were extracted by the transformer they used two strategies for the fusion one was the single path strategy
and the other was the multiple path strategy and used both of these strategies in their experiments experiments were
carried out on two different datasets and by following both strategies on mrbrains dataset the dsc came out to be
8298and8347forsigleandmultiplepathstrategiesrespectivelywhereasontheiseg2017datasetthesescoreswere
8675 and 8716 which were better than the models they used to do the comparison like hyperdensenet213
brainstructuresegmentation agooddealofdeeplearningarchitecturesusedtoperformthetaskofsegmentation
on medical images confront the problem of noise at the inference time and result in inaccurate result to address this
problem sinclair et al 214 proposed a network atlasistn atlas image and spatial transformer network that was
able to perform both registration as well as segmentation on 2d and 3d data of brain structure this network could
perform segmentation on numerous interest regions structures and to register the atlas label map to an inbetween
segmentationpixelwise this model was also able to do the fine tuning of the parameters at the inference time in
ordertoachievebetterpixelwisesegmentationduetowhichtheeffectofnoiseintheimagealsoreducedthismodel
was then tested on three different datasets two 3d and one 2d the results were compared with unet and this model
was performing better than unet139 giving a dsc of 0888
cardiacsegmentation combiningthesharedinformationofanyorganfromdifferentmodalitiesisveryhelpfulfor
learningandmultimodalityprocessinginordertoschievethischartsiasetal215proposeddisentangledalignand
fuse network dafnet that was able to learn the information present in different modalities input hence producing
a more precise segmentation mask anatomical factors from different inputs are combined and processed at the same
timedafnetcollectedtheinformationpresentindifferentmodalitiesdespiteofthefactthatfewlabelssupervisedare
  page 35 of 60
vision transformers in medical computer vision  a contemplative retrospection
there or even no labels unsupervised spatial transformer was used to align the anatomical factors in case of image
misregistration they evaluated their model by performing l2 t1 and cardiac segmentation on different datasets
theirmodelwasabletoperformonbothsinglemodelandmultimodelinputsanditoutperformedothermodelswhen
it was trained on single modality input whether with few labelssemi supervised or no labels unsupervised
definingtherightventriclervstructureincardiacsegmentationisastretchingworktodobecauseofitscomplex
andmultiplexstructurehenceitrequiresshortaxisaswellaslongaxisimagesinordertoaddressthisissuegaoetal
216 established a consistency based co training mechanism that used the geometric relationships between different
view cmr images for the segmentation along with this mechanism they also used the unet139 architecture in
ordertocapturesomelongrangedependenciesevaluationofthemodelwasdoneonthemms2challengedataset
and the dice score came out to be 083 and 086 for short axis and long axis respectively
colerectalcancersegmentation inastudydonetosegmentthecolorectalcancerregionsuietal217established
a novel approach based on transformer that performed the segmentation as well as detection of colorectal cancer
region collectively their model was based on two pipelines one for the detection and the other for the segmentation
in the detection part region proposals were generated they utilize image level decision approach that was based
on auto encoders whereas in the segmentation part they used patches of the image as input and to make the final
mask prediction class embeddings were used they compared their model with the faster cnn and yolov3 for the
detection task and their model performed exceptionally well on the used dataset giving an accuracy of 886 where
as the segmentation score came out to be 911 which was way better than unet139 and fcn201
633 xray or radiographic images
breast tumor segmentation correct and accurate segmentation of tumor in abvs is a difficult task because the
sizeofimageishugeanditsqualityislowinordertosegmenttumorfromtheabvsimagesliuetal218adopted
the use of both transformers and cnns and named their model as 3dunet they joined the attention module and
the unet139 model for further improvements in the performance they also made use of atrous spatial pyramid
poolingasppintheirmodelasppcanhelpcatchtheinformationatmultiscalestheycomparedtheirmodelwith
different3dsegmentationmodelslike3dfcn2193dpspnet220andrecordedthedicescorecoefficienttheir
score recorded as 7636 611 which was better than the networks that were used for the comparison
anatomicalstructuresegmentation mostofthesegmentationnetworksworkonsupervisedlearningwhereexpert
labeled image is required as a label and this is an obstacle if there arent much experts available hence in order to
make an annotation efficient lu et al 221 introduced contour transformer network ctn which is an annotation
efficient segmentation method for anatomical structures they copied the human ability doing the segmentation of
anatomical structures with very less exemplars available to achieve this they proposed a semi supervised learning
mechanismthatutilizetheresemblanceofstructureandappearanceofthedesiredobjectbetweenunlabeledandlabeled
images they made the segmentations of anatomy in the form of contour evolution process and model the behavior
by gcns they named their model as one shot anatomy segmentation model on performing the segmentation on
four different anatomies their model comprehensively performed better than u supervised learning mechanisms and
performed competitively against the supervised state of the art methods upon experiments the accuracy of one shot
model came out to be 9658 which was almost 15 better than braintorm which is another one shot based model
whereas in comparison with nonlearning based model the accuracy was 16 improved one shortcoming of their
networkisthattheirnetworkonlyperformedon2ddatahenceextendingthisarchitecturetoworkon3ddatawould
be an important step in the field of 3d segmentation
guide wire segmentation a study done by researchers tried to resolve the task of segmentation of guide wire
in xray fluoroscopy sequence zhang et al 222 proposed a network that takes in account current frame as well
as the previous frame while taking input for the guidewire segmentation by considering both frames helped them
in obtaining the temporary information their network contained two parts one was a cnn and the other was a
transformer the cnn wasnt able to capture the global features hence transformer came into play that can learn the
global features by using its attention mechanism cnn and transformer lied in the encoder part whereas decoder
contained up sampling concatenating operations and convolutions they evaluated their model on datasets from
three different hospitals and measured the f1score and compared their score with other state of the art models like
frrnet223 parnet224 and unet139 and their model was outperforming all other models
  page 36 of 60
vision transformers in medical computer vision  a contemplative retrospection
tooth root segmentation an accurate and precise segmentation of the boundaries present in the roots of the tooth
isnecessarytoattainaperfectrootcanaltherapyassessmentlietal225introducedagrouptransformernetwork
gtunetinordertoachievesegmentationofrootboundariestheirmodelsstructurewassimilartotheunetbut
they used group of transformers in place of encoders and decoders also in order to incorporate the prior knowledge
theyusedfourierdescriptorlosstheirmodelachievedanaccuracyof9631andf1scoreof8458outperforming
other state of the art models
634 oct or fundus images
drusen segmentation it is very crucial to diagnose the amd at an early stage in retinal oct images via drusen
segmentationinordertoachieveanaccuratesegmentationwangetal226proposedamultiscaletransformerglobal
attention network mstganet for the segmentation of drusen in retinal oct images their model was composed of
a ushaped architecture containing an encoder and decoder to collect the nonlocal features at different scales with
ling term dependencies from multiple encoder layers a novel multi scale transformer non local module is proposed
and used at encoders top another module msgcs was introduced to assist the model to join different semantic
knowledgebetweenencoderanddecodertheyalsointroducedasemisupervisedversionofmstganetthisversion
wascomprisedofpseudolabeleddataaugmentationstrategythismodelcanusedhugeamountofunlabeleddatain
order to increase the performance on segmentation upon experiments the dsc came out to be 08692 with a std of
00052outperformingtheotherstateoftheartmodelsthismodelwastrainedonasmallerdatasethoweveritwillbe
better to collect a larger set of data in order to see its efficiency also different semisupervised learning approaches
can also be used to further improve its performance
the table 6 given below summarized the performance gain by the reviewed articles of the segmentation category
table 6 a list of datasets and performance measures adopted by researchers for segmentation
modality publication dataset performance measures
mricheng et al210 brats2020107dice score  090
hausdorff distance  44
auc  9104
accuracy  90
sensitivity  8750
specificity  9211
chartasis et al215eri110 dice score  082
chaos111 dice score  085
sun et al212mrbrains108 dice score  083
iseg2017106 dice score  087
sinclair et al214 ukbb109 dice score  086
hausdorff distance  72
sagar et al211 brats2019 dice score  086
  page 37 of 60
vision transformers in medical computer vision  a contemplative retrospection
hausdorff distance  71
gao et al216 mms2dice score  086
hausdorff distance  96
xray imageslu et al221oaiiou  9732
hausdorff distance  60
jsrt113iou  9475
hausdorff distance  121
li et al 225 drive112 dice score  092
ct scans la et al203 kits2019114 dice score  088
octfundus images wang et al226 uscd115 dice score  086
64 clinical report generation
in this section we briefly describe various transformer models to generate the medical reports and address the
preceding challenges associated with automatic clinical report generation
641 supervised learning based approaches
supervisedlearningreferstoatypeoflearningalgorithmsthatlearnunderthepresenceofasupervisoraninput
fromthetrainingsetispassedthroughthenetworkthentheoutputofthenetworkiscomparedtothedesiredoutputand
learningweightsareupdatedaccordinglyfollowingstudieshaveemployedsupervisedlearningintheirmethodologies
incorporatinggloballevelfeatures globallevelfeaturesareextractedfromtheentiremedicalimageieencoded
features of both normal and disease regions in the image following studies have incorporated this notion into their
methodologiesyouetal144proposedatransformerbasedarchitecturealigntransformertheyresolvedthedata
bias and long sequence modeling problems to generate a coherent medical report by delineating the normal and
abnormalregionstheyusedresnet50pretrainedonimagenetandfinetunedonchexpertdatasettoextractvisual
features furthermore they fed the extracted visual features into the pretrained multilabel classification netwrok to
predict the disease tags align hierarchical attention as an encoder aligned the disease tags and visual regions by
learning the correlation and relationship between them moreover they acquired the multigrained diseasegrounded
visual features from the aligned disease tags and visual regions to alleviate the data bias problem multigrained
transformer as a decoder exploited the multigrained disease grounded visual features to generate a proper medical
report in automatic evaluation they compared their experimental results with the previous stateoftheart models
ier2genppkedandsentsatkgetcandachievedcompetitiveresultsoniuxrayandmimiccxrdatasets
in human evaluation the results of their model were far better than that of r2gen model similarly amjoud et al
227 also proposed a transformerbased deep learning model for generating long and detailed reports of chest xray
images they used a pretrained densenet121 82 instead of resnet50 144 to avoid gradient vanishing problem
and redundant feature maps they suppressed the last classification layer of the pretrained model to extract global
andregionalfeaturesfrommedicalimagesafterthattheextractedfeatureswerefedasinputintotheencodertomap
themintoasequenceofcontinuousrepresentationstheymodifiedthedecoderofthevanillatransformerbyaddinga
relational memory module to the normalization layer experiments demonstrated that their model generated detailed
findings reports for iu chest xrays test images and outperformed the stateoftheart models for blue1 blue2
and rouge metrics with 0479 0359 and 0380 scores respectively however the model could not perform well
  page 38 of 60
vision transformers in medical computer vision  a contemplative retrospection
forblue3blue4andmeteormetricsalsotheyusedasmallcorpusfortrainingasaresultsomesentences
were unseen during inference which lead to the scattering problem
in another work pahwa et al 143 leveraged the skip connections by proposing a transformerbased architecture
namedmedskipbymodifyingahighresolutionnetworkhrnettheymodifiedhrnet228forvisualfeatures
extraction by incorporating skip connections along with convolutional block attention modules cbam first they
extracted features representation from each downsampled layer and after extracting crucial features using attention
cbamconcatenatedthemcbamconstitutedspatiallyandchannelattentionsubmodulesforinferringa1dchannel
attention map and a 2d spatial attention map respectively the proposed architecture also contained a memory
driven transformer which constituted a standard encoder but the decoder contained a memorydriven conditional
normalization layer to incorporate relational memory the decoder facilitated the learning from patterns in reports
and recorded key information of the generated process extensive experiments on two publicly available datasets
peir gross and iu chest xrays showed that their proposed model had given the stateoftheart results for bleu
meteor and rogue metrics
incorporating global and local level features a medical image contains both normal and disease regions to
encode the disease regions of the image previous studies encoded the complete image which lead to the encoding of
irrelevant visual content which is adverse for radiology report generation some diseases have strong correlation and
findingthosecorrelationisbeneficialforgeneratingreportforrarediseasesvariousstudiestriedtotakeadvantageof
thisfeaturebyconstructingcorrelationmatrixintheencodingstagebydatadrivenmethodologiesorexpertknowledge
but these studies failed to decode these correlations effectively while decoding
to address these problems jia et al 229 leveraged the transformerbased architecture and proposed a fewshot
radiology report generation model namely transgen in the encoding stage they introduced a semanticaware visual
learningsvlmoduleinwhichtheyusedresnet101toidentifyandcapturethediseaseregionsofrarediseasesthey
capturedthediseaseregionsfromtheimageitselfandthefeaturemapgeneratedattimestept1bylearningthetwo
masksrespectivelytorefinethevisualrepresentationofrarediseasestheyadoptedaweightedsumofthesetwomasks
attimestepttolearnthevisualrepresentationsefficientlybyincorporatingbothglobalandlocallevelinformationfor
efficientdecodingofencodedcorrelationamongthediseasesthememoryaugmentedsemanticenhancementmodule
wasintroducedatthedecodingstageexperimentsdemonstratedthattheirmodeloutperformedthestateofartmodels
on the mimiccxr dataset but could not perform well for the iu xray dataset
similarly lee et al230 also incorporated both local and global level features by proposing cross encoder
decoder transformer cedt contained a globallocal visual extractor they used a convolution neural network
egresnet101asaglobalvisualextractortoencodethecompleteradiologyimageintoasequenceofpatchfeatures
toaccuratelycapturethefeaturesatthegloballeveliebonestructureorsizeoftheorganhoweverwhileincorporating
globallevelfeaturesitwasdifficulttoencodetheexactlocationandthesizeofthelesionareatoaddressthisproblem
theycroppedthediseaseregionsoftheimagewiththelastlayerofthecnnusingtheattentionguidedmaskinference
process and after resizing to the same size as the image used them as input to the local feature extractor to extract
the local level features then they concatenated the local visual features and global visual features and used them as
input to the cedt the standard transformer uses only the last layer information but they 230 also used lowlevel
features in addition to the highlevel features by using the concept of 231 they used multiple encoders to get the
alllevel information from them and utilize the outputs of all encoders on each decoder using parallel multiheaded
attentiontheyaddedtheextractedfeaturesofeachencoderlayerwhichresultedinbettercaptioningthanthebaseline
modelr2genfurthermoretheyalsoemployedmclnandrmforrecordingandutilizingtheimportantinformation
extensiveexperimentsdemonstratedthattheirmodeloutperformedthebaselinemodelforbleu1bleu2bleu
3meteorandrougeloniuxraydatasettheyalsoperformedexperimentwithpretrainedglvebutitcould
not perform well for the bleu4 metric
642 reinforcement learning based approach
previous studies 143 144 227 229 230 have used supervised learning approaches to generate medical reports
supervised learning approaches are prone to exposure bias problems in language modeling methods to address this
problemxiongetal232proposedanovelhierarchicalneuralnetworkarchitectureusingreinforcementlearningto
generate a long coherent medical report they incorporated the selfcritical reinforcement learning method into the
detector encoder and captioning decoder previous studies used only topdown visual encoders however this was
the first study that incorporated a bottomup visual detector as well to extract semantic rich features from the medical
  page 39 of 60
vision transformers in medical computer vision  a contemplative retrospection
imagesforthispurposefirstlytheyuseddensenet121pretrainedonchestxray14datasettodetecttheregionof
interestroiproposalsusingabottomupattentionmechanismtheregiondetectoroutputtedasetofroiproposals
alongwithclassifiedclassesandsomeassociatedattributessecondlytheyusedtopdowntransformervisualencoder
to extract further pixelwise visual information from proposed roi using pooling operations lastly the transformer
captioningmoduleusedimprovedroiproposalsasinputfromthetransformervisualencoderandgenerateddescriptive
sentences for each proposed roi by calculating reward directly using the cider metric their proposed architecture
outperformedthestateoftheartmethodsfortheciderevaluationmetricontheiuxraydatasetbutforthebelu1
metric their model could not perform stateoftheart their model overfitted as they used only the findings portion
of the generated medical report this problem can be resolved using a larger labelled dataset
thetable7givenbelowsummarizedtheperformancegainbythereviewedarticlesoftheclinicalreportgeneration
table 7 a list of datasets and performance measures adopted by researchers for clinical report generation
modality publication dataset performance measures
xray imagesyou et al 144iu xray 116bleu1  0484
bleu2  0313
bleu3  0225
bleu40173
metero0204
rougel0379
mimiccxr 117bleu10378
bleu20235
bleu30156
bleu40112
metero0158
rougel  0283
bleu10479
xray imagesamjoud et al227 iu xray 116bleu20359
rougel  0380
  page 40 of 60
vision transformers in medical computer vision  a contemplative retrospection
xrayspahwa et al 143iu xray 116bleu1 0467
bleu20297
bleu30214
bleu40162
metero0187
rougel0355
peir gross 118bleu1 0399
bleu20278
bleu30209
bleu40148
metero0176
rougel0414
jia et al229iu xray 116bleu10461
bleu20285
bleu30196
bleu40145
rougel 0367
ka   0367
mimiccxr 117bleu10368
bleu20243
bleu30178
  page 41 of 60
vision transformers in medical computer vision  a contemplative retrospection
bleu40138
rougel 0338
bleu105064
iu xray 116bleu203195
bleu302201
lee et al230bleu401924
rougel 03802
xray images
xiong et al 232 xray 116bleu10350
bleu20234
bleu30143
bleu40096
cider  0323
65 miscellaneous vit applications in medical imaging
tranformerbased architecture has also played a vital role in other applications of medical field ie in image
synthesis denoising the low dose computed tomography and positron emission tomography images enhancing the
resolution of medical images etc
651 functional magnetic resonance imaging fmri scans
visualizing regenerated neural visual content in the past decades few studies have been conducted to decode
thehumanbrainneuralactivitiesintonaturallanguagesentencesthemainpurposeofdecodingbrainneuralactivity
is basically to know the human brains perception of textual or visual content in the past most deep learning studies
focused on different task specific decodings ie detection classification recognition etc using functional magnetic
resonance imaging fmri data with the advancement in technology several research has been done in language
decoding to decode the human brain semantics evoked by linguistic stimuli into natural language words or sentences
inspiringfromthesetaskszhangetal59proposedahybridlanguagedecodingmodelcnntransformertodecode
the visual stimuli evoked at multitimes by natural images into descriptive sentences they exploited the concept of
neuralmachinetranslationnmt17butthedifferencewasinsourcesequenceienaturalimagesinnmtbutvisual
neuralactivitiesin59toachievethistaskfirstlytheyextractedmeaningfulsemanticlowdimensionalfeaturesfrom
highdimensionalvisualneuralactivitieslowlevelrawfmridatausingtwolayeronedimensionalcnnsecondly
the encoder part of the transformer encoded the semantic features into multilevel abstract representation lastly the
  page 42 of 60
vision transformers in medical computer vision  a contemplative retrospection
decoder of the transformer decoded the multilevel representation into descriptive natural language sentences they
comparedtheirmodelwithotherdecodingmodelsandachievedstateoftheartresultsforbleuciderandrouge
metricswith017066and018scoresrespectivelyinfuturethistransformerbasedbraindecodingtechnologywill
beusefulforthepeoplewhoareunabletotransmittheirvisualperceptionintospeechandwillalsobeabreakthrough
for neuroscientists in understanding and decoding the neural activities of human brain
652 petct scans
medical image enhancement computed tomography is a noninvasive imaging technique for medical diagnosis
since high exposure to xrays radiation is deadly for humans and has become the main concern for medical
practitioners to lessen this effect of xrays radiation it is used in less quantity in ct scans but it poses some serious
problems ie less contrast sharp features corners edges and stronger noise which affects the quality of ct scans
although lowdose computed tomography ldct is mainstream in clinical applications but the posed problems
causehindranceinaneffectiveclinicaldiagnosismanytraditionalmethodsiterativemethodsandconvolutionbased
deep learning approaches were employed to acquire high quality ldct images by deblurring and suppressing the
artifactsthehighfrequencysubbandofimagesarenoisyareaswhilethelowfrequencysubbandarenoisefreeareas
containingmainimagecontentsinceconvolutionbasedmethodsarelimitedtoextractingfeaturesfromthelocalareas
of images due to limited receptive fields therefore transformers came into the scientific field and revolutionized the
world with their facts of capturing longrange dependencies between image regions
keepinginaccountalltheseobservationszhangetal233proposedatransformerbasedarchitecturetodenoise
the ldct images by decomposing them into high frequency hf and lowfrequency parts hence the noise was
only retained in the hf part and it also contained a plethora of image textures to ensure the relationship between
hf and lf parts of the image they denoised the noisy hf part with the assistance of the latent texture of the lf
part for this purpose they employed cnns to extract corresponding texture and content features from the lf image
partfurthermoretheyacquiredthehighlevelfeaturesfromthetransformerusingthetexturefeaturesfromthenoisy
lf and embeddings from the hf part they used a modified transformer with three encoders and three decoders
finally they reconstructed the highquality ldct image piecewise by combining these highlevel features with the
content features from the lf part of the image ent features from the lf part of the image extensive experiments
demonstrated that their model outperformed all the baseline methods achieving 937 for vit metrics improved
structuresimilarityby123androotmeansquareerrorloweredby405onmayolowdosecomputedtomography
images dataset since convolutionbased methods cannot capture global contextual information wang et al 6 first
time proposed a convolutionfree tokentotoken vision transformerbased dilation network to denoise the ldct
imagestheycapturedthenoisefrominputmedicalimagesbylearningdeepfeaturesafterthattheyremovethenoisy
estimated residual images in order to clean them firstly they used tokenization block to tokenize the feature map
patches into tokens secondly they fed those tokens into transformer block further for enhancing the tokenization
they applied tokenization in cascaded form in tokentotoken block they further enlarged the receptive field and
refinedthecontextualinformationusingdilationintokenizationproceduretheyperformeddilationusingreshaping
soft split and cyclic shift to enhance the context they compared their model with other stateoftheart models and
their model outperformed for ssim and rmse metrics with 09144 and 87681 scores respectively in denoising the
images without downscaling tokenization of the image can be enhanced
petmricanconcurrentlyprovideanatomicalandmorphologicalimaginginformationthataidsinclinicaldisease
diagnosis pet acquires metabolic imaging information with the help of radiotracers while mri uses magnetic field
gradientsandradiowavestoacquireimagesofsoftbodytissuesalthoughtheseimagingmodalitieshaveapplications
indiseasediagnosisiecancertumorandbraindiseasesbutalsoposesomeseriousconcernssincetimerequirement
for pet imaging acquisition is high and as a result patient discomfort can affect the image quality ie low contrast
tonoise ratio the information from mri can assist in denoising the pet images using registration approach many
traditional deep learning and computer vision methods proposed to enhance the pet image quality using mri but
duetodiscrepancyinmodalitiestheycouldnotextractcontextualandspatialinformationefficientlytoaddressthese
problems zhang et al 234 proposed a spatial adaptive and transformer fusion network stfnet for denoising low
count pet with mri they adapted dual path using the spatialadaptive block to extract features for the fusion of
highlevelfeaturestheymodifiedthetraditionaltransformerencoderandincorporatedglobalattentiontoformapixel
topixelrelationshipbetweenmriandpetthefusedfeaturemapwasusedasinputtothedecoderforpetdenoising
their model obtained promising results for on rmsepsnrssim and pcc metrics
  page 43 of 60
vision transformers in medical computer vision  a contemplative retrospection
similarly luo et al 235 proposed 3d transformergan to build a standard dose pet image from the low
dosepetimagetheyleveragedthecnntransformerarchitecturetoincorporatebothglobalandlocalinformation
cnnbasedencoderextractedenrichedspatialinformationfromtheinputmedicalimagemoreoverthetransformer
captured the longrange dependency from the extracted features from the cnnbasedencoder the learned repre
sentation from the transformer are incorporated into the cnnbaseddecoder to restore them and reconstructed the
standard pet image extensive experiments demonstrated that their model outperformed the stateoftheart on real
human brain dataset
653 magnetic resonance imaging mri scans
medical image reconstruction mri is a prevalent noninvasive imaging technique but its acquisition process is
slowconsequentlythereisaneedtodevelopacceleratedmrimethodssimultaneouslyseveralstudiesondeepneural
networkshavebeenconductedtodevelopstateoftheartmethodsforacceleratedmrithereforekorkmazetal3
accelerated mri by reconstructing fullsampled mri images using unsupervised learning incorporating deep image
prior framework to alleviate the problem of undersampled acquisition they proposed generative vision transformer
basedunsupervisedmrireconstructionarchitecturetoincreasethereceptivefieldfirstlytheyperformedgenerative
nonlinear mapping over latent and noisy space to improve the invertibility of the model secondly they used cross
attention to improve the context ie both global and local context of image and latent features they did not use
anypretrainedmodellastlytheyperformedinferenceoneachindividualsubjecttoincreasethegeneralizationthey
performedextensiveexperimentsonacceleratedmulticontrastbrainmridatasetandoutperformedtheconvolutional
based generative models for psnr and ssim metrics
wang et al 236 proposed a superresolution approach to reconstruct the high resolution mri scans from low
resolution scans they proposed adjacent slices feature transformer asft network firstly they incorporated extra
slices in the consecutive inplane slices of anisotropic 3d mri images secondly to harness the similarity between
theconsecutiveslicestheyintroducedamultibranchfeaturestransformationandextractionmfteblockthirdlyto
enrichthetargethighresolutionsliceswiththeinformationfromthelowresolutionreferenceslicestheyfilteredoutthe
uselessinformationusingmfteblockmoreovertheyusedspatialadaptiveblocktorefinethefeaturesspatiallythey
used channel attention to incorporate the multiscale features and consequently they enhanced the super resolution
their model achieved the stateoftheart performance for superresolution task
medical image synthesis tissue morphology information acquired from multimodal medical images play an
important role in the clinical practice however it is not commonly used because of the expensive capturing of these
information generative models such as generative adversarial network gan are in practice to artificially synthesis
theseimagesganisacnnbasedarchitecturethatshowslocalitybiasandspatialinvarianceacrossallthepositions
huetal237introducedatransformerbaseddoublescaledeeplearningarchitectureforcrossmodalmedicalimage
synthesis to incorporate logrange dependencies doublescale gan showed efficient performance on benchmark ixi
mri dataset
medicalimageregistration tissuedeformationinahighlydeformableanatomyisestimatedusingimageregistra
tiondiffeomorphicregistrationisoneoftheimageregistrationtechniquesthatpreservestheinvertibleandonetoone
mappingbetweenimagescurrentdeeplearningtechniqueslacktheabilitytohandlelongrangerelevancethuslimiting
the meaningful contextual correspondence between images in the paper 238 a dual transformer network dtn
modelisproposedforthediffeomorphicregistrationofmrimagesdtnusingselfattentionmechanismsmodelthe
relevance from both the separate and concatenated images embeddings which facilitate contextual correspondence
between anatomies dtn consists of learnable embedding module relevance learning module and registration field
inferencemodulediffeomorphicregistrationfieldisestimatedusingmovingfixedandmovingimagesforonetoone
mapping dtn has two branches to learn the relevance based on the embeddings of separate onechannel images
andconcatenatedtwochannelimagesfirstthelowlevelimagefeaturefortheconcatenatedandseparateimagesare
extracted using cnn second the image features converted into sequences are fed to dtn for feature enhancement
basedonglobalcorrespondenceconcatenatedfeaturesformbothbranchesarethenusedtoinferthevelocityfieldand
registration filed the deformation field is represented as the exponential of the velocity which ensure the invertible
mapping metric space is used to optimize the proposed dtn in an unsupervised manner
  page 44 of 60
vision transformers in medical computer vision  a contemplative retrospection
654 endoscopy images
medicalimagereconstruction endoscopeisaminimallyinvasivemedicalimagingmodalityitassistsinmedical
diagnoses by acquiring accurate images of the internal organs of a patients body however the small imaging sensor
in the endoscope causes problems in acquiring the high magnified blood vessels images many traditional methods
incorporatinginterpolatedmethodsanddeeplearninghaveapplicationsinreconstructionofsuperresolutionofsingle
images mainly convolutionbased deep learning methods are there in reconstructing highresolution images which
have problems in capturing the global context consequently gu et al 239 leveraged transformers and proposed
hybrid architecture with the convolutional neural network to enhance the texture of blood vessels firstly cnn
extracted the lowlevel features from lowresolution lr images and the transformer sampled the lr image into
threedifferentsamplestoextracttexturefromtheimagesecondlysimilaritywasexaminedbetweendifferentfeatures
extracted from transformerbased extractor and cnnbased extractor thirdly they employed a texture migration
method to interchange the information between multiscale features extracted from the transformer and cnn to
synthesizetheimagelastlysubpixelconvolutionoperationwasperformedonmigratedbasicimagestosynthesizea
highresolutiontheirmodelacquiredpromisingqualitativeandquantitativeresultsthanthecnnbasedsingleimage
superresolution methods
655 fluorescence microscopy
quantitative characterization of anatomical structure using combination of markers in bone marrow
vasculatureandfetallivertissues fluorescencemicroscopyisavariantoftraditionalmicroscopywhichusesa
higherintensitylightsourcethatexcitesafluorescentspeciesinasampleofinterestitisusedfordifferentpurposessuch
asdetailedexaminationofanatomicallandmarkscellsandcellularnetworksalvarogomarizetal240proposeda
markersamplingandexcitenetworktoexploitthepotentialofattentionbasednetworkonthefluorescencemicroscopy
datasets which are underexplored by the deep learning the capability of the network is tested by the quantitative
characterization in various datasets of microvessels in fetal liver tissues and bone marrow vasculature in 3d confocal
microscopydatasetsproposedmodelgivesaconvincingperformancewithf1scoreof912forsinusoidsand712
for arteries in the liver vasculature dataset
denoising of celullar microscopy images for assisted augmented microscopy deep learning has greatly
assisted augmented microscopy that enable highquality microscopy images with using costly microscopy apparatus
zhengyang wang et al 241 proposed global voxel transformer networks gvtnets that uses attention operators
to address the limitations in already existing unet based neural instead of local operator that lack dynamic non
local information aggregation they used attention operators that allow global receptive field during prediction they
measured the performance of the model on three existing datasets for three different augmented microscopy tasks
656 histopathology images
wholeslideimagescontainrichcontentabouttheanatomicalandmorphologicalcharacteristicstobetterdepict
theimagecontentpathologistsfrequentlyexaminethetagsassociatedwiththeseimagessincethistaggingprocessis
labourintensive consequently li et al 242 first time proposed a patch transformer based architecture to automate
the multitagging whole slide images process they incorporated attention mechanism to extract global level features
from the patches of images they employed multitag attention module to build tag on the basis of weight extensive
experiments demonstrated that their model outperformed on 4920 wsi for macro and micro f1 metric
657 oct or fundus images
diabetes damages the retina and causes diabetic retinopathy this disease can lead to vision loss therefore early
detectioniscrucialvariousdeeplearningapproacheshaveautomatedtherecognitionofdiabeticretinopathygrading
however wu et al 158 proposed vision transformer based architecture to assist the ophthalmologist in recognising
the diabetic retinopathy grade they divided the fundus images into patches flatten them to generate sequence and
thenconvertedthemintolinearandpositionalembeddingstogeneratethefinalrepresentationstheyfedthepositional
embeddingsintomultiheadattentionlayerstheirmodeloutperformedthecnnbasedarchitecturewithanaccuracy
of 914
thetable8givenbelowsummarizedtheperformancegainbythereviewedarticlesofthemiscellaneouscategory
  page 45 of 60
vision transformers in medical computer vision  a contemplative retrospection
table 8 a list of datasets and performance measures adopted by researchers for miscellaneous application in vit
modality publication dataset performance measures
ct scanswang et al 6nihaapmmayo clinic
ldct grand challenge
119rmse  87681
ssim  09144
rmse  21199
2054
zhang et al 233nihaapmmayo
clinic ldct grand
challenge119ssim  0933
0012
vif  0144
0025
pet scanszhang et al234 upmr790rmse  00447
psnr db  275321
ssim  09291
pcc  09899
luo et al 235psnr  24818
clinical dataset which in
cludes eight normal control
nc subjectsssim  0986
nmse  00212
psnr  25249
clinical dataset which in
cludes eight mild cognitive
impairment mci subjectsssim  0987
nmse  00231
endoscopy images
gu et al 239diverse 2k resolution high
quality div2k images
dataset 121
set5 psnr db  3194
  page 46 of 60
vision transformers in medical computer vision  a contemplative retrospection
ssim  08935
set14 psnr db  2811
ssim  07842
b100 psnr db  2754
ssim  07464
urban100 psnr db  2587
ssim  07844
manga109 psnr db  3009
ssim  09077
mri scanswang et al 236kirby21 dataset kki01 to
kki05 120 psnr  4019
204
ssim 09882
00034
dice  091
tang et al 238t1weighted images of size
182218182of102drug
addicts and 10 healthy vol
unteershd  268
asd  059
ixi dataset
t1 r4 psnr  3255
177
ssim   9458
082
kokmaz et al 3 t1 r8 psnr  3028
168
ssim   9164
142
t2 r4 psnr  3271
073
ssim   8766
167
t2 r8 psnr  2990
070
ssim   8403
189
  page 47 of 60
vision transformers in medical computer vision  a contemplative retrospection
hu et al 237 ixi dataset
t1 psnr  3491
100
ssim  0895
007
t2 psnr  3534
095
ssim   0895
007
fmri scanscider  0741
zhang et al 59fmri experimentsthe vi
sual stimulus consisted of
2750 natural images from
imagenet datarougel 02009
bleu 0186
fluorescence
microscopygomariz et al 2403d confocal microscopy
datasetsf1 score  0912
39
fundus images wu et al 158diabeticretinopathydetec
tion datasetaccuracy   914
specificity  0977
precision  0928
sensitivity  0926
quadratic weighted kappa
score  0935
auc  0986
wsi li et al 2424920 wsis provided by a
histopathology service com
panymcro f1  0910
micro f1  0944
7 discussion and conclusion
vision transformers vit are now one of the hottest topics in the discipline of computer vision because of its
exemplaryperformanceandtremendouspotentialcomparedwithcnnsalthoughcnnsarematuredenoughforthe
development of applications that can ensure an efficient and accurate diagnosis however in the medical field where
an inaccurate output might endanger lives the concept of attention in vision transformers has paved its way for more
precise outcomes since vit models assess the global context of the image along with the interpretability through
  page 48 of 60
vision transformers in medical computer vision  a contemplative retrospection
the attention module their performance is more precise than cnns a variety of approaches have been proposed in
recentyearsasoutlinedandsummarizedinthisreviewtoexploreandutilizethecompetencyofvisiontransformers
theseapproachesshowedexcellentperformanceonawiderangeofvisualrecognitiontasksincludingclassification
lesionsdetectionanatomicalstructuresegmentationandclinicalreportgenerationneverthelesstherealpotentialof
transformersforcomputervisionhasyettobefullyexploredwhichmeansthatsignificantchallengesarestillthereto
be resolved the following section envisages these challenges as well as provides insights on future prospects
71 current trends and open challenges
althoughseveraltransformerbasedmodelshavebeenproposedbyresearcherstoaddressvisualrecognitiontasks
thesemethodsaresimplytheinitialstepsinthisfieldandthereisstillconsiderableroomforimprovementforexample
the transformer architecture in vit 19 is based on the standard transformer for nlp 16 although an enhanced
versionparticularlytailoredforcvneedstobeexploredfurthermoreitisnecessarytoemploytransformerstoother
medical domains such as orthodotics medical report generation and phenotype extraction etc to discover the power
of vision transformers
in this multidisciplinary study we provide a comprehensive view of how vision transformers employed medical
imaging modalities in visual recognition tasks for assisted diagnosis we have reviewed the research articles selected
from top tier journals and conferences in which researchers have proposed excellent frameworks that employed
vision transformers to accelerate the efficiency and performance of already proposed cnn based models the
figure 5 is a clear evidence that vision transformers are widely employed in classification and segmentation related
visual recognition tasks whereas their significance in registration related tasks have not been greatly explored the
miscellaneous studies in this review include the work on enhancing the resolutions for better outcomes denoising of
ct scans with a low dosage of xray radiation image registration etc nevertheless the undertaken studies are based
ondiscreteapplicationsthereforetheyarenotcomparablethefieldofresearchisquiteopenatthisstageasbaselines
areavailablethenreportgenerationcoveredonly7whichindicatesanareatoresearchuponasitsapplicationscan
assist both doctors and patients
similarly in the imaging modalities it can be observed in figure 6 that 61 of the papers found are related to
xraysandctscansthevisiontransformermodelwasfirstproposedintheyear2020andmostofthecorresponding
literature available was related to the diagnosis of coronavirus as it was prevailing in the same year the transformer
models achieved admirable results in regards to covid19 however other domains especially digital pathology
retinal breast etc require attention as not much literature is available also the literature related to covid19 is
eitherclassifyingordetectingthevirusthusnostudieswerefoundthatwereperformingsegmentationeventhough
mri produces images with better resolution than ct scans it covered only 13 of the total modalities the papers
aremostlyusedforimagesegmentationandhaveproducedrespectableoutcomestheseoutcomesarebasedonafew
studies targeting distinct diseases hence there is much more to explore
inadditionitwasobservedinclinicalreportgenerationsreviewthatamongvariousmodalitiesthestudiesrelated
to xray images were only available thus it means that the data required for report generation is only present in
correspondence to xray images as xrays are a costefficient method for examining various parts of the body it
is comprehensible that due to more reports and xray images available it was possible to collect enough data for
automaticreportgenerationsinceitassistsdoctorsinwritingmedicalreportsthecollectionofcorrespondingimages
and text should be collected to implement automated report generations in other domains
next one of the major limitations regarding medical datasets is that not enough data is available to train a
transformermodelthesemodelsrequiredatainhugeamountstoperformwellandadapttogeneralizabilityanother
observationisthenonavailabilityofabenchmarkasobservedinthefigure16sincealmostallthestudiesarebased
on different datasets the performances of the models are not comparable even if the datasets are the same in some
studies they are either used for different purposes or are combined with other datasets to form one large repository
now as long as there is no standard dataset for experimenting with different proposed models research can not take
its step forwards towards development using vision transformers
  page 49 of 60
vision transformers in medical computer vision  a contemplative retrospection
figure 16 longtail graphical representation of datasets employed in literature
72 future prospects
in order to propel the development of vision transformers in medical domain we propose various significant
future directions one direction is that several papers in this study have done a comparative analysis of cnns and
vit thus it can be inferred that transformer models have given better results in terms of efficiency computational
cost and accuracy since these models have achieve such outcomes where researchers are talking about cnns being
replaceablevisiontransformersrequiremoreresearchandimplementationastheyareunravelingapathtowardsmore
resourceefficient models
the paper demonstrates the significance of using vision transformers on medical images the future directions of
research involve working with more heterogeneous data sets and more extensive comparisons with other models to
give validity to the proposed transformer models next the studies may they be related to any category or modality
are using different datasets hence there cannot be a comparative analysis in terms of the proposed vision transformer
models therefore we should work on creating a benchmark dataset
similarly new knowledge emerging in cancer biology and deep learning enabled us to step into this rapidly evolving
domain genomic profiling is prevalent however it is crucial to correlate cancer genomic and phenotypic data to
fully understand cancer behavior cancer phenotype information includes tumor morphology eg histopathologic
diagnosislaboratoryresultseggeneamplificationstatusspecifictumorbehaviorsegmetastasisandresponse
totreatmentegtheeffectofachemotherapeuticagentontumorvolumevisiontransformerscanalsobeappliedto
medical images in order extract phenotypic information in order to better diagnose cancer related disorders another
directionisrelatedtothelimitationregardingcoronaviruscasesbeingdetectedandclassifiedbutnotbeingsegmented
the usage of segmenting the virus can help determine the rate of spread of the virus that is why it should be worked
uponinthefuturefurthermoretheliteratureforclinicalreportgenerationisrelatedtoxraysimagesandasdiscussed
abovemostofthevisiontransformersusedforxrayimagesarebeingusedtodetectcoronavirusasperourknowledge
none of the literature is related to medical report generation for coronavirus and considering the work that has been
done in this regard research in medical image captioning can propel towards the application side and assist the
practitioners overall since it covers only 7  of the total tasks in this study there is more to explore in terms of
xraysandotherimagingmodalitiesalthoughvisiontransformershaveachievedanothermilestoneforimprovedand
accuratediagnosisinthemedicaldomainthereisstillroomforimprovementintermsofresourceefficiencyinother
words we still have to discover the undiscovered
  page 50 of 60
vision transformers in medical computer vision  a contemplative retrospection
73 conclusion
thepaperenvisagesthesummaryanalysisofvisiontransformermodelsusedinthemedicaldomainthisstudywill
serve researchers from multidisciplinary backgrounds the visual recognition tasks considered in this paper include
classificationdetectionsegmentationandclinicalreportgenerationsotherthanthatsomemiscellaneoustaskswere
also included such as image registration image reconstruction etc furthermore medical imaging modalities such as
xraysctscansmrisetcwereusedasinputtorecognizemedicalimagingthroughvariousmodelsourgoalisto
presentthestudyinawaythatapprehendsthestatusandfuturedirectionsofvisiontransformershencewestructured
the information of datasets categories modalities and their results in a tabular form which will assist researchers to
move forward in the medical field conveniently
references
1 isabella castiglioni leonardo rundo marina codari giovanni di leo christian salvatore matteo interlenghi francesca gallivanone
andrea cozzi natascha claudia damico and francesco sardanelli ai applications to medical images from machine learning to deep
learning physica medica  83924 2021
2 s kevin zhou hayit greenspan christos davatzikos james s duncan bram van ginneken anant madabhushi jerry l prince daniel
rueckert and ronald m summers a review of deep learning in medical imaging imaging traits technology trends case studies with
progress highlights and future promises proceedings of the ieee  2021
3 yilmazkorkmazmahmutyurtsalmanulhassandarmuzafferzbeyandtolgacukur deepmrireconstructionwithgenerativevision
transformers in international workshop on machine learning for medical image reconstruction  pages 5464 springer 2021
4 james a diao jason k wang wan fung chui victoria mountain sai chowdary gullapally ramprakash srinivasan richard n mitchell
benjamin glass sara hoffman sudha k rao et al humaninterpretable image features derived from densely mapped cancer pathology
slides predict diverse molecular phenotypes nature communications  121115 2021
5 georgiosakaissismarcusrmakowskidanielrckertandrickmerfbraren secureprivacypreservingandfederatedmachinelearning
in medical imaging nature machine intelligence  26305311 2020
6 dayang wang zhan wu and hengyong yu tednet convolutionfree t2t vision transformerbased encoderdecoder dilation network for
lowdose ct denoising in international workshop on machine learning in medical imaging  pages 416425 springer 2021
7 justin m johnson and taghi m khoshgoftaar survey on deep learning with class imbalance journal of big data  61154 2019
8 nima tajbakhsh laura jeyaseelan qian li jeffrey n chiang zhihao wu and xiaowei ding embracing imperfect datasets a review of
deep learning solutions for medical image segmentation medical image analysis  63101693 2020
9 dandiyang cristhianmartinezlaravisua hardevkhandharchintan bhattandjesuscarretero detectionand analysisofcovid19 in
medical images using deep learning techniques scientific reports  111113 2021
10 chen li hao chen xiaoyan li ning xu zhijie hu dan xue shouliang qi he ma le zhang and hongzan sun a review for cervical
histopathology image analysis using machine vision approaches artificial intelligence review  53748214862 2020
11 chaoran yu and ernest johann helwig the role of ai technology in prediction diagnosis and treatment of colorectal cancer artificial
intelligence review  551323343 2022
12 ghulammurtazaliyanashuibainuddinwahidabdulwahabghulammujtabahenryfridaynwekemohammedalialgaradifariha
zulfiqarghulamrazaandnoranizaazmi deeplearningbasedbreastcancerclassificationthroughmedicalimagingmodalitiesstateof
the art and research challenges artificial intelligence review  53316551720 2020
13 ins domingues gisle pereira pedro martins hugo duarte joo santos and pedro henriques abreu using deep learning techniques in
medical imaging a systematic review of applications on ct and pet artificial intelligence review  53640934160 2020
14 geertlitjensthijskooibabakehteshamibejnordiarnaudarindraadiyososetiofrancescociompimohsenghafoorianjeroenawm
van der laak bram van ginneken and clara i snchez a survey on deep learning in medical image analysis medical image analysis 
426088 2017
15 asifullahkhananabiasohailummezahooraandaqsasaeedqureshi asurveyoftherecentarchitecturesofdeepconvolutionalneural
networks artificial intelligence review  53854555516 2020
16 ashishvaswaninoamshazeernikiparmarjakobuszkoreitllionjonesaidanngomezukaszkaiserandilliapolosukhin attention
is all you need advances in neural information processing systems  30 2017
17 dzmitrybahdanaukyunghyunchoandyoshuabengioneuralmachinetranslationbyjointlylearningtoalignandtranslate arxivpreprint
arxiv14090473  2014
18 minhthang luong hieu pham and christopher d manning effective approaches to attentionbased neural machine translation arxiv
preprint arxiv150804025  2015
19 alexeydosovitskiylucasbeyeralexanderkolesnikovdirkweissenbornxiaohuazhaithomasunterthinermostafadehghanimatthias
minderergeorgheigoldsylvaingellyetal animageisworth16x16wordstransformersforimagerecognitionatscale arxivpreprint
arxiv201011929  2020
20 salman khan muzammal naseer munawar hayat syed waqas zamir fahad shahbaz khan and mubarak shah transformers in vision
a survey acm computing surveys csur  2021
21 kaihanyunhewanghantingchenxinghaochenjianyuanguozhenhualiuyehuitanganxiaochunjingxuyixingxuetal a
survey on vision transformer ieee transactions on pattern analysis and machine intelligence  2022
22 nih national institutes of health us httpswwwnibibnihgovscienceeducationsciencetopics 
  page 51 of 60
vision transformers in medical computer vision  a contemplative retrospection
23 bin guan guoshan zhang jinkun yao xinbo wang and mengxuan wang arm fracture detection in xrays based on improved deep
convolutional neural network computers  electrical engineering  81106530 2020
24 amit kumar jaiswal prayag tiwari sachin kumar deepak gupta ashish khanna and joel jpc rodrigues identifying pneumonia in
chest xrays a deep learning approach measurement  145511518 2019
25 naveed chouhan asifullah khan jehan zeb shah mazhar hussnain and muhammad waleed khan deep convolutional neural network
and emotional learning based breast cancer detection using digital mammography computers in biology and medicine  132104318 2021
26 ademola enitan ilesanmi utairat chaumrattanakul and stanislav s makhanov a method for segmentation of tumors in breast ultrasound
images using the variant enhanced deep learning biocybernetics and biomedical engineering  412802818 2021
27 lifang chen tengfei mao and qian zhang identifying cardiomegaly in chest xrays using dual attention network applied intelligence 
pages 110 2022
28 pearlmarysamuelandthanikaiselvanveeramalai vsscnetvesselspecificskipchainconvolutionalnetworkforbloodvesselsegmentation
computer methods and programs in biomedicine  198105769 2021
29 abdelbakisouidnizarsakliandhedisakli classificationandpredictionsoflungdiseasesfromchestxraysusingmobilenetv2 applied
sciences 1162751 2021
30 abdul qayyum imran razzak m tanveer and ajay kumar depthwise dense neural network for automatic covid19 infection detection
and diagnosis annals of operations research  pages 121 2021
31 um prakash kottilingam kottursamy korhan cengiz utku kose and bui thanh hung 4xexpert systems for early prediction of
osteoporosis using multimodel algorithms measurement  180109543 2021
32 neslihanbayramoglumiikatnieminenandsimosaarakkala machinelearningbasedtextureanalysisofpatellafromxraysfordetecting
patellofemoral osteoarthritis international journal of medical informatics  157104627 2022
33 atf emre yksel sadullah gltekin enis simsar erife damla zdemir mustafa gndoar salih barkn tokgz and ibrahim ethem
hamamc dental enumeration and multiple treatment detection on panoramic xrays using deep learning scientific reports  111110
2021
34 rima arnaout lara curran yili zhao jami c levine erin chinn and anita j moongrady an ensemble of neural networks provides
expertlevel prenatal detection of complex congenital heart disease nature medicine  275882891 2021
35 hanemellethyshekharschandraandfatimaanasrallah thedetectionofmildtraumaticbraininjuryinpaediatricsusingartificialneural
networks computers in biology and medicine  135104614 2021
36 marcin woniak jakub sika and micha wieczorek deep neural network correlation learning mechanism for ct brain tumor detection
neural computing and applications  pages 116 2021
37 dhimandaskathyayinisivasubramanianpraveenbalajirajendranandmanojitpramaniklabelfreehighframerateimagingofcirculating
blood clots using a dual modal ultrasound and photoacoustic system journal of biophotonics  143e202000371 2021
38 jordan chamberlin madison r kocher jeffrey waltz madalyn snoddy natalie fc stringer joseph stephenson pooyan sahbaee puneet
sharma saikiran rapaka u joseph schoepf et al automated detection of lung nodules and coronary artery calcium using artificial
intelligence on lowdose ct scans for lung cancer screening accuracy and prognostic value bmc medicine  191114 2021
39 j akilandeswari g jothi a naveenkumar rs sabeenian p iyyanar and me paramasivam detecting pulmonary embolism using deep
neural networks international journal of performability engineering  173 2021
40 adel oulefki sos agaian thaweesak trongtirakul and azzeddine kassah laouar automatic covid19 lung infected region segmentation
and measurement using ctscans images pattern recognition  114107747 2021
41 sumitamondalanupksadhuandpranabkumardutta adaptivelocalternarypatternonparameteroptimizedfasterregionconvolutional
neural network for pulmonary emphysema diagnosis ieee access  9114135114152 2021
42 aao american academy of ophthalmology httpswwwaaoorg 
43 gabriella moraes dun jack fu marc wilson hagar khalid siegfried k wagner edward korot daniel ferraz livia faes christopher j
kellyterryspitzetal quantitativeanalysisofoctforneovascularagerelatedmaculardegenerationusingdeeplearning ophthalmology 
1285693705 2021
44 gahyungryukyungminleedonggeunparksanghyunparkandminsagong adeeplearningmodelforidentifyingdiabeticretinopathy
using optical coherence tomography angiography scientific reports  11119 2021
45 shotaro asano ryo asaoka hiroshi murata yohei hashimoto atsuya miki kazuhiko mori yoko ikeda takashi kanamoto junkichi
yamagami and kenji inoue predicting the central 10 degrees visual field in glaucoma by applying a deep learning algorithm to optical
coherence tomography images scientific reports  111110 2021
46 esther parramora alex cazaasgordon rui proena and lus a da silva cruz epiretinal membrane detection in optical coherence
tomography retinal images using deep learning ieee access  99920199219 2021
47 zhenhua wang yuanfu zhong mudi yao yan ma wenping zhang chaopeng li zhifu tao qin jiang and biao yan automated
segmentation of macular edema for the diagnosis of ocular disease using deep learning method scientific reports  111112 2021
48 syedalehassanshahzadakbarsahargullamjadrehmanandhindalaska deeplearningbasedautomaticdetectionofcentralserous
retinopathyusingopticalcoherencetomographicimages in 20211stinternationalconferenceonartificialintelligenceanddataanalytics
caida pages 206211 ieee 2021
49 m kashif yaqoob syed farooq ali irfan kareem and muhammad moazam fraz featurebased optimized deep residual network
architecture for diabetic retinopathy detection in 2020 ieee 23rd international multitopic conference inmic  pages 16 ieee 2020
50 shumpei obata yusuke ichiyama masashi kakinoki osamu sawada yoshitsugu saishin taku ito mari tomioka and masahito ohji
predictionofpostoperativevisualacuityaftervitrectomyformacularholeusingdeeplearningbasedartificialintelligence graefesarchive
for clinical and experimental ophthalmology  pages 111 2021
51 michaelabrmoffandchristinenkay chapter6imageprocessing instephenjryansrinivasrsaddadavidrhintonandrewp
schachat srinivas r sadda cp wilkinson peter wiedemann and andrew p schachat editors retina fifth edition  pages 151176
  page 52 of 60
vision transformers in medical computer vision  a contemplative retrospection
wb saunders london fifth edition edition 2013
52 quang tm pham sangil ahn jitae shin and su jeong song generating future fundus images for early agerelated macular degeneration
based on generative adversarial networks computer methods and programs in biomedicine  page 106648 2022
53 sufian a badawi muhammad moazam fraz muhammad shehzad imran mahmood sajid javed emad mosalam and ajay kamath
nileshwar detection and grading of hypertensive retinopathy using vessels tortuosity and arteriovenous ratio journal of digital imaging 
pages 121 2022
54 harimohanraiandkalyanchatterjee 2dmriimageanalysisandbraintumordetectionusingdeeplearningcnnmodelleunet multimedia
tools and applications  80283611136141 2021
55 zamirmeralijustinzwangjetanhbadhiwalachristopherdwitiwjeffersonrwilsonandmichaelgfehlings adeeplearningmodel
for detection of cervical spinal cord compression in mri scans scientific reports  111111 2021
56 saeedanazabidaashrafandahmadzaib transferlearningusingfreezefeaturesforalzheimerneurologicaldisorderdetectionusingadni
datasetmultimedia systems  2818594 2022
57 mei yang yiming zheng zhiying xie zhaoxia wang jiangxi xiao jue zhang and yun yuan a deep learning model for diagnosing
dystrophinopathies on thigh muscle mri images bmc neurology  21119 2021
58 mazhar javed awan mohd shafry mohd rahim naomie salim mazin abed mohammed begonya garciazapirain and karrar hameed
abdulkareem efficient detection of knee anterior cruciate ligament from magnetic resonance imaging using deep learning approach
diagnostics  111105 2021
59 jiang zhang chen li ganwanming liu min min chong wang jiyi li yuting wang hongmei yan zhentao zuo wei huang et al a
cnntransformerhybridapproachfordecodingvisualneuralactivityintotext computermethodsandprogramsinbiomedicine 214106586
2022
60 radiologyinfoorg radiologyinfoorg for patients httpswwwradiologyinfoorg 
61 zhenwangguangxulijingjiezhouandphilipoogunbona opticalflownetworksforheartbeatestimationin4dultrasoundimages in
2021 7th international conference on computing and artificial intelligence  pages 127131 2021
62 yuyu guo lei bi euijoon ahn dagan feng qian wang and jinman kim a spatiotemporal volumetric interpolation network for 4d
dynamic medical image in proceedings of the ieeecvf conference on computer vision and pattern recognition  pages 47264735
2020
63 anil v parwani whole slide imaging current applications and future directions  springer nature 2021
64 muhammad shaban syed ali khurram muhammad moazam fraz najah alsubaie iqra masood sajid mushtaq mariam hassan asif
loya and nasir m rajpoot a novel digital score for abundance of tumour infiltrating lymphocytes predicts disease free survival in oral
squamous cell carcinoma scientific reports  91113 2019
65 sajidjavedarifmahmoodmuhammadmoazamfraznavidalemikoohbananiksenijabenesyeewahtsangkatherinehewittdavid
epsteindavidsneadandnasirrajpootcellularcommunitydetectionfortissuephenotypingincolorectalcancerhistologyimages medical
image analysis  63101696 2020
66 rm saad bashir hanya mahmood muhammad shaban shan e ahmed raza m moazam fraz syed ali khurram and nasir m rajpoot
automated grade classification of oral epithelial dysplasia using morphometric analysis of histology images in medical imaging 2020
digital pathology  volume 11320 page 1132011 international society for optics and photonics 2020
67 yanning zhou simon graham navid alemi koohbanani muhammad shaban phengann heng and nasir rajpoot cgcnet cell graph
convolutional network for grading of colorectal cancer histology images in proceedings of the ieeecvf international conference on
computer vision workshops  pages 00 2019
68 muhammadshabanruqayyaawanmuhammadmoazamfrazayeshaazamyeewahtsangdavidsneadandnasirmrajpootcontext
aware convolutional neural network for grading of colorectal cancer histology images ieee transactions on medical imaging  2020
69 mmfrazsakhurramsgrahammshabanmhassanaloyaandnmrajpootfabnetfeatureattentionbasednetworkforsimultaneous
segmentationofmicrovesselsandnervesinroutinehistologyimagesoforalcancer neuralcomputingandapplications pages1142019
70 mmfrazmuhammadshabansimongrahamsyedalikhurramandnasirmrajpootuncertaintydrivenpoolingnetworkformicrovessel
segmentation in routine histology images in computational pathology and ophthalmic medical image analysis  pages 156164 springer
2018
71 snrashidmmfrazandsjaved multiscaledilatedunetforsegmentationofmultiorgannucleiindigitalhistologyimages in 2020ieee
17thinternationalconferenceonsmartcommunitiesimprovingqualityoflifeusingictiotandaihonet pages6872ieee2020
72 moritz schwyzer daniela a ferraro urs j muehlematter alessandra curionifontecedro martin w huellner gustav k von schulthess
philipp a kaufmann irene a burger and michael messerli automated detection of lung cancer at ultralow dose petct by deep neural
networksinitial results lung cancer  126170173 2018
73 benjamin h kann sanjay aneja gokoulakrichenane v loganadane jacqueline r kelly stephen m smith roy h decker james b yu
henrysparkwendellgyarbroughajaymalhotraetalpretreatmentidentificationofheadandneckcancernodalmetastasisandextranodal
extension using deep learning neural networks scientific reports  81111 2018
74 kobra etminani amira soliman anette davidsson jose r chang begoa martnezsanchis stefan byttner valle camacho matteo
bauckneht roxana stegeran marcus ressner et al a 3d deep learning model to predict the diagnosis of dementia with lewy bodies
alzheimersdiseaseandmildcognitiveimpairmentusingbrain18ffdgpet europeanjournalofnuclearmedicineandmolecularimaging 
492563584 2022
75 rikiyayamashitamizuhonishiorichardkinhgiandoandkaoritogashi convolutionalneuralnetworksanoverviewandapplication
in radiology insights into imaging  94611629 2018
76 alex krizhevsky ilya sutskever and geoffrey e hinton imagenet classification with deep convolutional neural networks advances in
neural information processing systems  25 2012
  page 53 of 60
vision transformers in medical computer vision  a contemplative retrospection
77 karen simonyan and andrew zisserman very deep convolutional networks for largescale image recognition arxiv preprint
arxiv14091556  2014
78 christian szegedy wei liu yangqing jia pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke and
andrew rabinovich going deeper with convolutions in proceedings of the ieee conference on computer vision and pattern recognition 
pages 19 2015
79 kaiming he xiangyu zhang shaoqing ren and jian sun deep residual learning for image recognition in proceedings of the ieee
conference on computer vision and pattern recognition  pages 770778 2016
80 saining xie ross girshick piotr dollr zhuowen tu and kaiming he aggregated residual transformations for deep neural networks in
proceedings of the ieee conference on computer vision and pattern recognition  pages 14921500 2017
81 jie hu li shen and gang sun squeezeandexcitation networks in proceedings of the ieee conference on computer vision and pattern
recognition  pages 71327141 2018
82 gao huang zhuang liu laurens van der maaten and kilian q weinberger densely connected convolutional networks in proceedings
of the ieee conference on computer vision and pattern recognition  pages 47004708 2017
83 mingxing tan and quoc le efficientnet rethinking model scaling for convolutional neural networks in international conference on
machine learning  pages 61056114 pmlr 2019
84 hafiz syed ahmed qasim muhammad shahzad and muhammad moazam fraz deep learning for face detection recent advancements
in2021 international conference on digital futures and transformative technologies icodt2  pages 16 ieee 2021
85 muhammad arslan hashmi qaiser riaz muhammad zeeshan muhammad shahzad and muhammad moazam fraz motion reveal
emotions identifying emotions from human walk using chest mounted smartphone ieee sensors journal  20221351113522 2020
86 weiwangandjianxungang applicationofconvolutionalneuralnetworkinnaturallanguageprocessing in 2018internationalconference
on information systems and computer aided education iciscae  pages 6470 ieee 2018
87 qi zhan wenjin wang and gerard de haan analysis of cnnbased remoteppg to understand limitations and sensitivities biomedical
optics express  11312681283 2020
88 arnaudarindraadiyososetioalbertotraversothomasdebelmoirasnberenscasvandenbogaardpiergiorgiocerellohaochen
qi dou maria evelina fantacci bram geurts et al validation comparison and combination of algorithms for automatic detection of
pulmonary nodules in computed tomography images the luna16 challenge medical image analysis  42113 2017
89 samuel g armato iii geoffrey mclennan luc bidaut michael f mcnittgray charles r meyer anthony p reeves binsheng zhao
denise r aberle claudia i henschke eric a hoffman et al the lung image database consortium lidc and image database resource
initiative idri a completed reference database of lung nodules on ct scans medical physics  382915931 2011
90 laugesorensensaherbshakerandmarleendebruijne quantitativeanalysisofpulmonaryemphysemausinglocalbinarypatterns ieee
transactions on medical imaging  292559569 2010
91 xingyiyangxuehaihejinyuzhaoyichenzhangshanghangzhangandpengtaoxie covidctdatasetactscandatasetaboutcovid19
arxiv preprint arxiv200313865  2020
92 plamenangelovandeduardoalmeidasoaressarscov2ctscandatasetalargedatasetofrealpatientsctscansforsarscov2identification
medrxiv 2020
93 dimitrioskolliasanastasiosarsenoslevonsoukissianandstefanoskollias miacov19dcovid19detectionthrough3dchestctimage
analysis in proceedings of the ieeecvf international conference on computer vision  pages 537544 2021
94 mohammad rahimzadeh abolfazl attar and seyed mohammad sakhaei a fully automated deep learningbased network for detecting
covid19 from a new and large lung ct scan dataset biomedical signal processing and control  68102588 2021
95 danielkermanykangzhangandmichaelgoldbaum labeledopticalcoherencetomographyoctandchestxrayimagesforclassification
mendeley data  2 2018
96 maria de la iglesia vay jose manuel saborit joaquim angel montell antonio pertusa aurelia bustos miguel cazorla joaquin galant
xavier barber domingo orozcobeltrn francisco garcagarca et al bimcv covid19 a large annotated dataset of rx and ct images
from covid19 patients arxiv preprint arxiv200601174  2020
97 unais sait gokul lal kv sunny prakash prajapati rahul bhaumik tarun kumar sanjana shivakumar and kriti bhalla curated dataset
for covid19 posterioranterior chest radiography images xrays mendeley data  3 2021
98 fathi elshafai walid abd elsamie extensive covid19 xray and ct chest images dataset mendeley data  3 2020
99 lindawangzhongqiulinandalexanderwong covidnetatailoreddeepconvolutionalneuralnetworkdesignfordetectionofcovid19
cases from chest xray images scientific reports  101112 2020
100 shirin hajeb mohammad alipour hossein rabbani and mohammad reza akhlaghi diabetic retinopathy grading by digital curvelet
transform computational and mathematical methods in medicine  2012 2012
101 jakobnikolaskatherfgzllnerfbianconismmelcherslrschadtgaiseramarxandcaweis collectionoftexturesincolorectal
cancer histology zenodo httpsdoi org10  5281 2016
102 siham tabik anabel gmezros jos luis martnrodrguez ivn sevillanogarca manuel reyarea david charte emilio guirado
juanluis surez julin luengo ma valerogonzlez et al covidgr dataset and covidsdnet methodology for predicting covid19 based
on chest xray images ieee journal of biomedical and health informatics  241235953605 2020
103 prasanna porwal samiksha pachade ravi kamble manesh kokare girish deshmukh vivek sahasrabuddhe and fabrice meriaudeau
indian diabetic retinopathy image dataset idrid a database for diabetic retinopathy screening research data 3325 2018
104 haydengunrajalisabridavidkoffandalexanderwong covidnetct2enhanceddeepneuralnetworksfordetectionofcovid19from
chest ct images through bigger more diverse learning arxiv preprint arxiv210107433  2021
105 john n weinstein eric a collisson gordon b mills kenna r shaw brad a ozenberger kyle ellrott ilya shmulevich chris sander and
joshua m stuart the cancer genome atlas pancancer analysis project nature genetics  451011131120 2013
  page 54 of 60
vision transformers in medical computer vision  a contemplative retrospection
106 li wang dong nie guannan li lodie puybareau jose dolz qian zhang fan wang jing xia zhengwang wu jiawei chen et al
benchmark on automatic sixmonthold infant brain segmentation algorithms the iseg2017 challenge ieee transactions on medical
imaging 38922192230 2019
107 spyridon bakas hamed akbari aristeidis sotiras michel bilello martin rozycki justin s kirby john b freymann keyvan farahani
andchristosdavatzikos advancingthecancergenomeatlasgliomamricollectionswithexpertsegmentationlabelsandradiomicfeatures
scientific data  41113 2017
108 adrinne m mendrik koen l vincken hugo j kuijf marcel breeuwer willem h bouvy jeroen de bresser amir alansary marleen
de bruijne aaron carass ayman elbaz et al mrbrains challenge online evaluation framework for brain image segmentation in 3t mri
scanscomputational intelligence and neuroscience  2015 2015
109 cathie sudlow john gallacher naomi allen valerie beral paul burton john danesh paul downey paul elliott jane green martin
landray et al uk biobank an open access resource for identifying the causes of a wide range of complex diseases of middle and old age
plos medicine  123e1001779 2015
110 colin g stirrat shirjel r alam thomas j macgillivray calum d gray marc r dweck jennifer raftis william sa jenkins william a
wallacerenzopessottokelvinhhlimetalferumoxytolenhancedmagneticresonanceimagingassessinginflammationaftermyocardial
infarction heart 1031915281535 2017
111 a emre kavur n sinem gezer mustafa bar sinem aslan pierrehenri conze vladimir groza duc duy pham soumick chatterjee
philipp ernst sava zkan et al chaos challengecombined ctmr healthy abdominal organ segmentation medical image analysis 
69101950 2021
112 joesstaalmichaeldabrmoffmeindertniemeijermaxaviergeverandbramvanginneken ridgebasedvesselsegmentationincolor
images of the retina ieee transactions on medical imaging  234501509 2004
113 junji shiraishi shigehiko katsuragawa junpei ikezoe tsuneo matsumoto takeshi kobayashi kenichi komatsu mitate matsui hiroshi
fujitayoshiekoderaandkuniodoidevelopmentofadigitalimagedatabaseforchestradiographswithandwithoutalungnodulereceiver
operatingcharacteristicanalysisofradiologistsdetectionofpulmonarynodules americanjournalofroentgenology 174171742000
114 nicholas heller fabian isensee klaus h maierhein xiaoshuai hou chunmei xie fengyi li yang nan guangrui mu zhiyong lin
miofei han et al the state of the art in kidney and kidney tumor segmentation in contrastenhanced ct imaging results of the kits19
challenge medical image analysis  67101821 2021
115 daniel s kermany michael goldbaum wenjia cai carolina cs valentim huiying liang sally l baxter alex mckeown ge yang
xiaokangwufangbingyanetal identifyingmedicaldiagnosesandtreatablediseasesbyimagebaseddeeplearning cell17251122
1131 2018
116 dina demnerfushman marc d kohli marc b rosenman sonya e shooshan laritza rodriguez sameer antani george r thoma and
clement j mcdonald preparing a collection of radiology examinations for distribution and retrieval journal of the american medical
informatics association  232304310 2016
117 alistair ew johnson tom j pollard seth j berkowitz nathaniel r greenbaum matthew p lungren chihying deng roger g mark and
steven horng mimiccxr a deidentified publicly available database of chest radiographs with freetext reports scientific data  6118
2019
118 baoyu jing pengtao xie and eric xing on the automatic generation of medical imaging reports arxiv preprint arxiv171108195  2017
119 baiyu chen xinhui duan zhicong yu shuai leng lifeng yu and cynthia mccollough development and validation of an open data
format for ct projection data medical physics  421269646972 2015
120 bennettalandmanalanjhuangaliyagifforddeeptisvikramisselannellimjonathanadfarrelljohnabogovicjunhuamin
chen samson jarso et al multiparametric neuroimaging reproducibility a 3t resource study neuroimage  54428542866 2011
121 eirikuragustssonandradutimofte ntire2017challengeonsingleimagesuperresolutiondatasetandstudy in proceedingsoftheieee
conference on computer vision and pattern recognition workshops  pages 126135 2017
122 jure zbontar florian knoll anuroop sriram tullie murrell zhengnan huang matthew j muckley aaron defazio ruben stern patricia
johnson mary bruno et al fastmri an open dataset and benchmarks for accelerated mri arxiv preprint arxiv181108839  2018
123 martin weigert uwe schmidt tobias boothe andreas mller alexandr dibrov akanksha jain benjamin wilhelm deborah schmidt
coleman broaddus sin culley et al contentaware image restoration pushing the limits of fluorescence microscopy nature methods 
151210901097 2018
124 mingyu kim jihye yun yongwon cho keewon shin ryoungwoo jang hyunjin bae and namkug kim deep learning in medical
imaging neurospine  164657 2019
125 md shahin ali md sipon miah jahurul haque md mahbubur rahman and md khairul islam an enhanced technique of skin cancer
classificationusingdeepconvolutionalneuralnetworkwithtransferlearningmodels machinelearningwithapplications 51000362021
126 jyostna devi bodapati nagur shareef shaik and veeranjaneyulu naralasetti composite deep neural network with gatedattention
mechanism for diabetic retinopathy severity classification journal of ambient intelligence and humanized computing  121098259839
2021
127 vinayakumar ravi harini narasimhan and tuan d pham efficientnetbased convolutional neural networks for tuberculosis classification
inadvances in artificial intelligence computation and data science  pages 227244 springer 2021
128 andrew g howard menglong zhu bo chen dmitry kalenichenko weijun wang tobias weyand marco andreetto and hartwig adam
mobilenets efficient convolutional neural networks for mobile vision applications arxiv preprint arxiv170404861  2017
129 muhammad sufyan arshad usman abdur rehman and muhammad moazam fraz plant disease identification using transfer learning in
2021 international conference on digital futures and transformative technologies icodt2  pages 15 ieee 2021
130 tufailsajjadshahhashminazeefulhaqmuhammadmoazamfrazandmuhammadshahzad applicationofdeeplearningforweapons
detection in surveillance videos in 2021 international conference on digital futures and transformative technologies icodt2  pages
16 ieee 2021
  page 55 of 60
vision transformers in medical computer vision  a contemplative retrospection
131 ross girshick jeff donahue trevor darrell and jitendra malik rich feature hierarchies for accurate object detection and semantic
segmentation in proceedings of the ieee conference on computer vision and pattern recognition  pages 580587 2014
132 ross girshick fast rcnn in proceedings of the ieee international conference on computer vision  pages 14401448 2015
133 shaoqing ren kaiming he ross girshick and jian sun faster rcnn towards realtime object detection with region proposal networks
advances in neural information processing systems  28 2015
134 joseph redmon santosh divvala ross girshick and ali farhadi you only look once unified realtime object detection in proceedings
of the ieee conference on computer vision and pattern recognition  pages 779788 2016
135 wei liu dragomir anguelov dumitru erhan christian szegedy scott reed chengyang fu and alexander c berg ssd single shot
multibox detector in european conference on computer vision  pages 2137 springer 2016
136 lohendran baskaran subhi j alaref gabriel maliakal benjamin c lee zhuoran xu jeong w choi sangeun lee ji min sung fay y
lin simon dunham et al automatic segmentation of multiple cardiovascular structures from cardiac computed tomography angiography
images using deep learning plos one 155e0232573 2020
137 kunal nagpal davis foote yun liu pohsuan cameron chen ellery wulczyn fraser tan niels olson jenny l smith arash
mohtashamian james h wren et al development and validation of a deep learning algorithm for improving gleason scoring of prostate
cancernpj digital medicine  21110 2019
138 sufian a badawi and muhammad moazam fraz optimizing the trainable bcosfire filter for retinal blood vessel segmentation peerj
6e5855 2018
139 olaf ronneberger philipp fischer and thomas brox unet convolutional networks for biomedical image segmentation in international
conference on medical image computing and computerassisted intervention  pages 234241 springer 2015
140 liangchieh chen george papandreou iasonas kokkinos kevin murphy and alan l yuille deeplab semantic image segmentation
withdeepconvolutionalnetsatrousconvolutionandfullyconnectedcrfs ieeetransactionsonpatternanalysisandmachineintelligence 
404834848 2017
141 kaiming he georgia gkioxari piotr dollr and ross girshick mask rcnn in proceedings of the ieee international conference on
computer vision  pages 29612969 2017
142 ruixin yang and yingyan yu artificial convolutional neural network in object detection and semantic segmentation for medical imaging
analysis frontiers in oncology  11573 2021
143 eshapahwadwijmehtasanjeetkapadiadevanshjainandachleshwarluthramedskipmedicalreportgenerationusingskipconnections
and integrated attention in proceedings of the ieeecvf international conference on computer vision  pages 34093415 2021
144 di you fenglin liu shen ge xiaoxia xie jing zhang and xian wu aligntransformer hierarchical alignment of visual regions and
disease tags for medical report generation in international conference on medical image computing and computerassisted intervention 
pages 7282 springer 2021
145 runyilizizhouwangandleizhangimagecaptionandmedicalreportgenerationbasedondeeplearningareviewandalgorithmanalysis
in2021 international conference on computer information science and artificial intelligence cisai  pages 373379 ieee 2021
146 oriol vinyals alexander toshev samy bengio and dumitru erhan show and tell a neural image caption generator in proceedings of
the ieee conference on computer vision and pattern recognition  pages 31563164 2015
147 kelvinxujimmybaryankiroskyunghyunchoaaroncourvilleruslansalakhudinovrichzemelandyoshuabengio showattend
andtellneuralimagecaptiongenerationwithvisualattention in internationalconferenceonmachinelearning pages20482057pmlr
2015
148 jiasen lu jianwei yang dhruv batra and devi parikh neural baby talk in proceedings of the ieee conference on computer vision and
pattern recognition  pages 72197228 2018
149 justinjohnsonandrejkarpathyandlifeifei densecapfullyconvolutionallocalizationnetworksfordensecaptioning in proceedings
of the ieee conference on computer vision and pattern recognition  pages 45654574 2016
150 alexander mathews lexing xie and xuming he semstyle learning to generate stylised image captions using unaligned text in
proceedings of the ieee conference on computer vision and pattern recognition  pages 85918600 2018
151 imrankhurrammmfrazmuhammadshahzadandnasirmrajpoot densecaptionnetasentencegenerationarchitectureforfinegrained
description of image semantics cognitive computation  133595611 2021
152 md zakir hossain ferdous sohel mohd fairuz shiratuddin and hamid laga a comprehensive survey of deep learning for image
captioning acm computing surveys csur  516136 2019
153 tomotakasobuenoriyukimoriyamamasahirokanekomasahikokusumototoshiakikobayashiryosuketsuchiyaryutarokakinuma
hironobuohmatsukanjinagaihiroyukinishiyamaetalscreeningforlungcancerwithlowdosehelicalcomputedtomographyantilung
cancer association project journal of clinical oncology  204911920 2002
154 weizhaojianchengyangyinglisunchengliweilanwuliangjinzhimingyangbingbingnipangaopeijunwangetal 3ddeep
learning from ct scans predicts tumor invasiveness of subcentimeter pulmonary adenocarcinomas cancer research  782468816889
2018
155 wei zhao jiancheng yang bingbing ni dexi bi yingli sun mengdi xu xiaoxia zhu cheng li liang jin pan gao et al toward
automaticpredictionofegfrmutationstatusinpulmonaryadenocarcinomawith3ddeeplearning cancermedicine 87353235432019
156 fangzhou liao ming liang zhe li xiaolin hu and sen song evaluate the malignancy of pulmonary nodules using the 3d deep leaky
noisyor network ieee transactions on neural networks and learning systems  301134843495 2019
157 jianchengyanghaorandengxiaoyanghuangbingbingniandyixu relationallearningbetweenmultiplepulmonarynodulesviadeep
set attention transformers in 2020 ieee 17th international symposium on biomedical imaging isbi  pages 18751878 ieee 2020
158 yanan wu shouliang qi yu sun shuyue xia yudong yao and wei qian a vision transformer for emphysema classification using ct
imagesphysics in medicine  biology  6624245016 2021
  page 56 of 60
vision transformers in medical computer vision  a contemplative retrospection
159 cuong do and lan vu an approach for recognizing covid19 cases using convolutional neural networks applied to ct scan images
in andrew g tescher and touradj ebrahimi editors applications of digital image processing xliii  volume 11510 pages 719  727
international society for optics and photonics spie 2020
160 halgurdsmaghdidarastasaadkayhanzrarghafooralisafaasadiqseyedalimirjaliliandmuhammadkhurramkhan diagnosing
covid19 pneumonia from xray and ct images using deep learning and transfer learning algorithms in multimodal image exploitation and
learning 2021  volume 11734 page 117340e international society for optics and photonics 2021
161 matteo polsinelli luigi cinque and giuseppe placidi a light cnn for detecting covid19 from ct scans of the chest pattern recognition
letters 14095100 2020
162 leiwenfubingyiwangtanweiyuanxiaotingchenyunlongaothomasfitzpatrickpeiyangliyiguozhouyifanlinqibinduan
et al clinical characteristics of coronavirus disease 2019 covid19 in china a systematic review and metaanalysis journal of infection 
806656665 2020
163 fengpantianheyepengsunshanguibolianglinglilidandanzhengjiazhengwangrichardlheskethlianyangetal time
course of lung changes on chest ct during recovery from 2019 novel coronavirus covid19 pneumonia radiology  2020
164 ara abigail e ambita eujene nikka v boquio and prospero c naval covitgan vision transformer forcovid19 detection in ct scan
imageswith selfattention gan fordataaugmentation in igor farka paolo masulli sebastian otte and stefan wermter editors artificial
neural networks and machine learning  icann 2021  pages 587598 cham 2021 springer international publishing
165 lei zhang and yan wen a transformerbased framework for automatic covid19 diagnosis in chest cts in 2021 ieeecvf international
conference on computer vision workshops iccvw  pages 513518 2021
166 xinggang wang xianbo deng qing fu qiang zhou jiapei feng hui ma wenyu liu and chuansheng zheng a weaklysupervised
frameworkforcovid19classificationandlesionlocalizationfromchestct ieeetransactionsonmedicalimaging 398261526252020
167 lin li lixin qin zeguo xu youbing yin xin wang bin kong junjie bai yi lu zhenghan fang qi song et al using artificial
intelligencetodetectcovid19andcommunityacquiredpneumoniabasedonpulmonaryctevaluationofthediagnosticaccuracy radiology 
2962e65e71 2020
168 joelcmthanpunliangthonomarmohdrijalrosminahmkassimashariyunusnorlizamnoorandpatrickthenpreliminarystudy
on patch sizes in vision transformers vit for covid19 and diseased lungs classification in 2021 ieee national biomedical engineering
conference nbec  pages 146150 ieee 2021
169 xiaole fan xiufang feng yunyun dong and huichao hou covid19 ct image recognition algorithm based on transformer and cnn
displays page 102150 2022
170 khushal tyagi gaurav pathak rahul nijhawan and ankush mittal detecting pneumonia using vision transformer and comparing with
other techniques in 2021 5th international conference on electronics communication and aerospace technology iceca  pages 1216
ieee 2021
171 linhtduongnhihletoanbtranvuongmngoandphuongtnguyen detectionoftuberculosisfromchestxrayimagesboosting
the performance with vision transformer and transfer learning expert systems with applications  184115519 2021
172 adambernheimxueyanmeimingqianhuangyangyangzahiafayadningzhangkaiyuediaobinlinxiqizhukunweilietal
chest ct findings in coronavirus disease19 covid19 relationship to duration of infection radiology  page 200463 2020
173 soumya ranjan nayak deepak ranjan nayak utkarsh sinha vaibhav arora and ram bilas pachori application of deep learning
techniques for detection of covid19 cases using chest xray images a comprehensive study biomedical signal processing and control 
64102365 2021
174 sangjoonparkgwanghyunkimyujinohjoonbeomseosangminleejinhwankimsungjunmoonjaekwanglimandjongchul
ye multitask vision transformer using lowlevel chest xray feature corpus for covid19 diagnosis and severity quantification medical
image analysis  75102299 2022
175 debaditya shome t kar sachi nandan mohanty prayag tiwari khan muhammad abdullah altameem yazhou zhang and abdul
khader jilani saudagar covidtransformer interpretable covid19 detection using vision transformer for healthcare international journal
of environmental research and public health  182111086 2021
176 mohamad mahmoud al rahhal yakoub bazi rami m jomaa ahmad alshibli naif alajlan mohamed lamine mekhalfi and farid
melgani covid19 detection in ctxray imagery using vision transformers journal of personalized medicine  122310 2022
177 muhammad aasharib nawshad usama aleem shami sana sajid and muhammad moazam fraz attention based residual network for
effectivedetectionofcovid19andviralpneumonia in 2021internationalconferenceondigitalfuturesandtransformativetechnologies
icodt2  pages 17 ieee 2021
178 yin dai yifan gao and fayu liu transmed transformers advance multimodal medical image classification diagnostics  1181384
2021
179 sharif amit kamran khondker fariha hossain alireza tavakkoli stewart lee zuckerbrod and salah a baker vtgan semisupervised
retinal image synthesis and disease prediction using vision transformers in proceedings of the ieeecvf international conference on
computer vision  pages 32353245 2021
180 magdy abdelghany zeid khaled elbahnasy and se aboyoussef multiclass colorectal cancer histology images classification using
vision transformers in 2021 tenth international conference on intelligent computing and information systems icicis  pages 224230
ieee 2021
181 haoyuan chen chen li xiaoyan li md mamunur rahaman weiming hu yixin li wanli liu changhao sun hongzan sun xinyu
huang et al ilmcam an interactive learning and multichannel attention mechanismbased weakly supervised colorectal histopathology
image classification approach computers in biology and medicine  page 105265 2022
182 jingxing li zhanglei yang and yifan yu a medical ai diagnosis platform based on vision transformer for coronavirus in 2021 ieee
international conference on computer science electronic information engineering and intelligent control technology cei  pages 246
252 ieee 2021
  page 57 of 60
vision transformers in medical computer vision  a contemplative retrospection
183 stefan jaeger sema candemir sameer antani yxing j wng puxuan lu and george thoma two public chest xray datasets for
computeraided screening of pulmonary diseases quantitative imaging in medicine and surgery  46475 2014
184 joseph paul cohen paul morrison lan dao karsten roth tim q duong and marzyeh ghassemi covid19 image data collection
prospective predictions are the future arxiv preprint arxiv200611988  2020
185 shuang liang weicun zhang and yu gu a hybrid and fast deep learning framework for covid19 detection via 3d chest ct images in
proceedings of the ieeecvf international conference on computer vision  pages 508512 2021
186 liyang chen zhiyuan you nian zhang juntong xi and xinyi le utrad anomaly detection and localization with utransformer neural
networks 1475362 2022
187 zhijie lin zhaoshui he shengli xie xu wang ji tan jun lu and beihai tan aanet adaptive attention network for covid19 detection
from chest xray images ieee transactions on neural networks and learning systems  321147814792 2021
188 arnab kumar mondal arnab bhattacharjee parag singla and ap prathosh xvitcos explainable vision transformer based covid19
screening using radiography ieee journal of translational engineering in health and medicine  10110 2021
189 emanuelepescesamueljosephwitheypetrospavlosypsilantisrobertbakewellvickygohandgiovannimontana learningtodetect
chest radiographs containing pulmonary lesions using visual attention networks medical image analysis  532638 2019
190 lizongzhangshuxinfengguiduoduanyingliandguisongliu detectionofmicroaneurysmsinfundusimagesbasedonanattention
mechanism genes 1010817 2019
191 liulimaixuhanruoliuyanglixiaofeiwanglaijiangzulinwangxiangfanandningliwang alargescaledatabaseandacnn
model for attentionbased glaucoma detection ieee transactions on medical imaging  392413424 2019
192 xi xu yu guan jianqiang li zerui ma li zhang and li li automatic glaucoma detection based on transfer induced attention network
biomedical engineering online  201119 2021
193 rodger c haggitt barretts esophagus dysplasia and adenocarcinoma human pathology  2510982993 1994
194 christopher p wild and laura j hardie reflux barretts oesophagus and adenocarcinoma burning questions nature reviews cancer 
39676684 2003
195 naofumitomitabehnazabdollahijasonweibingrenariefsuriawinataandsaeedhassanpour attentionbaseddeepneuralnetworks
for detection of cancerous and precancerous esophagus tissue on histopathological slides jama network open  211e1914645e1914645
2019
196 ming y lu drew fk williamson tiffany y chen richard j chen matteo barbieri and faisal mahmood dataefficient and weakly
supervised computational pathology on wholeslide images nature biomedical engineering  56555570 2021
197 richard j chen ming y lu weihung weng tiffany y chen drew fk williamson trevor manz maha shady and faisal mahmood
multimodalcoattentiontransformerforsurvivalpredictioningigapixelwholeslideimages in proceedingsoftheieeecvfinternational
conference on computer vision  pages 40154025 2021
198 yangningshouyizhangxiaomingxijieguopeideliuandcaimingzhang cacemvtefficientcoronaryarterycalciumsegmentation
withmultiscalevisiontransformers in 2021ieeeinternationalconferenceonbioinformaticsandbiomedicinebibm pages14621467
ieee 2021
199 matthew chung hai lee kersten petersen nick pawlowski ben glocker and michiel schaap tetris template transformer networks for
image segmentation with shape priors ieee transactions on medical imaging  381125962606 2019
200 maxjaderbergkarensimonyanandrewzissermanetalspatialtransformernetworks advancesinneuralinformationprocessingsystems 
28 2015
201 xiaomengliqidouhaochenchiwingfuxiaojuanqidaniellbelav ygabrielearmbrechtdieterfelsenbergguoyanzhengand
phengannheng 3dmultiscalefcnwithrandommodalityvoxeldropoutlearningforintervertebraldisclocalizationandsegmentationfrom
multimodality mr images medical image analysis  454154 2018
202 xiaohangfuleibiashnilkumarmichaelfulhamandjinmankim multimodalspatialattentionmodulefortargetingmultimodalpetct
lung tumor segmentation ieee journal of biomedical and health informatics  25935073516 2021
203 giammarco la barbera pietro gori haithem boussaid bruno belucci alessandro delmonte jeanne goulin sabine sarnacki laurence
rouet and isabelle bloch automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric
segmentation in 2021 ieee 18th international symposium on biomedical imaging isbi  pages 17731776 ieee 2021
204 daniel h pak andrs caballero wei sun and james s duncan efficient aortic valve multilabel segmentation using a spatial transformer
network in 2020 ieee 17th international symposium on biomedical imaging isbi  pages 17381742 ieee 2020
205 chunfeng lian fan wang hannah h deng li wang deqiang xiao tianshu kuang hungying lin jaime gateno steve gf shen
pewthianyapetal multitaskdynamictransformernetworkforconcurrentbonesegmentationandlargescalelandmarklocalizationwith
dentalcbct in internationalconferenceonmedicalimagecomputingandcomputerassistedintervention pages807816springer2020
206 chun luo jing zhang xinglin chen yinhao tang xiechuan weng and fan xu ucatr based on cnn and transformer encoding and
crossattention decoding for lesion segmentation of acute ischemic stroke in noncontrast computed tomography images in 2021 43rd
annual international conference of the ieee engineering in medicine  biology society embc  pages 35653568 ieee 2021
207 olivierpetitnicolasthomeclementrambourloicthemyrtobycollinsandlucsoler unettransformerselfandcrossattentionfor
medical image segmentation in international workshop on machine learning in medical imaging  pages 267276 springer 2021
208 jienengchenyongyiluqihangyuxiangdeluoehsanadeliyanwanglelualanlyuilleandyuyinzhoutransunettransformers
make strong encoders for medical image segmentation arxiv preprint arxiv210204306  2021
209 yutong xie jianpeng zhang chunhua shen and yong xia cotr efficiently bridging cnn and transformer for 3d medical image
segmentation in internationalconferenceonmedicalimagecomputingandcomputerassistedintervention pages171180springer2021
210 jianhong cheng jin liu hulin kuang and jianxin wang a fully automated multimodal mribased multitask learning for glioma
segmentation and idh genotyping ieee transactions on medical imaging  2022
  page 58 of 60
vision transformers in medical computer vision  a contemplative retrospection
211 abhinav sagar vitbis vision transformer for biomedical image segmentation in clinical imagebased procedures distributed and
collaborative learning artificial intelligence for combating covid19 and secure and privacypreserving machine learning  pages
3445 springer 2021
212 qixuan sun nianhua fang zhuo liu liang zhao youpeng wen and hongxiang lin hybridctrm bridging cnn and transformer for
multimodal brain image segmentation journal of healthcare engineering  2021 2021
213 jose dolz karthik gopinath jing yuan herve lombaert christian desrosiers and ismail ben ayed hyperdensenet a hyperdensely
connected cnn for multimodal image segmentation ieee transactions on medical imaging  38511161126 2018
214 matthewsinclairandreasschuhkarlhahnkerstenpetersenyingbaijamesbattenmichielschaapandbenglocker atlasistnjoint
segmentationregistrationandatlasconstructionwithimageandspatialtransformernetworks medicalimageanalysis page1023832022
215 agisilaoschartsiasgiorgospapanastasiouchengjiawangscottsempledavidenewbyrohandharmakumarandsotiriosatsaftaris
disentanglealignandfuseformultimodalandsemisupervisedimagesegmentation ieeetransactionsonmedicalimaging 403781792
2020
216 zheyao gao and xiahai zhuang consistency based cosegmentation for multiview cardiac mri using vision transformer in international
workshop on statistical atlases and computational models of the heart  pages 306314 springer 2021
217 dong sui kang zhang weifeng liu jing chen xiaoxuan ma and zhaofeng tian cst a multitask learning framework for colorectal
cancer region mining based on transformer biomed research international  2021 2021
218 yiyaoliuyiyangweijiangtianfuwangandbaiyinglei 3ddeepattentiveunetwithtransformerforbreasttumorsegmentationfrom
automated breast volume scanner in 2021 43rd annual international conference of the ieee engineering in medicine  biology society
embc pages 40114014 ieee 2021
219 holgerrrothhirohisaodaxiangrongzhounatsukishimizuyingyangyuichirohayashimasahiroodamichitakafujiwarakazunari
misawa and kensaku mori an application of cascaded 3d fully convolutional networks for medical image segmentation computerized
medical imaging and graphics  669099 2018
220 xiliang zhu zhaoyun cheng sheng wang xianjie chen and guoqing lu coronary angiography image segmentation based on pspnet
computer methods and programs in biomedicine  200105897 2021
221 yuhang lu kang zheng weijian li yirui wang adam p harrison chihung lin song wang jing xiao le lu changfu kuo et al
contourtransformernetworkforoneshotsegmentationofanatomicalstructures ieeetransactionsonmedicalimaging 401026722684
2020
222 guifangzhanghonchengwongchengwangjianjunzhuligongluandgaojuntengatemporarytransformernetworkforguidewire
segmentation in 202114thinternationalcongressonimageandsignalprocessingbiomedicalengineeringandinformaticscispbmei 
pages 15 ieee 2021
223 yanjiezhouxiaoliangxiezengguanghouguibinbianshiqiliuandxiaohuzhou frrnetfastrecurrentresidualnetworksfor
realtime catheter segmentation and tracking in endovascular aneurysm repair in 2020 ieee 17th international symposium on biomedical
imaging isbi  pages 961964 ieee 2020
224 yanjie zhou xiaoliang xie xiaohu zhou shiqi liu guibin bian and zengguang hou pyramid attention recurrent networks for
realtimeguidewiresegmentationandtrackinginintraoperativexrayfluoroscopy computerizedmedicalimagingandgraphics 83101734
2020
225 yunxiang li shuai wang jun wang guodong zeng wenjun liu qianni zhang qun jin and yaqi wang gt unet a unet like group
transformer network for tooth root segmentation in international workshop on machine learning in medical imaging  pages 386395
springer 2021
226 mengwangweifangzhufeishijinzhusuhaoyuchenkaiyuyizhouyuanyuanpengzhongyuechenandxinjianchen mstganet
automatic drusen segmentation from retinal oct images ieee transactions on medical imaging  2021
227 ayoubbenaliamjoudandmustaphaamrouch automaticgenerationofchestxrayreportsusingatransformerbaseddeeplearningmodel
in2021 fifth international conference on intelligent computing in data sciences icds  pages 15 ieee 2021
228 kesunbinxiaodongliuandjingdongwang deephighresolutionrepresentationlearningforhumanposeestimation in proceedings
of the ieeecvf conference on computer vision and pattern recognition  pages 56935703 2019
229 xing jia yun xiong jiawei zhang yao zhang blackley suzanne yangyong zhu and chunlei tang radiology report generation for rare
diseases via fewshot transformer in 2021 ieee international conference on bioinformatics and biomedicine bibm  pages 13471352
ieee 2021
230 hojun lee hyunjun cho jieun park jinyeong chae and jihie kim cross encoderdecoder transformer with globallocal visual extractor
for medical image captioning sensors 2241429 2022
231 marcellacorniamatteostefaninilorenzobaraldiandritacucchiara meshedmemorytransformerforimagecaptioning in proceedings
of the ieeecvf conference on computer vision and pattern recognition  pages 1057810587 2020
232 yuxuan xiong bo du and pingkun yan reinforced transformer for medical image captioning in international workshop on machine
learning in medical imaging  pages 673680 springer 2019
233 zhicheng zhang lequan yu xiaokun liang wei zhao and lei xing transct dualpath transformer for low dose computed tomography
ininternational conference on medical image computing and computerassisted intervention  pages 5564 springer 2021
234 lipei zhang zizheng xiao chao zhou jianmin yuan qiang he yongfeng yang xin liu dong liang hairong zheng wei fan et al
spatialadaptiveandtransformerfusionnetworkstfnetforlowcountpetblinddenoisingwithmri medicalphysics 4913433562022
235 yanmei luo yan wang chen zu bo zhan xi wu jiliu zhou dinggang shen and luping zhou 3d transformergan for highquality pet
reconstruction in international conference on medical image computing and computerassisted intervention  pages 276285 springer
2021
236 luluwanghuazhengzhuzhongshiheyuanyuanjiaandjinglongdu adjacentslicesfeaturetransformernetworkforsingleanisotropic
3d brain mri image superresolution biomedical signal processing and control  72103339 2022
  page 59 of 60
vision transformers in medical computer vision  a contemplative retrospection
237 zebinhuhaoliuzhendongliandzekuanyu dataenabledintelligenceincomplexindustrialsystemscrossmodeltransformermethod
for medical image synthesis complexity  2021 2021
238 kuntangzhilililitianlihuiwangandyueminzhu admiraffineanddeformablemedicalimageregistrationfordrugaddictedbrain
imagesieee access  87096070968 2020
239 xiaoganggufeixiangzhourongfeichenxinzhenrenandwenjuzhou endoscopicsingleimagesuperresolutionbasedontransformer
and convolutional neural network in intelligent life system modelling image processing and analysis  pages 2432 springer 2021
240 alvaro gomariz tiziano portenier patrick m helbling stephan isringhausen ute suessbier csar nombelaarrieta and orcun goksel
modalityattentionandsamplingenablesdeeplearningwithheterogeneousmarkercombinationsinfluorescencemicroscopy naturemachine
intelligence  39799811 2021
241 zhengyang wang yaochen xie and shuiwang ji global voxel transformer networks for augmented microscopy nature machine
intelligence  32161171 2021
242 weijian li vietduy nguyen haofu liao matt wilder ke cheng and jiebo luo patch transformer for multitagging whole slide
histopathology images in international conference on medical image computing and computerassisted intervention  pages 532540
springer 2019
  page 60 of 60","['detectionanddiagnosis72determiningspreadofthecancerdeterminingtherecurrenceofcancermetastasis73', 'cloudwhichisshowninfigure3thetagcloudillustratesthetrendingtermsinvitapplicationsinmedicalcomputer', 'thehumanbrainneuralactivitiesintonaturallanguagesentencesthemainpurposeofdecodingbrainneuralactivity', 'thecourseoftimeandleadingtoattentionmechanisms16usingattentionmechanismtheregionsoftheimageare', 'forvisiontransformershasgrownthroughouttheyearsfrom2019to2022astheapplicationsofvisiontransformersin', 'cnnbasedencoderextractedenrichedspatialinformationfromtheinputmedicalimagemoreoverthetransformer', 'accuracyandspecificitytheproposedmodeloutperformedthecnnmodelsreachingtheaccuracyof096and098', 'anddeeplearningmodelsmoreoverattentionmechanismscanalsobeincludedinvariouslocationsofadeeplearning', 'figure5showsthecategorizationofresearcharticlesbasedonvisualrecognitiontasksusingvisiontransformers', 'metricswith017066and018scoresrespectivelyinfuturethistransformerbasedbraindecodingtechnologywill']"
"SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for
  Large-scale Vision-Language Models","['Youngjoon Yu', 'Sangyun Chung', 'Byung-Kwan Lee', 'Yong Man Ro']",2024,http://arxiv.org/abs/2408.12114v3,"/bâ™€tSPARK: Multi-Vision Sensor Perception and Reasoning Benchmark
for Large-scale Vision-Language Models
Youngjoon Yuâ€ , Sangyun Chungâ€ , Byung-Kwan Lee, and Yong Man Ro*
Integrated Vision Language Lab, KAIST, South Korea
{greatday, jelarum, leebk, ymro }@kaist.ac.kr
Abstract
Large-scale Vision-Language Models (LVLMs) have signif-
icantly advanced with text-aligned vision inputs. They have
made remarkable progress in computer vision tasks by align-
ing text modality with vision inputs. There are also endeav-
ors to incorporate multi-vision sensors beyond RGB, includ-
ing thermal, depth, and medical X-ray images. However, we
observe that current LVLMs view images taken from multi-
vision sensors as if they were in the same RGB domain with-
out considering the physical characteristics of multi-vision
sensors. They fail to convey the fundamental multi-vision
sensor information from the dataset and the correspond-
ing contextual knowledge properly. Consequently, alignment
between the information from the actual physical environ-
ment and the text is not achieved correctly, making it dif-
ficult to answer complex sensor-related questions that con-
sider the physical environment. In this paper, we aim to
establish a multi-vision Sensor Perception AndReasoning
benchmar Kcalled SPARK that can reduce the fundamen-
tal multi-vision sensor information gap between images and
multi-vision sensors. We generated 6,248 vision-language
test samples to investigate multi-vision sensory perception
and multi-vision sensory reasoning on physical sensor knowl-
edge proficiency across different formats, covering different
types of sensor-related questions. We utilized these samples
to assess ten leading LVLMs. The results showed that most
models displayed deficiencies in multi-vision sensory rea-
soning to varying extents. Codes and data are available at
https://github.com/top-yun/SPARK
Introduction
In recent days, Large-scale Vision-Language Models
(LVLMs) have achieved significant breakthroughs in ar-
eas such as visual dialogue (Koh, Salakhutdinov, and Fried
2023), video analysis (Ren et al. 2024), and document under-
standing (Ye et al. 2023), establishing themselves as critical
tools in the pursuit of artificial general intelligence (AGI).
These models function similarly to the human brain by
processing multimodal information and generating sophis-
ticated inferences. For instance, the latest LVLMs, like Ope-
nAIâ€™s GPT-4o (OpenAI 2024), have exhibited exceptional
reasoning abilities that not only rival but in some cases ex-
ceed human performance.
*Corresponding author.â€ Both authors are equally contributed.
Sensory Reasoning Performance Across Different LVLMs and Vision Sensors
Yes! the ribs are visible in this chest X-ray image Are there ribs visible in this 
image?
What could be the likely reason for capturing this image? 
[A] To measure the temperature of items. 
[B] To describe the lighting condition.
[C] To detect hazy, foggy atmosphere.
[D] To analyze the spatial arrangement of objects in a room .
[C] To detect hazy, foggy atmosphere.
XR image
Depth image
Multi -Vision SensorSensory Reasoning (Acc)LLAVA -v1.5 CogVLM InternVL2 TroL
Meteor IXC 2.5 QwenVL GPT4oFigure 1: The comparison of sensory reasoning performance
across different multi-vision sensors with respect to the re-
cent LVLMs. Note that, sensory reasoning performance sig-
nificantly drops across different multi-vision sensors.
One emerging concept in modern AI research gaining sig-
nificant attention is the development of large vision language
models (LVLMs) capable of handling a variety of multi-
modal inputs, surpassing the capabilities of previous large
language models (LLMs). LVLMs can process diverse forms
of data simultaneously, including images, videos, and text
(OpenAI 2024; OpenGVLab 2024; Zhang et al. 2024). This
ability also allows them to use multi-vision sensor data as
input, including thermal sensors, depth sensors, and medi-
cal imaging (Girdhar et al. 2023; Su et al. 2023). To fully
harness the potential of LVLMs, recent research has focused
on effectively integrating various multi-vision sensor data to
develop more sophisticated and practical AI systems for the
real world.arXiv:2408.12114v3  [cs.CV]  11 Oct 2024
However, despite the remarkable advancements in LVLM
models, significant challenges still remain in fully utilizing
multi-vision sensors. LVLMs often overlook the nuances of
the physical properties of individual vision sensors. Instead,
they tend to make judgments based on prior visual or lin-
guistic information from images they have learned using
low-level features in two-dimensional data. This results in
the models recognizing only superficial patterns in image
inputs, missing the underlying logical structures or contex-
tual understanding. When identifying specific objects in an
image input, a model might rely on patterns learned from
similar-looking images rather than considering the actual
physical properties of the multi-vision sensors used to cap-
ture the image. This can hinder accurate identification and
a deep understanding of the input images in fields where
the LVLMâ€™s decision-making is crucial such as autonomous
driving (Mao et al. 2023; Xu et al. 2024), security sys-
tems (Shi et al. 2024), and medical image diagnosis (Bazi
et al. 2023).
We evaluate the behavior of the recent LVLMs using
multi-vision sensor images as input in Figure 1. The perfor-
mance of sensory reasoning, which we devised to assess the
understanding of fundamental knowledge of multi-vision
sensors in the real world, significantly drops across differ-
ent multi-vision sensors such as thermal infrared, depth,
and X-ray (XR) images. This highlights the challenges that
LVLMs face in accurately interpreting multi-vision sensor
data and making correct inferences based on the physical
properties of sensors. Additionally, from the interaction ex-
ample shown below in Figure 1, while the LVLM can ac-
curately identify the vision sensor used to capture the im-
age for a relatively simple question, it struggles with under-
standing the actual purpose or context of the image in the
sensor-related, more complicated questions. This indicates
that current LVLMs have difficulty in understanding the fun-
damental knowledge of physical vision sensors beyond what
the image looks like.
For example, as illustrated in Figure 1, when humans look
at a photograph of an X-ray medical image, they interpret it
deeply, drawing upon their knowledge base and their physi-
cal understanding of the human body beyond the X-ray im-
age itself. Despite never having seen their internal organs
and the structure of bones with the naked eye, humans can
comprehend the image through scientific contextual knowl-
edge and their inherent understanding of the physical world.
In contrast, current LVLMs try to understand the inside of
the human body based solely on the two-dimensional data
they have been trained on, revealing their limitations in
fully grasping the physical environment of the real world.
Therefore, establishing a comprehensive evaluation bench-
mark is necessary before LVLMs are implemented in crit-
ical and sensitive real-world applications. However, the as-
sessment of Large Vision-Language Models (LVLMs) has
significantly lagged behind their rapid development. Several
initiatives are striving to close this gap by introducing a vari-
ety of multimodal evaluation benchmarks. Notable examples
include MME (Fu et al. 2024), MMBench (Liu et al. 2024b),
LVLM-eHub (Xu et al. 2023), and SEED-Bench (Li et al.
2023a). These benchmarks aim to define key dimensions ofmultimodal capabilities and provide corresponding test sam-
ples. But, they cover a relatively narrow range of multimodal
tasks, primarily focusing on fundamental abilities such as vi-
sual recognition and OCR.
In this paper, to handle the aforementioned challenge, we
design the SPARK benchmark to evaluate multi-vision input
LVLMs on two fronts: multi-vision perception and multi-
vision reasoning. Multi-vision perception pertains to the in-
formation needed, which measures the LVLMâ€™s effective-
ness in satisfying visual perception needs. Multi-vision rea-
soning measures the LVLMâ€™s ability to base its responses on
fundamental information from the provided sensor knowl-
edge. To be specific, we generated 6,248 vision-language
test samples to investigate multi-vision sensory perception
and reasoning related to physical sensor knowledge profi-
ciency, covering 6 types of multi-vision sensory instruction
tasks across 2 different question-and-answer formats. We
used these samples to assess 10 leading large-scale vision
language models. The experiment results validate that most
LVLMs displayed deficiencies in sensory reasoning to vary-
ing extents.
In summary, the contributions of this work are as follows:
â€¢ To the best of our knowledge, we first reveal the inca-
pability of current LVLMs, which suffer from limited
multi-vision sensory reasoning across different multi-
vision sensors due to an absence of fundamental under-
standing of sensors in the physical world.
â€¢ We propose a novel benchmark, SPARK, to rigorously
test and evaluate the capabilities of LVLMs in under-
standing sensory knowledge, providing a comprehensive
framework for assessing their performance.
â€¢ We evaluated a total of 10 state-of-the-art LVLMs using
our SPARK benchmark, which is designed to rigorously
assess the capability of the LVLMs in handling funda-
mental knowledge related to multi-vision sensors.
Related work
Large-scale Vison-Language Models. Recently, there has
been significant interest in visual language multimodal
learning. Visual language models such as LLA V A (Liu et al.
2023b, 2024a), CollaVO (Lee et al. 2024c), MoAI (Lee
et al. 2024d), TroL (Lee et al. 2024a), Meteor (Lee et al.
2024b), IXC2.5 (Zhang et al. 2024), and QwenVL (Bai
et al. 2023) have shown impressive performance in a variety
of downstream tasks. In addition, to obtain richer contex-
tual information, LVLMs have developed the capability to
handle multimodal inputs. Wang et al. introduces CogVLM,
an advanced visual language foundation multimodal model
that integrates a trainable visual expert module with a pre-
trained language model. InternVL2 (Chen et al. 2024) is an
open-source multimodal large language model that bridges
the gap between open-source and commercial models by
enhancing visual understanding, dynamic high-resolution
processing, and bilingual dataset quality. GPT4o (OpenAI
2024) possesses advanced multimodal capabilities, allow-
ing it to process and generate diverse multimodalities. This
enables the model to understand and create content that in-
tegrates visual and textual information, making it suitable
Position (Thermal)
[Y] Is the person standing to the leftof the dog ?
[N] Is the dog positioned behind the person?Existence (RGB)
[Y] Is there a flower on the man's suit in this image?
[N] Is there a hat being worn by anyone in this image?
General Description (XR)
[Y] Are the ribs visible in the image?
[N]Is there a distinct liver shape visible in the image?Counting (Depth)
[Y] Are there two bottles in this image?
[N] Are there two monitors in this image?
Multi
 -
vision 
Perception
Multi
 -
vision 
Reasoning
Sensory Reasoning (Thermal)
Q: What could be the likely reason for capturing this 
image? 
[A] To assess the environmental conditions of the area.
[B] To study the feeding habits of livestock.
[C] To evaluate the breeding patterns of the animal.
[D] To monitor the health and temperature of animal.Contextual Reasoning (Thermal)
Q: What might be the reason for the vehicles lined up in the image?
[A] They are waiting for a traffic signal .
[B] They are parked for the night.
[C] They are part of a parade.
[D] They are in a car wash.
SPARKFigure 2: In the proposed SPARK, we build the first benchmark for evaluating the abilities of LVLMs in multi-vision sensor
understanding, which covers four types of multi-vision perception tasks (Existence, Counting, Position, and General Descrip-
tion) and two types of multi-vision reasoning tasks (Contextual Reasoning and Sensory Reasoning).
for a wide range of applications that require various modal-
ities. Consequently, many LVLMs have emerged that take
multi-vision sensor images as input. Girdhar et al. presents
ImageBind, which creates a joint embedding space across
multi-vision sensors including depth and thermal sensor
data. PandaGPT (Su et al. 2023) is a LVLM that integrates
multimodal encoders and large language models to enable
multi-vision and auditory instruction-following capabilities,
performing complex tasks. However, relatively less attention
has been devoted to whether LVLMs truly understand the
physical meanings of multi-vision sensors used to capture
the input image.
Evaluation Benchmark for LVLMs. Numerous studies
have leveraged existing vision-language datasets to develop
benchmarks for assessing the reliability of LVLMs (Li and
Lu 2024). MME (Fu et al. 2024) includes 14 sub-tasks
based on publicly available images with manually created
annotations, evaluating both the recognition and perception
capabilities of LVLMs through yes/no question answering.
SEED-benchmark (Li et al. 2023a) designed to evaluate the
generative comprehension capabilities of multimodal LVLM
through human-annotated multi-choice questions across 12
evaluation dimensions. Other comparable benchmarks in-
clude LVLM-eHub (Xu et al. 2023), MM-Vet (Yu et al.
2023), and MMBench (Liu et al. 2024b). Additionally, there
are benchmarks aimed at assessing specific target propertiesof LVLMs. POPE (Li et al. 2023b) focuses on evaluating
object hallucination by asking yes/no questions about the
presence of objects in the input image. M-HalDetect (Gun-
jal, Yin, and Bas 2024) introduces hallucination tasks us-
ing human-annotated labels for sentence-level classification.
Unlike those previous evaluation benchmarks, the proposed
SPARK is designed to rigorously test and evaluate the ca-
pabilities of understanding the physical meaning of multi-
vision sensors.
Evaluation and Instruction Design
There are multiple formats available for evaluating the
multi-sensor perception and reasoning capabilities of
LVLM, each with distinct advantages and limitations. Free-
form questions (Yarom et al. 2024) offer flexibility and ease
of creation but demand labor-intensive human assessment
and present challenges in maintaining consistent scoring.
Similarity-based assessment are less resource-intensive but
can be significantly affected by biases present in the similar-
ity metrics. Yes-or-No questions (Fu et al. 2024) are straight-
forward and easier to assess, but they may oversimplify the
evaluation, failing to capture the full extent of LVLMâ€™s com-
prehension of multi-vision reasoning ability.
First of all, to enable quantitative performance metrics for
multi-vision perception, the instruction design aims to elicit
â€œyesâ€ or â€œnoâ€ responses from the model. This binary re-
SPARKExistenceCounting
Sensory
ReasoningPosition
General
DescriptionContextual
Reasoning1423720Figure 3: Distribution of data sources of the SPARK bench-
mark. In SPARK, we demonstrate six core multi-vision sen-
sory tasks in the inner ring, and the outer ring displays the
number of samples for each specific task.
sponse format simplifies the evaluation process, allowing for
clear, objective performance measurement. As a result, each
instruction comprises two parts: a brief, targeted question
and an explanation corresponding to either â€œyesâ€ or â€œno.â€
This structure ensures that the LVLMâ€™s comprehension can
be precisely assessed. For every test image, two instructions
are manually crafted, each posing a different question to the
model. These questions are designed to test different aspects
of the imageâ€™s content and context. The rationale behind this
approach is to ensure that the modelâ€™s answers are not based
on chance. When the LVLMs correctly answer both ques-
tions, it demonstrates an understanding of the image and its
related information, rather than merely guessing.
In addition, we also introduce a multi-vision sensor under-
standing evaluation design based on multi-choice questions.
This format presents questions with a set of predetermined
choices, allowing respondents to select the correct options.
The multi-choice question format is advantageous for sev-
eral reasons. First, it enables efficient grading and analysis of
responses, as answers can be objectively evaluated against a
fixed set of possible responses. Also, the multi-choice ques-
tion format allows for precise control over the difficulty level
of the questions. By varying the validity of each option, we
can create questions that test different levels of understand-
ing and comprehension. For example, including more plau-
sible but incorrect options can increase the difficulty, ensur-
ing that only models with a deeper understanding can con-
sistently choose the correct answer. This flexibility in ques-
tion design makes multi-choice questions a powerful tool
for assessing the nuanced capabilities of multi-vision sen-
sor systems. Furthermore, the Yes-or-No format can be seenas a specific case of multi-choice question, where the op-
tions are limited to â€œ(A) Yesâ€ and â€œ(B) No.â€ This simplifica-
tion retains the benefits of the multi-choice question format
while providing a straightforward way to measure binary de-
cisions.
Using accuracy as the evaluation metric for both multi-
choice questions and Yes-or-No questions ensures consis-
tency in how we assess the modelâ€™s performance. Accuracy,
defined as the proportion of correctly answered questions,
provides a clear and intuitive measure of how well the model
understands the given inputs. The adoption of the multi-
choice question based evaluation design supports the de-
velopment of a more comprehensive evaluation framework.
The incorporation of both simple Yes-or-No questions and
more complex multi-choice questions ensures that the eval-
uation covers both basic and advanced aspects of LVLMâ€™s
understanding.
Evaluation on Multi-vision Sensor Tasks
Our instruction dataset was collected according to two
multi-vision tasks: multi-vision perception and multi-vision
reasoning. As illustrated in Figure 2, first of all, multi-vision
perception focuses on the LVLMâ€™s ability to accurately in-
terpret and identify objects, scenes, and relationships from
various multi-vision inputs. This involves tasks such as ob-
ject detection, image classification, scene recognition, and
relationship detection, where the model must process and
understand the content of images from multiple vision sen-
sors. The goal is to ensure that the model can consistently
recognize and categorize visual elements across different
contexts from different vision sensors. On the other hand,
multi-vision reasoning requires the model to not only per-
ceive but also make inferences based on the multi-vision
sensory data. This involves higher-order cognitive tasks such
as understanding relationships between objects, prediction
of intent of sensor use, and understanding sensor knowl-
edge. For instance, the model might need to infer the cause
of an event depicted in an image sequence or predict the
purpose of a captured image. Multi-vision reasoning tests
the LVLMâ€™s capability to integrate multi-vision information
with contextual sensory knowledge, making logical deduc-
tions that go beyond mere perception.
Multi-vision Perception
Multi-vision perception is the foundational process by which
Large Vision-Language Models (LVLMs) analyze images
captured by various multi-vision sensors, including RGB,
thermal, depth, and X-ray images. This process involves rec-
ognizing and interpreting the fundamental elements within
each visual input based on cognitive science (Kahneman,
Treisman, and Gibbs 1992; Broadbent 2013).
â€¢ Existence: LVLMs can identify and list common objects
present in the image, such as people, vehicles, animals,
furniture, and so on.
â€¢ Count: LVLMs can count the number of identified ob-
jects or entities, providing a quantitative understanding
of the scene.
Models Vision Sensors Existence Count PositionGeneral
DescriptionMulti-vision
PerceptionContextual
ReasoningSensory
ReasoningMulti-vision
Reasoning
Open Source Large-scale Vision-Language Models
RGB 93.9 68.5 62.6 97.9 80.7 95.1 97.2 96.1Qwen-VL-ChatThermal 86.1 66.9 59.3 95.3 76.9 90.3 83.5 86.9(Bai et al. 2023)Depth 76.6 59.6 53.3 84.9 68.6 78.1 68.4 73.3
XR 68.0 71.3 55.1 74.1 67.1 81.8 74.7 78.3
RGB 94.2 75.5 59.8 96.9 81.6 88.7 94.8 91.8LLA V A-v1.5-7BThermal 93.3 76.1 62.4 95.1 81.7 85.5 51.0 68.2(Liu et al. 2023a)Depth 87.1 70.7 53.3 93.7 76.2 87.4 73.8 80.6
XR 74.2 57.4 67.4 72.3 67.8 62.1 50.7 56.4
RGB 96.5 73.4 61.4 97.2 82.1 98.0 97.2 97.6CogVLM-ChatThermal 94.9 76.1 64.6 96.2 82.9 96.2 59.0 77.6(Wang et al. 2023)Depth 94.9 76.1 64.5 96.5 83.0 90.1 71.7 80.9
XR 86.1 72.8 61.6 79.4 74.9 90.9 84.0 87.5
RGB 97.2 78.5 72.2 97.9 86.4 98.0 99.5 98.8Meteor-7BThermal 93.5 68.9 71.7 95.3 82.3 90.9 62.0 76.4(Lee et al. 2024b)Depth 83.5 65.9 62.2 91.6 75.8 89.5 77.3 83.4
XR 79.5 70.6 63.8 76.6 72.6 86.4 84.0 85.2
RGB 96.9 81.2 69.3 96.5 85.9 98.0 99.5 98.8TroL-7BThermal 93.9 72.8 68.1 92.8 81.9 94.1 65.5 79.8(Lee et al. 2024a)Depth 83.3 67.7 67.3 90.7 77.2 84.8 73.8 79.3
XR 82.8 69.1 71.0 78.7 75.4 83.3 84.0 83.7
RGB 96.5 76.9 69.3 98.6 85.3 98.6 99.5 99.1IXC2.5-VL-7BThermal 93.0 70.6 66.8 95.5 81.5 92.5 60.0 76.2(Zhang et al. 2024)Depth 86.1 59.9 59.4 93.3 74.7 90.6 74.3 82.4
XR 86.1 73.5 63.8 76.6 75.0 89.4 88.0 88.7
RGB 97.2 78.3 72.4 97.9 86.5 97.6 99.1 98.3InternVL2-8BThermal 90.5 75.8 61.1 93.7 80.3 94.6 61.5 78.1(OpenGVLab 2024)Depth 83.0 60.2 60.3 91.4 73.7 86.9 79.9 83.5
XR 92.7 77.9 71.7 84.9 81.8 89.4 82.7 86.0
Closed Source Large-scale Vision-Language Models
RGB 94.6 79.6 65.2 95.3 83.7 97.6 98.6 98.1Gemini 1.5 ProThermal 91.4 73.6 68.8 93.9 81.9 90.3 93.0 91.7(Team et al. 2024)Depth 87.8 73.7 62.6 94.2 79.6 78.0 88.4 83.2
XR 89.9 81.6 63.0 82.0 79.2 92.4 88.0 90.2
RGB 95.1 79.0 69.7 95.8 84.9 99.5 97.2 98.3Claude 3.5 SonnetThermal 92.1 79.2 62.9 95.0 82.3 94.1 85.0 89.6(Anthropic 2024)Depth 72.9 67.7 55.6 84.4 70.2 86.4 75.5 80.9
XR 83.2 76.5 74.6 83.5 79.5 93.9 82.7 88.3
RGB 96.9 80.9 71.4 97.4 86.7 98.5 98.6 98.6GPT-4oThermal 96.1 75.6 71.4 98.2 85.3 95.2 92.0 93.6(OpenAI 2024)Depth 87.6 77.3 71.0 94.4 82.6 95.8 85.8 90.8
XR 91.9 83.8 65.2 85.6 81.7 95.5 82.7 89.1
Table 1: Evaluation results of different models on SPARK benchmark. Accuracy is the metric. â€œMulti-vision Perceptionâ€ shows
the average performance on four dimensions (Existence, Count, Position, and General Description) for evaluating visual per-
ception, and â€œMulti-vision Reasoningâ€ shows the average performance on two dimensions (Contextual Reasoning and Sensory
Reasoning) for evaluating vision sensory understanding. LVLMs are sorted in ascending order of release date.
â€¢ Position: LVLMs can determine the spatial arrangement
of objects within the image, noting their positions relative
to one another.
â€¢ General Description: LVLMs are also equipped to gener-
ate nuanced descriptions of the overall scene depicted in
an image. They can articulate what is happening, identify
objects, and provide factual information that enhances
the understanding of the image itself.At the perception stage, LVLMs focus on extracting essen-
tial information directly from raw image data captured by
multi-vision sensors. This foundational perception is critical
for all subsequent reasoning tasks, serving as the foundation
upon which more complex interpretations are built.
Multi-vision Reasoning
Multi-vision reasoning is where LVLMs truly showcase
their advanced capabilities. Beyond simply perceiving im-
Vision Sensors RGB Thermal Depth XR
ModelsMulti-Vison
PerceptionMulti-Vision
ReasoningMulti-Vison
PerceptionMulti-Vision
ReasoningMulti-Vison
PerceptionMulti-Vision
ReasoningMulti-Vison
PerceptionMulti-Vision
ReasoningALL
Open Source Large-scale Vision-Language Models
LLaV A-v1.5-7B
(Liu et al. 2023a)81.6 91.8 81.7 68.2 76.2 80.6 67.8 56.4 75.6
Qwen-VL-Chat
(Bai et al. 2023)80.7 96.1 76.9 86.9 68.6 73.3 67.1 78.3 78.5
Meteor-7B
(Lee et al. 2024b)86.4 98.8 82.3 76.4 75.8 83.4 72.6 85.2 82.6
TroL-7B
(Lee et al. 2024a)85.9 98.8 81.9 79.8 77.2 79.3 75.4 83.7 82.8
IXC2.5-VL-7B
(Zhang et al. 2024)85.3 99.1 81.5 76.2 74.7 82.4 75.0 88.7 82.9
CogVLM-Chat
(Wang et al. 2023)82.1 97.6 82.9 77.6 83.0 80.9 74.9 87.5 83.3
InternVL2-8B
(OpenGVLab 2024)86.5 98.3 80.3 78.1 73.7 83.5 81.8 86.0 83.5
Closed Source Large-scale Vision-Language Models
Claude 3.5 Sonnet
(Anthropic 2024)84.9 98.3 82.3 89.6 70.2 80.9 79.5 88.3 84.3
Gemini 1.5 Pro
(Team et al. 2024)83.7 98.1 81.9 91.7 79.6 83.2 79.2 90.2 85.9
GPT-4o
(OpenAI 2024)86.7 98.6 85.3 93.6 82.6 90.8 81.7 89.1 88.5
Table 2: Leaderboards of 10 advanced leading LVLMs on proposed SPARK benchmark according to different multi-vision
sensors. Accuracy is the metric and the best accuracy is denoted in bold and underlined. LVLMs are sorted in ascending order
of overall accuracy (ALL).
ages, LVLMs can engage in logical reasoning to derive
deeper insights and make informed decisions. This distin-
guishes the recent LVLMs from traditional computer vision
models, which primarily focus on understanding and inter-
acting with the real world.
â€¢ Contextual reasoning: LVLMs can utilize fundamen-
tal knowledge and contextual clues to make judgments
about a given scenario. This type of reasoning allows
LVLMs to refer to the underlying basis of physical sensor
knowledge and ensure that the reasoning process remains
consistent with the context provided by the image and the
associated information.
â€¢ Sensory reasoning: A more complex reasoning ability
requires LVLMs to map 2D image data to the physical
meanings associated with different multi-vision sensors.
This process not only involves processing the raw data
from images but also integrates it with contextual infor-
mation about the underlying physical sensor knowledge
in the real world. By combining fundamental sensor in-
formation, LVLMs can derive conclusions that are both
accurate and contextually relevant. Sensory reasoning re-
quires a deep understanding of the knowledge underlying
the physical meaning of multi-vision sensor data. This
goes beyond surface-level image recognition, demanding
that LVLMs make sense of the sensor data in a way that
reflects real-world physics and usage scenarios.
Next, we integrate both visual and textual inputs into
GPT-4, guided by meticulously crafted prompts. These
prompts are specifically designed to align with various eval-
uation dimensions, ensuring that the generated questions areboth relevant and focused. To further enhance the quality
of the benchmark, we introduce an additional filtering step.
In the final stages of development, human annotators play a
crucial role, selecting the correct answers and categorizing
the questions according to their respective evaluation dimen-
sions.
Experiment
Implementation Details
Dataset Collection We collect six subsets for each multi-
sensor vision task type, together with 4k images and 6k
unique questions and answers. These instructions are built
from five public datasets: MS-COCO (Lin et al. 2015),
M3FD (Liu et al. 2022), Dog&People (Roboflow 2022),
RGB-D scene dataset (Cho et al. 2021), and UNIFESP X-
ray Body Part Classifier Competition dataset (Eduardo Fa-
rina 2022). The MS-COCO dataset is a commonly used ob-
ject detection dataset that contains RGB images with fine-
grained object bounding boxes, categories, and attribute an-
notations. We sampled 1.2k images from validation dataset.
Furthermore, for thermal sensor datasets, we sampled 1.2k
images from two different thermal datasets (M3FD and
Dog&People) in order to collect a thermal dataset covering
the widest possible range of diverse situations and objects.
Additionally, we sampled 1.2k images from RGB-D scene
dataset (Cho et al. 2021) for depth sensor because it covers
a variety of indoor and outdoor scenes. Finally, we sampled
0.4k images from the public X-ray body part dataset for the
XR sensor dataset because of the diversity of multiple hu-
man body parts. We described the overall distribution of data
sources of the SPARK benchmark in Figure 3.
Large Vision Language Models In our evaluation, we
selected 10 state-of-the-art (SOTA) Large Vision-Language
Models (LVLMs) that represent the leading edge of current
research. These models were chosen to provide a compre-
hensive assessment of the capabilities and performance of
both open-source and closed-source LVLMs across a vari-
ety of multi-vision sensor tasks on the SPARK benchmark.
â€¢ Open source: CogVLM-Chat (Wang et al. 2023),
LLA V A-v1.5-7B (Liu et al. 2023b), InternVL2-
8B (OpenGVLab 2024), TroL-7B (Lee et al. 2024a),
Meteor-7B (Lee et al. 2024b), IXC2.5-VL-7B (Zhang
et al. 2024), Qwen-VL-Chat (Bai et al. 2023)
â€¢ Closed source: GPT-4o (OpenAI 2024), Claude 3.5 Son-
net (Anthropic 2024), Gemini-Pro1.5 (Team et al. 2024)
Experiment Result
In this section, we conduct a comprehensive evaluation us-
ing the proposed SPARK benchmark, a rigorous framework
designed to assess the capabilities of Large Vision-Language
Models (LVLMs) in two target tasks: Multi-vision Percep-
tion and Multi-vision Reasoning. Multi-vision Perception
presents the averaged performance on four dimensions for
evaluating visual perception. Meanwhile, Multi-vision Rea-
soning demonstrates the averaged performance on two di-
mensions for evaluating the LVLMsâ€™ ability to understand
and reason about multi-vision sensory data.
As shown in Table 1, the evaluation revealed that perfor-
mance varies significantly depending on the type of multi-
vision sensor used to capture the input images. LVLMs gen-
erally perform well in simple Multi-vision perception tasks
such as generating general descriptions, but more complex
reasoning tasks like Multi-vision Reasoning reveal signif-
icant differences in model capabilities. Since they mainly
trained with general RGB images, the performance of multi-
vision perception and reasoning in RGB sensor is consis-
tently maintained at high levels. However, the performance
of LVLMs drops noticeably when dealing with images cap-
tured using thermal, depth, and X-ray(XR) sensors. This de-
cline is particularly evident in the Multi-vision Reasoning
task, especially in Sensory Reasoning.
Sensory Reasoning requires LVLMs to not only recognize
and describe images but also to understand the physical prin-
ciples underlying the sensor data. For example, interpreting
thermal data involves understanding heat signatures, while
depth data requires an understanding of the need for spa-
tial geometry beyond simple 2D interpretation. The experi-
ment demonstrates LVLMsâ€™ limited proficiency in interpret-
ing and mapping sensor data to its physical meaning.
Table 2 provides a clear comparison of the performance
of various LVLMs across different multi-vision sensors and
tasks. It highlights the strengths and weaknesses of each
model, particularly the advantage that closed-source models
have in maintaining high performance across more complex
reasoning tasks with diverse vision sensor types. Consider-
ing the overall accuracy score (ALL), GPT-4o excels in the
proposed SPARK benchmark.ModelVision
SensorSensor Reasoning
w/o Sensor Info.Sensor Reasoning
w/ Sensor Info.âˆ†
LLA V A-v1.5-7B Thermal 51.0 81.0 +30.0
(Liu et al. 2023b) Depth 73.8 87.6 +13.8
Open source LVLM XR 50.7 54.0 +3.3
TroL-7B Thermal 65.5 97.0 +31.5
(Lee et al. 2024a) Depth 73.8 99.1 +25.3
Open source LVLM XR 84.0 84.0 -
InternVL2-8B Thermal 61.5 85.5 +24.0
(OpenGVLab 2024) Depth 79.9 99.6 +19.7
Open source LVLM XR 82.7 85.3 +2.6
Claude 3.5 Sonnet Thermal 85.0 96.5 +11.5
(Anthropic 2024) Depth 75.5 99.6 +24.1
Closed source LVLM XR 82.7 82.7 -
GPT-4o Thermal 92.0 94.0 +2.0
(OpenAI 2024) Depth 85.8 99.6 +13.8
Closed source LVLM XR 82.7 84.0 +1.3
Table 3: Ablation study on sensor reasoning performance
change whether the sensor information is given. We choose
three LVLMs from open source and two from closed source.
Ablation study
In the previous section, we observed that LVLMs frequently
struggle to accurately infer the purpose or context of an im-
age when the data is sourced from multi-vision sensors other
than RGB. However, as demonstrated in Figure 1, even when
the input image lacks explicit information about the sen-
sor type, LVLMs can still identify the sensor correctly. This
suggests that while LVLMs have already acquired sensor-
related knowledge through textual data, they face challenges
in mapping fundamental knowledge to real-world scenarios.
Thus, in Table 3, we conducted an ablation experiment
on data-centric enhancement by adding sensor information
as a text prompt at the beginning of the question (â€œThis
is a{Thermal, Depth, X-Ray }image.â€) and measured the
sensory reasoning performance change. The experiment
demonstrated that sensor information can enhance the rea-
soning capabilities of LVLMs, particularly for thermal and
depth images, while XR data showed the least impact. This
implies that LVLM models, including GPT-4o, are not fully
utilizing the knowledge they already possess to understand
multi-vision sensory data.
Conclusion
In this study, we focus on evaluating the ability of Large
Vision-Language Models (LVLMs) to understand and pro-
cess multi-vision sensory inputs. As LVLMs are increas-
ingly deployed in real-world applications, their ability to
accurately interpret and reason about data from diverse vi-
sion sensors has become crucial. To address this, we propose
an evaluation benchmark called SPARK, which generates
instruction tuning samples aimed at specific physical sen-
sor understanding in various question-and-answer formats.
Through extensive experiments, we assess the performance
of understanding sensory knowledge in the latest state-of-
the-art LVLMs handling multi-vision input. We believe this
approach, integrating a sensory knowledge annotated evalu-
ation benchmark paves the way for promising future appli-
cations of LVLMs.
References
Anthropic. 2024. Claude 3.5 sonnet. https://www.anthropic.
com/news/claude-3-5-sonnet.
Bai, J.; Bai, S.; Yang, S.; Wang, S.; Tan, S.; Wang, P.; Lin,
J.; Zhou, C.; and Zhou, J. 2023. Qwen-VL: A Versatile
Vision-Language Model for Understanding, Localization,
Text Reading, and Beyond. arXiv:2308.12966.
Bazi, Y .; Rahhal, M. M. A.; Bashmal, L.; and Zuair, M.
2023. Visionâ€“language model for visual question answer-
ing in medical imagery. Bioengineering , 10(3): 380.
Broadbent, D. E. 2013. Perception and communication . El-
sevier.
Chen, Z.; Wang, W.; Tian, H.; Ye, S.; Gao, Z.; Cui, E.;
Tong, W.; Hu, K.; Luo, J.; Ma, Z.; et al. 2024. How Far
Are We to GPT-4V? Closing the Gap to Commercial Mul-
timodal Models with Open-Source Suites. arXiv preprint
arXiv:2404.16821 .
Cho, J.; Min, D.; Kim, Y .; and Sohn, K. 2021. DIML/CVL
RGB-D Dataset: 2M RGB-D Images of Natural Indoor and
Outdoor Scenes. arXiv:2110.11590.
Eduardo Farina, M. P., FelipeKitamura. 2022. UNIFESP X-
ray Body Part Classifier Competition.
Fu, C.; Chen, P.; Shen, Y .; Qin, Y .; Zhang, M.; Lin, X.;
Yang, J.; Zheng, X.; Li, K.; Sun, X.; Wu, Y .; and Ji, R. 2024.
MME: A Comprehensive Evaluation Benchmark for Multi-
modal Large Language Models. arXiv:2306.13394.
Girdhar, R.; El-Nouby, A.; Liu, Z.; Singh, M.; Alwala, K. V .;
Joulin, A.; and Misra, I. 2023. Imagebind: One embedding
space to bind them all. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
15180â€“15190.
Gunjal, A.; Yin, J.; and Bas, E. 2024. Detecting and Pre-
venting Hallucinations in Large Vision Language Models.
arXiv:2308.06394.
Kahneman, D.; Treisman, A.; and Gibbs, B. J. 1992. The
reviewing of object files: Object-specific integration of in-
formation. Cognitive psychology , 24(2): 175â€“219.
Koh, J. Y .; Salakhutdinov, R.; and Fried, D. 2023. Ground-
ing language models to images for multimodal inputs and
outputs. In International Conference on Machine Learning ,
17283â€“17300. PMLR.
Lee, B.-K.; Chung, S.; Kim, C. W.; Park, B.; and Ro, Y . M.
2024a. TroL: Traversal of Layers for Large Language and
Vision Models. arXiv:2406.12246.
Lee, B.-K.; Kim, C. W.; Park, B.; and Ro, Y . M. 2024b. Me-
teor: Mamba-based Traversal of Rationale for Large Lan-
guage and Vision Models. arXiv:2405.15574.
Lee, B.-K.; Park, B.; Kim, C. W.; and Ro, Y . M. 2024c.
CoLLaVO: Crayon Large Language and Vision mOdel.
arXiv:2402.11248.
Lee, B.-K.; Park, B.; Kim, C. W.; and Ro, Y . M. 2024d.
MoAI: Mixture of All Intelligence for Large Language and
Vision Models. arXiv:2403.07508.
Li, B.; Wang, R.; Wang, G.; Ge, Y .; Ge, Y .; and
Shan, Y . 2023a. Seed-bench: Benchmarking multimodalllms with generative comprehension. arXiv preprint
arXiv:2307.16125 .
Li, J.; and Lu, W. 2024. A Survey on Benchmarks
of Multimodal Large Language Models. arXiv preprint
arXiv:2408.08632 .
Li, Y .; Du, Y .; Zhou, K.; Wang, J.; Zhao, W. X.; and Wen, J.-
R. 2023b. Evaluating Object Hallucination in Large Vision-
Language Models. arXiv:2305.10355.
Lin, T.-Y .; Maire, M.; Belongie, S.; Bourdev, L.; Girshick,
R.; Hays, J.; Perona, P.; Ramanan, D.; Zitnick, C. L.; and
DollÂ´ar, P. 2015. Microsoft COCO: Common Objects in Con-
text. arXiv:1405.0312.
Liu, H.; Li, C.; Li, Y .; and Lee, Y . J. 2023a. Improved Base-
lines with Visual Instruction Tuning.
Liu, H.; Li, C.; Li, Y .; Li, B.; Zhang, Y .; Shen, S.; and Lee,
Y . J. 2024a. LLaV A-NeXT: Improved reasoning, OCR, and
world knowledge.
Liu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023b. Visual Instruc-
tion Tuning.
Liu, J.; Fan, X.; Huang, Z.; Wu, G.; Liu, R.; Zhong, W.; and
Luo, Z. 2022. Target-aware Dual Adversarial Learning and a
Multi-scenario Multi-Modality Benchmark to Fuse Infrared
and Visible for Object Detection. arXiv:2203.16220.
Liu, Y .; Duan, H.; Zhang, Y .; Li, B.; Zhang, S.; Zhao, W.;
Yuan, Y .; Wang, J.; He, C.; Liu, Z.; Chen, K.; and Lin, D.
2024b. MMBench: Is Your Multi-modal Model an All-
around Player? arXiv:2307.06281.
Mao, J.; Qian, Y .; Zhao, H.; and Wang, Y . 2023. Gpt-
driver: Learning to drive with gpt. arXiv preprint
arXiv:2310.01415 .
OpenAI. 2024. Hello GPT-4o. https://openai.com/index/
hello-gpt-4o/.
OpenGVLab. 2024. InternVL2: Better than the
Bestâ€”Expanding Performance Boundaries of Open-Source
Multimodal Models with the Progressive Scaling Strategy.
https://internvl.github.io/blog/2024-07-02-InternVL-2.0/.
Ren, S.; Yao, L.; Li, S.; Sun, X.; and Hou, L. 2024.
Timechat: A time-sensitive multimodal large language
model for long video understanding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 14313â€“14323.
Roboflow. 2022. thermal dogs and people x6ejw
Dataset. https://universe.roboflow.com/object-detection/
thermal-dogs-and-people-x6ejw. Visited on 2023-03-29.
Shi, Y .; Gao, Y .; Lai, Y .; Wang, H.; Feng, J.; He, L.; Wan,
J.; Chen, C.; Yu, Z.; and Cao, X. 2024. Shield: An eval-
uation benchmark for face spoofing and forgery detection
with multimodal large language models. arXiv preprint
arXiv:2402.04178 .
Su, Y .; Lan, T.; Li, H.; Xu, J.; Wang, Y .; and Cai, D. 2023.
Pandagpt: One model to instruction-follow them all. arXiv
preprint arXiv:2305.16355 .
Team, G.; Georgiev, P.; Lei, V . I.; Burnell, R.; Bai, L.;
Gulati, A.; Tanzer, G.; Vincent, D.; Pan, Z.; Wang, S.;
et al. 2024. Gemini 1.5: Unlocking multimodal understand-
ing across millions of tokens of context. arXiv preprint
arXiv:2403.05530 .
Wang, W.; Lv, Q.; Yu, W.; Hong, W.; Qi, J.; Wang, Y .; Ji,
J.; Yang, Z.; Zhao, L.; Song, X.; et al. 2023. Cogvlm: Vi-
sual expert for pretrained language models. arXiv preprint
arXiv:2311.03079 .
Xu, P.; Shao, W.; Zhang, K.; Gao, P.; Liu, S.; Lei, M.; Meng,
F.; Huang, S.; Qiao, Y .; and Luo, P. 2023. LVLM-eHub:
A Comprehensive Evaluation Benchmark for Large Vision-
Language Models. arXiv:2306.09265.
Xu, Z.; Zhang, Y .; Xie, E.; Zhao, Z.; Guo, Y .; Wong, K.-
Y . K.; Li, Z.; and Zhao, H. 2024. Drivegpt4: Interpretable
end-to-end autonomous driving via large language model.
IEEE Robotics and Automation Letters .
Yarom, M.; Bitton, Y .; Changpinyo, S.; Aharoni, R.; Herzig,
J.; Lang, O.; Ofek, E.; and Szpektor, I. 2024. What you see is
what you read? improving text-image alignment evaluation.
Advances in Neural Information Processing Systems , 36.
Ye, J.; Hu, A.; Xu, H.; Ye, Q.; Yan, M.; Dan, Y .; Zhao, C.;
Xu, G.; Li, C.; Tian, J.; et al. 2023. mplug-docowl: Modu-
larized multimodal large language model for document un-
derstanding. arXiv preprint arXiv:2307.02499 .
Yu, W.; Yang, Z.; Li, L.; Wang, J.; Lin, K.; Liu, Z.; Wang, X.;
and Wang, L. 2023. MM-Vet: Evaluating Large Multimodal
Models for Integrated Capabilities. arXiv:2308.02490.
Zhang, P.; Dong, X.; Zang, Y .; Cao, Y .; Qian, R.; Chen, L.;
Guo, Q.; Duan, H.; Wang, B.; Ouyang, L.; Zhang, S.; Zhang,
W.; Li, Y .; Gao, Y .; Sun, P.; Zhang, X.; Li, W.; Li, J.; Wang,
W.; Yan, H.; He, C.; Zhang, X.; Chen, K.; Dai, J.; Qiao, Y .;
Lin, D.; and Wang, J. 2024. InternLM-XComposer-2.5: A
Versatile Large Vision Language Model Supporting Long-
Contextual Input and Output. arXiv:2407.03320.","btspark multivision sensor perception and reasoning benchmark
for largescale visionlanguage models
youngjoon yu sangyun chung byungkwan lee and yong man ro
integrated vision language lab kaist south korea
greatday jelarum leebk ymro kaistackr
abstract
largescale visionlanguage models lvlms have signif
icantly advanced with textaligned vision inputs they have
made remarkable progress in computer vision tasks by align
ing text modality with vision inputs there are also endeav
ors to incorporate multivision sensors beyond rgb includ
ing thermal depth and medical xray images however we
observe that current lvlms view images taken from multi
vision sensors as if they were in the same rgb domain with
out considering the physical characteristics of multivision
sensors they fail to convey the fundamental multivision
sensor information from the dataset and the correspond
ing contextual knowledge properly consequently alignment
between the information from the actual physical environ
ment and the text is not achieved correctly making it dif
ficult to answer complex sensorrelated questions that con
sider the physical environment in this paper we aim to
establish a multivision sensor perception andreasoning
benchmar kcalled spark that can reduce the fundamen
tal multivision sensor information gap between images and
multivision sensors we generated 6248 visionlanguage
test samples to investigate multivision sensory perception
and multivision sensory reasoning on physical sensor knowl
edge proficiency across different formats covering different
types of sensorrelated questions we utilized these samples
to assess ten leading lvlms the results showed that most
models displayed deficiencies in multivision sensory rea
soning to varying extents codes and data are available at
httpsgithubcomtopyunspark
introduction
in recent days largescale visionlanguage models
lvlms have achieved significant breakthroughs in ar
eas such as visual dialogue koh salakhutdinov and fried
2023 video analysis ren et al 2024 and document under
standing ye et al 2023 establishing themselves as critical
tools in the pursuit of artificial general intelligence agi
these models function similarly to the human brain by
processing multimodal information and generating sophis
ticated inferences for instance the latest lvlms like ope
nais gpt4o openai 2024 have exhibited exceptional
reasoning abilities that not only rival but in some cases ex
ceed human performance
corresponding authorboth authors are equally contributed
sensory reasoning performance across different lvlms and vision sensors
yes the ribs are visible in this chest xray image are there ribs visible in this 
image
what could be the likely reason for capturing this image 
a to measure the temperature of items 
b to describe the lighting condition
c to detect hazy foggy atmosphere
d to analyze the spatial arrangement of objects in a room 
c to detect hazy foggy atmosphere
xr image
depth image
multi vision sensorsensory reasoning accllava v15 cogvlm internvl2 trol
meteor ixc 25 qwenvl gpt4ofigure 1 the comparison of sensory reasoning performance
across different multivision sensors with respect to the re
cent lvlms note that sensory reasoning performance sig
nificantly drops across different multivision sensors
one emerging concept in modern ai research gaining sig
nificant attention is the development of large vision language
models lvlms capable of handling a variety of multi
modal inputs surpassing the capabilities of previous large
language models llms lvlms can process diverse forms
of data simultaneously including images videos and text
openai 2024 opengvlab 2024 zhang et al 2024 this
ability also allows them to use multivision sensor data as
input including thermal sensors depth sensors and medi
cal imaging girdhar et al 2023 su et al 2023 to fully
harness the potential of lvlms recent research has focused
on effectively integrating various multivision sensor data to
develop more sophisticated and practical ai systems for the
real worldarxiv240812114v3  cscv  11 oct 2024
however despite the remarkable advancements in lvlm
models significant challenges still remain in fully utilizing
multivision sensors lvlms often overlook the nuances of
the physical properties of individual vision sensors instead
they tend to make judgments based on prior visual or lin
guistic information from images they have learned using
lowlevel features in twodimensional data this results in
the models recognizing only superficial patterns in image
inputs missing the underlying logical structures or contex
tual understanding when identifying specific objects in an
image input a model might rely on patterns learned from
similarlooking images rather than considering the actual
physical properties of the multivision sensors used to cap
ture the image this can hinder accurate identification and
a deep understanding of the input images in fields where
the lvlms decisionmaking is crucial such as autonomous
driving mao et al 2023 xu et al 2024 security sys
tems shi et al 2024 and medical image diagnosis bazi
et al 2023
we evaluate the behavior of the recent lvlms using
multivision sensor images as input in figure 1 the perfor
mance of sensory reasoning which we devised to assess the
understanding of fundamental knowledge of multivision
sensors in the real world significantly drops across differ
ent multivision sensors such as thermal infrared depth
and xray xr images this highlights the challenges that
lvlms face in accurately interpreting multivision sensor
data and making correct inferences based on the physical
properties of sensors additionally from the interaction ex
ample shown below in figure 1 while the lvlm can ac
curately identify the vision sensor used to capture the im
age for a relatively simple question it struggles with under
standing the actual purpose or context of the image in the
sensorrelated more complicated questions this indicates
that current lvlms have difficulty in understanding the fun
damental knowledge of physical vision sensors beyond what
the image looks like
for example as illustrated in figure 1 when humans look
at a photograph of an xray medical image they interpret it
deeply drawing upon their knowledge base and their physi
cal understanding of the human body beyond the xray im
age itself despite never having seen their internal organs
and the structure of bones with the naked eye humans can
comprehend the image through scientific contextual knowl
edge and their inherent understanding of the physical world
in contrast current lvlms try to understand the inside of
the human body based solely on the twodimensional data
they have been trained on revealing their limitations in
fully grasping the physical environment of the real world
therefore establishing a comprehensive evaluation bench
mark is necessary before lvlms are implemented in crit
ical and sensitive realworld applications however the as
sessment of large visionlanguage models lvlms has
significantly lagged behind their rapid development several
initiatives are striving to close this gap by introducing a vari
ety of multimodal evaluation benchmarks notable examples
include mme fu et al 2024 mmbench liu et al 2024b
lvlmehub xu et al 2023 and seedbench li et al
2023a these benchmarks aim to define key dimensions ofmultimodal capabilities and provide corresponding test sam
ples but they cover a relatively narrow range of multimodal
tasks primarily focusing on fundamental abilities such as vi
sual recognition and ocr
in this paper to handle the aforementioned challenge we
design the spark benchmark to evaluate multivision input
lvlms on two fronts multivision perception and multi
vision reasoning multivision perception pertains to the in
formation needed which measures the lvlms effective
ness in satisfying visual perception needs multivision rea
soning measures the lvlms ability to base its responses on
fundamental information from the provided sensor knowl
edge to be specific we generated 6248 visionlanguage
test samples to investigate multivision sensory perception
and reasoning related to physical sensor knowledge profi
ciency covering 6 types of multivision sensory instruction
tasks across 2 different questionandanswer formats we
used these samples to assess 10 leading largescale vision
language models the experiment results validate that most
lvlms displayed deficiencies in sensory reasoning to vary
ing extents
in summary the contributions of this work are as follows
 to the best of our knowledge we first reveal the inca
pability of current lvlms which suffer from limited
multivision sensory reasoning across different multi
vision sensors due to an absence of fundamental under
standing of sensors in the physical world
 we propose a novel benchmark spark to rigorously
test and evaluate the capabilities of lvlms in under
standing sensory knowledge providing a comprehensive
framework for assessing their performance
 we evaluated a total of 10 stateoftheart lvlms using
our spark benchmark which is designed to rigorously
assess the capability of the lvlms in handling funda
mental knowledge related to multivision sensors
related work
largescale visonlanguage models recently there has
been significant interest in visual language multimodal
learning visual language models such as lla v a liu et al
2023b 2024a collavo lee et al 2024c moai lee
et al 2024d trol lee et al 2024a meteor lee et al
2024b ixc25 zhang et al 2024 and qwenvl bai
et al 2023 have shown impressive performance in a variety
of downstream tasks in addition to obtain richer contex
tual information lvlms have developed the capability to
handle multimodal inputs wang et al introduces cogvlm
an advanced visual language foundation multimodal model
that integrates a trainable visual expert module with a pre
trained language model internvl2 chen et al 2024 is an
opensource multimodal large language model that bridges
the gap between opensource and commercial models by
enhancing visual understanding dynamic highresolution
processing and bilingual dataset quality gpt4o openai
2024 possesses advanced multimodal capabilities allow
ing it to process and generate diverse multimodalities this
enables the model to understand and create content that in
tegrates visual and textual information making it suitable
position thermal
y is the person standing to the leftof the dog 
n is the dog positioned behind the personexistence rgb
y is there a flower on the mans suit in this image
n is there a hat being worn by anyone in this image
general description xr
y are the ribs visible in the image
nis there a distinct liver shape visible in the imagecounting depth
y are there two bottles in this image
n are there two monitors in this image
multi
 
vision 
perception
multi
 
vision 
reasoning
sensory reasoning thermal
q what could be the likely reason for capturing this 
image 
a to assess the environmental conditions of the area
b to study the feeding habits of livestock
c to evaluate the breeding patterns of the animal
d to monitor the health and temperature of animalcontextual reasoning thermal
q what might be the reason for the vehicles lined up in the image
a they are waiting for a traffic signal 
b they are parked for the night
c they are part of a parade
d they are in a car wash
sparkfigure 2 in the proposed spark we build the first benchmark for evaluating the abilities of lvlms in multivision sensor
understanding which covers four types of multivision perception tasks existence counting position and general descrip
tion and two types of multivision reasoning tasks contextual reasoning and sensory reasoning
for a wide range of applications that require various modal
ities consequently many lvlms have emerged that take
multivision sensor images as input girdhar et al presents
imagebind which creates a joint embedding space across
multivision sensors including depth and thermal sensor
data pandagpt su et al 2023 is a lvlm that integrates
multimodal encoders and large language models to enable
multivision and auditory instructionfollowing capabilities
performing complex tasks however relatively less attention
has been devoted to whether lvlms truly understand the
physical meanings of multivision sensors used to capture
the input image
evaluation benchmark for lvlms numerous studies
have leveraged existing visionlanguage datasets to develop
benchmarks for assessing the reliability of lvlms li and
lu 2024 mme fu et al 2024 includes 14 subtasks
based on publicly available images with manually created
annotations evaluating both the recognition and perception
capabilities of lvlms through yesno question answering
seedbenchmark li et al 2023a designed to evaluate the
generative comprehension capabilities of multimodal lvlm
through humanannotated multichoice questions across 12
evaluation dimensions other comparable benchmarks in
clude lvlmehub xu et al 2023 mmvet yu et al
2023 and mmbench liu et al 2024b additionally there
are benchmarks aimed at assessing specific target propertiesof lvlms pope li et al 2023b focuses on evaluating
object hallucination by asking yesno questions about the
presence of objects in the input image mhaldetect gun
jal yin and bas 2024 introduces hallucination tasks us
ing humanannotated labels for sentencelevel classification
unlike those previous evaluation benchmarks the proposed
spark is designed to rigorously test and evaluate the ca
pabilities of understanding the physical meaning of multi
vision sensors
evaluation and instruction design
there are multiple formats available for evaluating the
multisensor perception and reasoning capabilities of
lvlm each with distinct advantages and limitations free
form questions yarom et al 2024 offer flexibility and ease
of creation but demand laborintensive human assessment
and present challenges in maintaining consistent scoring
similaritybased assessment are less resourceintensive but
can be significantly affected by biases present in the similar
ity metrics yesorno questions fu et al 2024 are straight
forward and easier to assess but they may oversimplify the
evaluation failing to capture the full extent of lvlms com
prehension of multivision reasoning ability
first of all to enable quantitative performance metrics for
multivision perception the instruction design aims to elicit
yes or no responses from the model this binary re
sparkexistencecounting
sensory
reasoningposition
general
descriptioncontextual
reasoning1423720figure 3 distribution of data sources of the spark bench
mark in spark we demonstrate six core multivision sen
sory tasks in the inner ring and the outer ring displays the
number of samples for each specific task
sponse format simplifies the evaluation process allowing for
clear objective performance measurement as a result each
instruction comprises two parts a brief targeted question
and an explanation corresponding to either yes or no
this structure ensures that the lvlms comprehension can
be precisely assessed for every test image two instructions
are manually crafted each posing a different question to the
model these questions are designed to test different aspects
of the images content and context the rationale behind this
approach is to ensure that the models answers are not based
on chance when the lvlms correctly answer both ques
tions it demonstrates an understanding of the image and its
related information rather than merely guessing
in addition we also introduce a multivision sensor under
standing evaluation design based on multichoice questions
this format presents questions with a set of predetermined
choices allowing respondents to select the correct options
the multichoice question format is advantageous for sev
eral reasons first it enables efficient grading and analysis of
responses as answers can be objectively evaluated against a
fixed set of possible responses also the multichoice ques
tion format allows for precise control over the difficulty level
of the questions by varying the validity of each option we
can create questions that test different levels of understand
ing and comprehension for example including more plau
sible but incorrect options can increase the difficulty ensur
ing that only models with a deeper understanding can con
sistently choose the correct answer this flexibility in ques
tion design makes multichoice questions a powerful tool
for assessing the nuanced capabilities of multivision sen
sor systems furthermore the yesorno format can be seenas a specific case of multichoice question where the op
tions are limited to a yes and b no this simplifica
tion retains the benefits of the multichoice question format
while providing a straightforward way to measure binary de
cisions
using accuracy as the evaluation metric for both multi
choice questions and yesorno questions ensures consis
tency in how we assess the models performance accuracy
defined as the proportion of correctly answered questions
provides a clear and intuitive measure of how well the model
understands the given inputs the adoption of the multi
choice question based evaluation design supports the de
velopment of a more comprehensive evaluation framework
the incorporation of both simple yesorno questions and
more complex multichoice questions ensures that the eval
uation covers both basic and advanced aspects of lvlms
understanding
evaluation on multivision sensor tasks
our instruction dataset was collected according to two
multivision tasks multivision perception and multivision
reasoning as illustrated in figure 2 first of all multivision
perception focuses on the lvlms ability to accurately in
terpret and identify objects scenes and relationships from
various multivision inputs this involves tasks such as ob
ject detection image classification scene recognition and
relationship detection where the model must process and
understand the content of images from multiple vision sen
sors the goal is to ensure that the model can consistently
recognize and categorize visual elements across different
contexts from different vision sensors on the other hand
multivision reasoning requires the model to not only per
ceive but also make inferences based on the multivision
sensory data this involves higherorder cognitive tasks such
as understanding relationships between objects prediction
of intent of sensor use and understanding sensor knowl
edge for instance the model might need to infer the cause
of an event depicted in an image sequence or predict the
purpose of a captured image multivision reasoning tests
the lvlms capability to integrate multivision information
with contextual sensory knowledge making logical deduc
tions that go beyond mere perception
multivision perception
multivision perception is the foundational process by which
large visionlanguage models lvlms analyze images
captured by various multivision sensors including rgb
thermal depth and xray images this process involves rec
ognizing and interpreting the fundamental elements within
each visual input based on cognitive science kahneman
treisman and gibbs 1992 broadbent 2013
 existence lvlms can identify and list common objects
present in the image such as people vehicles animals
furniture and so on
 count lvlms can count the number of identified ob
jects or entities providing a quantitative understanding
of the scene
models vision sensors existence count positiongeneral
descriptionmultivision
perceptioncontextual
reasoningsensory
reasoningmultivision
reasoning
open source largescale visionlanguage models
rgb 939 685 626 979 807 951 972 961qwenvlchatthermal 861 669 593 953 769 903 835 869bai et al 2023depth 766 596 533 849 686 781 684 733
xr 680 713 551 741 671 818 747 783
rgb 942 755 598 969 816 887 948 918lla v av157bthermal 933 761 624 951 817 855 510 682liu et al 2023adepth 871 707 533 937 762 874 738 806
xr 742 574 674 723 678 621 507 564
rgb 965 734 614 972 821 980 972 976cogvlmchatthermal 949 761 646 962 829 962 590 776wang et al 2023depth 949 761 645 965 830 901 717 809
xr 861 728 616 794 749 909 840 875
rgb 972 785 722 979 864 980 995 988meteor7bthermal 935 689 717 953 823 909 620 764lee et al 2024bdepth 835 659 622 916 758 895 773 834
xr 795 706 638 766 726 864 840 852
rgb 969 812 693 965 859 980 995 988trol7bthermal 939 728 681 928 819 941 655 798lee et al 2024adepth 833 677 673 907 772 848 738 793
xr 828 691 710 787 754 833 840 837
rgb 965 769 693 986 853 986 995 991ixc25vl7bthermal 930 706 668 955 815 925 600 762zhang et al 2024depth 861 599 594 933 747 906 743 824
xr 861 735 638 766 750 894 880 887
rgb 972 783 724 979 865 976 991 983internvl28bthermal 905 758 611 937 803 946 615 781opengvlab 2024depth 830 602 603 914 737 869 799 835
xr 927 779 717 849 818 894 827 860
closed source largescale visionlanguage models
rgb 946 796 652 953 837 976 986 981gemini 15 prothermal 914 736 688 939 819 903 930 917team et al 2024depth 878 737 626 942 796 780 884 832
xr 899 816 630 820 792 924 880 902
rgb 951 790 697 958 849 995 972 983claude 35 sonnetthermal 921 792 629 950 823 941 850 896anthropic 2024depth 729 677 556 844 702 864 755 809
xr 832 765 746 835 795 939 827 883
rgb 969 809 714 974 867 985 986 986gpt4othermal 961 756 714 982 853 952 920 936openai 2024depth 876 773 710 944 826 958 858 908
xr 919 838 652 856 817 955 827 891
table 1 evaluation results of different models on spark benchmark accuracy is the metric multivision perception shows
the average performance on four dimensions existence count position and general description for evaluating visual per
ception and multivision reasoning shows the average performance on two dimensions contextual reasoning and sensory
reasoning for evaluating vision sensory understanding lvlms are sorted in ascending order of release date
 position lvlms can determine the spatial arrangement
of objects within the image noting their positions relative
to one another
 general description lvlms are also equipped to gener
ate nuanced descriptions of the overall scene depicted in
an image they can articulate what is happening identify
objects and provide factual information that enhances
the understanding of the image itselfat the perception stage lvlms focus on extracting essen
tial information directly from raw image data captured by
multivision sensors this foundational perception is critical
for all subsequent reasoning tasks serving as the foundation
upon which more complex interpretations are built
multivision reasoning
multivision reasoning is where lvlms truly showcase
their advanced capabilities beyond simply perceiving im
vision sensors rgb thermal depth xr
modelsmultivison
perceptionmultivision
reasoningmultivison
perceptionmultivision
reasoningmultivison
perceptionmultivision
reasoningmultivison
perceptionmultivision
reasoningall
open source largescale visionlanguage models
llav av157b
liu et al 2023a816 918 817 682 762 806 678 564 756
qwenvlchat
bai et al 2023807 961 769 869 686 733 671 783 785
meteor7b
lee et al 2024b864 988 823 764 758 834 726 852 826
trol7b
lee et al 2024a859 988 819 798 772 793 754 837 828
ixc25vl7b
zhang et al 2024853 991 815 762 747 824 750 887 829
cogvlmchat
wang et al 2023821 976 829 776 830 809 749 875 833
internvl28b
opengvlab 2024865 983 803 781 737 835 818 860 835
closed source largescale visionlanguage models
claude 35 sonnet
anthropic 2024849 983 823 896 702 809 795 883 843
gemini 15 pro
team et al 2024837 981 819 917 796 832 792 902 859
gpt4o
openai 2024867 986 853 936 826 908 817 891 885
table 2 leaderboards of 10 advanced leading lvlms on proposed spark benchmark according to different multivision
sensors accuracy is the metric and the best accuracy is denoted in bold and underlined lvlms are sorted in ascending order
of overall accuracy all
ages lvlms can engage in logical reasoning to derive
deeper insights and make informed decisions this distin
guishes the recent lvlms from traditional computer vision
models which primarily focus on understanding and inter
acting with the real world
 contextual reasoning lvlms can utilize fundamen
tal knowledge and contextual clues to make judgments
about a given scenario this type of reasoning allows
lvlms to refer to the underlying basis of physical sensor
knowledge and ensure that the reasoning process remains
consistent with the context provided by the image and the
associated information
 sensory reasoning a more complex reasoning ability
requires lvlms to map 2d image data to the physical
meanings associated with different multivision sensors
this process not only involves processing the raw data
from images but also integrates it with contextual infor
mation about the underlying physical sensor knowledge
in the real world by combining fundamental sensor in
formation lvlms can derive conclusions that are both
accurate and contextually relevant sensory reasoning re
quires a deep understanding of the knowledge underlying
the physical meaning of multivision sensor data this
goes beyond surfacelevel image recognition demanding
that lvlms make sense of the sensor data in a way that
reflects realworld physics and usage scenarios
next we integrate both visual and textual inputs into
gpt4 guided by meticulously crafted prompts these
prompts are specifically designed to align with various eval
uation dimensions ensuring that the generated questions areboth relevant and focused to further enhance the quality
of the benchmark we introduce an additional filtering step
in the final stages of development human annotators play a
crucial role selecting the correct answers and categorizing
the questions according to their respective evaluation dimen
sions
experiment
implementation details
dataset collection we collect six subsets for each multi
sensor vision task type together with 4k images and 6k
unique questions and answers these instructions are built
from five public datasets mscoco lin et al 2015
m3fd liu et al 2022 dogpeople roboflow 2022
rgbd scene dataset cho et al 2021 and unifesp x
ray body part classifier competition dataset eduardo fa
rina 2022 the mscoco dataset is a commonly used ob
ject detection dataset that contains rgb images with fine
grained object bounding boxes categories and attribute an
notations we sampled 12k images from validation dataset
furthermore for thermal sensor datasets we sampled 12k
images from two different thermal datasets m3fd and
dogpeople in order to collect a thermal dataset covering
the widest possible range of diverse situations and objects
additionally we sampled 12k images from rgbd scene
dataset cho et al 2021 for depth sensor because it covers
a variety of indoor and outdoor scenes finally we sampled
04k images from the public xray body part dataset for the
xr sensor dataset because of the diversity of multiple hu
man body parts we described the overall distribution of data
sources of the spark benchmark in figure 3
large vision language models in our evaluation we
selected 10 stateoftheart sota large visionlanguage
models lvlms that represent the leading edge of current
research these models were chosen to provide a compre
hensive assessment of the capabilities and performance of
both opensource and closedsource lvlms across a vari
ety of multivision sensor tasks on the spark benchmark
 open source cogvlmchat wang et al 2023
lla v av157b liu et al 2023b internvl2
8b opengvlab 2024 trol7b lee et al 2024a
meteor7b lee et al 2024b ixc25vl7b zhang
et al 2024 qwenvlchat bai et al 2023
 closed source gpt4o openai 2024 claude 35 son
net anthropic 2024 geminipro15 team et al 2024
experiment result
in this section we conduct a comprehensive evaluation us
ing the proposed spark benchmark a rigorous framework
designed to assess the capabilities of large visionlanguage
models lvlms in two target tasks multivision percep
tion and multivision reasoning multivision perception
presents the averaged performance on four dimensions for
evaluating visual perception meanwhile multivision rea
soning demonstrates the averaged performance on two di
mensions for evaluating the lvlms ability to understand
and reason about multivision sensory data
as shown in table 1 the evaluation revealed that perfor
mance varies significantly depending on the type of multi
vision sensor used to capture the input images lvlms gen
erally perform well in simple multivision perception tasks
such as generating general descriptions but more complex
reasoning tasks like multivision reasoning reveal signif
icant differences in model capabilities since they mainly
trained with general rgb images the performance of multi
vision perception and reasoning in rgb sensor is consis
tently maintained at high levels however the performance
of lvlms drops noticeably when dealing with images cap
tured using thermal depth and xrayxr sensors this de
cline is particularly evident in the multivision reasoning
task especially in sensory reasoning
sensory reasoning requires lvlms to not only recognize
and describe images but also to understand the physical prin
ciples underlying the sensor data for example interpreting
thermal data involves understanding heat signatures while
depth data requires an understanding of the need for spa
tial geometry beyond simple 2d interpretation the experi
ment demonstrates lvlms limited proficiency in interpret
ing and mapping sensor data to its physical meaning
table 2 provides a clear comparison of the performance
of various lvlms across different multivision sensors and
tasks it highlights the strengths and weaknesses of each
model particularly the advantage that closedsource models
have in maintaining high performance across more complex
reasoning tasks with diverse vision sensor types consider
ing the overall accuracy score all gpt4o excels in the
proposed spark benchmarkmodelvision
sensorsensor reasoning
wo sensor infosensor reasoning
w sensor info
lla v av157b thermal 510 810 300
liu et al 2023b depth 738 876 138
open source lvlm xr 507 540 33
trol7b thermal 655 970 315
lee et al 2024a depth 738 991 253
open source lvlm xr 840 840 
internvl28b thermal 615 855 240
opengvlab 2024 depth 799 996 197
open source lvlm xr 827 853 26
claude 35 sonnet thermal 850 965 115
anthropic 2024 depth 755 996 241
closed source lvlm xr 827 827 
gpt4o thermal 920 940 20
openai 2024 depth 858 996 138
closed source lvlm xr 827 840 13
table 3 ablation study on sensor reasoning performance
change whether the sensor information is given we choose
three lvlms from open source and two from closed source
ablation study
in the previous section we observed that lvlms frequently
struggle to accurately infer the purpose or context of an im
age when the data is sourced from multivision sensors other
than rgb however as demonstrated in figure 1 even when
the input image lacks explicit information about the sen
sor type lvlms can still identify the sensor correctly this
suggests that while lvlms have already acquired sensor
related knowledge through textual data they face challenges
in mapping fundamental knowledge to realworld scenarios
thus in table 3 we conducted an ablation experiment
on datacentric enhancement by adding sensor information
as a text prompt at the beginning of the question this
is athermal depth xray image and measured the
sensory reasoning performance change the experiment
demonstrated that sensor information can enhance the rea
soning capabilities of lvlms particularly for thermal and
depth images while xr data showed the least impact this
implies that lvlm models including gpt4o are not fully
utilizing the knowledge they already possess to understand
multivision sensory data
conclusion
in this study we focus on evaluating the ability of large
visionlanguage models lvlms to understand and pro
cess multivision sensory inputs as lvlms are increas
ingly deployed in realworld applications their ability to
accurately interpret and reason about data from diverse vi
sion sensors has become crucial to address this we propose
an evaluation benchmark called spark which generates
instruction tuning samples aimed at specific physical sen
sor understanding in various questionandanswer formats
through extensive experiments we assess the performance
of understanding sensory knowledge in the latest stateof
theart lvlms handling multivision input we believe this
approach integrating a sensory knowledge annotated evalu
ation benchmark paves the way for promising future appli
cations of lvlms
references
anthropic 2024 claude 35 sonnet httpswwwanthropic
comnewsclaude35sonnet
bai j bai s yang s wang s tan s wang p lin
j zhou c and zhou j 2023 qwenvl a versatile
visionlanguage model for understanding localization
text reading and beyond arxiv230812966
bazi y  rahhal m m a bashmal l and zuair m
2023 visionlanguage model for visual question answer
ing in medical imagery bioengineering  103 380
broadbent d e 2013 perception and communication  el
sevier
chen z wang w tian h ye s gao z cui e
tong w hu k luo j ma z et al 2024 how far
are we to gpt4v closing the gap to commercial mul
timodal models with opensource suites arxiv preprint
arxiv240416821 
cho j min d kim y  and sohn k 2021 dimlcvl
rgbd dataset 2m rgbd images of natural indoor and
outdoor scenes arxiv211011590
eduardo farina m p felipekitamura 2022 unifesp x
ray body part classifier competition
fu c chen p shen y  qin y  zhang m lin x
yang j zheng x li k sun x wu y  and ji r 2024
mme a comprehensive evaluation benchmark for multi
modal large language models arxiv230613394
girdhar r elnouby a liu z singh m alwala k v 
joulin a and misra i 2023 imagebind one embedding
space to bind them all in proceedings of the ieeecvf
conference on computer vision and pattern recognition 
1518015190
gunjal a yin j and bas e 2024 detecting and pre
venting hallucinations in large vision language models
arxiv230806394
kahneman d treisman a and gibbs b j 1992 the
reviewing of object files objectspecific integration of in
formation cognitive psychology  242 175219
koh j y  salakhutdinov r and fried d 2023 ground
ing language models to images for multimodal inputs and
outputs in international conference on machine learning 
1728317300 pmlr
lee bk chung s kim c w park b and ro y  m
2024a trol traversal of layers for large language and
vision models arxiv240612246
lee bk kim c w park b and ro y  m 2024b me
teor mambabased traversal of rationale for large lan
guage and vision models arxiv240515574
lee bk park b kim c w and ro y  m 2024c
collavo crayon large language and vision model
arxiv240211248
lee bk park b kim c w and ro y  m 2024d
moai mixture of all intelligence for large language and
vision models arxiv240307508
li b wang r wang g ge y  ge y  and
shan y  2023a seedbench benchmarking multimodalllms with generative comprehension arxiv preprint
arxiv230716125 
li j and lu w 2024 a survey on benchmarks
of multimodal large language models arxiv preprint
arxiv240808632 
li y  du y  zhou k wang j zhao w x and wen j
r 2023b evaluating object hallucination in large vision
language models arxiv230510355
lin ty  maire m belongie s bourdev l girshick
r hays j perona p ramanan d zitnick c l and
dollar p 2015 microsoft coco common objects in con
text arxiv14050312
liu h li c li y  and lee y  j 2023a improved base
lines with visual instruction tuning
liu h li c li y  li b zhang y  shen s and lee
y  j 2024a llav anext improved reasoning ocr and
world knowledge
liu h li c wu q and lee y  j 2023b visual instruc
tion tuning
liu j fan x huang z wu g liu r zhong w and
luo z 2022 targetaware dual adversarial learning and a
multiscenario multimodality benchmark to fuse infrared
and visible for object detection arxiv220316220
liu y  duan h zhang y  li b zhang s zhao w
yuan y  wang j he c liu z chen k and lin d
2024b mmbench is your multimodal model an all
around player arxiv230706281
mao j qian y  zhao h and wang y  2023 gpt
driver learning to drive with gpt arxiv preprint
arxiv231001415 
openai 2024 hello gpt4o httpsopenaicomindex
hellogpt4o
opengvlab 2024 internvl2 better than the
bestexpanding performance boundaries of opensource
multimodal models with the progressive scaling strategy
httpsinternvlgithubioblog20240702internvl20
ren s yao l li s sun x and hou l 2024
timechat a timesensitive multimodal large language
model for long video understanding in proceedings of
the ieeecvf conference on computer vision and pattern
recognition  1431314323
roboflow 2022 thermal dogs and people x6ejw
dataset httpsuniverseroboflowcomobjectdetection
thermaldogsandpeoplex6ejw visited on 20230329
shi y  gao y  lai y  wang h feng j he l wan
j chen c yu z and cao x 2024 shield an eval
uation benchmark for face spoofing and forgery detection
with multimodal large language models arxiv preprint
arxiv240204178 
su y  lan t li h xu j wang y  and cai d 2023
pandagpt one model to instructionfollow them all arxiv
preprint arxiv230516355 
team g georgiev p lei v  i burnell r bai l
gulati a tanzer g vincent d pan z wang s
et al 2024 gemini 15 unlocking multimodal understand
ing across millions of tokens of context arxiv preprint
arxiv240305530 
wang w lv q yu w hong w qi j wang y  ji
j yang z zhao l song x et al 2023 cogvlm vi
sual expert for pretrained language models arxiv preprint
arxiv231103079 
xu p shao w zhang k gao p liu s lei m meng
f huang s qiao y  and luo p 2023 lvlmehub
a comprehensive evaluation benchmark for large vision
language models arxiv230609265
xu z zhang y  xie e zhao z guo y  wong k
y  k li z and zhao h 2024 drivegpt4 interpretable
endtoend autonomous driving via large language model
ieee robotics and automation letters 
yarom m bitton y  changpinyo s aharoni r herzig
j lang o ofek e and szpektor i 2024 what you see is
what you read improving textimage alignment evaluation
advances in neural information processing systems  36
ye j hu a xu h ye q yan m dan y  zhao c
xu g li c tian j et al 2023 mplugdocowl modu
larized multimodal large language model for document un
derstanding arxiv preprint arxiv230702499 
yu w yang z li l wang j lin k liu z wang x
and wang l 2023 mmvet evaluating large multimodal
models for integrated capabilities arxiv230802490
zhang p dong x zang y  cao y  qian r chen l
guo q duan h wang b ouyang l zhang s zhang
w li y  gao y  sun p zhang x li w li j wang
w yan h he c zhang x chen k dai j qiao y 
lin d and wang j 2024 internlmxcomposer25 a
versatile large vision language model supporting long
contextual input and output arxiv240703320","['httpsinternvlgithubioblog20240702internvl20', 'worldarxiv240812114v3', 'reasoning1423720figure', '988meteor7bthermal', '976cogvlmchatthermal', '991ixc25vl7bthermal', 'ixc25vl7b', '983internvl28bthermal', 'thermaldogsandpeoplex6ejw', '986gpt4othermal']"
Adapting Computer Vision Algorithms for Omnidirectional Video,['Hannes Fassold'],2019,http://arxiv.org/abs/1907.09233v1,"Adapting Computer Vision Algorithms
for Omnidirectional Video
Hannes Fassold
JOANNEUM RESEARCH, DIGITAL - Institute for Information and Communication Technologies
Steyrergasse 17
Graz, Austria 8010
hannes.fassold@joanneum.at
ABSTRACT
Omnidirectional (360) video has got quite popular because it pro-
vides a highly immersive viewing experience. For computer vision
algorithms, it poses several challenges, like the special (equirectan-
gular) projection commonly employed and the huge image size. In
this work, we give a high-level overview of these challenges and
outline strategies how to adapt computer vision algorithm for the
speci/f_ics of omnidirectional video.
CCS CONCEPTS
â€¢Human-centered computing !Virtual reality; â€¢Computing
methodologies!Scene understanding;
KEYWORDS
omnidirectional video, VR, deep learning, object detection
1 INTRODUCTION
Omnidirectional (360) video content recently got very popular
in the media industry as well as in robotics, because it allows the
viewer to experience the content in an immersive and interactive
way. Omnidirectional consumer video cameras like the Samsung
Gear 360 or the Ricoh /T_heta V have multiple lenses and capture
images which cover the whole viewing sphere, typically in 4K or Ul-
traHD resolution. Omnidirectional videos are typically consumed
with a head-mounted display (HMD), so that the user is free to
choose the area (viewport) within the sphere he is currently inter-
ested in. /T_he whole viewing sphere is encoded in one 2D image for
each timepoint, usually in equirectangular projection [ 1]. Coordi-
nates on the viewing sphere are usually given in a longitude-latidue
representation (see Figure 1 for the relation between the viewing
sphere and the 2D image). In the following, the longitude is always
denoted by Ï•and has the rangeÂ»","adapting computer vision algorithms
for omnidirectional video
hannes fassold
joanneum research digital  institute for information and communication technologies
steyrergasse 17
graz austria 8010
hannesfassoldjoanneumat
abstract
omnidirectional 360 video has got quite popular because it pro
vides a highly immersive viewing experience for computer vision
algorithms it poses several challenges like the special equirectan
gular projection commonly employed and the huge image size in
this work we give a highlevel overview of these challenges and
outline strategies how to adapt computer vision algorithm for the
specifics of omnidirectional video
ccs concepts
humancentered computing virtual reality computing
methodologiesscene understanding
keywords
omnidirectional video vr deep learning object detection
1 introduction
omnidirectional 360 video content recently got very popular
in the media industry as well as in robotics because it allows the
viewer to experience the content in an immersive and interactive
way omnidirectional consumer video cameras like the samsung
gear 360 or the ricoh theta v have multiple lenses and capture
images which cover the whole viewing sphere typically in 4k or ul
trahd resolution omnidirectional videos are typically consumed
with a headmounted display hmd so that the user is free to
choose the area viewport within the sphere he is currently inter
ested in the whole viewing sphere is encoded in one 2d image for
each timepoint usually in equirectangular projection  1 coordi
nates on the viewing sphere are usually given in a longitudelatidue
representation see figure 1 for the relation between the viewing
sphere and the 2d image in the following the longitude is always
denoted by and has the range","['omnidirectional', 'equirectan', 'immersive', 'equirectangular', 'methodologiesscene', 'highlevel', 'hannesfassoldjoanneumat', 'humancentered', 'steyrergasse', 'longitudelatidue']"
